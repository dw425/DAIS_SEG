# Databricks notebook source
# MAGIC %md
# MAGIC # Pillar 2 — Generate Synthetic Environment
# MAGIC
# MAGIC Generates **statistically faithful synthetic Delta Tables** from a Source Blueprint.
# MAGIC Preserves distributions, referential integrity, null patterns, and edge cases —
# MAGIC without extracting any production data.
# MAGIC
# MAGIC ### Prerequisites
# MAGIC - Source Blueprint generated by notebook 01
# MAGIC - `dais-seg` wheel installed on the cluster

# COMMAND ----------

# MAGIC %pip install /Workspace/Users/$current_user/dais-seg-*.whl --quiet

# COMMAND ----------

dbutils.widgets.text("blueprint_id", "", "Blueprint ID")
dbutils.widgets.text("catalog", "dais_seg", "Target Catalog")
dbutils.widgets.text("target_schema", "synthetic", "Synthetic Schema Name")
dbutils.widgets.text("scale_factor", "0.1", "Scale Factor (e.g., 0.1 = 10%)")
dbutils.widgets.text("tables", "", "Specific Tables (comma-separated, empty = all)")

# COMMAND ----------

blueprint_id = dbutils.widgets.get("blueprint_id")
catalog = dbutils.widgets.get("catalog")
target_schema = dbutils.widgets.get("target_schema")
scale_factor = float(dbutils.widgets.get("scale_factor"))
tables_str = dbutils.widgets.get("tables")
specific_tables = [t.strip() for t in tables_str.split(",") if t.strip()] or None

assert blueprint_id, "blueprint_id widget is required"

print(f"Blueprint: {blueprint_id}")
print(f"Target: {catalog}.{target_schema}")
print(f"Scale: {scale_factor}x")
print(f"Tables: {specific_tables or 'all'}")

# COMMAND ----------

from dais_seg.config import SEGConfig, set_config

config = SEGConfig(catalog=catalog, schema="blueprints")
set_config(config)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 1: Load Blueprint

# COMMAND ----------

from dais_seg.profiler import BlueprintGenerator

bp_loader = BlueprintGenerator(spark)
blueprint = bp_loader.load_blueprint(blueprint_id)

print(f"Loaded blueprint: {blueprint['blueprint_id']}")
print(f"  Source: {blueprint['source_system']['name']} ({blueprint['source_system']['type']})")
print(f"  Tables: {len(blueprint['tables'])}")
print(f"  Relationships: {len(blueprint['relationships'])}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 2: Generate Synthetic Delta Tables

# COMMAND ----------

from dais_seg.generator import SyntheticDeltaTableGenerator

generator = SyntheticDeltaTableGenerator(
    spark,
    target_catalog=catalog,
    target_schema=target_schema,
)

generated_tables = generator.generate_from_blueprint(
    blueprint,
    scale_factor=scale_factor,
    tables=specific_tables,
)

print(f"\nGenerated {len(generated_tables)} synthetic Delta Tables:")
for name, fqn in generated_tables.items():
    print(f"  {name} -> {fqn}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 3: Verify Generated Data

# COMMAND ----------

for table_name, fqn in generated_tables.items():
    df = spark.read.table(fqn)
    print(f"\n{table_name}: {df.count():,} rows, {len(df.columns)} columns")
    display(df.limit(5))

# COMMAND ----------

print(f"\n✅ Synthetic generation complete: {len(generated_tables)} tables in {catalog}.{target_schema}")
print(f"Next step: Run notebook 03_conform_medallion")

# COMMAND ----------

import json
dbutils.notebook.exit(json.dumps({
    "tables_generated": len(generated_tables),
    "target_schema": target_schema,
    "tables": list(generated_tables.keys()),
}))
