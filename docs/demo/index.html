<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>SEG Demo â€” Synthetic Environment Generator</title>
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/@anthropic-ai/stlite-mountable@0.73.0/build/stlite.css"
    onerror="this.href='https://cdn.jsdelivr.net/npm/@stlite/mountable@0.68.1/build/stlite.css'"
  />
  <style>
    body { margin: 0; padding: 0; }
    #loading {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      background: #0e1117;
      color: #fafafa;
    }
    #loading h2 { margin-bottom: 8px; }
    #loading p { color: #808495; font-size: 14px; }
    .spinner {
      width: 40px; height: 40px;
      border: 4px solid #262730;
      border-top: 4px solid #ff4b4b;
      border-radius: 50%;
      animation: spin 1s linear infinite;
      margin-bottom: 20px;
    }
    @keyframes spin { to { transform: rotate(360deg); } }
  </style>
</head>
<body>
  <div id="loading">
    <div class="spinner"></div>
    <h2>Loading SEG Demo</h2>
    <p>Installing Python runtime + NumPy + Pandas in your browser...</p>
    <p style="font-size:12px; margin-top:20px;">This may take 30-60 seconds on first load</p>
  </div>
  <div id="root"></div>

  <script src="https://cdn.jsdelivr.net/npm/@stlite/mountable@0.68.1/build/stlite.js"></script>
  <script>
    document.getElementById("loading").style.display = "flex";

    stlite.mount({
      requirements: ["numpy", "pandas", "scipy"],
      entrypoint: "app.py",
      files: {
        "app.py": `
import streamlit as st
import json
import io
import re
import csv
import zipfile
import random
import string
import xml.etree.ElementTree as ET
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Optional
from datetime import date, datetime, timedelta, timezone
from uuid import uuid4

import numpy as np
import pandas as pd
from scipy import stats as scipy_stats


# ================================================================
# BASE TYPES
# ================================================================

class InputFormat(str, Enum):
    DDL = "ddl"
    ETL_MAPPING = "etl_mapping"
    SCHEMA_JSON = "schema_json"
    SCHEMA_YAML = "schema_yaml"
    SCHEMA_XML = "schema_xml"
    RAW_TEXT = "raw_text"
    UNKNOWN = "unknown"

@dataclass
class ParsedColumn:
    name: str
    data_type: str
    raw_type: str = ""
    nullable: bool = True
    is_primary_key: bool = False
    is_unique: bool = False
    default_value: Optional[str] = None
    check_constraints: list = field(default_factory=list)
    precision: Optional[int] = None
    scale: Optional[int] = None
    max_length: Optional[int] = None

@dataclass
class ParsedForeignKey:
    fk_column: str
    referenced_table: str
    referenced_column: str

@dataclass
class ParsedTable:
    name: str
    schema: str = "dbo"
    columns: list = field(default_factory=list)
    foreign_keys: list = field(default_factory=list)
    row_count: int = 1000

@dataclass
class ParsedSchema:
    source_name: str
    source_type: str = "parsed"
    tables: list = field(default_factory=list)
    input_format: InputFormat = InputFormat.UNKNOWN
    raw_input: str = ""
    parse_warnings: list = field(default_factory=list)


# ================================================================
# FORMAT DETECTOR
# ================================================================

class FormatDetector:
    @classmethod
    def detect(cls, content, filename=None):
        if filename:
            ext = filename.lower().rsplit(".", 1)[-1] if "." in filename else ""
            mapping = {
                "sql": InputFormat.DDL, "ddl": InputFormat.DDL,
                "csv": InputFormat.ETL_MAPPING, "tsv": InputFormat.ETL_MAPPING,
                "json": InputFormat.SCHEMA_JSON,
                "yaml": InputFormat.SCHEMA_YAML, "yml": InputFormat.SCHEMA_YAML,
                "xml": InputFormat.SCHEMA_XML,
            }
            fmt = mapping.get(ext, InputFormat.UNKNOWN)
            if fmt != InputFormat.UNKNOWN:
                return fmt
        stripped = content.strip()
        if not stripped:
            return InputFormat.UNKNOWN
        if stripped.startswith("<") and re.search(r"<(tables|schema|database|table)\\b", stripped, re.IGNORECASE):
            return InputFormat.SCHEMA_XML
        if re.search(r"\\bCREATE\\s+TABLE\\b", stripped, re.IGNORECASE):
            return InputFormat.DDL
        if stripped.startswith("{") and '"tables"' in stripped:
            return InputFormat.SCHEMA_JSON
        first_line = stripped.split("\\n")[0].lower()
        etl_h = ["source_table","target_table","src_table","tgt_table","source_column","target_column"]
        if any(h in first_line for h in etl_h):
            return InputFormat.ETL_MAPPING
        if re.search(r"\\b(INT|VARCHAR|DECIMAL|TIMESTAMP|BOOLEAN|BIGINT|SMALLINT|FLOAT|DOUBLE)\\b", stripped, re.IGNORECASE):
            return InputFormat.RAW_TEXT
        return InputFormat.UNKNOWN


# ================================================================
# DDL PARSER
# ================================================================

TYPE_NORMALIZATION = {
    "number": "decimal", "varchar2": "varchar", "nvarchar2": "varchar",
    "clob": "text", "nclob": "text", "blob": "binary", "raw": "binary",
    "nvarchar": "varchar", "nchar": "char", "bit": "boolean",
    "money": "decimal", "smallmoney": "decimal", "datetime2": "timestamp",
    "datetimeoffset": "timestamp", "uniqueidentifier": "varchar",
    "image": "binary", "ntext": "text",
    "serial": "int", "bigserial": "bigint", "smallserial": "smallint",
    "real": "float", "double precision": "double",
    "character varying": "varchar", "character": "char",
    "bytea": "binary", "json": "text", "jsonb": "text",
    "uuid": "varchar", "interval": "varchar",
    "mediumint": "int", "mediumtext": "text", "longtext": "text",
    "tinytext": "text", "enum": "varchar", "set": "varchar", "year": "int",
}

MULTI_WORD_TYPES = {
    "double precision", "character varying", "long raw",
    "timestamp with time zone", "timestamp without time zone",
}

class DDLParser:
    def can_parse(self, content):
        return bool(re.search(r"\\bCREATE\\s+TABLE\\b", content, re.IGNORECASE))

    def parse(self, content, **kwargs):
        source_name = kwargs.get("source_name", "DDL Import")
        tables, warnings = [], []
        for raw_name, body in self._extract_create_tables(content):
            tname, sname = self._parse_table_name(raw_name)
            cols, fks, tw = self._parse_table_body(body)
            tables.append(ParsedTable(name=tname, schema=sname, columns=cols, foreign_keys=fks))
            warnings.extend(tw)
        for tn, fk in self._parse_alter_table_fks(content):
            for t in tables:
                if t.name.lower() == tn.lower():
                    t.foreign_keys.append(fk)
        return ParsedSchema(source_name=source_name, source_type="sql", tables=tables,
                           input_format=InputFormat.DDL, raw_input=content, parse_warnings=warnings)

    def _extract_create_tables(self, content):
        results = []
        hp = re.compile(r"CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?([^\\s(]+)\\s*\\(", re.IGNORECASE)
        for h in hp.finditer(content):
            tn = h.group(1)
            start, depth, i = h.end(), 1, h.end()
            while i < len(content) and depth > 0:
                if content[i] == "(": depth += 1
                elif content[i] == ")": depth -= 1
                i += 1
            if depth == 0:
                results.append((tn, content[start:i-1]))
        return results

    def _parse_table_name(self, raw):
        clean = re.sub(r'[\\[\\]\\'\\"]', "", raw)
        parts = clean.split(".")
        return (parts[-1], parts[-2]) if len(parts) >= 2 else (parts[0], "dbo")

    def _parse_table_body(self, body):
        cols, fks, warnings = [], [], []
        for el in self._split_defs(body):
            el = el.strip()
            if not el: continue
            upper = el.upper().strip()
            if upper.startswith("PRIMARY KEY"):
                pk_cols = self._extract_parens_list(el)
                for c in cols:
                    if c.name.lower() in [p.lower() for p in pk_cols]:
                        c.is_primary_key = True
                continue
            if upper.startswith("FOREIGN KEY"):
                fk = self._parse_table_fk(el)
                if fk: fks.append(fk)
                continue
            if upper.startswith(("UNIQUE","CHECK","CONSTRAINT","INDEX")): continue
            col, ifk = self._parse_col_def(el, warnings)
            if col:
                cols.append(col)
                if ifk: fks.append(ifk)
        return cols, fks, warnings

    def _parse_col_def(self, el, warnings):
        stripped = el.strip()
        nm = re.match(r'^([\\'\\"\\'\\[\\]]?\\w+[\\'\\"\\'\\]\\]]?)\\s+', stripped)
        if not nm: return None, None
        raw_name = re.sub(r'[\\[\\]\\'\\"]', "", nm.group(1))
        rest = stripped[nm.end():]
        raw_type, type_args = "", None
        rl = rest.lower()
        for mwt in sorted(MULTI_WORD_TYPES, key=len, reverse=True):
            if rl.startswith(mwt):
                raw_type = mwt
                rest = rest[len(mwt):].strip()
                break
        if not raw_type:
            tm = re.match(r'([A-Za-z_]\\w*)', rest)
            if not tm: return None, None
            raw_type = tm.group(1).lower()
            rest = rest[tm.end():].strip()
        if rest.startswith("("):
            pe = rest.find(")")
            if pe > 0:
                type_args = rest[1:pe]
                rest = rest[pe+1:].strip()
        mods_orig = rest
        mods = rest.upper()
        ctype = TYPE_NORMALIZATION.get(raw_type, raw_type)
        prec, scl, maxl = None, None, None
        if type_args:
            parts = [p.strip() for p in type_args.split(",")]
            if ctype in ("decimal","numeric"):
                prec = int(parts[0]) if parts[0].isdigit() else None
                scl = int(parts[1]) if len(parts)>1 and parts[1].isdigit() else None
            elif ctype in ("varchar","char","text"):
                if parts[0].isdigit(): maxl = int(parts[0])
        nullable = "NOT NULL" not in mods
        is_pk = "PRIMARY KEY" in mods
        is_unique = "UNIQUE" in mods
        check_vals = []
        cm = re.search(r"CHECK\\s*\\(.*?IN\\s*\\(([^)]+)\\)", mods_orig, re.IGNORECASE)
        if cm:
            check_vals = [v.strip().strip("'\\"") for v in cm.group(1).split(",")]
        default_val = None
        dm = re.search(r"DEFAULT\\s+(\\S+)", mods_orig, re.IGNORECASE)
        if dm: default_val = dm.group(1).strip("'\\"")
        inline_fk = None
        rm = re.search(r"REFERENCES\\s+([\\w\\'\\"\\'\\[\\]]+)(?:\\s*\\(\\s*(\\w+)\\s*\\))?", mods_orig, re.IGNORECASE)
        if rm:
            rt = re.sub(r'[\\[\\]\\'\\"]', "", rm.group(1))
            rc = rm.group(2) or "id"
            inline_fk = ParsedForeignKey(fk_column=raw_name, referenced_table=rt, referenced_column=rc)
        if any(kw in mods for kw in ("IDENTITY","AUTO_INCREMENT")): is_pk = True
        if raw_type in ("serial","bigserial","smallserial"): is_pk = True
        col = ParsedColumn(name=raw_name, data_type=ctype,
            raw_type=f"{raw_type}({type_args})" if type_args else raw_type,
            nullable=nullable, is_primary_key=is_pk, is_unique=is_unique,
            default_value=default_val, check_constraints=check_vals,
            precision=prec, scale=scl, max_length=maxl)
        return col, inline_fk

    def _split_defs(self, body):
        elems, cur, depth = [], [], 0
        for c in body:
            if c == "(": depth += 1; cur.append(c)
            elif c == ")": depth -= 1; cur.append(c)
            elif c == "," and depth == 0: elems.append("".join(cur)); cur = []
            else: cur.append(c)
        if cur: elems.append("".join(cur))
        return elems

    def _extract_parens_list(self, el):
        m = re.search(r"\\(([^)]+)\\)", el)
        return [re.sub(r'[\\[\\]\\'\\"]', "", n.strip()) for n in m.group(1).split(",")] if m else []

    def _parse_table_fk(self, el):
        m = re.search(r"FOREIGN\\s+KEY\\s*\\(\\s*(\\w+)\\s*\\)\\s*REFERENCES\\s+(\\S+)\\s*\\(\\s*(\\w+)\\s*\\)", el, re.IGNORECASE)
        if m:
            return ParsedForeignKey(fk_column=m.group(1),
                referenced_table=re.sub(r'[\\[\\]\\'\\"]', "", m.group(2)).split(".")[-1],
                referenced_column=m.group(3))
        return None

    def _parse_alter_table_fks(self, content):
        results = []
        p = re.compile(r"ALTER\\s+TABLE\\s+(\\S+)\\s+ADD\\s+(?:CONSTRAINT\\s+\\S+\\s+)?FOREIGN\\s+KEY\\s*\\(\\s*[\\w\\'\\"\\'\\[\\]]*(\\w+)[\\w\\'\\"\\'\\]\\]]*\\s*\\)\\s*REFERENCES\\s+(\\S+)\\s*\\(\\s*[\\w\\'\\"\\'\\[\\]]*(\\w+)[\\w\\'\\"\\'\\]\\]]*\\s*\\)", re.IGNORECASE)
        for m in p.finditer(content):
            tn = re.sub(r'[\\[\\]\\'\\"]', "", m.group(1)).split(".")[-1]
            fk = ParsedForeignKey(fk_column=m.group(2),
                referenced_table=re.sub(r'[\\[\\]\\'\\"]', "", m.group(3)).split(".")[-1],
                referenced_column=m.group(4))
            results.append((tn, fk))
        return results


# ================================================================
# JSON SCHEMA PARSER
# ================================================================

class SchemaDefinitionParser:
    def can_parse(self, content):
        s = content.strip()
        if s.startswith("{"):
            try: return "tables" in json.loads(s)
            except: return False
        return False

    def parse(self, content, **kwargs):
        source_name = kwargs.get("source_name", "Schema Definition")
        data = json.loads(content.strip())
        tables = []
        for t in data.get("tables", []):
            cols = []
            for c in t.get("columns", []):
                if isinstance(c, str):
                    parts = c.split(":")
                    cols.append(ParsedColumn(name=parts[0].strip(), data_type=parts[1].strip() if len(parts)>1 else "varchar"))
                elif isinstance(c, dict):
                    cols.append(ParsedColumn(
                        name=c.get("name", c.get("column_name", "")),
                        data_type=c.get("type", c.get("data_type", "varchar")),
                        nullable=c.get("nullable", True),
                        is_primary_key=c.get("pk", c.get("primary_key", c.get("is_primary_key", False))),
                        max_length=c.get("length", c.get("max_length")),
                        check_constraints=c.get("values", c.get("allowed_values", [])),
                    ))
            fks = [ParsedForeignKey(fk_column=fk.get("column",""), referenced_table=fk.get("references",""),
                    referenced_column=fk.get("references_column","id"))
                   for fk in t.get("foreign_keys", t.get("fks", []))]
            tables.append(ParsedTable(name=t.get("name",""), schema=t.get("schema","dbo"), columns=cols, foreign_keys=fks))
        return ParsedSchema(source_name=data.get("name", source_name), source_type=data.get("type","parsed"),
            tables=tables, input_format=InputFormat.SCHEMA_JSON, raw_input=content[:5000])


# ================================================================
# XML PARSER
# ================================================================

class XMLParser:
    def can_parse(self, content):
        s = content.strip()
        if not s.startswith("<"): return False
        try:
            root = ET.fromstring(s)
            return root.tag.lower() in ("tables","schema","database") or any(c.tag.lower()=="table" for c in root)
        except: return False

    def parse(self, content, **kwargs):
        source_name = kwargs.get("source_name", "XML Import")
        root = ET.fromstring(content.strip())
        tables, warnings = [], []
        if root.tag.lower() in ("tables","schema","database"):
            tel = [c for c in root if c.tag.lower() == "table"]
        elif root.tag.lower() == "table":
            tel = [root]
        else:
            tel = list(root.iter("table")) + list(root.iter("Table"))
        for te in tel:
            t = self._parse_table(te, warnings)
            if t: tables.append(t)
        return ParsedSchema(source_name=source_name, source_type="xml", tables=tables,
            input_format=InputFormat.SCHEMA_XML, raw_input=content[:5000], parse_warnings=warnings)

    def _parse_table(self, el, w):
        name = el.get("name", el.get("Name", ""))
        if not name: return None
        cols, fks = [], []
        for ch in el:
            tag = ch.tag.lower()
            if tag == "column":
                c, ifk = self._parse_col(ch)
                if c: cols.append(c)
                if ifk: fks.append(ifk)
            elif tag in ("foreignkey","foreign_key","fk"):
                fk = self._parse_fk(ch)
                if fk: fks.append(fk)
        return ParsedTable(name=name, schema=el.get("schema","dbo"), columns=cols, foreign_keys=fks)

    def _parse_col(self, el):
        name = el.get("name", el.get("Name", ""))
        if not name: return None, None
        rt = el.get("type", el.get("Type", el.get("dataType", "varchar")))
        dt = rt.lower().split("(")[0]
        nullable = el.get("nullable","true").lower() not in ("false","no","0")
        is_pk = el.get("primaryKey", el.get("pk","false")).lower() in ("true","yes","1")
        maxl = int(el.get("length","0")) if el.get("length","").isdigit() else None
        check_str = el.get("check", el.get("values", ""))
        checks = [v.strip() for v in check_str.split(",") if v.strip()] if check_str else []
        ifk = None
        ref = el.get("references")
        if ref: ifk = self._parse_ref(name, ref)
        return ParsedColumn(name=name, data_type=dt, raw_type=rt, nullable=nullable,
            is_primary_key=is_pk, check_constraints=checks, max_length=maxl), ifk

    def _parse_fk(self, el):
        col = el.get("column","")
        ref = el.get("references","")
        if col and ref: return self._parse_ref(col, ref)
        return None

    def _parse_ref(self, fk_col, ref_str):
        m = re.match(r"(\\w+)\\((\\w+)\\)", ref_str)
        if m: return ParsedForeignKey(fk_column=fk_col, referenced_table=m.group(1), referenced_column=m.group(2))
        if "." in ref_str:
            p = ref_str.split(".")
            return ParsedForeignKey(fk_column=fk_col, referenced_table=p[0], referenced_column=p[1])
        return ParsedForeignKey(fk_column=fk_col, referenced_table=ref_str.strip(), referenced_column="id")


# ================================================================
# ETL MAPPING PARSER (CSV)
# ================================================================

class ETLMappingParser:
    SYNONYMS = {
        "source_table": ["src_table","from_table","source_tbl"],
        "source_column": ["src_column","src_col","from_column"],
        "source_type": ["src_type","src_data_type","from_type"],
        "target_table": ["tgt_table","to_table","target_tbl","dest_table"],
        "target_column": ["tgt_column","tgt_col","to_column","dest_column"],
        "target_type": ["tgt_type","tgt_data_type","to_type","dest_type"],
        "nullable": ["is_nullable","null","allow_null"],
        "primary_key": ["pk","is_pk","is_primary_key"],
        "foreign_key_table": ["fk_table","ref_table","references_table"],
        "foreign_key_column": ["fk_column","ref_column","references_column"],
    }

    def can_parse(self, content):
        fl = content.strip().split("\\n")[0].lower()
        return any(h in fl for h in ["source_table","target_table","src_table","tgt_table"])

    def parse(self, content, **kwargs):
        source_name = kwargs.get("source_name", "ETL Mapping")
        use_target = kwargs.get("use_target_side", True)
        reader = csv.DictReader(io.StringIO(content))
        hmap = self._map_headers(reader.fieldnames or [])
        tables_dict = {}
        for row in reader:
            tbl = row.get(hmap.get("target_table",""),"") if use_target else row.get(hmap.get("source_table",""),"")
            col = row.get(hmap.get("target_column",""),"") if use_target else row.get(hmap.get("source_column",""),"")
            dtype = row.get(hmap.get("target_type",""),"") or row.get(hmap.get("source_type",""),"") or "varchar"
            if not tbl or not col: continue
            if tbl not in tables_dict: tables_dict[tbl] = {"columns":[], "fks":[]}
            nullable_v = row.get(hmap.get("nullable",""),"true")
            nullable = nullable_v.lower() not in ("false","no","0","n") if nullable_v else True
            pk_v = row.get(hmap.get("primary_key",""),"")
            is_pk = pk_v.lower() in ("true","yes","1","y","pk") if pk_v else False
            tables_dict[tbl]["columns"].append(ParsedColumn(name=col, data_type=dtype.lower().split("(")[0],
                raw_type=dtype, nullable=nullable, is_primary_key=is_pk))
            fk_tbl = row.get(hmap.get("foreign_key_table",""),"")
            fk_col = row.get(hmap.get("foreign_key_column",""),"") or "id"
            if fk_tbl:
                tables_dict[tbl]["fks"].append(ParsedForeignKey(fk_column=col, referenced_table=fk_tbl, referenced_column=fk_col))
        tables = [ParsedTable(name=n, columns=d["columns"], foreign_keys=d["fks"]) for n,d in tables_dict.items()]
        return ParsedSchema(source_name=source_name, source_type="etl_mapping", tables=tables,
            input_format=InputFormat.ETL_MAPPING, raw_input=content[:5000])

    def _map_headers(self, headers):
        hmap = {}
        for h in headers:
            hl = h.strip().lower()
            for canonical, syns in self.SYNONYMS.items():
                if hl == canonical or hl in syns:
                    hmap[canonical] = h
                    break
        return hmap


# ================================================================
# DEFAULT STATS GENERATOR
# ================================================================

def generate_default_stats(col, row_count=1000):
    dt = col.data_type.lower()
    stats = {"null_ratio": 0.0 if not col.nullable else 0.05}
    if col.check_constraints:
        n = len(col.check_constraints)
        stats["top_values"] = [{"value":v, "frequency": round(1.0/n, 4)} for v in col.check_constraints]
        stats["distinct_count"] = n
        return stats
    if dt in ("int","integer","smallint","tinyint"):
        if col.is_primary_key:
            stats.update({"min":1,"max":row_count,"mean":row_count/2,"stddev":row_count/6,"distinct_count":row_count})
        else:
            stats.update({"min":1,"max":1000,"mean":500,"stddev":300,"distinct_count":min(500,row_count)})
    elif dt in ("bigint","long"):
        stats.update({"min":1,"max":100000,"mean":50000,"stddev":30000,"distinct_count":min(10000,row_count)})
    elif dt in ("float","double"):
        stats.update({"min":0.0,"max":10000.0,"mean":100.0,"stddev":50.0,"distinct_count":min(row_count,row_count)})
    elif dt in ("decimal","numeric"):
        p = col.precision or 10
        s = col.scale or 2
        mx = 10**(p-s) - 1
        stats.update({"min":0,"max":mx,"mean":mx/10,"stddev":mx/20,"distinct_count":min(row_count,row_count)})
    elif dt in ("varchar","char","text","string"):
        ml = col.max_length or 50
        stats.update({"min_length":3,"max_length":min(ml,100),"distinct_count":min(row_count,row_count)})
    elif dt == "date":
        stats.update({"min":"2020-01-01","max":"2025-12-31","distinct_count":min(row_count,2000)})
    elif dt in ("timestamp","datetime"):
        stats.update({"min":"2020-01-01","max":"2025-12-31","distinct_count":min(row_count,row_count)})
    elif dt in ("boolean","bool"):
        dv = col.default_value
        if dv and dv.lower() in ("true","1"):
            stats["top_values"] = [{"value":True,"frequency":0.7},{"value":False,"frequency":0.3}]
        elif dv and dv.lower() in ("false","0"):
            stats["top_values"] = [{"value":True,"frequency":0.3},{"value":False,"frequency":0.7}]
        else:
            stats["top_values"] = [{"value":True,"frequency":0.5},{"value":False,"frequency":0.5}]
        stats["distinct_count"] = 2
    else:
        stats.update({"min_length":5,"max_length":20,"distinct_count":min(row_count,row_count)})
    return stats


# ================================================================
# BLUEPRINT ASSEMBLER
# ================================================================

class BlueprintAssembler:
    def assemble(self, parsed, row_count=1000):
        bid = str(uuid4())
        tables = []
        for pt in parsed.tables:
            er = pt.row_count if pt.row_count != 1000 else row_count
            cols = [{"name":c.name,"data_type":c.data_type,"nullable":c.nullable,
                     "is_primary_key":c.is_primary_key,"stats":generate_default_stats(c, er)} for c in pt.columns]
            fks = [{"column":fk.fk_column,"references_table":fk.referenced_table,
                    "references_column":fk.referenced_column} for fk in pt.foreign_keys]
            tables.append({"name":pt.name,"schema":pt.schema,"row_count":er,"columns":cols,"foreign_keys":fks})
        rels = []
        for pt in parsed.tables:
            for fk in pt.foreign_keys:
                rels.append({"from_table":f"{pt.schema}.{pt.name}","to_table":f"{pt.schema}.{fk.referenced_table}",
                    "relationship_type":"one_to_many","join_columns":[{"from_column":fk.fk_column,"to_column":fk.referenced_column}]})
        return {"blueprint_id":bid,"source_system":{"name":parsed.source_name,"type":parsed.source_type},
            "profiled_at":datetime.now(timezone.utc).isoformat(),"profiled_by":"source_loader",
            "tables":tables,"relationships":rels}


# ================================================================
# DISTRIBUTION SAMPLER
# ================================================================

class DistributionSampler:
    def __init__(self, seed=42):
        self.rng = np.random.default_rng(seed)
        self.py_rng = random.Random(seed)

    def sample_column(self, col_spec, row_count):
        stats = col_spec.get("stats", {})
        dt = col_spec.get("data_type", "string").lower().split("(")[0]
        nr = stats.get("null_ratio", 0.0)
        nn = int(row_count * (1 - nr))
        nc = row_count - nn
        if dt in ("int","bigint","smallint","tinyint","integer","long"): vals = self._int(stats, nn)
        elif dt in ("float","double","decimal","numeric"): vals = self._float(stats, nn)
        elif dt in ("string","varchar","char","text"): vals = self._str(stats, nn)
        elif dt == "date": vals = self._date(stats, nn)
        elif dt in ("timestamp","datetime"): vals = self._ts(stats, nn)
        elif dt in ("boolean","bool"): vals = self._bool(stats, nn)
        else: vals = self._str(stats, nn)
        result = list(vals) + [None]*nc
        self.py_rng.shuffle(result)
        return result

    def _int(self, s, n):
        tv = s.get("top_values")
        if tv and s.get("distinct_count",0) <= 100: return self._freq(tv, n)
        mn, mx, mean, std = s.get("min",0), s.get("max",1000), s.get("mean"), s.get("stddev")
        if mean is not None and std is not None and std > 0:
            v = np.clip(self.rng.normal(mean, std, n), mn, mx)
            return [int(round(x)) for x in v]
        return [int(x) for x in self.rng.integers(int(mn), int(mx)+1, n)]

    def _float(self, s, n):
        mean, std = s.get("mean",0.0), s.get("stddev",1.0)
        mn, mx = s.get("min",float("-inf")), s.get("max",float("inf"))
        if mean is not None and std is not None and std > 0:
            return [round(float(x),4) for x in np.clip(self.rng.normal(mean,std,n),mn,mx)]
        return [round(float(x),4) for x in self.rng.uniform(mn,mx,n)]

    def _str(self, s, n):
        tv = s.get("top_values")
        if tv: return self._freq(tv, n)
        mnl, mxl = s.get("min_length",5), s.get("max_length",20)
        if mnl is None: mnl = 5
        if mxl is None: mxl = 20
        return ["".join(self.py_rng.choices(string.ascii_letters+string.digits, k=self.py_rng.randint(mnl,mxl))) for _ in range(n)]

    def _date(self, s, n):
        mn = self._pd(s.get("min","2020-01-01"))
        mx = self._pd(s.get("max","2025-12-31"))
        dd = max((mx-mn).days, 1)
        return [(mn+timedelta(days=int(self.rng.integers(0,dd+1)))).isoformat() for _ in range(n)]

    def _ts(self, s, n):
        dates = self._date(s, n)
        return [f"{d}T{self.py_rng.randint(0,23):02d}:{self.py_rng.randint(0,59):02d}:{self.py_rng.randint(0,59):02d}" for d in dates]

    def _bool(self, s, n):
        tv = s.get("top_values",[])
        tr = 0.5
        for t in tv:
            if str(t.get("value","")).lower() in ("true","1"): tr = t.get("frequency",0.5); break
        return [bool(x) for x in self.rng.choice([True,False],n,p=[tr,1-tr])]

    def _freq(self, tv, n):
        vals = [t["value"] for t in tv]
        freqs = np.array([t.get("frequency",1.0/len(tv)) for t in tv])
        freqs = freqs/freqs.sum()
        return [vals[i] for i in self.rng.choice(len(vals),n,p=freqs)]

    def _pd(self, v):
        if isinstance(v, date): return v
        try: return date.fromisoformat(str(v)[:10])
        except: return date(2020,1,1)


# ================================================================
# RELATIONSHIP PRESERVER
# ================================================================

class RelationshipPreserver:
    def __init__(self, blueprint):
        self.bp = blueprint
        self.tables = {t["name"]:t for t in blueprint.get("tables",[])}
        self.rels = blueprint.get("relationships",[])
        self.deps = {}
        self._build()

    def get_generation_order(self):
        visited, order = set(), []
        def visit(tn):
            if tn in visited: return
            visited.add(tn)
            for p in self.deps.get(tn,[]):
                visit(p)
            order.append(tn)
        for tn in self.deps: visit(tn)
        return order

    def get_fk_constraints(self, tn):
        ts = self.tables.get(tn,{})
        return [{"child_column":fk.get("column",""),"parent_table":fk.get("references_table",""),
                 "parent_column":fk.get("references_column","")} for fk in ts.get("foreign_keys",[])]

    def build_pk_pool(self, tn, data):
        ts = self.tables.get(tn,{})
        pool = {}
        for c in ts.get("columns",[]):
            if c.get("is_primary_key") and c["name"] in data:
                pool[c["name"]] = [v for v in data[c["name"]] if v is not None]
        return pool

    def apply_fk_values(self, tn, data, pk_pools):
        for con in self.get_fk_constraints(tn):
            pt, pc, cc = con["parent_table"], con["parent_column"], con["child_column"]
            pool = pk_pools.get(pt,{}).get(pc,[])
            if not pool or cc not in data: continue
            data[cc] = [random.choice(pool) for _ in range(len(data[cc]))]
        return data

    def _build(self):
        for tn in self.tables: self.deps[tn] = []
        for r in self.rels:
            ft = r.get("from_table","").split(".")[-1]
            tt = r.get("to_table","").split(".")[-1]
            if ft in self.deps: self.deps[ft].append(tt)
        for tn, ts in self.tables.items():
            for fk in ts.get("foreign_keys",[]):
                p = fk.get("references_table","")
                if p and p != tn and p not in self.deps.get(tn,[]): self.deps.setdefault(tn,[]).append(p)


# ================================================================
# LOCAL GENERATOR
# ================================================================

class LocalGenerator:
    def generate(self, blueprint, seed=42, on_progress=None):
        sampler = DistributionSampler(seed=seed)
        preserver = RelationshipPreserver(blueprint)
        order = preserver.get_generation_order()
        pk_pools, results = {}, {}
        total = len(order)
        for idx, tn in enumerate(order):
            ts = next((t for t in blueprint["tables"] if t["name"]==tn), None)
            if not ts: continue
            rc = ts.get("row_count",1000)
            data = {c["name"]: sampler.sample_column(c, rc) for c in ts.get("columns",[])}
            data = preserver.apply_fk_values(tn, data, pk_pools)
            pk_pools[tn] = preserver.build_pk_pool(tn, data)
            results[tn] = pd.DataFrame(data)
            if on_progress: on_progress(tn, idx+1, total)
        return results

    def conform_medallion(self, tables, blueprint):
        bronze, silver, gold = {}, {}, {}
        rules, rule_results = [], []
        total_dropped, total_nulls = 0, 0
        for tn, df in tables.items():
            ts = next((t for t in blueprint["tables"] if t["name"]==tn), None)
            bronze[tn] = df.copy()
            sdf = df.copy()
            before = len(sdf)
            sdf = sdf.dropna(how="all")
            total_dropped += before - len(sdf)
            if ts:
                pks = [c["name"] for c in ts.get("columns",[]) if c.get("is_primary_key")]
                existing_pks = [c for c in pks if c in sdf.columns]
                if existing_pks:
                    bd = len(sdf)
                    sdf = sdf.drop_duplicates(subset=existing_pks, keep="first")
                    total_dropped += bd - len(sdf)
                for cs in ts.get("columns",[]):
                    cn = cs["name"]
                    if not cs.get("nullable",True) and cn in sdf.columns:
                        nc = int(sdf[cn].isna().sum())
                        if nc > 0:
                            total_nulls += nc
                            fills = {"int":0,"bigint":0,"float":0.0,"double":0.0,"decimal":0.0,
                                     "varchar":"","char":"","text":"","boolean":False,"date":"2020-01-01"}
                            sdf[cn] = sdf[cn].fillna(fills.get(cs.get("data_type",""),""))
            silver[tn] = sdf
            # Gold aggregation
            agg = []
            for col in sdf.columns:
                row = {"column":col, "non_null":int(sdf[col].notna().sum()), "nulls":int(sdf[col].isna().sum())}
                num = pd.to_numeric(sdf[col], errors="coerce")
                if num.notna().sum() > len(sdf)*0.5:
                    row.update({"min":round(float(num.min()),2),"max":round(float(num.max()),2),
                                "mean":round(float(num.mean()),2),"std":round(float(num.std()),2)})
                else:
                    row["distinct"] = int(sdf[col].nunique())
                    top = sdf[col].value_counts().head(3)
                    row["top_values"] = ", ".join(f"{v}({c})" for v,c in top.items())
                agg.append(row)
            gold[tn] = pd.DataFrame(agg)
            # Quality rules
            if ts:
                for cs in ts.get("columns",[]):
                    cn = cs["name"]
                    if cn not in sdf.columns: continue
                    if not cs.get("nullable",True):
                        rules.append({"name":f"{tn}_{cn}_not_null","table":tn,"expression":f"{cn} IS NOT NULL","action":"warn"})
                        nc = int(sdf[cn].isna().sum())
                        rule_results.append({"rule":f"{tn}_{cn}_not_null","table":tn,"column":cn,"passed":nc==0,"violations":nc,"total_rows":len(sdf)})
                    if cs.get("is_primary_key"):
                        rules.append({"name":f"{tn}_{cn}_unique","table":tn,"expression":f"{cn} is unique","action":"warn"})
                        dc = int(sdf[cn].duplicated().sum())
                        rule_results.append({"rule":f"{tn}_{cn}_unique","table":tn,"column":cn,"passed":dc==0,"violations":dc,"total_rows":len(sdf)})
        return {"bronze":bronze,"silver":silver,"gold":gold,"rules":rules,"results":rule_results,
                "stats":{"rows_dropped":total_dropped,"nulls_cleaned":total_nulls,"tables_processed":len(tables)}}

    def validate_local(self, blueprint, generated, quality_results=None):
        results = []
        TYPE_EQ = {"int":{"integer","int","int64","int32"},"bigint":{"long","bigint","int64"},
            "float":{"float","float64","float32"},"double":{"double","float64"},
            "decimal":{"decimal","float64","float"},"varchar":{"string","varchar","text","object"},
            "char":{"string","object"},"text":{"string","object"},"boolean":{"boolean","bool"},
            "date":{"date","object"},"timestamp":{"timestamp","datetime64[ns]","object"}}
        for ts in blueprint.get("tables",[]):
            tn = ts["name"]
            df = generated.get(tn)
            if df is None:
                results.append({"table":tn,"schema_score":0,"fidelity_score":0,"quality_score":0,
                    "pipeline_score":0,"overall_score":0,"recs":[f"Table {tn} not generated"]})
                continue
            # Schema
            expected = {c["name"]:c for c in ts.get("columns",[])}
            missing = [c for c in expected if c not in df.columns]
            matched = sum(1 for cn,cs in expected.items() if cn in df.columns and
                (cs["data_type"]==str(df[cn].dtype) or str(df[cn].dtype) in TYPE_EQ.get(cs["data_type"],{cs["data_type"]})))
            ss = matched/max(len(expected),1)
            # Fidelity
            rc_match = abs(len(df)-ts.get("row_count",0))/max(ts.get("row_count",1),1) <= 0.05
            fs = (1.0 if rc_match else 0.5)*0.2 + 0.8*1.0
            # Quality
            qs = 1.0
            if quality_results:
                viol = [r for r in quality_results if r.get("table")==tn and not r.get("passed",True)]
                if viol: qs = max(0.0, 1.0-len(viol)*0.1)
            overall = ss*0.25 + fs*0.35 + qs*0.20 + 1.0*0.20
            recs = []
            if missing: recs.append(f"Missing columns: {', '.join(missing)}")
            results.append({"table":tn,"schema_score":round(ss,4),"fidelity_score":round(fs,4),
                "quality_score":round(qs,4),"pipeline_score":1.0,"overall_score":round(overall,4),"recs":recs,"missing":missing})
        return results


# ================================================================
# STREAMLIT UI
# ================================================================

st.set_page_config(page_title="SEG Demo", page_icon="ðŸ”¬", layout="wide", initial_sidebar_state="collapsed")
st.title("SEG â€” Synthetic Environment Generator")
st.caption("Upload DDL, Excel, JSON, YAML, or XML â€” generate a full synthetic environment in your browser")

for k in ("parsed","blueprint","tables","medallion","validation"):
    if k not in st.session_state: st.session_state[k] = None

tab1, tab2, tab3, tab4, tab5 = st.tabs(["1. Load & Parse","2. Blueprint","3. Generate","4. Conform","5. Validate"])

# ---- STEP 1 ----
with tab1:
    st.header("Step 1: Load & Parse Input")
    c1, c2 = st.columns(2)
    with c1:
        st.subheader("Upload File")
        uploaded = st.file_uploader("Choose a file", type=["sql","ddl","csv","json","yaml","yml","xml"], key="up1")
    with c2:
        st.subheader("Paste Text")
        pasted = st.text_area("Paste DDL, schema, or mapping", height=200,
            placeholder="CREATE TABLE customers (\\n  id INT PRIMARY KEY,\\n  name VARCHAR(100) NOT NULL\\n);", key="pa1")

    if st.button("Parse Input", type="primary", key="bp1"):
        content, filename, parsed = "", None, None
        if uploaded:
            filename = uploaded.name
            content = uploaded.read().decode("utf-8")
        elif pasted.strip():
            content = pasted.strip()
        if content:
            fmt = FormatDetector.detect(content, filename)
            parser = None
            if fmt in (InputFormat.DDL, InputFormat.RAW_TEXT): parser = DDLParser()
            elif fmt == InputFormat.ETL_MAPPING: parser = ETLMappingParser()
            elif fmt in (InputFormat.SCHEMA_JSON,): parser = SchemaDefinitionParser()
            elif fmt == InputFormat.SCHEMA_XML: parser = XMLParser()
            elif fmt == InputFormat.UNKNOWN: parser = DDLParser()
            if parser:
                try:
                    parsed = parser.parse(content, source_name=filename or "Pasted Input")
                    st.info(f"Detected format: **{fmt.value}**")
                except Exception as e:
                    st.error(f"Parse error: {e}")
            if parsed:
                st.session_state.parsed = parsed
                for k in ("blueprint","tables","medallion","validation"):
                    st.session_state[k] = None

    parsed = st.session_state.parsed
    if parsed:
        st.markdown("---")
        tc = sum(len(t.columns) for t in parsed.tables)
        tf = sum(len(t.foreign_keys) for t in parsed.tables)
        m1,m2,m3,m4 = st.columns(4)
        m1.metric("Tables", len(parsed.tables))
        m2.metric("Columns", tc)
        m3.metric("Foreign Keys", tf)
        m4.metric("Source Type", parsed.source_type)
        for t in parsed.tables:
            with st.expander(f"Table: **{t.name}** ({len(t.columns)} columns)", expanded=True):
                rows = [{"Column":c.name,"Type":c.data_type,"Raw":c.raw_type or c.data_type,
                    "Nullable":"Yes" if c.nullable else "No","PK":"Y" if c.is_primary_key else "",
                    "Check":", ".join(c.check_constraints) if c.check_constraints else ""} for c in t.columns]
                st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
                if t.foreign_keys:
                    st.markdown("**Foreign Keys:**")
                    for fk in t.foreign_keys:
                        st.markdown(f"- \`{fk.fk_column}\` â†’ \`{fk.referenced_table}({fk.referenced_column})\`")
        st.success(f"Parsed **{len(parsed.tables)} tables** â€” proceed to Step 2")

# ---- STEP 2 ----
with tab2:
    st.header("Step 2: Blueprint Assembly")
    parsed = st.session_state.parsed
    if not parsed:
        st.info("Complete Step 1 first.")
    else:
        c1,c2 = st.columns(2)
        with c1: row_count = st.number_input("Rows per table", 10, 10000, 1000, 100, key="rc2")
        with c2: seed = st.number_input("Random seed", 0, 99999, 42, key="sd2")
        if st.button("Assemble Blueprint", type="primary", key="bp2"):
            bp = BlueprintAssembler().assemble(parsed, row_count=row_count)
            st.session_state.blueprint = bp
            for k in ("tables","medallion","validation"): st.session_state[k] = None
        bp = st.session_state.blueprint
        if bp:
            st.markdown("---")
            b1,b2,b3,b4 = st.columns(4)
            b1.metric("Blueprint ID", bp["blueprint_id"][:8]+"...")
            b2.metric("Tables", len(bp["tables"]))
            b3.metric("Relationships", len(bp["relationships"]))
            b4.metric("Source", bp["source_system"]["type"])
            if bp["relationships"]:
                st.subheader("Relationships")
                for r in bp["relationships"]:
                    ft = r["from_table"].split(".")[-1]
                    tt = r["to_table"].split(".")[-1]
                    j = r["join_columns"][0]
                    st.markdown(f"\`{ft}\`.{j['from_column']} â†’ \`{tt}\`.{j['to_column']}")
            st.subheader("Table Specs")
            for t in bp["tables"]:
                with st.expander(f"**{t['name']}** â€” {t['row_count']} rows"):
                    sr = [{"Column":c["name"],"Type":c["data_type"],"PK":"Y" if c.get("is_primary_key") else "",
                        "Null%":f"{c['stats'].get('null_ratio',0):.0%}",
                        "Min":c["stats"].get("min",""),"Max":c["stats"].get("max",""),
                        "Distinct":c["stats"].get("distinct_count","")} for c in t["columns"]]
                    st.dataframe(pd.DataFrame(sr), use_container_width=True, hide_index=True)
            st.download_button("Download Blueprint JSON", json.dumps(bp,indent=2,default=str),
                f"blueprint_{bp['blueprint_id'][:8]}.json", "application/json")
            st.success("Blueprint assembled â€” proceed to Step 3")

# ---- STEP 3 ----
with tab3:
    st.header("Step 3: Generate Synthetic Data")
    bp = st.session_state.blueprint
    if not bp:
        st.info("Complete Step 2 first.")
    else:
        if st.button("Generate Synthetic Data", type="primary", key="bp3"):
            prog = st.progress(0, text="Starting...")
            gen = LocalGenerator()
            def on_p(tn, i, t): prog.progress(i/t, text=f"Generating {tn} ({i}/{t})...")
            seed = st.session_state.get("sd2", 42)
            tables = gen.generate(bp, seed=seed, on_progress=on_p)
            prog.progress(1.0, text="Done!")
            st.session_state.tables = tables
            for k in ("medallion","validation"): st.session_state[k] = None
        tables = st.session_state.tables
        if tables:
            st.markdown("---")
            total = sum(len(df) for df in tables.values())
            g1,g2,g3 = st.columns(3)
            g1.metric("Tables", len(tables))
            g2.metric("Total Rows", f"{total:,}")
            g3.metric("Total Columns", sum(len(df.columns) for df in tables.values()))
            for name, df in tables.items():
                with st.expander(f"**{name}** â€” {len(df):,} rows", expanded=True):
                    st.markdown("**Preview** (20 rows)")
                    st.dataframe(df.head(20), use_container_width=True, hide_index=True)
                    st.markdown("**Statistics**")
                    st.dataframe(df.describe(include="all").transpose(), use_container_width=True)
            buf = io.BytesIO()
            with zipfile.ZipFile(buf,"w",zipfile.ZIP_DEFLATED) as zf:
                for n,df in tables.items(): zf.writestr(f"{n}.csv", df.to_csv(index=False))
            buf.seek(0)
            st.download_button("Download All (CSV ZIP)", buf, "synthetic_data.zip", "application/zip")
            st.success(f"Generated **{total:,} rows** â€” proceed to Step 4")

# ---- STEP 4 ----
with tab4:
    st.header("Step 4: Conform â€” Medallion Architecture")
    tables = st.session_state.tables
    bp = st.session_state.blueprint
    if not tables or not bp:
        st.info("Complete Step 3 first.")
    else:
        if st.button("Run Medallion Pipeline", type="primary", key="bp4"):
            with st.spinner("Running Bronze â†’ Silver â†’ Gold..."):
                med = LocalGenerator().conform_medallion(tables, bp)
                st.session_state.medallion = med
                st.session_state.validation = None
        med = st.session_state.medallion
        if med:
            st.markdown("---")
            s1,s2,s3 = st.columns(3)
            s1.metric("Tables Processed", med["stats"]["tables_processed"])
            s2.metric("Rows Dropped", med["stats"]["rows_dropped"])
            s3.metric("Nulls Cleaned", med["stats"]["nulls_cleaned"])
            for tn in med["bronze"]:
                st.markdown(f"### Table: \`{tn}\`")
                cb,cs,cg = st.columns(3)
                with cb:
                    bdf = med["bronze"][tn]
                    st.markdown("**Bronze** (Raw)")
                    st.metric("Rows", f"{len(bdf):,}")
                    st.dataframe(bdf.head(8), use_container_width=True, hide_index=True)
                with cs:
                    sdf = med["silver"][tn]
                    d = len(bdf)-len(sdf)
                    st.markdown("**Silver** (Cleaned)")
                    st.metric("Rows", f"{len(sdf):,}", delta=f"-{d}" if d else None)
                    st.dataframe(sdf.head(8), use_container_width=True, hide_index=True)
                with cg:
                    gdf = med["gold"][tn]
                    st.markdown("**Gold** (Aggregated)")
                    st.metric("Columns Profiled", len(gdf))
                    st.dataframe(gdf, use_container_width=True, hide_index=True)
            st.markdown("---")
            st.subheader("Quality Rules")
            if med["rules"]:
                st.markdown("**DLT Expectations:**")
                dlt = "\\n".join(f"  CONSTRAINT {r['name']} EXPECT ({r['expression']})" for r in med["rules"])
                st.code(dlt, language="sql")
                qr = [{"Status":"PASS" if r["passed"] else "FAIL","Rule":r["rule"],"Table":r["table"],
                    "Violations":r.get("violations",0),"Total":r.get("total_rows",0)} for r in med["results"]]
                st.dataframe(pd.DataFrame(qr), use_container_width=True, hide_index=True)
                pc = sum(1 for r in med["results"] if r["passed"])
                st.metric("Pass Rate", f"{pc}/{len(med['results'])} ({pc/max(len(med['results']),1):.0%})")
            st.success("Medallion complete â€” proceed to Step 5")

# ---- STEP 5 ----
with tab5:
    st.header("Step 5: Validate & Confidence Score")
    tables = st.session_state.tables
    bp = st.session_state.blueprint
    med = st.session_state.medallion
    if not tables or not bp:
        st.info("Complete Steps 3-4 first.")
    else:
        if st.button("Run Validation", type="primary", key="bp5"):
            with st.spinner("Validating..."):
                qr = med["results"] if med else None
                val = LocalGenerator().validate_local(bp, tables, qr)
                st.session_state.validation = val
        val = st.session_state.validation
        if val:
            st.markdown("---")
            avg = sum(v["overall_score"] for v in val)/len(val)
            gc = sum(1 for v in val if v["overall_score"]>=0.90)
            ac = sum(1 for v in val if 0.70<=v["overall_score"]<0.90)
            rc = sum(1 for v in val if v["overall_score"]<0.70)
            if avg>=0.90: lvl,icon="GREEN","ðŸŸ¢"
            elif avg>=0.70: lvl,icon="AMBER","ðŸŸ¡"
            else: lvl,icon="RED","ðŸ”´"
            st.markdown(f"## {icon} Overall: **{lvl}** â€” {avg:.1%}")
            st.markdown(f"**{len(val)} tables:** {gc} green, {ac} amber, {rc} red")
            w1,w2,w3,w4 = st.columns(4)
            w1.metric("Schema (25%)", f"{sum(v['schema_score'] for v in val)/len(val):.1%}")
            w2.metric("Fidelity (35%)", f"{sum(v['fidelity_score'] for v in val)/len(val):.1%}")
            w3.metric("Quality (20%)", f"{sum(v['quality_score'] for v in val)/len(val):.1%}")
            w4.metric("Pipeline (20%)", f"{sum(v['pipeline_score'] for v in val)/len(val):.1%}")
            st.markdown("---")
            for v in val:
                ic = "ðŸŸ¢" if v["overall_score"]>=0.90 else "ðŸŸ¡" if v["overall_score"]>=0.70 else "ðŸ”´"
                with st.expander(f"{ic} **{v['table']}** â€” {v['overall_score']:.1%}", expanded=v["overall_score"]<0.90):
                    d1,d2,d3,d4 = st.columns(4)
                    d1.metric("Schema", f"{v['schema_score']:.0%}")
                    d2.metric("Fidelity", f"{v['fidelity_score']:.0%}")
                    d3.metric("Quality", f"{v['quality_score']:.0%}")
                    d4.metric("Pipeline", f"{v['pipeline_score']:.0%}")
                    st.progress(v["schema_score"], text=f"Schema: {v['schema_score']:.0%}")
                    st.progress(v["fidelity_score"], text=f"Fidelity: {v['fidelity_score']:.0%}")
                    st.progress(v["quality_score"], text=f"Quality: {v['quality_score']:.0%}")
                    st.progress(v["pipeline_score"], text=f"Pipeline: {v['pipeline_score']:.0%}")
                    if v.get("recs"):
                        st.markdown("**Recommendations:**")
                        for r in v["recs"]: st.markdown(f"- {r}")
            st.markdown("---")
            st.markdown("| Dimension | Weight | Description |\\n|---|---|---|\\n| Schema | 25% | Columns, types match |\\n| Fidelity | 35% | Distributions, nulls match |\\n| Quality | 20% | DLT pass rate |\\n| Pipeline | 20% | Medallion success |")
            st.markdown("| Level | Threshold |\\n|---|---|\\n| ðŸŸ¢ Green | >= 90% |\\n| ðŸŸ¡ Amber | >= 70% |\\n| ðŸ”´ Red | < 70% |")
            st.success("Validation complete! Synthetic environment is ready.")
`
      },
    }, document.getElementById("root"));

    // Hide loading once stlite starts
    const observer = new MutationObserver(() => {
      const root = document.getElementById("root");
      if (root && root.children.length > 0) {
        document.getElementById("loading").style.display = "none";
        observer.disconnect();
      }
    });
    observer.observe(document.getElementById("root"), { childList: true, subtree: true });
  </script>
</body>
</html>
