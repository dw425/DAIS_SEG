<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>NiFi Flow Analyzer</title>
  <script type="module" crossorigin>function Ia(e,n){for(var t=0;t<n.length;t++){const r=n[t];if(typeof r!="string"&&!Array.isArray(r)){for(const i in r)if(i!=="default"&&!(i in e)){const o=Object.getOwnPropertyDescriptor(r,i);o&&Object.defineProperty(e,i,o.get?o:{enumerable:!0,get:()=>r[i]})}}}return Object.freeze(Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}))}(function(){const n=document.createElement("link").relList;if(n&&n.supports&&n.supports("modulepreload"))return;for(const i of document.querySelectorAll('link[rel="modulepreload"]'))r(i);new MutationObserver(i=>{for(const o of i)if(o.type==="childList")for(const a of o.addedNodes)a.tagName==="LINK"&&a.rel==="modulepreload"&&r(a)}).observe(document,{childList:!0,subtree:!0});function t(i){const o={};return i.integrity&&(o.integrity=i.integrity),i.referrerPolicy&&(o.referrerPolicy=i.referrerPolicy),i.crossOrigin==="use-credentials"?o.credentials="include":i.crossOrigin==="anonymous"?o.credentials="omit":o.credentials="same-origin",o}function r(i){if(i.ep)return;i.ep=!0;const o=t(i);fetch(i.href,o)}})();const Fa="modulepreload",La=function(e,n){return new URL(e,n).href},Qr={},kt=function(n,t,r){let i=Promise.resolve();if(t&&t.length>0){let a=function(d){return Promise.all(d.map(u=>Promise.resolve(u).then(g=>({status:"fulfilled",value:g}),g=>({status:"rejected",reason:g}))))};const s=document.getElementsByTagName("link"),c=document.querySelector("meta[property=csp-nonce]"),l=c?.nonce||c?.getAttribute("nonce");i=a(t.map(d=>{if(d=La(d,r),d in Qr)return;Qr[d]=!0;const u=d.endsWith(".css"),g=u?'[rel="stylesheet"]':"";if(!!r)for(let f=s.length-1;f>=0;f--){const h=s[f];if(h.href===d&&(!u||h.rel==="stylesheet"))return}else if(document.querySelector(`link[href="${d}"]${g}`))return;const m=document.createElement("link");if(m.rel=u?"stylesheet":Fa,u||(m.as="script"),m.crossOrigin="",m.href=d,l&&m.setAttribute("nonce",l),document.head.appendChild(m),u)return new Promise((f,h)=>{m.addEventListener("load",f),m.addEventListener("error",()=>h(new Error(`Unable to preload CSS for ${d}`)))})}))}function o(a){const s=new Event("vite:preloadError",{cancelable:!0});if(s.payload=a,window.dispatchEvent(s),!s.defaultPrevented)throw a}return i.then(a=>{for(const s of a||[])s.status==="rejected"&&o(s.reason);return n().catch(o)})},Dr=Object.freeze({parsed:null,analysis:null,assessment:null,notebook:null,migrationReport:null,finalReport:null,manifest:null,validation:null,valueAnalysis:null});let Ne={...Dr};const On=new Set;function Oa(e={}){return Ne={...Dr,...e},On.clear(),{getState:ce,setState:Xe,subscribe:Na,resetState:Gt}}function ce(e){return e!==void 0?Ne[e]:{...Ne}}function Xe(e){if(!e||typeof e!="object")return;const n={...Ne};Object.assign(Ne,e),Tr(n)}function Na(e){if(typeof e!="function")throw new TypeError("subscribe() expects a function");return On.add(e),()=>On.delete(e)}function Gt(){const e={...Ne};Ne={...Dr},Tr(e)}const Ma=Object.freeze({analyze:["parsed"],assess:["parsed","analysis"],convert:["parsed","assessment"],report:["parsed","notebook"],reportFinal:["parsed","notebook","migrationReport"],validate:["parsed","notebook"],value:["parsed","notebook"]});function tt(){return Ct(Ne)}function Ct(e){if(e===null||typeof e!="object")return e;if(e instanceof Set)return new Set([...e].map(Ct));if(e instanceof Map)return new Map([...e].map(([t,r])=>[Ct(t),Ct(r)]));if(Array.isArray(e))return e.map(Ct);const n={};for(const t of Object.keys(e))n[t]=Ct(e[t]);return n}function nt(e){if(!e||typeof e!="object")return;const n={...Ne};Ne={...e},Tr(n)}function zn(e){const n=Ma[e];if(!n)return{ok:!0,missing:[]};const t=n.filter(r=>Ne[r]==null);return{ok:t.length===0,missing:t}}function Tr(e){const n={...Ne};for(const t of On)try{t(n,e)}catch(r){console.error("[state] subscriber threw:",r)}}class Ba{constructor(){this._handlers=new Map}on(n,t){return this._handlers.has(n)||this._handlers.set(n,new Set),this._handlers.get(n).add(t),()=>this.off(n,t)}once(n,t){const r=(...i)=>{this.off(n,r),t(...i)};return this.on(n,r)}off(n,t){const r=this._handlers.get(n);r&&(r.delete(t),r.size===0&&this._handlers.delete(n))}emit(n,...t){const r=this._handlers.get(n);if(r)for(const i of r)try{i(...t)}catch(o){console.error(`[EventBus] handler for "${n}" threw:`,o)}}clear(n){n?this._handlers.delete(n):this._handlers.clear()}}const Xn=new Ba,Nn=new Map;let Ut=null;function to(){if(Ut!==null)return Ut;try{const e="__ls_probe__";localStorage.setItem(e,"1"),localStorage.removeItem(e),Ut=!0}catch{Ut=!1}return Ut}function ja(e,n=null){try{if(to()){const t=localStorage.getItem(e);if(t===null)return n;try{return JSON.parse(t)}catch{return t}}return Nn.has(e)?Nn.get(e):n}catch{return n}}function Ua(e,n){const t=typeof n=="string"?n:JSON.stringify(n);try{return to()?(localStorage.setItem(e,t),!0):(Nn.set(e,n),!0)}catch{return Nn.set(e,n),!1}}const no="dbx_config",qa=Object.freeze([{value:"14.3",label:"14.3 LTS",sparkVersion:"14.3.x-scala2.12"},{value:"15.4",label:"15.4 LTS",sparkVersion:"15.4.x-scala2.12"},{value:"16.0",label:"16.0",sparkVersion:"16.0.x-scala2.12"}]),Jr=Object.freeze({azure:[{value:"Standard_DS3_v2",label:"Standard_DS3_v2 (4 vCPU, 14 GB)",tier:"small"},{value:"Standard_DS4_v2",label:"Standard_DS4_v2 (8 vCPU, 28 GB)",tier:"medium"},{value:"Standard_DS5_v2",label:"Standard_DS5_v2 (16 vCPU, 56 GB)",tier:"large"},{value:"Standard_E4ds_v5",label:"Standard_E4ds_v5 (4 vCPU, 32 GB, memory-opt)",tier:"memory"},{value:"Standard_NC6s_v3",label:"Standard_NC6s_v3 (6 vCPU, 112 GB, GPU)",tier:"gpu"}],aws:[{value:"i3.xlarge",label:"i3.xlarge (4 vCPU, 30.5 GB)",tier:"small"},{value:"i3.2xlarge",label:"i3.2xlarge (8 vCPU, 61 GB)",tier:"medium"},{value:"i3.4xlarge",label:"i3.4xlarge (16 vCPU, 122 GB)",tier:"large"},{value:"r5.xlarge",label:"r5.xlarge (4 vCPU, 32 GB, memory-opt)",tier:"memory"},{value:"p3.2xlarge",label:"p3.2xlarge (8 vCPU, 61 GB, GPU)",tier:"gpu"}],gcp:[{value:"n2-standard-4",label:"n2-standard-4 (4 vCPU, 16 GB)",tier:"small"},{value:"n2-standard-8",label:"n2-standard-8 (8 vCPU, 32 GB)",tier:"medium"},{value:"n2-standard-16",label:"n2-standard-16 (16 vCPU, 64 GB)",tier:"large"},{value:"n2-highmem-4",label:"n2-highmem-4 (4 vCPU, 32 GB, memory-opt)",tier:"memory"},{value:"a2-highgpu-1g",label:"a2-highgpu-1g (12 vCPU, 85 GB, GPU)",tier:"gpu"}]}),xe=Object.freeze({catalog:"",schema:"",secretScope:"",cloudProvider:"azure",sparkVersion:"14.3.x-scala2.12",nodeType:"Standard_DS3_v2",numWorkers:2,workspacePath:"/Workspace/Migrations/NiFi",computeType:"cluster",runtimeVersion:"14.3"});function za(){try{const e=ja(no);return e&&typeof e=="object"?{...xe,...e}:{...xe}}catch{return{...xe}}}function Ga(e){try{Ua(no,e)}catch{}}function ro(){const e=document.getElementById("cfgRuntimeVersion")?.value||xe.runtimeVersion,n=qa.find(t=>t.value===e);return{catalog:document.getElementById("cfgCatalog")?.value||"",schema:document.getElementById("cfgSchema")?.value||"",secretScope:document.getElementById("cfgScope")?.value||"",cloudProvider:document.getElementById("cfgCloud")?.value||xe.cloudProvider,sparkVersion:n?n.sparkVersion:xe.sparkVersion,nodeType:document.getElementById("cfgNodeType")?.value||xe.nodeType,numWorkers:parseInt(document.getElementById("cfgWorkers")?.value)||xe.numWorkers,workspacePath:document.getElementById("cfgWorkspacePath")?.value||xe.workspacePath,computeType:document.getElementById("cfgComputeType")?.value||xe.computeType,runtimeVersion:e}}function Ha(e,n){return!n||!n.catalog?e:e.replace(/<catalog>|\{catalog\}/g,n.catalog).replace(/<schema>|\{schema\}/g,n.schema||"default").replace(/<scope>|\{scope\}/g,n.secretScope||"migration_secrets").replace(/<workspace_path>/g,n.workspacePath||xe.workspacePath).replace(/<spark_version>/g,n.sparkVersion||xe.sparkVersion).replace(/<node_type>/g,n.nodeType||xe.nodeType)}const so=500,Wa={PARSE_XML_MALFORMED:{severity:"high",message:"XML is malformed or incomplete",fix:"Check XML syntax â€” ensure all tags are properly closed",recoverable:!1},PARSE_JSON_INVALID:{severity:"high",message:"JSON is not valid",fix:"Validate JSON format at jsonlint.com",recoverable:!1},PARSE_ARCHIVE_CORRUPT:{severity:"high",message:"Archive file is corrupt or unreadable",fix:"Re-download or re-export the NiFi flow",recoverable:!1},PARSE_EMPTY_FLOW:{severity:"medium",message:"Flow contains no processors",fix:"Verify this is the correct NiFi flow file",recoverable:!1},ANALYZE_CIRCULAR_DEP:{severity:"low",message:"Circular dependency detected in flow",fix:"Review cycle groups in the tier diagram",recoverable:!0},MAP_UNKNOWN_PROCESSOR:{severity:"medium",message:"Unknown processor type â€” no mapping template",fix:"This processor requires manual Databricks implementation",recoverable:!0},MAP_TEMPLATE_UNRESOLVED:{severity:"high",message:"Template placeholders could not be resolved",fix:"Check processor properties match template expectations",recoverable:!0},MAP_PROPERTY_MISSING:{severity:"medium",message:"Required processor property not found",fix:"Verify NiFi processor configuration is complete",recoverable:!0},GEN_UNRESOLVED_PLACEHOLDER:{severity:"high",message:"Generated code contains unresolved placeholders",fix:"Configure Databricks settings in Step 4",recoverable:!0},GEN_INVALID_PYTHON:{severity:"high",message:"Generated code has syntax errors",fix:"Review flagged cells and fix Python syntax",recoverable:!0},GEN_MISSING_IMPORT:{severity:"medium",message:"Generated code uses unimported modules",fix:"Add missing imports to the requirements cell",recoverable:!0},GEN_DEPRECATED_API:{severity:"low",message:"Generated code uses deprecated Databricks API",fix:"Update to current API â€” see Databricks release notes",recoverable:!0},VALIDATE_INTENT_MISMATCH:{severity:"medium",message:"Processor intent not preserved in notebook",fix:"Review the generated cell and verify logic matches NiFi behavior",recoverable:!0},VALIDATE_SCHEMA_VIOLATION:{severity:"high",message:"Output schema does not match expected schema",fix:"Check column names and types in generated code",recoverable:!0},CONFIG_INVALID_CLOUD:{severity:"low",message:"Invalid cloud provider selected",fix:"Choose azure, aws, or gcp",recoverable:!0},CONFIG_MISSING_REQUIRED:{severity:"medium",message:"Required configuration field is empty",fix:"Fill in catalog, schema, and secret scope in Step 4",recoverable:!0}},Ar="nifi_analyzer_errors";function Va(){try{const e=localStorage.getItem(Ar);if(e){const n=JSON.parse(e);if(Array.isArray(n))return n.map(t=>{const r=new we(t.message||"",{code:t.code,phase:t.phase,severity:t.severity,context:t.context});return r.timestamp=t.timestamp||Date.now(),r}).slice(-so)}}catch{}return[]}function Ka(){try{const e=$t.map(n=>({message:n.message,code:n.code,phase:n.phase,severity:n.severity,context:n.context,timestamp:n.timestamp,stack:n.stack}));localStorage.setItem(Ar,JSON.stringify(e))}catch{}}const $t=Va();class we extends Error{constructor(n,{code:t,phase:r,severity:i,cause:o,context:a}={}){super(n),this.name="AppError",this.code=t||"UNKNOWN",this.phase=r||"",this.severity=i||"medium",this.cause=o||null,this.context=a||{},this.timestamp=Date.now()}}const oo=[];function Qa(e){typeof e=="function"&&oo.push(e)}function $e(e,n={}){const t=e instanceof we?e:new we(e.message||String(e),{code:"UNHANDLED",cause:e,...n});console.error(`[${t.code}] ${t.message}`,t),$t.push(t),$t.length>so&&$t.shift(),Ka(),oo.forEach(r=>{try{r(t)}catch{}})}function An(){return[...$t]}function Ht(){$t.length=0;try{localStorage.removeItem(Ar)}catch{}}function Ja(e){return Wa[e]}function Za(e){const n=document.createElement("div");n.id="errorPanel",n.className="error-panel collapsed",n.innerHTML=`
    <div class="error-panel-header" id="errorPanelToggle">
      <span>Errors &amp; Warnings</span>
      <span id="errorBadge" class="error-badge hidden">0</span>
      <span class="error-panel-arrow">â–²</span>
    </div>
    <div class="error-panel-body">
      <div class="error-panel-toolbar">
        <button id="errorCopyBtn" class="btn btn-sm">Copy All</button>
        <button id="errorDownloadBtn" class="btn btn-sm">Download Log</button>
        <button id="errorClearBtn" class="btn btn-sm btn-secondary">Clear</button>
      </div>
      <div id="errorList" class="error-list"></div>
    </div>
  `,e.appendChild(n),n.querySelector("#errorPanelToggle").addEventListener("click",()=>{n.classList.toggle("collapsed")});const r=n.querySelector("#errorCopyBtn");r.addEventListener("click",()=>{const c=An().map(l=>`[${new Date(l.timestamp).toISOString()}] [${l.severity}] [${l.code}] ${l.message}${l.phase?" (phase: "+l.phase+")":""}`).join(`
`);navigator.clipboard.writeText(c).then(()=>{r.textContent="Copied!",setTimeout(()=>{r.textContent="Copy All"},1500)}).catch(()=>{r.textContent="Failed",setTimeout(()=>{r.textContent="Copy All"},1500)})}),n.querySelector("#errorDownloadBtn").addEventListener("click",()=>{const c=An().map(g=>({timestamp:new Date(g.timestamp).toISOString(),code:g.code,severity:g.severity,phase:g.phase,message:g.message,context:g.context,stack:g.stack})),l=new Blob([JSON.stringify(c,null,2)],{type:"application/json"}),d=URL.createObjectURL(l),u=document.createElement("a");u.href=d,u.download="nifi-analyzer-errors-"+new Date().toISOString().slice(0,19).replace(/:/g,"-")+".json",document.body.appendChild(u),u.click(),document.body.removeChild(u),URL.revokeObjectURL(d)}),n.querySelector("#errorClearBtn").addEventListener("click",()=>{Ht();const s=n.querySelector("#errorList");s&&(s.innerHTML=""),pt()}),pt(),An().forEach(s=>{ao(s,!0)})}function Xa(e){switch(e){case"critical":case"high":return"ðŸ”´";case"medium":return"ðŸŸ¡";case"low":return"ðŸ”µ";default:return"ðŸ”µ"}}function qt(e){const n=document.createElement("div");return n.textContent=e,n.innerHTML}function ao(e,n=!1){const t=document.getElementById("errorList");if(!t)return;const i=new Date(e.timestamp).toLocaleTimeString(),o=e.code?Ja(e.code):null,a=document.createElement("div");a.className="error-item",a.setAttribute("role","listitem");let s="";o&&o.fix&&(s=`<div class="error-fix">Fix: ${qt(o.fix)}</div>`);let c="";e.stack&&(c=`<div class="error-stack">${qt(e.stack)}</div>`),a.innerHTML=`
    <span class="error-severity">${Xa(e.severity)}</span>
    <span class="error-phase">${qt(e.phase||e.code||"unknown")}</span>
    <span class="error-message">
      ${qt(e.message)}
      ${s}
      ${c}
    </span>
    <span class="error-timestamp">${qt(i)}</span>
  `,a.addEventListener("click",()=>{a.classList.toggle("expanded")}),t.insertBefore(a,t.firstChild),n||pt()}function pt(){const e=document.getElementById("errorBadge");if(!e)return;const n=An().length;e.textContent=String(n),n>0?e.classList.remove("hidden"):e.classList.add("hidden")}function Ya(){document.querySelectorAll(".tab").forEach(e=>{e.addEventListener("click",()=>{if(e.classList.contains("locked"))return;document.querySelectorAll(".tab").forEach(t=>t.classList.remove("active")),document.querySelectorAll(".panel").forEach(t=>t.classList.remove("active")),e.classList.add("active");const n=document.getElementById("panel-"+e.dataset.tab);n&&n.classList.add("active")})})}function it(e){document.querySelectorAll(".tab").forEach(n=>{n.classList.toggle("active",n.dataset.tab===e)}),document.querySelectorAll(".panel").forEach(n=>{n.classList.toggle("active",n.id==="panel-"+e)})}function ge(e,n){const t=document.querySelector(`.tab[data-tab="${e}"]`);t&&(t.classList.remove("locked","processing","done"),n==="locked"?t.classList.add("locked"):n==="processing"?t.classList.add("processing"):n==="done"&&t.classList.add("done"),n!=="locked"&&(t.style.pointerEvents="",t.style.opacity=""))}function gt(e){ge(e,"ready")}function B(e){return e==null?"":String(e).replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;").replace(/"/g,"&quot;").replace(/'/g,"&#39;")}function at(e){return'<div class="metrics">'+e.map(n=>{const t=B(Array.isArray(n)?n[0]:n.label),r=B(Array.isArray(n)?n[1]:n.value),i=Array.isArray(n)?n[2]:n.delta,o=Array.isArray(n)?"":n.color||"";return`<div class="metric"><div class="label">${t}</div><div class="value"${o?' style="color:'+B(o)+'"':""}>${r}</div>`+(i?`<div class="delta">${B(i)}</div>`:"")+"</div>"}).join("")+"</div>"}function Kt(e,n){const t=e.map(i=>`<th>${B(i)}</th>`).join(""),r=n.map(i=>`<tr>${i.map(o=>`<td>${o??""}</td>`).join("")}</tr>`).join("");return`<div class="table-scroll"><table><thead><tr>${t}</tr></thead><tbody>${r}</tbody></table></div>`}function yr(e,n,t=!1){return`<div class="expander ${t?"open":""}"><div class="expander-header" data-expander-toggle><span>${e}</span><span class="expander-arrow">&#9654;</span></div><div class="expander-body">${n}</div></div>`}typeof document<"u"&&document.addEventListener("click",e=>{const n=e.target.closest("[data-expander-toggle]");n&&n.parentElement.classList.toggle("open")});let Rt="",Gn="",Dt=null;const ei=[".gz",".zip",".nar",".jar",".tgz",".docx",".xlsx"];function io(e){const n=e.toLowerCase();return n.endsWith(".tar.gz")||n.endsWith(".xml.gz")||n.endsWith(".json.gz")?!0:ei.some(t=>n.endsWith(t))}function co(){return Rt}function Ir(e,n){Rt=e,Gn=n,Dt=null}function lo(){return Gn}function po(){return Dt}function ti(){const e=document.getElementById("fileInput");if(!e)return Promise.resolve();const n=e.files[0];if(!n)return Promise.resolve();Gn=n.name;const t=document.getElementById("fileName");t&&(t.textContent="Loaded: "+n.name,t.classList.remove("hidden"));const r=io(n.name);return new Promise(i=>{const o=new FileReader;o.onload=a=>{r?(Dt=new Uint8Array(a.target.result),Rt=""):(Rt=a.target.result,Dt=null),i()},o.onerror=()=>i(),r?o.readAsArrayBuffer(n):o.readAsText(n)})}function uo(){const e=document.getElementById("fileInput"),n=document.getElementById("fileDropZone");!e||!n||(e.setAttribute("accept",".xml,.json,.sql,.txt,.csv,.gz,.zip,.nar,.jar,.tgz,.tar.gz,.docx,.xlsx"),n.addEventListener("click",()=>e.click()),n.addEventListener("dragover",t=>{t.preventDefault(),n.style.borderColor="var(--primary)"}),n.addEventListener("dragleave",()=>{n.style.borderColor="var(--border)"}),n.addEventListener("drop",async t=>{t.preventDefault(),n.style.borderColor="var(--border)";const r=t.dataTransfer.files[0];if(r){Gn=r.name;const i=document.getElementById("fileName");i&&(i.textContent="Loaded: "+r.name,i.classList.remove("hidden"));const o=io(r.name);await new Promise(a=>{const s=new FileReader;s.onload=c=>{o?(Dt=new Uint8Array(c.target.result),Rt=""):(Rt=c.target.result,Dt=null),a()},s.onerror=()=>a(),o?s.readAsArrayBuffer(r):s.readAsText(r)}),e.dispatchEvent(new Event("change",{bubbles:!0}))}}))}const ni=Object.freeze(Object.defineProperty({__proto__:null,getUploadedBytes:po,getUploadedContent:co,getUploadedName:lo,handleFile:ti,initFileUpload:uo,setUploadedContent:Ir},Symbol.toStringTag,{value:"Module"})),ri={etl:`<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<flowController encoding-version="1.4">
  <rootGroup><name>ETL_Demo_Pipeline</name>
    <processor><id>p1</id><name>Read Source CSV</name><class>org.apache.nifi.processors.standard.GetFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>5 min</schedulingPeriod><state>RUNNING</state>
      <property><name>Input Directory</name><value>/data/input/sales</value></property>
      <property><name>File Filter</name><value>[^\\\\.].*\\\\.csv</value></property>
      <autoTerminatedRelationship>failure</autoTerminatedRelationship>
    </processor>
    <processor><id>p2</id><name>Validate Schema</name><class>org.apache.nifi.processors.standard.ValidateRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Record Reader</name><value>CSVReader</value></property>
      <property><name>Record Writer</name><value>CSVWriter</value></property>
    </processor>
    <processor><id>p3</id><name>Route by Region</name><class>org.apache.nifi.processors.standard.RouteOnAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Routing Strategy</name><value>Route to Property name</value></property>
      <property><name>us_east</name><value>\${region:equals("US-East")}</value></property>
      <property><name>us_west</name><value>\${region:equals("US-West")}</value></property>
      <property><name>europe</name><value>\${region:equals("EU")}</value></property>
    </processor>
    <processor><id>p4</id><name>Transform Sales Data</name><class>org.apache.nifi.processors.standard.ReplaceText</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Search Value</name><value>"amount":"(\\d+)"</value></property>
      <property><name>Replacement Value</name><value>"amount_cents":"\${1}00"</value></property>
      <property><name>Replacement Strategy</name><value>Regex Replace</value></property>
    </processor>
    <processor><id>p5</id><name>Query Sales Summary</name><class>org.apache.nifi.processors.standard.ExecuteSQL</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Database Connection Pooling Service</name><value>DBCPService</value></property>
      <property><name>SQL select query</name><value>SELECT region, product, SUM(amount) as total, COUNT(*) as cnt FROM sales.transactions WHERE trade_date >= CURRENT_DATE - 7 GROUP BY region, product</value></property>
    </processor>
    <processor><id>p6</id><name>Update Attributes</name><class>org.apache.nifi.processors.standard.UpdateAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>output.filename</name><value>\${filename:substringBefore('.')}_processed_\${now():format('yyyyMMdd')}.csv</value></property>
      <property><name>batch.id</name><value>\${UUID()}</value></property>
    </processor>
    <processor><id>p7</id><name>Write to Data Lake</name><class>org.apache.nifi.processors.standard.PutFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Directory</name><value>/data/output/processed_sales</value></property>
      <property><name>Conflict Resolution Strategy</name><value>replace</value></property>
    </processor>
    <processor><id>p8</id><name>Insert to Warehouse</name><class>org.apache.nifi.processors.standard.PutDatabaseRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Database Connection Pooling Service</name><value>DBCPService</value></property>
      <property><name>Table Name</name><value>warehouse.sales_processed</value></property>
      <property><name>Statement Type</name><value>INSERT</value></property>
    </processor>
    <processor><id>p9</id><name>Log Completion</name><class>org.apache.nifi.processors.standard.LogMessage</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Log Level</name><value>info</value></property>
      <property><name>Log Message</name><value>ETL batch complete: \${batch.id}</value></property>
    </processor>
    <connection><id>c1</id><source><id>p1</id><type>PROCESSOR</type></source><destination><id>p2</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c2</id><source><id>p2</id><type>PROCESSOR</type></source><destination><id>p3</id><type>PROCESSOR</type></destination><relationship>valid</relationship></connection>
    <connection><id>c3</id><source><id>p3</id><type>PROCESSOR</type></source><destination><id>p4</id><type>PROCESSOR</type></destination><relationship>us_east</relationship></connection>
    <connection><id>c4</id><source><id>p3</id><type>PROCESSOR</type></source><destination><id>p4</id><type>PROCESSOR</type></destination><relationship>us_west</relationship></connection>
    <connection><id>c5</id><source><id>p3</id><type>PROCESSOR</type></source><destination><id>p4</id><type>PROCESSOR</type></destination><relationship>europe</relationship></connection>
    <connection><id>c6</id><source><id>p4</id><type>PROCESSOR</type></source><destination><id>p5</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c7</id><source><id>p5</id><type>PROCESSOR</type></source><destination><id>p6</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c8</id><source><id>p6</id><type>PROCESSOR</type></source><destination><id>p7</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c9</id><source><id>p6</id><type>PROCESSOR</type></source><destination><id>p8</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c10</id><source><id>p8</id><type>PROCESSOR</type></source><destination><id>p9</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
  </rootGroup>
  <controllerServices>
    <controllerService><id>cs1</id><name>DBCPService</name><class>org.apache.nifi.dbcp.DBCPConnectionPool</class><property><name>Database Connection URL</name><value>jdbc:postgresql://db.example.com:5432/analytics</value></property><property><name>Database User</name><value>etl_user</value></property><property><name>Password</name><value>\${DB_PASSWORD}</value></property></controllerService>
  </controllerServices>
</flowController>`,streaming:`<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<flowController encoding-version="1.4">
  <rootGroup><name>Streaming_IoT_Pipeline</name>
    <processGroup><name>IoT Ingestion</name>
      <processor><id>s1</id><name>Consume Kafka Events</name><class>org.apache.nifi.processors.kafka.pubsub.ConsumeKafka_2_6</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>100 ms</schedulingPeriod><state>RUNNING</state>
        <property><name>Kafka Brokers</name><value>kafka-broker-1:9092,kafka-broker-2:9092</value></property>
        <property><name>Topic Name(s)</name><value>iot.sensor.readings</value></property>
        <property><name>Group ID</name><value>nifi-iot-consumer</value></property>
      </processor>
      <processor><id>s2</id><name>Parse JSON Payload</name><class>org.apache.nifi.processors.standard.EvaluateJsonPath</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Destination</name><value>flowfile-attribute</value></property>
        <property><name>sensor_id</name><value>$.sensor_id</value></property>
        <property><name>temperature</name><value>$.readings.temperature</value></property>
        <property><name>humidity</name><value>$.readings.humidity</value></property>
        <property><name>timestamp</name><value>$.event_time</value></property>
      </processor>
      <processor><id>s3</id><name>Route by Threshold</name><class>org.apache.nifi.processors.standard.RouteOnAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Routing Strategy</name><value>Route to Property name</value></property>
        <property><name>alert</name><value>\${temperature:gt(100):or(\${humidity:gt(95)})}</value></property>
        <property><name>normal</name><value>\${temperature:le(100):and(\${humidity:le(95)})}</value></property>
      </processor>
      <connection><id>sc1</id><source><id>s1</id><type>PROCESSOR</type></source><destination><id>s2</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>sc2</id><source><id>s2</id><type>PROCESSOR</type></source><destination><id>s3</id><type>PROCESSOR</type></destination><relationship>matched</relationship></connection>
    </processGroup>
    <processGroup><name>Alert Processing</name>
      <processor><id>s4</id><name>Enrich Alert Data</name><class>org.apache.nifi.processors.standard.LookupAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Lookup Service</name><value>DeviceRegistry</value></property>
        <property><name>device.name</name><value>\${sensor_id}</value></property>
      </processor>
      <processor><id>s5</id><name>Format Alert Notification</name><class>org.apache.nifi.processors.standard.ReplaceText</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Replacement Value</name><value>{"alert":"THRESHOLD_EXCEEDED","sensor":"\${sensor_id}","temp":"\${temperature}","humidity":"\${humidity}","device":"\${device.name}","time":"\${timestamp}"}</value></property>
        <property><name>Replacement Strategy</name><value>Always Replace</value></property>
      </processor>
      <processor><id>s6</id><name>Send Alert to API</name><class>org.apache.nifi.processors.standard.InvokeHTTP</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Remote URL</name><value>https://alerts.example.com/api/v2/notify</value></property>
        <property><name>HTTP Method</name><value>POST</value></property>
        <property><name>Content-Type</name><value>application/json</value></property>
      </processor>
      <connection><id>sc3</id><source><id>s4</id><type>PROCESSOR</type></source><destination><id>s5</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>sc4</id><source><id>s5</id><type>PROCESSOR</type></source><destination><id>s6</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <processGroup><name>Data Storage</name>
      <processor><id>s7</id><name>Batch Readings</name><class>org.apache.nifi.processors.standard.MergeContent</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Merge Strategy</name><value>Bin-Packing Algorithm</value></property>
        <property><name>Minimum Number of Entries</name><value>100</value></property>
        <property><name>Maximum Number of Entries</name><value>1000</value></property>
        <property><name>Max Bin Age</name><value>30 sec</value></property>
      </processor>
      <processor><id>s8</id><name>Convert to Parquet</name><class>org.apache.nifi.processors.standard.ConvertRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Record Reader</name><value>JsonTreeReader</value></property>
        <property><name>Record Writer</name><value>ParquetRecordSetWriter</value></property>
      </processor>
      <processor><id>s9</id><name>Write to Delta Lake</name><class>org.apache.nifi.processors.standard.PutHDFS</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Directory</name><value>/data/iot/sensor_readings/\${now():format('yyyy/MM/dd')}</value></property>
        <property><name>Conflict Resolution Strategy</name><value>replace</value></property>
      </processor>
      <processor><id>s10</id><name>Insert to Timeseries DB</name><class>org.apache.nifi.processors.standard.PutDatabaseRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Database Connection Pooling Service</name><value>TimeseriesDBCP</value></property>
        <property><name>Table Name</name><value>iot.sensor_readings</value></property>
        <property><name>Statement Type</name><value>INSERT</value></property>
      </processor>
      <connection><id>sc5</id><source><id>s7</id><type>PROCESSOR</type></source><destination><id>s8</id><type>PROCESSOR</type></destination><relationship>merged</relationship></connection>
      <connection><id>sc6</id><source><id>s8</id><type>PROCESSOR</type></source><destination><id>s9</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>sc7</id><source><id>s8</id><type>PROCESSOR</type></source><destination><id>s10</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <connection><id>sc_g1</id><source><id>s3</id><type>PROCESSOR</type></source><destination><id>s4</id><type>PROCESSOR</type></destination><relationship>alert</relationship></connection>
    <connection><id>sc_g2</id><source><id>s3</id><type>PROCESSOR</type></source><destination><id>s7</id><type>PROCESSOR</type></destination><relationship>normal</relationship></connection>
    <connection><id>sc_g3</id><source><id>s3</id><type>PROCESSOR</type></source><destination><id>s7</id><type>PROCESSOR</type></destination><relationship>alert</relationship></connection>
  </rootGroup>
</flowController>`,full:`<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<flowController encoding-version="1.4">
  <rootGroup><name>Manufacturing_Data_Pipeline</name>
    <processGroup><name>Data Ingestion</name>
      <processor><id>f1</id><name>Scan Input Directory</name><class>org.apache.nifi.processors.standard.GetFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>1 min</schedulingPeriod><state>RUNNING</state>
        <property><name>Input Directory</name><value>/data/mfg/incoming</value></property>
        <property><name>File Filter</name><value>.*\\.(csv|json|xml)</value></property>
        <property><name>Keep Source File</name><value>false</value></property>
      </processor>
      <processor><id>f2</id><name>List SFTP Uploads</name><class>org.apache.nifi.processors.standard.ListFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>5 min</schedulingPeriod><state>RUNNING</state>
        <property><name>Input Directory</name><value>/sftp/uploads/mfg_data</value></property>
        <property><name>File Filter</name><value>production_.*\\.csv</value></property>
      </processor>
      <processor><id>f3</id><name>Fetch Upload Contents</name><class>org.apache.nifi.processors.standard.FetchFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>File to Fetch</name><value>\${absolute.path}/\${filename}</value></property>
      </processor>
      <processor><id>f4</id><name>Query Production Metrics</name><class>org.apache.nifi.processors.standard.ExecuteSQL</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>10 min</schedulingPeriod><state>RUNNING</state>
        <property><name>Database Connection Pooling Service</name><value>MfgDBCP</value></property>
        <property><name>SQL select query</name><value>SELECT lot_id, wafer_id, step_name, measurement, result, operator, meas_time FROM mfg_data.production_steps WHERE meas_time >= CURRENT_TIMESTAMP - INTERVAL '1' HOUR ORDER BY meas_time</value></property>
      </processor>
      <connection><id>fc1</id><source><id>f2</id><type>PROCESSOR</type></source><destination><id>f3</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <processGroup><name>Data Transformation</name>
      <processor><id>f5</id><name>Route by File Type</name><class>org.apache.nifi.processors.standard.RouteOnAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Routing Strategy</name><value>Route to Property name</value></property>
        <property><name>csv_files</name><value>\${filename:endsWith('.csv')}</value></property>
        <property><name>json_files</name><value>\${filename:endsWith('.json')}</value></property>
        <property><name>xml_files</name><value>\${filename:endsWith('.xml')}</value></property>
      </processor>
      <processor><id>f6</id><name>Parse JSON Metrics</name><class>org.apache.nifi.processors.standard.EvaluateJsonPath</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Destination</name><value>flowfile-attribute</value></property>
        <property><name>lot_id</name><value>$.lot_id</value></property>
        <property><name>status</name><value>$.quality_status</value></property>
        <property><name>yield_pct</name><value>$.yield_percentage</value></property>
      </processor>
      <processor><id>f7</id><name>Normalize Data Format</name><class>org.apache.nifi.processors.standard.ConvertRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Record Reader</name><value>InferAvroReader</value></property>
        <property><name>Record Writer</name><value>CSVRecordSetWriter</value></property>
      </processor>
      <processor><id>f8</id><name>Add Processing Metadata</name><class>org.apache.nifi.processors.standard.UpdateAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>processing.timestamp</name><value>\${now():format('yyyy-MM-dd HH:mm:ss')}</value></property>
        <property><name>source.system</name><value>nifi_mfg_pipeline</value></property>
        <property><name>batch.id</name><value>\${UUID()}</value></property>
        <property><name>output.filename</name><value>\${filename:substringBefore('.')}_enriched_\${now():format('yyyyMMdd_HHmmss')}.csv</value></property>
      </processor>
      <connection><id>fc2</id><source><id>f5</id><type>PROCESSOR</type></source><destination><id>f6</id><type>PROCESSOR</type></destination><relationship>json_files</relationship></connection>
      <connection><id>fc3</id><source><id>f5</id><type>PROCESSOR</type></source><destination><id>f7</id><type>PROCESSOR</type></destination><relationship>csv_files</relationship></connection>
      <connection><id>fc4</id><source><id>f6</id><type>PROCESSOR</type></source><destination><id>f8</id><type>PROCESSOR</type></destination><relationship>matched</relationship></connection>
      <connection><id>fc5</id><source><id>f7</id><type>PROCESSOR</type></source><destination><id>f8</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <processGroup><name>Data Loading</name>
      <processor><id>f9</id><name>Write to Staging</name><class>org.apache.nifi.processors.standard.PutFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Directory</name><value>/data/mfg/staging</value></property>
        <property><name>Conflict Resolution Strategy</name><value>replace</value></property>
      </processor>
      <processor><id>f10</id><name>Upload to HDFS</name><class>org.apache.nifi.processors.hadoop.PutHDFS</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Directory</name><value>/data/warehouse/mfg_production</value></property>
        <property><name>Conflict Resolution Strategy</name><value>replace</value></property>
      </processor>
      <processor><id>f11</id><name>Insert Production Records</name><class>org.apache.nifi.processors.standard.PutDatabaseRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Database Connection Pooling Service</name><value>MfgDBCP</value></property>
        <property><name>Table Name</name><value>mfg_data.production_processed</value></property>
        <property><name>Statement Type</name><value>INSERT</value></property>
      </processor>
      <processor><id>f12</id><name>Transfer to Partner SFTP</name><class>org.apache.nifi.processors.standard.PutSFTP</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Hostname</name><value>sftp.partner.example.com</value></property>
        <property><name>Port</name><value>22</value></property>
        <property><name>Username</name><value>mfg_data_xfer</value></property>
        <property><name>Password</name><value>\${SFTP_PASSWORD}</value></property>
        <property><name>Remote Path</name><value>/incoming/mfg/\${now():format('yyyyMMdd')}</value></property>
      </processor>
      <connection><id>fc6</id><source><id>f9</id><type>PROCESSOR</type></source><destination><id>f10</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>fc7</id><source><id>f9</id><type>PROCESSOR</type></source><destination><id>f11</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>fc8</id><source><id>f10</id><type>PROCESSOR</type></source><destination><id>f12</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <processGroup><name>Orchestration</name>
      <processor><id>f13</id><name>Signal Data Ready</name><class>org.apache.nifi.processors.standard.Notify</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Signal Counter Name</name><value>mfg_data_ready</value></property>
        <property><name>Signal Counter Delta</name><value>1</value></property>
      </processor>
      <processor><id>f14</id><name>Wait for All Sources</name><class>org.apache.nifi.processors.standard.Wait</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>5 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Signal Counter Name</name><value>mfg_data_ready</value></property>
        <property><name>Target Signal Count</name><value>3</value></property>
      </processor>
      <processor><id>f15</id><name>Run Aggregation Script</name><class>org.apache.nifi.processors.standard.ExecuteStreamCommand</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Command</name><value>/opt/scripts/aggregate_mfg.sh</value></property>
        <property><name>Command Arguments</name><value>/data/mfg/staging /data/mfg/aggregated</value></property>
      </processor>
      <processor><id>f16</id><name>Refresh Impala Tables</name><class>org.apache.nifi.processors.standard.ExecuteStreamCommand</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Command</name><value>impala-shell</value></property>
        <property><name>Command Arguments</name><value>-q INVALIDATE METADATA mfg_data.production_processed; COMPUTE STATS mfg_data.production_processed;</value></property>
      </processor>
      <processor><id>f17</id><name>Log Pipeline Status</name><class>org.apache.nifi.processors.standard.LogMessage</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Log Level</name><value>info</value></property>
        <property><name>Log Message</name><value>Manufacturing pipeline complete: batch=\${batch.id}, files=\${file.count}, timestamp=\${processing.timestamp}</value></property>
      </processor>
      <connection><id>fc9</id><source><id>f14</id><type>PROCESSOR</type></source><destination><id>f15</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>fc10</id><source><id>f15</id><type>PROCESSOR</type></source><destination><id>f16</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>fc11</id><source><id>f16</id><type>PROCESSOR</type></source><destination><id>f17</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <connection><id>fc_g1</id><source><id>f1</id><type>PROCESSOR</type></source><destination><id>f5</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g2</id><source><id>f3</id><type>PROCESSOR</type></source><destination><id>f5</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g3</id><source><id>f4</id><type>PROCESSOR</type></source><destination><id>f8</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g4</id><source><id>f8</id><type>PROCESSOR</type></source><destination><id>f9</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g5</id><source><id>f11</id><type>PROCESSOR</type></source><destination><id>f13</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g6</id><source><id>f12</id><type>PROCESSOR</type></source><destination><id>f13</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
  </rootGroup>
  <controllerServices>
    <controllerService><id>cs_mfg</id><name>MfgDBCP</name><class>org.apache.nifi.dbcp.DBCPConnectionPool</class>
      <property><name>Database Connection URL</name><value>jdbc:oracle:thin:@mfg-db.example.com:1521/MFGPRD</value></property>
      <property><name>Database User</name><value>mfg_reader</value></property>
      <property><name>Password</name><value>\${MFG_DB_PASSWORD}</value></property>
      <property><name>Database Driver Class Name</name><value>oracle.jdbc.driver.OracleDriver</value></property>
    </controllerService>
  </controllerServices>
</flowController>`};async function si(e,n){const t=ri[e];if(!t)return;const r={etl:"ETL Pipeline (9 processors)",streaming:"Streaming IoT (10 processors)",full:"Manufacturing Migration (17 processors)"};Ir(t,`sample_${e}_flow.xml`);const i=document.getElementById("fileName");i&&(i.textContent="Sample: "+(r[e]||e),i.classList.remove("hidden"));const o=document.getElementById("pasteInput");o&&(o.value=""),typeof n=="function"&&await n()}async function oi(e,n,t){const r=document.getElementById("fileName");r&&(r.textContent="Loading: "+n+"...",r.classList.remove("hidden"));try{const i=await fetch(e);if(!i.ok)throw new Error("HTTP "+i.status);const o=await i.text();Ir(o,n),r&&(r.textContent="Sample: "+n);const a=document.getElementById("pasteInput");a&&(a.value=""),typeof t=="function"&&await t()}catch(i){r&&(r.textContent="Failed to load "+n+" â€” "+i.message,r.style.color="var(--red)",setTimeout(()=>{r.style.color=""},3e3))}}const fo=/password|secret|token|key|auth|credential|cert|private|keytab|passphrase/i;function ai(e,n){return fo.test(e)?"********":n}function mo(e){return fo.test(e)}function ho(e){let n=0;const t=[],r=new Set,i={},o={},a=[];function s(c){i[c]=o[c]=n++,t.push(c),r.add(c);for(const l of e[c]||[])i[l]===void 0?(s(l),o[c]=Math.min(o[c],o[l])):r.has(l)&&(o[c]=Math.min(o[c],i[l]));if(o[c]===i[c]){const l=[];let d;do d=t.pop(),r.delete(d),l.push(d);while(d!==c);l.length>1&&a.push(l)}}for(const c of Object.keys(e))i[c]===void 0&&s(c);return a}const Fr={GetFile:{role:"source",subcategory:"file-source",action:"read"},GetSFTP:{role:"source",subcategory:"file-source",action:"read"},GetFTP:{role:"source",subcategory:"file-source",action:"read"},FetchFile:{role:"source",subcategory:"file-source",action:"read"},ListFile:{role:"source",subcategory:"file-source",action:"read"},ListSFTP:{role:"source",subcategory:"file-source",action:"read"},ListFTP:{role:"source",subcategory:"file-source",action:"read"},TailFile:{role:"source",subcategory:"file-source",action:"read"},GetHTTP:{role:"source",subcategory:"api-source",action:"read"},ListenHTTP:{role:"source",subcategory:"api-source",action:"read"},HandleHttpRequest:{role:"source",subcategory:"api-source",action:"read"},ConsumeKafka:{role:"source",subcategory:"streaming-source",action:"read"},ConsumeKafka_2_6:{role:"source",subcategory:"streaming-source",action:"read"},ConsumeKafkaRecord_2_6:{role:"source",subcategory:"streaming-source",action:"read"},ConsumeJMS:{role:"source",subcategory:"streaming-source",action:"read"},ConsumeMQTT:{role:"source",subcategory:"streaming-source",action:"read"},ConsumeAMQP:{role:"source",subcategory:"streaming-source",action:"read"},ConsumeAzureEventHub:{role:"source",subcategory:"streaming-source",action:"read"},ConsumeGCPubSub:{role:"source",subcategory:"streaming-source",action:"read"},ConsumeKinesisStream:{role:"source",subcategory:"streaming-source",action:"read"},GetSQS:{role:"source",subcategory:"streaming-source",action:"read"},QueryDatabaseTable:{role:"source",subcategory:"db-source",action:"read"},QueryDatabaseTableRecord:{role:"source",subcategory:"db-source",action:"read"},GetMongo:{role:"source",subcategory:"db-source",action:"read"},GetElasticsearch:{role:"source",subcategory:"db-source",action:"read"},GetHBase:{role:"source",subcategory:"db-source",action:"read"},GetCouchbaseKey:{role:"source",subcategory:"db-source",action:"read"},GetDynamoDB:{role:"source",subcategory:"db-source",action:"read"},GetCypher:{role:"source",subcategory:"db-source",action:"read"},ListS3:{role:"source",subcategory:"cloud-source",action:"read"},FetchS3Object:{role:"source",subcategory:"cloud-source",action:"read"},FetchAzureBlobStorage:{role:"source",subcategory:"cloud-source",action:"read"},FetchGCS:{role:"source",subcategory:"cloud-source",action:"read"},ListGCSBucket:{role:"source",subcategory:"cloud-source",action:"read"},GetHDFS:{role:"source",subcategory:"cloud-source",action:"read"},FetchHDFS:{role:"source",subcategory:"cloud-source",action:"read"},ListHDFS:{role:"source",subcategory:"cloud-source",action:"read"},GenerateFlowFile:{role:"source",subcategory:"generator",action:"read"},ReplaceText:{role:"transform",subcategory:"content-transform",action:"transform"},JoltTransformJSON:{role:"transform",subcategory:"content-transform",action:"transform"},JoltTransformRecord:{role:"transform",subcategory:"content-transform",action:"transform"},FlattenJson:{role:"transform",subcategory:"content-transform",action:"transform"},TransformXml:{role:"transform",subcategory:"content-transform",action:"transform"},ModifyBytes:{role:"transform",subcategory:"content-transform",action:"transform"},EvaluateJsonPath:{role:"transform",subcategory:"extract",action:"extract"},EvaluateXPath:{role:"transform",subcategory:"extract",action:"extract"},EvaluateXQuery:{role:"transform",subcategory:"extract",action:"extract"},ExtractText:{role:"transform",subcategory:"extract",action:"extract"},ExtractGrok:{role:"transform",subcategory:"extract",action:"extract"},ExtractHL7Attributes:{role:"transform",subcategory:"extract",action:"extract"},ParseCEF:{role:"transform",subcategory:"extract",action:"extract"},ParseEvtx:{role:"transform",subcategory:"extract",action:"extract"},ParseNetflowv5:{role:"transform",subcategory:"extract",action:"extract"},ParseSyslog5424:{role:"transform",subcategory:"extract",action:"extract"},IdentifyMimeType:{role:"transform",subcategory:"extract",action:"extract"},UpdateAttribute:{role:"transform",subcategory:"attribute-transform",action:"enrich"},PutAttribute:{role:"transform",subcategory:"attribute-transform",action:"enrich"},AttributesToJSON:{role:"transform",subcategory:"attribute-transform",action:"transform"},ConvertRecord:{role:"transform",subcategory:"format-convert",action:"transform"},ConvertAvroToJSON:{role:"transform",subcategory:"format-convert",action:"transform"},ConvertAvroToORC:{role:"transform",subcategory:"format-convert",action:"transform"},ConvertCSVToAvro:{role:"transform",subcategory:"format-convert",action:"transform"},ConvertJSONToAvro:{role:"transform",subcategory:"format-convert",action:"transform"},ConvertJSONToSQL:{role:"transform",subcategory:"format-convert",action:"transform"},ConvertCharacterSet:{role:"transform",subcategory:"format-convert",action:"transform"},SplitJson:{role:"transform",subcategory:"split-merge",action:"transform"},SplitContent:{role:"transform",subcategory:"split-merge",action:"transform"},SplitXml:{role:"transform",subcategory:"split-merge",action:"transform"},SplitText:{role:"transform",subcategory:"split-merge",action:"transform"},SplitAvro:{role:"transform",subcategory:"split-merge",action:"transform"},SplitRecord:{role:"transform",subcategory:"split-merge",action:"transform"},MergeContent:{role:"transform",subcategory:"split-merge",action:"transform"},MergeRecord:{role:"transform",subcategory:"split-merge",action:"transform"},ForkRecord:{role:"transform",subcategory:"split-merge",action:"transform"},SampleRecord:{role:"transform",subcategory:"split-merge",action:"transform"},SegmentContent:{role:"transform",subcategory:"split-merge",action:"transform"},DuplicateFlowFile:{role:"transform",subcategory:"split-merge",action:"transform"},UnpackContent:{role:"transform",subcategory:"split-merge",action:"transform"},CompressContent:{role:"transform",subcategory:"encoding",action:"transform"},EncryptContent:{role:"transform",subcategory:"encoding",action:"transform"},HashAttribute:{role:"transform",subcategory:"encoding",action:"transform"},ExecuteScript:{role:"transform",subcategory:"scripted",action:"transform"},ExecuteGroovyScript:{role:"transform",subcategory:"scripted",action:"transform"},RouteOnAttribute:{role:"route",subcategory:"conditional",action:"filter"},RouteOnContent:{role:"route",subcategory:"conditional",action:"filter"},RouteText:{role:"route",subcategory:"conditional",action:"filter"},RouteHL7:{role:"route",subcategory:"conditional",action:"filter"},ValidateRecord:{role:"route",subcategory:"validation",action:"validate"},DistributeLoad:{role:"route",subcategory:"load-balance",action:"filter"},DetectDuplicate:{role:"route",subcategory:"dedup",action:"filter"},ExecuteSQL:{role:"process",subcategory:"database",action:"read"},InvokeHTTP:{role:"process",subcategory:"api-call",action:"enrich"},LookupRecord:{role:"process",subcategory:"enrichment",action:"enrich"},LookupAttribute:{role:"process",subcategory:"enrichment",action:"enrich"},HandleHttpResponse:{role:"process",subcategory:"api-call",action:"write"},ExecuteStreamCommand:{role:"process",subcategory:"external-process",action:"transform"},ExecuteProcess:{role:"process",subcategory:"external-process",action:"transform"},InvokeAWSGatewayApi:{role:"process",subcategory:"api-call",action:"enrich"},PutFile:{role:"sink",subcategory:"file-sink",action:"write"},PutSFTP:{role:"sink",subcategory:"file-sink",action:"write"},PutFTP:{role:"sink",subcategory:"file-sink",action:"write"},PutDatabaseRecord:{role:"sink",subcategory:"db-sink",action:"write"},PutSQL:{role:"sink",subcategory:"db-sink",action:"write"},PutMongo:{role:"sink",subcategory:"db-sink",action:"write"},PutElasticsearch:{role:"sink",subcategory:"db-sink",action:"write"},PutHBaseJSON:{role:"sink",subcategory:"db-sink",action:"write"},PutHBaseCell:{role:"sink",subcategory:"db-sink",action:"write"},PutDynamoDB:{role:"sink",subcategory:"db-sink",action:"write"},PutS3Object:{role:"sink",subcategory:"cloud-sink",action:"write"},PutHDFS:{role:"sink",subcategory:"cloud-sink",action:"write"},PutAzureBlobStorage:{role:"sink",subcategory:"cloud-sink",action:"write"},PutAzureDataLakeStorage:{role:"sink",subcategory:"cloud-sink",action:"write"},PutGCSObject:{role:"sink",subcategory:"cloud-sink",action:"write"},PublishKafka:{role:"sink",subcategory:"streaming-sink",action:"write"},PublishKafka_2_6:{role:"sink",subcategory:"streaming-sink",action:"write"},PublishKafkaRecord_2_6:{role:"sink",subcategory:"streaming-sink",action:"write"},PutKinesisStream:{role:"sink",subcategory:"streaming-sink",action:"write"},PublishGCPubSub:{role:"sink",subcategory:"streaming-sink",action:"write"},PutSNS:{role:"sink",subcategory:"streaming-sink",action:"write"},PutEmail:{role:"sink",subcategory:"notification-sink",action:"write"},PutSyslog:{role:"sink",subcategory:"notification-sink",action:"write"},PutTCP:{role:"sink",subcategory:"notification-sink",action:"write"},LogMessage:{role:"utility",subcategory:"logging",action:"monitor"},LogAttribute:{role:"utility",subcategory:"logging",action:"monitor"},DebugFlow:{role:"utility",subcategory:"logging",action:"monitor"},CountText:{role:"utility",subcategory:"metrics",action:"monitor"},Wait:{role:"utility",subcategory:"flow-control",action:"monitor"},Notify:{role:"utility",subcategory:"flow-control",action:"monitor"}};Object.entries(Fr).forEach(([e,n])=>{n.role});function ii(e){return/^(Get|List|Consume|Fetch|Query|Tail)/i.test(e)?"read":/^(Put|Publish|Send|Post)/i.test(e)?"write":/^(Route|Distribute|Validate|Detect)/i.test(e)?"filter":/^(Convert|Split|Merge|Replace|Transform|Extract|Evaluate|Flatten|Compress|Encrypt|Hash|Parse|Fork|Sample|Unpack)/i.test(e)?"transform":/^(Lookup|Invoke)/i.test(e)?"enrich":/^(Log|Debug|Count|Wait|Notify)/i.test(e)?"monitor":"process"}function ci(e,n){return n==="source"?/File|SFTP|FTP/i.test(e)?"file-source":/HTTP|API|Gateway/i.test(e)?"api-source":/Kafka|JMS|MQTT|AMQP|EventHub|PubSub|Kinesis|SQS/i.test(e)?"streaming-source":/SQL|Database|Mongo|Elasticsearch|HBase|Couchbase|DynamoDB|Cypher|Redis/i.test(e)?"db-source":/S3|Azure|GCS|HDFS|Cloud/i.test(e)?"cloud-source":"file-source":n==="sink"?/File|SFTP|FTP/i.test(e)?"file-sink":/SQL|Database|Mongo|Elasticsearch|HBase|Couchbase|DynamoDB|Redis/i.test(e)?"db-sink":/Kafka|JMS|MQTT|AMQP|EventHub|PubSub|Kinesis|SNS|SQS/i.test(e)?"streaming-sink":/S3|Azure|GCS|HDFS|Cloud/i.test(e)?"cloud-sink":/Email|Syslog|TCP|Slack/i.test(e)?"notification-sink":"cloud-sink":n==="transform"?/Extract|Evaluate|Parse|Grok|HL7|CEF|Syslog|Identify/i.test(e)?"extract":/Attribute|PutAttribute/i.test(e)?"attribute-transform":/Convert|CharacterSet/i.test(e)?"format-convert":/Split|Merge|Fork|Sample|Segment|Duplicate|Unpack/i.test(e)?"split-merge":/Compress|Encrypt|Hash/i.test(e)?"encoding":/Script|Groovy/i.test(e)?"scripted":"content-transform":n==="route"?/Validate/i.test(e)?"validation":/Duplicate|Dedup/i.test(e)?"dedup":/Distribute|Balance/i.test(e)?"load-balance":"conditional":n==="process"?/SQL|Database/i.test(e)?"database":/HTTP|API|Gateway|Invoke/i.test(e)?"api-call":/Lookup/i.test(e)?"enrichment":"external-process":n==="utility"?/Log|Debug/i.test(e)?"logging":/Count/i.test(e)?"metrics":"flow-control":n+"-unknown"}function Pe(e){const n=Fr[e];return n?n.role:/^(Get|List|Consume|Listen|Fetch|Tail|Query)/i.test(e)?"source":/^(Put|Publish|Send|Post)/i.test(e)?"sink":/^(Route|Distribute|Control|Validate|Detect)/i.test(e)?"route":/^(Convert|Split|Merge|Replace|Transform|Extract|Evaluate|Flatten|Compress|Encrypt|Hash)/i.test(e)?"transform":/^(Execute|Invoke|Lookup|Handle)/i.test(e)?"process":/^(Log|Debug|Count|Wait|Notify)/i.test(e)?"utility":"process"}function go(e){const n=Fr[e];if(n)return{...n};const t=Pe(e);return{role:t,subcategory:ci(e,t),action:ii(e)}}const _o=[{id:"ingestion",label:"INGESTION",color:"#3B82F6",match:(e,n,t)=>e==="source"},{id:"extraction",label:"EXTRACTION",color:"#8B5CF6",match:(e,n)=>n==="extract"},{id:"routing",label:"ROUTING",color:"#EAB308",match:e=>e==="route"},{id:"enrichment",label:"ENRICHMENT",color:"#06B6D4",match:(e,n,t)=>t==="enrich"||n==="enrichment"||n==="api-call"},{id:"transformation",label:"TRANSFORMATION",color:"#A855F7",match:e=>e==="transform"},{id:"loading",label:"LOADING",color:"#21C354",match:e=>e==="sink"},{id:"monitoring",label:"MONITORING",color:"#808495",match:(e,n,t)=>e==="utility"||t==="monitor"},{id:"processing",label:"PROCESSING",color:"#6366F1",match:()=>!0}];function li(e){const n={};return Object.entries(e||{}).forEach(([t,r])=>{const i={};(r.processors||[]).forEach(a=>{if(!a||!a.type)return;const s=go(a.type),c=_o.find(l=>l.match(s.role,s.subcategory,s.action));i[c.id]=(i[c.id]||0)+1});const o=Object.entries(i).sort((a,s)=>s[1]-a[1]);n[t]=o.length>0?o[0][0]:"processing"}),n}function di(e){return _o.find(n=>n.id===e)}const pi=new Set(["Destination","Return Type","Path Not Found Behavior","Null Value Representation","Character Set","Maximum Buffer Size","Maximum Capture Group Length","Enable Canonical Equivalence","Enable Case-insensitive Matching","Permit Whitespace and Comments","Include Zero Capture Groups","Delete Attributes Expression","Store State","Stateful Variables Initial Value","Canonical Value Lookup Cache Size"]),ui=new Set(["UpdateAttribute","PutAttribute","EvaluateJsonPath","EvaluateXPath","EvaluateXQuery","ExtractText","ExtractGrok","ExtractHL7Attributes"]);function fi(e,n){const t={},r={};function i(s){return t[s]||(t[s]={creators:[],readers:[],modifiers:[]}),t[s]}function o(s){return r[s]||(r[s]={creates:[],reads:[]}),r[s]}(e||[]).forEach(s=>{if(!s||!s.name)return;const c=s.properties||{},l=o(s.name);ui.has(s.type)&&Object.keys(c).forEach(d=>{if(pi.has(d)||d.startsWith("nifi-")||d.startsWith("Record "))return;const u=i(d);u.creators.length>0&&(s.type==="UpdateAttribute"||s.type==="PutAttribute")?u.modifiers.includes(s.name)||u.modifiers.push(s.name):u.creators.includes(s.name)||u.creators.push(s.name),l.creates.includes(d)||l.creates.push(d)}),Object.values(c).forEach(d=>{if(!d||typeof d!="string"||!d.includes("${"))return;const u=d.match(/\$\{([^}:]+)/g);u&&u.forEach(g=>{const m=g.slice(2).trim().split(":")[0].split(".")[0].trim();if(!m||m.includes("(")||m.length>80||/^(now|nextInt|random|UUID|uuid|hostname|ip|literal|thread|entryDate)$/i.test(m))return;const f=i(m);f.readers.includes(s.name)||f.readers.push(s.name),l.reads.includes(m)||l.reads.push(m)})})});const a={};return Object.entries(t).forEach(([s,c])=>{const l=[];c.creators.forEach(d=>l.push({proc:d,action:"create"})),c.modifiers.forEach(d=>l.push({proc:d,action:"modify"})),c.readers.forEach(d=>l.push({proc:d,action:"read"})),l.length>0&&(a[s]=l)}),{attributeMap:t,processorAttributes:r,attributeLineage:a}}function mi(e,n){const t=new Set;return(e||[]).forEach(r=>{const i=n[r.name];i&&i.creates.forEach(o=>t.add(o))}),[...t]}function hi(e,n){const t=new Set;return(e||[]).forEach(r=>{const i=n[r.name];i&&i.reads.forEach(o=>t.add(o))}),[...t]}const Qt=["source","route","transform","process","sink","utility"],gi={source:"#3B82F6",route:"#EAB308",transform:"#A855F7",process:"#6366F1",sink:"#21C354",utility:"#808495"},_i={source:"SOURCES",route:"ROUTING",transform:"TRANSFORMS",process:"PROCESSING",sink:"SINKS",utility:"UTILITY"};function yi(e){const n=[["source",e.sources],["route",e.routes],["transform",e.transforms],["process",e.processes],["sink",e.sinks],["utility",e.utilities]];return n.sort((t,r)=>r[1]!==t[1]?r[1]-t[1]:Qt.indexOf(t[0])-Qt.indexOf(r[0])),n[0][1]>0?n[0][0]:"process"}function bi(e,n){return n&&n._nifi?vi(n._nifi):{nodes:[],connections:[],tierLabels:{},tierColors:{},cycles:[]}}function vi(e,n){const t=[],r=[],i={},o=e.processors||[],a=e.connections||[];e.processGroups;const s={};o.forEach(A=>{const w=A.group||"(root)";s[w]||(s[w]={sources:0,sinks:0,routes:0,transforms:0,processes:0,utilities:0,total:0,processors:[],typeCount:{}});const R=Pe(A.type);s[w][R+"s"]=(s[w][R+"s"]||0)+1,s[w].total++,s[w].processors.push(A),s[w].typeCount[A.type]=(s[w].typeCount[A.type]||0)+1});const c=li(s),l=fi(o),d={};o.forEach(A=>{d[A.name]=go(A.type)});const u={},g={};Object.entries(s).forEach(([A,w])=>{const R=new Set,y=new Set;w.processors.forEach(O=>{const J=d[O.name];J&&(J.subcategory&&R.add(J.subcategory),J.action&&y.add(J.action))}),u[A]=[...R],g[A]=[...y]});const p={};o.forEach(A=>{p[A.name]=A.group||"(root)"});const m={},f={};a.forEach(A=>{const w=p[A.sourceName]||"(root)",R=p[A.destinationName]||"(root)";if(w!==R){const y=w+"|"+R;m[y]=(m[y]||0)+1}else f[w]=(f[w]||0)+1});const h=Object.keys(s),_={};Object.keys(m).forEach(A=>{const[w,R]=A.split("|");_[w]||(_[w]=new Set),_[w].add(R)}),h.forEach(A=>{_[A]||(_[A]=new Set)});const v=ho(_),k=new Set,E={};v.forEach((A,w)=>{A.forEach(R=>{k.add(R),E[R]=w})});const D={};h.forEach(A=>{D[A]=yi(s[A])});const P={};Qt.forEach(A=>{P[A]=[]}),h.forEach(A=>{P[D[A]].push(A)}),Qt.forEach(A=>{const w=A+"s";P[A].sort((R,y)=>{const O=(s[R][w]||0)/(s[R].total||1),J=(s[y][w]||0)/(s[y].total||1);return J!==O?J-O:s[y].total-s[R].total})});let $=0;Qt.forEach(A=>{const w=P[A];if(!w.length)return;$++;const R=gi[A],y=R.replace("#","").match(/.{2}/g).map(O=>parseInt(O,16));i[$]={label:_i[A],color:R,bg:`rgba(${y[0]},${y[1]},${y[2]},0.06)`,role:A},w.forEach(O=>{const J=s[O],M=Object.entries(J.typeCount).sort((V,ee)=>ee[1]-V[1]).slice(0,3).map(([V,ee])=>`${V}(${ee})`).join(", "),z=k.has(O),U=E[O],X=z?v[U]:[],F=z?Object.entries(m).filter(([V])=>{const[ee,se]=V.split("|");return v[U].includes(ee)&&v[U].includes(se)}).map(([V,ee])=>{const[se,te]=V.split("|");return{from:se,to:te,count:ee}}):[],N=di(c[O]),H=mi(J.processors,l.processorAttributes),Q=hi(J.processors,l.processorAttributes);t.push({id:"pg_"+O,name:O,tier:$,type:"process_group",dominantRole:D[O],subtype:D[O]+"s",procCount:J.total,srcCount:J.sources,sinkCount:J.sinks,routeCount:J.routes,transformCount:J.transforms,processCount:J.processes,utilityCount:J.utilities,intraConns:f[O]||0,topTypes:M,inCycle:z,sccMembers:X,cycleEdges:F,expandable:!0,stage:c[O],stageLabel:N?N.label:"",stageColor:N?N.color:"#6366F1",subcategories:u[O]||[],actionTypes:g[O]||[],attrCreates:H,attrReads:Q,detail:{processors:J.processors,typeCount:J.typeCount,intraConns:f[O]||0,procFullMeta:d}})})}),Object.entries(m).forEach(([A,w])=>{const[R,y]=A.split("|"),O=k.has(R)&&k.has(y)&&E[R]===E[y];r.push({from:"pg_"+R,to:"pg_"+y,label:w>1?w+" flows":"1 flow",type:"flow",color:O?"#EF4444":"#4B5563",width:Math.min(1+w*.3,4),inCycle:O})});const x=[],S={};o.forEach(A=>{S[A.type]=(S[A.type]||0)+1}),Object.entries(S).sort((A,w)=>w[1]-A[1]).forEach(([A,w])=>{const R=Pe(A);x.push({name:A,writers:w,readers:0,lookups:0,total:w,role:R})});const C=v.map((A,w)=>({id:w,groups:A,edgeCount:Object.keys(m).filter(R=>{const[y,O]=R.split("|");return A.includes(y)&&A.includes(O)}).length}));return{nodes:t,connections:r,tierLabels:i,diagramType:"nifi_flow",densityData:x,cycleData:C,stageData:c,attrFlowData:l}}function yo(e,n=250){let t=null,r=null,i=null;function o(...a){r=a,i=this,clearTimeout(t),t=setTimeout(()=>{t=null,e.apply(i,r),r=i=null},n)}return o.cancel=()=>{clearTimeout(t),t=null,r=i=null},o.flush=()=>{t!==null&&(clearTimeout(t),t=null,e.apply(i,r),r=i=null)},o}function ki(e,n,t,r,i){const o=new Set([e]),a=new Set;function s(d){t.forEach(u=>{const g=u.from+"|"+u.to;u.from===d&&!o.has(u.to)&&(a.add(g),o.add(u.to),s(u.to))})}function c(d){t.forEach(u=>{const g=u.from+"|"+u.to;u.to===d&&!o.has(u.from)&&(a.add(g),o.add(u.from),c(u.from))})}s(e),c(e),Object.entries(r).forEach(([d,u])=>{o.has(d)?(u.classList.add("highlighted"),u.classList.remove("dimmed"),d===e&&u.classList.add("selected")):(u.classList.add("dimmed"),u.classList.remove("highlighted","selected"))});const l=i.querySelector("svg.tier-svg");l&&l.querySelectorAll("path[data-from]").forEach(d=>{const u=d.dataset.from+"|"+d.dataset.to;a.has(u)?(d.setAttribute("opacity","1"),d.setAttribute("stroke","#FAFAFA"),d.setAttribute("stroke-width","3"),d.setAttribute("filter","url(#glow)"),d.setAttribute("marker-end","url(#arrow-white)"),d.style.transition="all 0.2s ease"):(d.setAttribute("opacity","0.08"),d.removeAttribute("filter"))})}const Si=yo(ki,30);function wi(e,n){Object.values(e).forEach(r=>{r.classList.remove("highlighted","dimmed","selected")});const t=n.querySelector("svg.tier-svg");t&&t.querySelectorAll("path[data-from]").forEach(r=>{r.setAttribute("opacity","0.35"),r.setAttribute("stroke",r.dataset.origColor||"#4B5563"),r.setAttribute("stroke-width",r.dataset.origWidth||"1.5"),r.removeAttribute("filter");const i=r.dataset.origColor||"",o=i.includes("EF44")?"arrow-red":i.includes("F59E")||i.includes("F5")?"arrow-amber":i.includes("6366")?"arrow-purple":i.includes("3B82")?"arrow-blue":i.includes("21C3")?"arrow-green":"arrow-default";r.setAttribute("marker-end",`url(#${o})`),r.style.transition=""})}function wt(e,n,t){const r={};e.forEach(s=>{r[s.from]||(r[s.from]=[]),r[s.from].push({to:s.to,key:s.from+"|"+s.to})});const i=new Set([n]),o={},a=[n];for(;a.length;){const s=a.shift();if(s===t){const c=[],l=[];let d=t;for(;d!==n;)c.unshift(d),l.unshift(o[d].key),d=o[d].from;return c.unshift(n),{pathNodes:c,pathEdgeKeys:l,found:!0}}for(const c of r[s]||[])i.has(c.to)||(i.add(c.to),o[c.to]={from:s,key:c.key},a.push(c.to))}return{pathNodes:[],pathEdgeKeys:[],found:!1}}let gn=null;function bo(e){let n=document.getElementById("pathTraceToast");n||(n=document.createElement("div"),n.id="pathTraceToast",n.className="path-trace-toast",document.body.appendChild(n));const t=e.selected.length,r=e.pathNodes.size,i=t===1?"1 node selected â€” click another to trace route":`${t} nodes selected â€” ${r} in path`;n.innerHTML=`<span>${i}</span><span class="toast-hint">Click nodes to build route</span><span class="toast-clear" id="pathTraceToastClear">âœ• Clear</span>`,n.style.display="flex";const o=document.getElementById("pathTraceToastClear");gn&&o&&o.removeEventListener("click",gn),gn=()=>{n.style.display="none"},o&&o.addEventListener("click",gn)}function Ei(){const e=document.getElementById("pathTraceToast");e&&(e.style.display="none")}function Zr(){const e=document.getElementById("pathTraceToast");if(e){const n=document.createElement("span");n.style.cssText="color:var(--red);margin-left:8px",n.textContent="No direct path",e.appendChild(n),setTimeout(()=>{n.parentNode&&n.remove()},2500)}}function xi(e,n){const t=new Set([e]),r=new Set,i=[e],o=new Set([e]);for(;i.length;){const c=i.shift();n.forEach(l=>{l.from===c&&!o.has(l.to)&&(o.add(l.to),t.add(l.to),r.add(l.from+"|"+l.to),i.push(l.to))})}const a=[e],s=new Set([e]);for(;a.length;){const c=a.shift();n.forEach(l=>{l.to===c&&!s.has(l.from)&&(s.add(l.from),t.add(l.from),r.add(l.from+"|"+l.to),a.push(l.from))})}return{reachNodes:t,reachEdges:r}}function vo(e,n,t){Object.entries(n).forEach(([i,o])=>{o.classList.remove("path-selected","path-member","path-dimmed","highlighted","dimmed","selected"),e.selected.includes(i)?o.classList.add("path-selected"):e.pathNodes.has(i)?o.classList.add("path-member"):o.classList.add("path-dimmed")});const r=t.querySelector("svg.tier-svg");r&&r.querySelectorAll("path[data-from]").forEach(i=>{const o=i.dataset.from+"|"+i.dataset.to,a=i.dataset.to+"|"+i.dataset.from;e.pathEdgeKeys.has(o)||e.pathEdgeKeys.has(a)?(i.setAttribute("opacity","1"),i.setAttribute("stroke","#FACA15"),i.setAttribute("stroke-width","3"),i.setAttribute("filter","url(#glow)"),i.setAttribute("marker-end","url(#arrow-white)"),i.style.transition="all 0.2s ease"):(i.setAttribute("opacity","0.04"),i.removeAttribute("filter"),i.style.transition="all 0.2s ease")})}function Ci(e,n,t,r,i){n.selected=[e],n.active=!0;const{reachNodes:o,reachEdges:a}=xi(e,t);n.pathNodes=o,n.pathEdgeKeys=a,vo(n,r,i),bo(n)}function $i(e,n,t,r,i){if(!n.selected.includes(e)){if(n.selected.push(e),n.selected.length===2){const o=n.selected[0],a=n.selected[1];let s=wt(t,o,a);if(s.found||(s=wt(t,a,o)),!s.found){const c=t.flatMap(l=>[l,{from:l.to,to:l.from,label:l.label,type:l.type,color:l.color,width:l.width}]);s=wt(c,o,a)}s.found?(n.pathNodes=new Set(s.pathNodes),n.pathEdgeKeys=new Set(s.pathEdgeKeys)):(n.pathNodes=new Set(n.selected),n.pathEdgeKeys=new Set,Zr())}else{const o=n.selected[n.selected.length-2];let a=wt(t,o,e);if(a.found||(a=wt(t,e,o)),!a.found){const s=t.flatMap(c=>[c,{from:c.to,to:c.from,label:c.label,type:c.type,color:c.color,width:c.width}]);a=wt(s,o,e)}a.found?(a.pathNodes.forEach(s=>n.pathNodes.add(s)),a.pathEdgeKeys.forEach(s=>n.pathEdgeKeys.add(s))):(n.pathNodes.add(e),Zr())}vo(n,r,i),bo(n)}}function Yn(e,n,t){e.selected=[],e.pathNodes=new Set,e.pathEdgeKeys=new Set,e.active=!1,Object.values(n).forEach(i=>{i.classList.remove("path-selected","path-member","path-dimmed")});const r=t.querySelector("svg.tier-svg");r&&r.querySelectorAll("path[data-from]").forEach(i=>{i.setAttribute("opacity","0.35"),i.setAttribute("stroke",i.dataset.origColor||"#4B5563"),i.setAttribute("stroke-width",i.dataset.origWidth||"1.5"),i.removeAttribute("filter");const o=i.dataset.origColor||"",a=o.includes("EF44")?"arrow-red":o.includes("F59E")||o.includes("F5")?"arrow-amber":o.includes("6366")?"arrow-purple":o.includes("3B82")?"arrow-blue":o.includes("21C3")?"arrow-green":"arrow-default";i.setAttribute("marker-end",`url(#${a})`),i.style.transition=""}),Ei()}function br(e,n,t){const r=e.querySelector("svg.tier-svg");if(r&&r.remove(),!n.length)return;const i=document.createElementNS("http://www.w3.org/2000/svg","svg");i.classList.add("tier-svg"),i.style.position="absolute",i.style.top="0",i.style.left="0",i.style.width=e.scrollWidth+"px",i.style.height=e.scrollHeight+"px",i.style.pointerEvents="none",i.style.zIndex="1",i.setAttribute("viewBox",`0 0 ${e.scrollWidth} ${e.scrollHeight}`);const o=document.createElementNS("http://www.w3.org/2000/svg","defs"),a=document.createElementNS("http://www.w3.org/2000/svg","filter");a.setAttribute("id","glow"),a.setAttribute("x","-50%"),a.setAttribute("y","-50%"),a.setAttribute("width","200%"),a.setAttribute("height","200%");const s=document.createElementNS("http://www.w3.org/2000/svg","feGaussianBlur");s.setAttribute("stdDeviation","3"),s.setAttribute("result","blur"),a.appendChild(s);const c=document.createElementNS("http://www.w3.org/2000/svg","feMerge"),l=document.createElementNS("http://www.w3.org/2000/svg","feMergeNode");l.setAttribute("in","blur");const d=document.createElementNS("http://www.w3.org/2000/svg","feMergeNode");d.setAttribute("in","SourceGraphic"),c.appendChild(l),c.appendChild(d),a.appendChild(c),o.appendChild(a),Object.entries({default:"#4B5563",blue:"#3B82F6",purple:"#6366F1",red:"#EF4444",amber:"#F59E0B",green:"#21C354",white:"#FAFAFA"}).forEach(([p,m])=>{const f=document.createElementNS("http://www.w3.org/2000/svg","marker");f.setAttribute("id","arrow-"+p),f.setAttribute("viewBox","0 0 10 8"),f.setAttribute("refX","10"),f.setAttribute("refY","4"),f.setAttribute("markerWidth","8"),f.setAttribute("markerHeight","6"),f.setAttribute("orient","auto");const h=document.createElementNS("http://www.w3.org/2000/svg","path");h.setAttribute("d","M0,0 L10,4 L0,8 Z"),h.setAttribute("fill",m),f.appendChild(h),o.appendChild(f)}),i.appendChild(o);const g=e.getBoundingClientRect();n.forEach(p=>{const m=t[p.from],f=t[p.to];if(!m||!f)return;const h=m.getBoundingClientRect(),_=f.getBoundingClientRect(),v=h.left+h.width/2-g.left+e.scrollLeft,k=h.top+h.height-g.top+e.scrollTop,E=_.left+_.width/2-g.left+e.scrollLeft,D=_.top-g.top+e.scrollTop,P=D-k,$=Math.max(Math.abs(P)*.35,30),x=(E-v)*.15,S=document.createElementNS("http://www.w3.org/2000/svg","path");S.setAttribute("d",`M${v},${k} C${v+x},${k+$} ${E-x},${D-$} ${E},${D}`);const C=p.color||"#4B5563";S.setAttribute("stroke",C),S.setAttribute("stroke-width",String(p.width||1.5)),S.setAttribute("fill","none");const A=C.includes("EF44")?"arrow-red":C.includes("F59E")||C.includes("F5")?"arrow-amber":C.includes("6366")?"arrow-purple":C.includes("3B82")?"arrow-blue":C.includes("21C3")?"arrow-green":"arrow-default";S.setAttribute("marker-end",`url(#${A})`),S.setAttribute("opacity","0.35"),S.dataset.from=p.from,S.dataset.to=p.to,S.dataset.origColor=C,S.dataset.origWidth=String(p.width||1.5),p.dash&&S.setAttribute("stroke-dasharray","6,4"),p.inCycle&&S.setAttribute("stroke-dasharray","8,4"),i.appendChild(S)}),e.style.position="relative",e.insertBefore(i,e.firstChild)}const Xr={source:"#3B82F6",route:"#EAB308",transform:"#A855F7",process:"#6366F1",sink:"#21C354",utility:"#808495"};function Yr(e,n,t,r,i,o,a,s){const c="sub_"+e.id,l=o.querySelector(`[data-sub-band="${c}"]`);if(l){l.remove(),n.classList.remove("expanded");const f=n.querySelector(".expand-indicator");f&&(f.textContent="â–¶ expand"),Object.keys(i).forEach(h=>{h.startsWith("proc_"+e.name+"|")&&delete i[h]}),requestAnimationFrame(()=>br(o,r.connections,i));return}n.classList.add("expanded");const d=n.querySelector(".expand-indicator");d&&(d.textContent="â–¼ collapse");const u=e.detail&&e.detail.processors||[],g=["source","route","transform","process","sink","utility"],p={source:"Sources",route:"Routing",transform:"Transforms",process:"Processing",sink:"Sinks",utility:"Utility"},m=document.createElement("div");m.className="tier-sub-band",m.dataset.subBand=c,g.forEach(f=>{const h=u.filter(k=>Pe(k.type)===f);if(!h.length)return;const _=document.createElement("div");_.className="tier-band-label",_.style.color=Xr[f]||"#808495",_.textContent=`${e.name} â†’ ${p[f]} (${h.length})`,m.appendChild(_);const v=document.createElement("div");v.className="tier-nodes",h.forEach(k=>{const E=document.createElement("div");E.className="tier-node";const D="proc_"+e.name+"|"+k.name;E.dataset.nodeId=D,(k.state==="DISABLED"||k.state==="STOPPED")&&(E.style.opacity="0.5"),E.style.borderTopColor=Xr[f]||"#808495",E.style.borderTopWidth="3px";const P=document.createElement("div");P.className="node-name",P.textContent=k.name.length>20?k.name.substring(0,17)+"...":k.name,P.title=k.name,E.appendChild(P);const $=document.createElement("div");$.className="node-meta",$.textContent=k.type,E.appendChild($),E.addEventListener("click",x=>{x.stopPropagation(),kt(()=>Promise.resolve().then(()=>Pi),void 0,import.meta.url).then(S=>{S.showNodeDetail({name:k.name,type:"processor",subtype:f,meta:k.type,group:e.name,state:k.state,propCount:Object.keys(k.properties||{}).length,detail:k},a,s)})}),v.appendChild(E),i[D]=E}),m.appendChild(v)}),t.after(m),requestAnimationFrame(()=>br(o,r.connections,i))}function In(e,n,t){if(!n)return;let r='<div class="node-detail">';if(r+=`<h4>${B(e.name)}</h4>`,t==="nifi_flow"&&e.type==="process_group"&&e.detail){const i=e.detail;if(e.stageLabel&&(r+=`<div style="margin-bottom:6px"><span class="ns" style="background:${B(e.stageColor||"#6366F1")};color:white;font-size:0.7rem">${B(e.stageLabel)}</span></div>`),r+=`<p><strong>Processors:</strong> ${e.procCount} &middot; <strong>Internal Connections:</strong> ${i.intraConns||0}</p>`,r+='<div style="display:flex;gap:6px;flex-wrap:wrap;margin:8px 0">',e.srcCount&&(r+=`<span class="ns ns-tx">${e.srcCount} sources</span>`),e.routeCount&&(r+=`<span class="ns" style="background:#EAB308;color:#000">${e.routeCount} routes</span>`),e.transformCount&&(r+=`<span class="ns" style="background:#A855F7;color:white">${e.transformCount} transforms</span>`),e.processCount&&(r+=`<span class="ns ns-ext">${e.processCount} processors</span>`),e.sinkCount&&(r+=`<span class="ns" style="background:#21C354;color:white">${e.sinkCount} sinks</span>`),e.utilityCount&&(r+=`<span class="ns" style="background:#808495;color:white">${e.utilityCount} utility</span>`),r+="</div>",e.subcategories&&e.subcategories.length&&(r+='<div style="display:flex;gap:4px;flex-wrap:wrap;margin:4px 0">',e.subcategories.forEach(o=>{r+=`<span class="ns ns-sub" style="font-size:0.65rem">${B(o)}</span>`}),r+="</div>"),e.actionTypes&&e.actionTypes.length&&(r+='<p style="margin:4px 0;font-size:0.75rem"><strong>Actions:</strong> '+e.actionTypes.map(B).join(", ")+"</p>"),(e.attrCreates&&e.attrCreates.length||e.attrReads&&e.attrReads.length)&&(r+='<div style="margin:8px 0;padding:8px;border:1px solid var(--border);border-radius:6px;font-size:0.75rem">',r+="<strong>Attribute Flow</strong>",e.attrCreates&&e.attrCreates.length&&(r+=`<div style="margin-top:4px"><span style="color:#21C354">â†‘ Creates (${e.attrCreates.length}):</span> `,r+=e.attrCreates.slice(0,8).map(o=>`<code class="attr-chip attr-chip-create" data-attr-name="${B(o)}">${B(o)}</code>`).join(" "),e.attrCreates.length>8&&(r+=` <em>+${e.attrCreates.length-8} more</em>`),r+="</div>"),e.attrReads&&e.attrReads.length&&(r+=`<div style="margin-top:4px"><span style="color:#3B82F6">â†“ Reads (${e.attrReads.length}):</span> `,r+=e.attrReads.slice(0,8).map(o=>`<code class="attr-chip attr-chip-read" data-attr-name="${B(o)}">${B(o)}</code>`).join(" "),e.attrReads.length>8&&(r+=` <em>+${e.attrReads.length-8} more</em>`),r+="</div>"),r+="</div>"),i.typeCount){const o=Object.entries(i.typeCount).sort((a,s)=>s[1]-a[1]);r+='<table style="font-size:0.75rem"><thead><tr><th>Processor Type</th><th>Count</th></tr></thead><tbody>',o.slice(0,15).forEach(([a,s])=>{r+=`<tr><td>${B(a)}</td><td>${s}</td></tr>`}),o.length>15&&(r+=`<tr><td colspan="2" style="color:var(--text2)">+${o.length-15} more types</td></tr>`),r+="</tbody></table>"}i.processors&&i.processors.length&&(r+='<p style="margin-top:8px"><strong>Processors (first 10):</strong></p>',r+='<ul style="font-size:0.75rem;margin:4px 0 4px 16px">',i.processors.slice(0,10).forEach(o=>{r+=`<li>${B(o.name)} <code style="font-size:0.65rem">${B(o.type)}</code></li>`}),i.processors.length>10&&(r+=`<li style="color:var(--text2)">+${i.processors.length-10} more</li>`),r+="</ul>"),e.inCycle&&e.sccMembers&&(r+='<div style="margin:8px 0;padding:8px 12px;border:1px solid #EF4444;border-radius:6px;background:rgba(239,68,68,0.08);font-size:0.8rem">',r+='<strong style="color:#EF4444">Circular Dependency Detected</strong><br>',r+="Cycle with: "+e.sccMembers.filter(o=>o!==e.name).map(B).join(", "),e.cycleEdges&&e.cycleEdges.length&&(r+="<br><br><strong>Cycle edges:</strong><br>",e.cycleEdges.forEach(o=>{r+=`${B(o.from)} â†’ ${B(o.to)} (${o.count} flow${o.count>1?"s":""})<br>`})),r+="</div>")}else if(t==="nifi_flow"&&e.detail){const i=e.detail;r+=`<p><strong>Type:</strong> ${B(i.type)} <code style="font-size:0.7rem">${B(i.fullType||"")}</code></p>`,r+=`<p><strong>Group:</strong> ${B(i.group||"(root)")}</p>`,r+=`<p><strong>State:</strong> ${B(i.state||"N/A")}</p>`,i.schedulingStrategy&&(r+=`<p><strong>Scheduling:</strong> ${B(i.schedulingStrategy)} / ${B(i.schedulingPeriod)}</p>`);const o=Object.keys(i.properties||{}),a=o.filter(s=>mo(s)).length;o.length&&(r+=`<p><strong>Properties (${o.length}):</strong>${a?` <span style="color:#EAB308;font-size:0.75rem">&#x26A0; ${a} sensitive masked</span>`:""}</p><pre style="max-height:200px;overflow:auto;font-size:0.75rem">`,o.slice(0,20).forEach(s=>{r+=`${B(s)}: ${B(ai(s,(i.properties[s]||"").substring(0,100)))}
`}),o.length>20&&(r+=`... +${o.length-20} more
`),r+="</pre>")}else if(t==="dependency_graph"&&e.type==="session"&&e.detail){const i=e.detail;r+=`<p><strong>Sources:</strong> ${i.sources} &middot; <strong>Targets:</strong> ${i.targets} &middot; <strong>Lookups:</strong> ${i.lookups}</p>`,e.seq&&(r+=`<p><strong>Execution Order:</strong> #${e.seq}</p>`),i.source_tables&&i.source_tables.length&&(r+='<p style="margin-top:6px"><strong>Source Tables:</strong></p><ul style="font-size:0.8rem;margin:4px 0 4px 16px">',i.source_tables.slice(0,10).forEach(o=>{r+=`<li>${B(o.name)}</li>`}),i.source_tables.length>10&&(r+=`<li style="color:var(--text2)">+${i.source_tables.length-10} more</li>`),r+="</ul>"),i.target_tables&&i.target_tables.length&&(r+='<p><strong>Target Tables:</strong></p><ul style="font-size:0.8rem;margin:4px 0 4px 16px">',i.target_tables.forEach(o=>{r+=`<li>${B(o.name)}${o.load_type?' <code style="font-size:0.7rem">'+B(o.load_type)+"</code>":""}</li>`}),r+="</ul>"),e.hasConflict&&e.conflictDetails&&e.conflictDetails.length&&(r+='<div class="alert alert-warn" style="margin:8px 0;padding:8px 12px;font-size:0.8rem"><strong>Conflicts:</strong><br>',e.conflictDetails.forEach(o=>{r+=`${B(o.table_name)} &mdash; ${B(o.conflict_type)}<br>`}),r+="</div>")}else if(t==="dependency_graph"&&(e.type==="table_output"||e.type==="conflict_gate")&&e.detail){const i=e.detail;i.writers&&i.writers.length&&(r+=`<p><strong>Writers:</strong> ${i.writers.map(B).join(", ")}</p>`),i.readers&&i.readers.length&&(r+=`<p><strong>Readers:</strong> ${i.readers.map(B).join(", ")}</p>`),i.lookups&&i.lookups.length&&(r+=`<p><strong>Lookup Readers:</strong> ${i.lookups.map(B).join(", ")}</p>`),i.conflicts&&i.conflicts.length&&(r+='<div class="alert alert-warn" style="margin:8px 0;padding:8px 12px;font-size:0.8rem"><strong>Conflicts:</strong><br>',i.conflicts.forEach(o=>{r+=`${B(o.conflict_type)}${o.writers?" &mdash; Writers: "+o.writers.map(B).join(", "):""}<br>`}),r+="</div>")}else if(e.detail&&e.detail.columns){const i=e.detail;r+=`<p><strong>Schema:</strong> ${B(i.schema||"dbo")} &middot; <strong>Rows:</strong> ${i.row_count}</p>`,r+='<table style="font-size:0.75rem"><thead><tr><th>Column</th><th>Type</th><th>PK</th><th>Null</th></tr></thead><tbody>',i.columns.slice(0,15).forEach(o=>{r+=`<tr><td>${B(o.name)}</td><td>${B(o.data_type)}</td><td>${o.is_primary_key?"Y":""}</td><td>${o.nullable?"Y":"N"}</td></tr>`}),i.columns.length>15&&(r+=`<tr><td colspan="4" style="color:var(--text2)">+${i.columns.length-15} more columns</td></tr>`),r+="</tbody></table>",i.foreign_keys&&i.foreign_keys.length&&(r+='<p style="margin-top:8px"><strong>Foreign Keys:</strong></p>',i.foreign_keys.forEach(o=>{r+=`<p style="font-size:0.8rem"><code>${B(o.column||o.fk_column)}</code> &rarr; <code>${B(o.references_table)}(${B(o.references_column)})</code></p>`}))}r+="</div>",n.innerHTML=r}const Pi=Object.freeze(Object.defineProperty({__proto__:null,showNodeDetail:In},Symbol.toStringTag,{value:"Module"})),Ri={source:"#3B82F6",route:"#EAB308",transform:"#A855F7",process:"#6366F1",sink:"#21C354",utility:"#808495"};function Di(e,n,t,r,i,o){const{nodes:a,connections:s,tierLabels:c}=n,l={};a.forEach(u=>{l[u.tier]||(l[u.tier]=[]),l[u.tier].push(u)});const d=Object.keys(l).map(Number).sort((u,g)=>u-g);return d.forEach(u=>{const g=c[u]||{label:`TIER ${u}`,color:"#808495",bg:"rgba(128,132,149,0.06)"},p=document.createElement("div");p.className="tier-band",p.style.background=g.bg,p.style.borderLeft=`3px solid ${g.color}`;const m=document.createElement("div");m.className="tier-band-label",m.style.color=g.color,m.textContent=g.label,p.appendChild(m);const f=document.createElement("div");f.className="tier-nodes",l[u].forEach(h=>{const _=document.createElement("div");if(_.dataset.nodeId=h.id,_.dataset.role=h.subtype||h.dominantRole||"",_.dataset.conf=String(h.conf||0),_.dataset.name=h.name||"",_.dataset.type=h.procType||h.type||"",h.type==="session"){if(_.className="tier-node",h.hasConflict?(_.style.borderColor="#EF4444",_.style.borderTopColor="#EF4444"):h.subtype==="root"?_.style.borderTopColor="#3B82F6":_.style.borderTopColor="#6366F1",_.style.borderTopWidth="3px",h.seq){const D=document.createElement("div");D.className="node-seq",D.textContent=h.seq,h.hasConflict&&(D.style.background="#EF4444"),_.appendChild(D)}const v=document.createElement("div");v.className="node-name";const k=(h.name||"").replace(/^s_m_(?:Load_|LOAD_)?/i,"");v.textContent=k.length>22?k.substring(0,19)+"...":k,v.title=h.name||"",_.appendChild(v);const E=document.createElement("div");if(E.className="node-stats",E.innerHTML=`<span class="ns ns-tx">${h.srcCount||0} tx</span><span class="ns ns-ext">${h.tgtCount||0} ext</span>`+(h.lkpCount?`<span class="ns ns-lkp">${h.lkpCount} lkp</span>`:""),_.appendChild(E),h.hasConflict){const D=document.createElement("div");D.className="node-badge red",D.textContent="!",D.title=(h.conflictDetails||[]).map(P=>(P.table_name||"")+": "+(P.conflict_type||"")).join(", "),_.appendChild(D)}}else if(h.type==="table_output"){_.className="tier-node table-output",h.isConflict?_.style.borderColor="#EF4444":h.isChain?_.style.borderColor="#F59E0B":_.style.borderColor="#21C354";const v=document.createElement("div");v.style.cssText="font-size:0.8rem;margin-bottom:2px",v.textContent=h.isConflict?"âš ":h.isChain?"â…¡":"âœ“",_.appendChild(v);const k=document.createElement("div");k.className="node-name",k.textContent=(h.name||"").length>20?(h.name||"").substring(0,17)+"...":h.name||"",k.title=h.name||"",_.appendChild(k);const E=document.createElement("div");E.className="node-class",E.style.color=h.isConflict?"#FCA5A5":h.isChain?"#FDE68A":"#86EFAC",E.textContent=h.isConflict?"CONFLICT":h.isChain?"CHAIN":"INDEPENDENT",_.appendChild(E)}else if(h.type==="conflict_gate"){_.className="tier-node conflict-gate";const v=document.createElement("div");v.style.cssText="font-size:1.2rem;margin-bottom:2px",v.textContent="âš ",_.appendChild(v);const k=document.createElement("div");k.className="node-name",k.textContent=(h.name||"").length>25?(h.name||"").substring(0,22)+"...":h.name||"",k.title=h.name||"",_.appendChild(k);const E=document.createElement("div");E.className="node-meta",E.textContent=`${h.writerCount||0}W / ${h.readerCount||0}R / ${h.lookupCount||0}L`,_.appendChild(E);const D=document.createElement("div");D.className="node-class",D.style.color="#FCA5A5",D.textContent="CONFLICT",_.appendChild(D)}else if(h.type==="process_group"){_.className="tier-node expandable",h.inCycle&&_.classList.add("in-cycle");const v=Ri[h.dominantRole]||"#6366F1";if(_.style.borderTopColor=v,_.style.borderTopWidth="3px",_.style.minWidth="160px",_.style.maxWidth="260px",h.actionTypes&&h.actionTypes.length&&(_.dataset.actions=h.actionTypes.join(",")),h.stage&&(_.dataset.stage=h.stage),h.stageLabel){const x=document.createElement("div");x.className="node-stage-label",x.style.color=h.stageColor||"#6366F1",x.style.borderColor=h.stageColor||"#6366F1",x.textContent=h.stageLabel,_.appendChild(x)}if(h.inCycle){const x=document.createElement("div");x.className="cycle-badge",x.textContent="â†»",x.title="Cycle: "+(h.sccMembers||[]).filter(S=>S!==h.name).join(", "),_.appendChild(x)}const k=document.createElement("div");k.className="node-name",k.textContent=(h.name||"").length>28?(h.name||"").substring(0,25)+"...":h.name||"",k.title=h.name||"",_.appendChild(k);const E=document.createElement("div");if(E.className="node-badge",E.textContent=h.procCount||0,E.title=(h.procCount||0)+" processors",_.appendChild(E),h.subcategories&&h.subcategories.length){const x=document.createElement("div");if(x.className="node-subcategories",h.subcategories.slice(0,3).forEach(S=>{const C=document.createElement("span");C.className="ns ns-sub",C.textContent=S,x.appendChild(C)}),h.subcategories.length>3){const S=document.createElement("span");S.className="ns ns-sub-more",S.textContent="+"+(h.subcategories.length-3),x.appendChild(S)}_.appendChild(x)}const D=document.createElement("div");D.className="node-stats";let P="";if(h.srcCount&&(P+=`<span class="ns ns-tx">${h.srcCount} src</span>`),(h.transformCount||0)+(h.routeCount||0)&&(P+=`<span class="ns" style="background:#A855F7;color:white">${(h.transformCount||0)+(h.routeCount||0)} xfm</span>`),h.processCount&&(P+=`<span class="ns ns-ext">${h.processCount} proc</span>`),h.sinkCount&&(P+=`<span class="ns" style="background:#21C354;color:white">${h.sinkCount} sink</span>`),h.utilityCount&&(P+=`<span class="ns" style="background:#808495;color:white">${h.utilityCount} util</span>`),D.innerHTML=P,_.appendChild(D),h.attrCreates&&h.attrCreates.length||h.attrReads&&h.attrReads.length){const x=document.createElement("div");if(x.className="node-attr-indicators",h.attrCreates.length){const S=document.createElement("span");S.className="ns ns-attr-create",S.textContent=h.attrCreates.length+" attrâ†‘",S.title="Creates: "+h.attrCreates.slice(0,5).join(", ")+(h.attrCreates.length>5?"...":""),x.appendChild(S)}if(h.attrReads.length){const S=document.createElement("span");S.className="ns ns-attr-read",S.textContent=h.attrReads.length+" attrâ†“",S.title="Reads: "+h.attrReads.slice(0,5).join(", ")+(h.attrReads.length>5?"...":""),x.appendChild(S)}_.appendChild(x)}const $=document.createElement("div");$.className="expand-indicator",$.textContent="â–¶ expand",_.appendChild($)}else{_.className="tier-node",(h.state==="DISABLED"||h.state==="STOPPED")&&(_.style.opacity="0.5"),h.subtype==="source"?_.style.borderTopColor="#3B82F6":h.subtype==="sink"?_.style.borderTopColor="#21C354":h.subtype==="route"?_.style.borderTopColor="#EAB308":h.subtype==="transform"?_.style.borderTopColor="#A855F7":h.type==="table"&&(_.style.borderTopColor="#3B82F6"),_.style.borderTopWidth="3px";const v=document.createElement("div");if(v.className="node-name",v.textContent=(h.name||"").length>25?(h.name||"").substring(0,22)+"...":h.name||"",v.title=h.name||"",_.appendChild(v),h.meta){const k=document.createElement("div");k.className="node-meta",k.textContent=h.meta,_.appendChild(k)}if(h.rows){const k=document.createElement("div");k.className="node-badge",k.textContent=h.rows>=1e3?Math.round(h.rows/1e3)+"K":h.rows,_.appendChild(k)}}_.addEventListener("mouseenter",()=>{r.active||Si(h.id,a,s,t,e)}),_.addEventListener("mouseleave",()=>{r.active||wi(t,e)}),_.addEventListener("click",v=>{v.stopPropagation(),r.active&&!r.selected.includes(h.id)?($i(h.id,r,s,t,e),In(h,i,o)):r.active&&r.selected.includes(h.id)?(h.type==="process_group"&&h.expandable&&Yr(h,_,p,n,t,e,i,o),In(h,i,o)):(Ci(h.id,r,s,t,e),h.type==="process_group"&&h.expandable&&Yr(h,_,p,n,t,e,i,o),In(h,i,o))}),f.appendChild(_),t[h.id]=_}),p.appendChild(f),e.appendChild(p)}),{sortedTiers:d}}function Ti(e,n,t,r,i,o,a){const s=document.getElementById("sidebarClearBtn");if(!e.activeTypes.size){ko(i,o),a.querySelectorAll(".density-row").forEach(u=>u.classList.remove("filter-dimmed")),s&&(s.style.display="none");return}s&&(s.style.display="block");const c=new Set;e.activeTypes.forEach(u=>{n[u]&&n[u].forEach(g=>c.add(g))});const l=new Set;r.forEach(u=>{c.has(u.from)&&c.has(u.to)&&l.add(u.from+"|"+u.to)}),Object.entries(i).forEach(([u,g])=>{g.classList.remove("path-selected","path-member","path-dimmed","highlighted","dimmed","selected"),c.has(u)?g.classList.add("path-member"):g.classList.add("path-dimmed")});const d=o.querySelector("svg.tier-svg");d&&d.querySelectorAll("path[data-from]").forEach(u=>{const g=u.dataset.from+"|"+u.dataset.to;l.has(g)?(u.setAttribute("opacity","0.8"),u.setAttribute("stroke",u.dataset.origColor||"#4B5563"),u.setAttribute("stroke-width",u.dataset.origWidth||"1.5")):u.setAttribute("opacity","0.04")}),a.querySelectorAll(".density-row").forEach(u=>{e.activeTypes.has(u.dataset.typeName)?u.classList.remove("filter-dimmed"):u.classList.add("filter-dimmed")})}function ko(e,n){Object.values(e).forEach(r=>{r.classList.remove("path-selected","path-member","path-dimmed")});const t=n.querySelector("svg.tier-svg");t&&t.querySelectorAll("path[data-from]").forEach(r=>{r.setAttribute("opacity","0.35"),r.setAttribute("stroke",r.dataset.origColor||"#4B5563"),r.setAttribute("stroke-width",r.dataset.origWidth||"1.5"),r.removeAttribute("filter");const i=r.dataset.origColor||"",o=i.includes("EF44")?"arrow-red":i.includes("F59E")||i.includes("F5")?"arrow-amber":i.includes("6366")?"arrow-purple":i.includes("3B82")?"arrow-blue":i.includes("21C3")?"arrow-green":"arrow-default";r.setAttribute("marker-end",`url(#${o})`),r.style.transition=""})}const Se=Object.freeze({MAPPED:.7,EXACT:.85,PARTIAL:.4});let Le={role:"all",conf:"all",search:"",action:"all"};function So(e,n,t){Le[n]=t;const r=e.parentElement;r&&r.querySelectorAll("[data-node-id]").forEach(i=>{const o=i.dataset.role||"",a=parseFloat(i.dataset.conf||0),s=(i.dataset.name||"").toLowerCase(),c=(i.dataset.type||"").toLowerCase(),l=(i.dataset.actions||"").toLowerCase();let d=!0;Le.role!=="all"&&o!==Le.role&&(d=!1),Le.conf==="high"&&a<Se.MAPPED&&(d=!1),Le.conf==="med"&&(a<Se.PARTIAL||a>=Se.MAPPED)&&(d=!1),Le.conf==="low"&&a>=Se.PARTIAL&&(d=!1),Le.search&&!s.includes(Le.search.toLowerCase())&&!c.includes(Le.search.toLowerCase())&&(d=!1),Le.action!=="all"&&!l.includes(Le.action)&&(d=!1),i.style.opacity=d?"1":"0.15",i.style.pointerEvents=d?"":"none"})}const Ai=yo((e,n)=>{So(e,"search",n)},150);function _n(e,n,t){n==="search"?Ai(e,t):So(e,n,t)}function Ii(){Le={role:"all",conf:"all",search:"",action:"all"}}const Fi=[{value:"all",label:"All Actions"},{value:"read",label:"Read"},{value:"write",label:"Write"},{value:"transform",label:"Transform"},{value:"filter",label:"Filter"},{value:"enrich",label:"Enrich"},{value:"validate",label:"Validate"},{value:"monitor",label:"Monitor"},{value:"extract",label:"Extract"}];let yn=null;function Li(e,n,t,r){Ii();const i=document.getElementById(n),o=document.getElementById(t),a=document.getElementById(r);if(!i)return;i.innerHTML="",i.style.minHeight="200px";const{nodes:s,connections:c,tierLabels:l,diagramType:d,densityData:u}=e;if(!s.length){i.innerHTML='<p style="text-align:center;padding:20px;color:var(--text2)">No nodes to display</p>';return}const g=document.createElement("div");g.className="filter-toolbar";let p='<div class="filter-group"><label>Role:</label>';["all","source","transform","route","process","sink","utility"].forEach(P=>{const $={all:"",source:"#3B82F6",transform:"#A855F7",route:"#EAB308",process:"#6366F1",sink:"#21C354",utility:"#808495"},x=$[P]?' style="border-color:'+$[P]+";color:"+$[P]+'"':"";p+='<button class="filter-btn'+(P==="all"?" active":"")+'"'+x+' data-filter-role="'+P+'">'+(P==="all"?"All":P.charAt(0).toUpperCase()+P.slice(1))+"</button>"}),p+='</div><div class="filter-group"><label>Confidence:</label>',[{k:"all",l:"All",s:""},{k:"high",l:"High",s:"border-color:var(--green);color:var(--green)"},{k:"med",l:"Med",s:"border-color:var(--amber);color:var(--amber)"},{k:"low",l:"Low",s:"border-color:var(--red);color:var(--red)"}].forEach(P=>{p+='<button class="filter-btn'+(P.k==="all"?" active":"")+'"'+(P.s?' style="'+P.s+'"':"")+' data-filter-conf="'+P.k+'">'+P.l+"</button>"}),p+="</div>",d==="nifi_flow"&&(p+='<div class="filter-group"><label>Action:</label><select class="filter-action-select">',Fi.forEach(P=>{p+=`<option value="${P.value}">${P.label}</option>`}),p+="</select></div>"),p+='<div class="filter-group"><input class="filter-search" type="text" placeholder="Search processors..."></div>',g.innerHTML=p,g.querySelectorAll("[data-filter-role]").forEach(P=>{P.addEventListener("click",()=>{P.parentElement.querySelectorAll(".filter-btn").forEach($=>$.classList.remove("active")),P.classList.add("active"),_n(g,"role",P.dataset.filterRole)})}),g.querySelectorAll("[data-filter-conf]").forEach(P=>{P.addEventListener("click",()=>{P.parentElement.querySelectorAll(".filter-btn").forEach($=>$.classList.remove("active")),P.classList.add("active"),_n(g,"conf",P.dataset.filterConf)})});const m=g.querySelector(".filter-action-select");m&&m.addEventListener("change",()=>{_n(g,"action",m.value)});const f=g.querySelector(".filter-search");f&&f.addEventListener("input",()=>{_n(g,"search",f.value)}),i.appendChild(g);const h={selected:[],pathNodes:new Set,pathEdgeKeys:new Set,active:!1},_={};Di(i,e,_,h,o,d),requestAnimationFrame(()=>{br(i,c,_)}),yn&&document.removeEventListener("keydown",yn),yn=P=>{P.key==="Escape"&&h.active&&Yn(h,_,i)},document.addEventListener("keydown",yn),i.addEventListener("click",P=>{(P.target===i||P.target.classList.contains("tier-band")||P.target.classList.contains("tier-band-label"))&&h.active&&Yn(h,_,i)});const v=document.getElementById("tierDensitySidebar"),k=document.getElementById("densityBars"),E={activeTypes:new Set};if(v&&k&&u&&u.length){v.classList.remove("hidden");const P=v.querySelector("h4");if(P&&(P.textContent=d==="nifi_flow"?"Filter by Type":"Connection Density"),k.innerHTML="",d==="nifi_flow"){const S=document.createElement("div");S.className="sidebar-filter-hint",S.textContent="Click to filter diagram",k.appendChild(S)}const $=Math.max(...u.map(S=>S.total)),x={};if(d==="nifi_flow"&&s.forEach(S=>{S.type==="process_group"&&S.detail&&S.detail.typeCount&&Object.keys(S.detail.typeCount).forEach(C=>{x[C]||(x[C]=new Set),x[C].add(S.id)})}),u.forEach(S=>{const C=document.createElement("div");if(C.className="density-row",C.dataset.typeName=S.name,S.role){const A={source:"#3B82F6",sink:"#21C354",route:"#EAB308",transform:"#A855F7",process:"#6366F1",utility:"#808495"},w=Math.max(4,S.total/$*80);C.innerHTML=`<span class="density-bar" style="width:${w}px;background:${A[S.role]||"#808495"}" title="${S.total}x"></span><span class="density-label" title="${B(S.name)}">${B(S.name)} (${S.total})</span>`}else{const A=Math.max(2,S.writers/$*60),w=Math.max(0,S.readers/$*60),R=Math.max(0,S.lookups/$*60);let y=`<span class="density-bar" style="width:${A}px;background:#EF4444" title="${S.writers} writer(s)"></span>`;S.readers&&(y+=`<span class="density-bar" style="width:${w}px;background:#3B82F6" title="${S.readers} reader(s)"></span>`),S.lookups&&(y+=`<span class="density-bar" style="width:${R}px;background:#F59E0B" title="${S.lookups} lookup(s)"></span>`),C.innerHTML=y+`<span class="density-label" title="${B(S.name)}">${B(S.name)}</span>`}d==="nifi_flow"&&C.addEventListener("click",()=>{h.active&&Yn(h,_,i);const A=S.name;E.activeTypes.has(A)?(E.activeTypes.delete(A),C.classList.remove("filter-active")):(E.activeTypes.add(A),C.classList.add("filter-active")),Ti(E,x,s,c,_,i,k)}),k.appendChild(C)}),d==="nifi_flow"){const S=document.createElement("div");S.className="sidebar-clear-btn",S.id="sidebarClearBtn",S.textContent="âœ• Clear filter",S.addEventListener("click",()=>{E.activeTypes.clear(),k.querySelectorAll(".density-row").forEach(C=>C.classList.remove("filter-active","filter-dimmed")),ko(_,i),S.style.display="none"}),k.appendChild(S)}}else v&&v.classList.add("hidden");if(a){a.innerHTML="";const P=s.filter(x=>x.type==="session").length,$=s.filter(x=>x.type==="table_output"||x.type==="conflict_gate").length;if(d==="dependency_graph")a.innerHTML=[`<span>${P} Sessions</span>`,`<span>${$} Tables</span>`,`<span style="color:#EF4444">${s.filter(x=>x.hasConflict||x.type==="conflict_gate").length} Conflicts</span>`,'<span><span class="leg-line" style="background:#6366F1"></span> Dependency</span>','<span><span class="leg-line" style="background:#F59E0B;border-top:2px dashed #F59E0B"></span> Lookup</span>','<span><span class="leg-line" style="background:#EF4444"></span> Conflict</span>','<span><span class="leg-line" style="background:#21C354"></span> Independent</span>','<span><span class="leg-line" style="background:#F59E0B"></span> Chain</span>'].join("");else if(d==="nifi_flow"){const x=s.filter(A=>A.type==="process_group").length,S=s.filter(A=>A.type==="processor").length,C=e.cycleData?e.cycleData.length:0;a.innerHTML=[`<span>${x} Process Groups</span>`,S?`<span>${S} Processors</span>`:"",`<span>${c.length} Connections</span>`,C?`<span style="color:#EF4444">${C} Cycle(s)</span>`:"",'<span><span class="leg-line" style="background:#3B82F6"></span> Source</span>','<span><span class="leg-line" style="background:#EAB308"></span> Route</span>','<span><span class="leg-line" style="background:#A855F7"></span> Transform</span>','<span><span class="leg-line" style="background:#6366F1"></span> Process</span>','<span><span class="leg-line" style="background:#21C354"></span> Sink</span>','<span><span class="leg-line" style="background:#EF4444;border-top:2px dashed #EF4444"></span> Cycle Edge</span>','<span><span class="leg-line" style="background:#06B6D4"></span> Attr Create</span>','<span><span class="leg-line" style="background:#F59E0B"></span> Attr Read</span>','<span style="color:var(--text2);font-size:0.7rem">Click nodes to trace route &middot; Click attribute to trace flow &middot; Esc to clear</span>'].join("")}else d==="sql_tables"?a.innerHTML=['<span><span class="leg-line" style="background:#3B82F6;border-top:2px solid #3B82F6"></span> Foreign Key</span>',`<span>${s.length} tables &middot; ${c.length} relationships</span>`].join(""):a.innerHTML=[`<span>${s.length} objects</span>`,c.length?'<span><span class="leg-line" style="background:#4B5563;border-top:2px dashed #4B5563"></span> Shared columns</span>':""].join("")}const D=document.getElementById("tierDiagramContainer");D&&D.classList.remove("hidden")}function pe(e,n){const t=e.querySelector(":scope > "+n);return t?t.textContent.trim():""}function es(e){const n={};return e.querySelectorAll("config > properties > entry").forEach(t=>{const r=t.querySelector(":scope > key")?.textContent||"",i=t.querySelector(":scope > value");r&&i&&(n[r]=i.textContent||"")}),Object.keys(n).length||e.querySelectorAll(":scope > properties > entry").forEach(t=>{const r=t.querySelector(":scope > key")?.textContent||"",i=t.querySelector(":scope > value");r&&i&&(n[r]=i.textContent||"")}),Object.keys(n).length||e.querySelectorAll(":scope > property").forEach(t=>{const r=t.querySelector(":scope > name")?.textContent||"",i=t.querySelector(":scope > value")?.textContent||"";r&&(n[r]=i)}),n}function ts(e,n){const t=[],r=[],i=[],o=[],a=[],s={};function c(S,C){const A=S.querySelector(":scope > contents")||S,w=A.querySelectorAll(":scope > processors"),R=w.length===0?A.querySelectorAll(":scope > processor"):[];(w.length>0?w:R).forEach(F=>{const N=pe(F,"name"),H=pe(F,"type")||pe(F,"class"),Q=H.split(".").pop(),V=pe(F,"state"),ee=es(F),se=F.querySelector("config > schedulingPeriod")?.textContent||pe(F,"schedulingPeriod")||"",te=F.querySelector("config > schedulingStrategy")?.textContent||pe(F,"schedulingStrategy")||"",ae=pe(F,"id");ae&&(s[ae]=N||Q),r.push({name:N,type:Q,fullType:H,state:V,properties:ee,group:C,schedulingPeriod:se,schedulingStrategy:te,_id:ae||"gen_"+r.length})});const O=A.querySelectorAll(":scope > connections"),J=O.length===0?A.querySelectorAll(":scope > connection"):[];(O.length>0?O:J).forEach(F=>{const N=F.querySelector("source > id")?.textContent||pe(F,"sourceId")||"",H=F.querySelector("destination > id")?.textContent||pe(F,"destinationId")||"",Q=F.querySelector("source > type")?.textContent||"",V=F.querySelector("destination > type")?.textContent||"",ee=[];F.querySelectorAll(":scope > selectedRelationships").forEach(te=>{te.textContent&&ee.push(te.textContent)}),ee.length||F.querySelectorAll(":scope > relationship").forEach(te=>{te.textContent&&ee.push(te.textContent)});const se=pe(F,"backPressureObjectThreshold");i.push({sourceId:N,destinationId:H,sourceType:Q,destinationType:V,relationships:ee,backPressure:se})}),A.querySelectorAll(":scope > inputPorts > inputPort, :scope > inputPort").forEach(F=>{const N=pe(F,"id"),H=pe(F,"name");N&&(s[N]=H||"InputPort",r.push({name:H||"InputPort",type:"InputPort",id:N,group:C,properties:{}}))}),A.querySelectorAll(":scope > outputPorts > outputPort, :scope > outputPort").forEach(F=>{const N=pe(F,"id"),H=pe(F,"name");N&&(s[N]=H||"OutputPort",r.push({name:H||"OutputPort",type:"OutputPort",id:N,group:C,properties:{}}))}),A.querySelectorAll(":scope > funnels > funnel, :scope > funnel").forEach(F=>{const N=pe(F,"id");N&&(s[N]="Funnel",r.push({name:"Funnel",type:"Funnel",id:N,group:C,properties:{}}))});const z=A.querySelectorAll(":scope > processGroups"),U=z.length===0?A.querySelectorAll(":scope > processGroup"):[];(z.length>0?z:U).forEach(F=>{const N=pe(F,"name"),H=pe(F,"id");H&&(s[H]=N),a.push({name:N,parentGroup:C}),c(F,N)})}const l=e.querySelector("template > snippet")||e.querySelector("snippet")||e.querySelector("flowController > rootGroup")||e.querySelector("rootGroup")||e.querySelector("processGroupFlow > flow")||e.documentElement,d=e.querySelector("controllerServices"),u=l.querySelectorAll(":scope > controllerServices > controllerService"),g=u.length===0?l.querySelectorAll(":scope > controllerService"):[],p=u.length===0&&g.length===0&&d?d.querySelectorAll(":scope > controllerService"):[];(u.length>0?u:g.length>0?g:p).forEach(S=>{const C=pe(S,"name"),A=pe(S,"type")||pe(S,"class"),w=pe(S,"state"),R={};S.querySelectorAll(":scope > properties > entry").forEach(y=>{const O=y.querySelector(":scope > key")?.textContent||"",J=y.querySelector(":scope > value");O&&J&&(R[O]=J.textContent||"")}),S.querySelectorAll(":scope > property").forEach(y=>{const O=y.querySelector(":scope > name")?.textContent||"",J=y.querySelector(":scope > value")?.textContent||"";O&&(R[O]=J)}),o.push({name:C,type:A.split(".").pop(),fullType:A,state:w,properties:R})});const f=l.querySelectorAll(":scope > processGroups"),h=f.length===0?l.querySelectorAll(":scope > processGroup"):[];(f.length>0?f:h).forEach(S=>{const C=pe(S,"name"),A=pe(S,"id");A&&(s[A]=C),a.push({name:C,parentGroup:"(root)"}),c(S,C)});const v=l.querySelectorAll(":scope > processors"),k=v.length===0?l.querySelectorAll(":scope > processor"):[];(v.length>0?v:k).forEach(S=>{const C=pe(S,"name"),A=pe(S,"type")||pe(S,"class"),w=pe(S,"id");w&&(s[w]=C||A.split(".").pop());const R=es(S),y=S.querySelector("config > schedulingPeriod")?.textContent||pe(S,"schedulingPeriod")||"",O=S.querySelector("config > schedulingStrategy")?.textContent||pe(S,"schedulingStrategy")||"";r.push({name:C,type:A.split(".").pop(),fullType:A,state:pe(S,"state"),properties:R,group:"(root)",schedulingPeriod:y,schedulingStrategy:O,_id:w||"gen_"+r.length})});const D=l.querySelectorAll(":scope > connections"),P=D.length===0?l.querySelectorAll(":scope > connection"):[],$=D.length>0?D:P,x=new Set;return i.forEach(S=>{x.add(`${S.sourceId}|${S.destinationId}|${[...S.relationships].sort().join(",")}`)}),$.forEach(S=>{const C=S.querySelector("source > id")?.textContent||pe(S,"sourceId")||"",A=S.querySelector("destination > id")?.textContent||pe(S,"destinationId")||"",w=S.querySelector("source > type")?.textContent||"",R=S.querySelector("destination > type")?.textContent||"",y=[];S.querySelectorAll(":scope > selectedRelationships").forEach(M=>{M.textContent&&y.push(M.textContent)}),y.length||S.querySelectorAll(":scope > relationship").forEach(M=>{M.textContent&&y.push(M.textContent)});const O=pe(S,"backPressureObjectThreshold"),J=`${C}|${A}|${[...y].sort().join(",")}`;x.has(J)||(x.add(J),i.push({sourceId:C,destinationId:A,sourceType:w,destinationType:R,relationships:y,backPressure:O}))}),i.forEach(S=>{S.sourceName=s[S.sourceId]||S.sourceId.substring(0,12)+"...",S.destinationName=s[S.destinationId]||S.destinationId.substring(0,12)+"..."}),{tables:t,processors:r,connections:i,controllerServices:o,processGroups:a,idToName:s}}function ns(e,n){const t=[],r=[],i=[],o=[];function a(d,u){const g=d.name||u||"root";d.name&&i.push({name:g,id:d.identifier||g}),(d.processors||[]).forEach(p=>{const m={};Array.isArray(p.properties)?p.properties.forEach(f=>{f.name!==void 0&&f.name!==null&&(m[f.name]=f.value)}):p.properties&&Object.entries(p.properties).forEach(([f,h])=>{h!==null&&(m[f]=String(h))}),t.push({id:p.identifier||p.id||"p_"+t.length,name:p.name||p.type||"Unknown",type:(p.type||"").replace(/^org\.apache\.nifi\.processors?\.\w+\./,""),class:p.type||"",group:g,state:p.scheduledState||p.state||"STOPPED",schedulingStrategy:p.schedulingStrategy||"TIMER_DRIVEN",schedulingPeriod:p.schedulingPeriod||"0 sec",properties:m,relationships:p.autoTerminatedRelationships||[]})}),(d.connections||[]).forEach(p=>{r.push({sourceId:p.source?.id||p.sourceId||"",destinationId:p.destination?.id||p.destinationId||"",sourceName:p.source?.name||p.sourceName||"",destinationName:p.destination?.name||p.destinationName||"",relationships:p.selectedRelationships||[],backPressure:p.backPressureObjectThreshold||""})}),(d.controllerServices||[]).forEach(p=>{o.push({name:p.name||p.type||"Unknown",type:(p.type||"").replace(/^org\.apache\.nifi\.\w+\./,""),state:p.scheduledState||"ENABLED",properties:p.properties||{}})}),(d.processGroups||[]).forEach(p=>a(p,p.name||g))}a(e,"root");const s={};t.forEach(d=>{s[d.id]=d.name}),r.forEach(d=>{!d.sourceName&&d.sourceId&&(d.sourceName=s[d.sourceId]||d.sourceId),!d.destinationName&&d.destinationId&&(d.destinationName=s[d.destinationId]||d.destinationId)});const c=/password|secret|token|key|auth|credential|cert|private|keytab|passphrase/i,l={processors:t,connections:r,processGroups:i,controllerServices:o,clouderaTools:[],deepPropertyInventory:{filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set},sqlTables:[],sqlTableMeta:{}};return t.forEach(d=>{Object.entries(d.properties).forEach(([u,g])=>{g&&(/\$\{/.test(g)&&(l.deepPropertyInventory.nifiEL[g]||(l.deepPropertyInventory.nifiEL[g]=[]),l.deepPropertyInventory.nifiEL[g].push(d.name)),/jdbc:/i.test(g)&&(l.deepPropertyInventory.jdbcUrls[g]||(l.deepPropertyInventory.jdbcUrls[g]=[]),l.deepPropertyInventory.jdbcUrls[g].push(d.name)),c.test(u)&&(l.deepPropertyInventory.credentialRefs[u]||(l.deepPropertyInventory.credentialRefs[u]=[]),l.deepPropertyInventory.credentialRefs[u].push(d.name)))})}),{source_name:n,source_type:"nifi_registry_json",_nifi:l,parse_warnings:[],tables:[],_deferredProcessorWork:null}}function Oi(e){if(!e)return"";let n=e;return n.charCodeAt(0)===65279&&(n=n.substring(1)),n=n.replace(/\r\n/g,`
`).replace(/\r/g,`
`),n=n.replace(/\x00/g,""),n=n.replace(/\u00A0/g," "),n=n.replace(/[\u201C\u201D]/g,'"').replace(/[\u2018\u2019]/g,"'"),n.trim()}const cn=50*1024*1024;async function rs(e){try{if(!e||e.length===0)return"";if(e.length>cn)return{error:"File exceeds 50MB limit"};const{default:n}=await kt(async()=>{const{default:r}=await Promise.resolve().then(()=>Pa);return{default:r}},void 0,import.meta.url);return n.inflate(e,{to:"string"})}catch(n){return{error:"Gzip decompression failed: "+n.message}}}async function Ni(e){try{if(e&&e.length>cn)return{error:"File exceeds 50MB limit"};const{default:n}=await kt(async()=>{const{default:a}=await Promise.resolve().then(()=>Wr);return{default:a}},void 0,import.meta.url),t=await n.loadAsync(e),r=[],i=[/flow\.xml$/i,/flow\.json$/i,/template.*\.xml$/i,/\.xml$/i,/\.json$/i,/\.sql$/i,/META-INF\/MANIFEST\.MF$/i],o=Object.keys(t.files).filter(a=>!t.files[a].dir);for(const a of o){const s=a.split(".").pop().toLowerCase();if(["xml","json","sql","txt","properties","cfg","yaml","yml","mf"].includes(s)){const c=await t.files[a].async("string");r.push({filename:a,content:c,type:s})}}return r.sort((a,s)=>{const c=i.findIndex(d=>d.test(a.filename)),l=i.findIndex(d=>d.test(s.filename));return(c===-1?999:c)-(l===-1?999:l)}),r}catch(n){return{error:"ZIP extraction failed: "+n.message}}}async function Mi(e){try{if(e&&e.length>cn)return{error:"File exceeds 50MB limit"};const{default:n}=await kt(async()=>{const{default:r}=await Promise.resolve().then(()=>Pa);return{default:r}},void 0,import.meta.url),t=n.inflate(e);return wo(t)}catch(n){return{error:"TAR.GZ extraction failed: "+n.message}}}function wo(e){try{if(!e||e.length===0)return[];const n=[];let t=0;const r=new TextDecoder,i=1e4;let o=0;for(;t<e.length-512;){if(++o>i){n.push({name:"__tar_limit_exceeded__",content:"TAR archive exceeds 10,000 entry limit"});break}const a=e.slice(t,t+512);let s=r.decode(a.slice(0,100)).replace(/\0/g,"").trim();if(!s)break;const c=r.decode(a.slice(124,136)).replace(/\0/g,"").trim(),l=parseInt(c,8)||0,d=r.decode(a.slice(156,157));if(t+=512,d==="L"){const u=r.decode(e.slice(t,t+l)).replace(/\0/g,"").trim();if(t+=Math.ceil(l/512)*512,t>=e.length-512)break;const g=e.slice(t,t+512),p=r.decode(g.slice(124,136)).replace(/\0/g,"").trim(),m=parseInt(p,8)||0,f=r.decode(g.slice(156,157));if(t+=512,f==="0"||f===""||f==="\0"){const h=u.split(".").pop().toLowerCase();if(["xml","json","sql","txt","properties","cfg","yaml","yml"].includes(h)){const _=r.decode(e.slice(t,t+m));n.push({filename:u,content:_,type:h})}}t+=Math.ceil(m/512)*512;continue}if(d==="x"||d==="g"){t+=Math.ceil(l/512)*512;continue}if(d==="0"||d===""||d==="\0"){const u=s.split(".").pop().toLowerCase();if(["xml","json","sql","txt","properties","cfg","yaml","yml"].includes(u)){const g=r.decode(e.slice(t,t+l));n.push({filename:s,content:g,type:u})}}t+=Math.ceil(l/512)*512}return n}catch(n){return{error:"Tar parsing failed: "+n.message}}}function Bi(e){const n=[];let t="",r=0;for(;r<e.length;){const o=e[r];if(o==="-"&&e[r+1]==="-"){const a=e.indexOf(`
`,r);a===-1?(t+=e.slice(r),r=e.length):(t+=e.slice(r,a+1),r=a+1);continue}if(o==="/"&&e[r+1]==="*"){const a=e.indexOf("*/",r+2);a===-1?(t+=e.slice(r),r=e.length):(t+=e.slice(r,a+2),r=a+2);continue}if(o==="'"){let a=r+1;for(t+=o;a<e.length;)if(e[a]==="'"&&e[a+1]==="'")t+="''",a+=2;else if(e[a]==="'"){t+="'",a+=1;break}else t+=e[a],a+=1;r=a;continue}if(o==='"'){let a=r+1;for(t+=o;a<e.length;)if(e[a]==='"'&&e[a+1]==='"')t+='""',a+=2;else if(e[a]==='"'){t+='"',a+=1;break}else t+=e[a],a+=1;r=a;continue}if(o===";"){const a=t.trim();a.length>0&&n.push(a),t="",r+=1;continue}t+=o,r+=1}const i=t.trim();return i.length>0&&n.push(i),n}function ji(e,n){try{const t=Bi(e),r=[],i=[];let o=0;for(const a of t){const s=a.toUpperCase().trimStart();let c="unknown",l="SQL_Statement_"+ ++o;if(s.startsWith("CREATE TABLE")||s.startsWith("CREATE EXTERNAL TABLE")){c="CreateTable";const d=a.match(/CREATE\s+(?:EXTERNAL\s+)?TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?(?:`?(\w+)`?\.)?`?(\w+)`?/i);d&&(l=d[2]||l)}else if(s.startsWith("CREATE VIEW")||s.startsWith("CREATE OR REPLACE VIEW")){c="CreateView";const d=a.match(/CREATE\s+(?:OR\s+REPLACE\s+)?VIEW\s+(?:`?(\w+)`?\.)?`?(\w+)`?/i);d&&(l=d[2]||l)}else if(s.startsWith("INSERT")){c="InsertData";const d=a.match(/INSERT\s+(?:INTO|OVERWRITE)\s+(?:`?(\w+)`?\.)?`?(\w+)`?/i);d&&(l="Insert_"+(d[2]||l))}else if(s.startsWith("SELECT"))c="SelectQuery",l="Query_"+o;else if(s.startsWith("ALTER"))c="AlterTable";else if(s.startsWith("DROP"))c="DropObject";else continue;r.push({name:l,type:"SQL."+c,state:"RUNNING",group:"SQL Script",schedulingStrategy:"SEQUENTIAL",schedulingPeriod:"N/A",properties:{"SQL Statement":a.substring(0,2e3)}})}for(let a=0;a<r.length-1;a++)i.push({sourceId:r[a].name,sourceName:r[a].name,destinationId:r[a+1].name,destinationName:r[a+1].name,relationships:["success"]});return{source_name:n,source_type:"sql_script",_nifi:{processors:r,connections:i,controllerServices:[],processGroups:[{name:"SQL Script",parentGroup:""}],idToName:{},clouderaTools:[],deepPropertyInventory:{filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set},sqlTables:r.filter(a=>a.type.includes("Create")).map(a=>a.name),sqlTableMeta:{}},tables:[],parse_warnings:["Parsed as SQL script â€” processors represent SQL statements, not NiFi processors."],_deferredProcessorWork:null}}catch(t){return{error:"SQL parsing failed: "+t.message}}}async function Ui(e){try{if(e&&e.length>cn)return{error:"File exceeds 50MB limit"};const{default:n}=await kt(async()=>{const{default:m}=await Promise.resolve().then(()=>Wr);return{default:m}},void 0,import.meta.url),r=(await n.loadAsync(e)).files["word/document.xml"];if(!r)return{text:"",tables:[]};const i=await r.async("string"),a=new DOMParser().parseFromString(i,"text/xml"),s=a.getElementsByTagNameNS("http://schemas.openxmlformats.org/wordprocessingml/2006/main","t"),c=[],l=new Set;for(let m=0;m<s.length;m++){let h=s[m].parentNode;for(;h&&h.nodeName!=="w:p"&&h.nodeName!=="body";)h=h.parentNode;h&&h.nodeName==="w:p"&&!l.has(h)&&(l.add(h),c.push(h))}const u=c.map(m=>{const f=m.querySelectorAll("w\\:t, t");return Array.from(f).map(h=>h.textContent).join("")}).filter(Boolean).join(`
`),g=[],p=a.getElementsByTagNameNS("http://schemas.openxmlformats.org/wordprocessingml/2006/main","tbl");for(let m=0;m<p.length;m++){const f=p[m].getElementsByTagNameNS("http://schemas.openxmlformats.org/wordprocessingml/2006/main","tr"),h=[];for(let _=0;_<f.length;_++){const v=f[_].getElementsByTagNameNS("http://schemas.openxmlformats.org/wordprocessingml/2006/main","tc"),k=[];for(let E=0;E<v.length;E++){const D=v[E].getElementsByTagNameNS("http://schemas.openxmlformats.org/wordprocessingml/2006/main","t");let P="";for(let $=0;$<D.length;$++)P+=D[$].textContent+" ";k.push(P.trim())}h.push(k)}g.push(h)}return{text:u,tables:g}}catch(n){return{error:"DOCX extraction failed: "+n.message}}}function qi(e){const n=e.match(/^([A-Z]+)/);if(!n)return 0;const t=n[1];let r=0;for(let i=0;i<t.length;i++)r=r*26+(t.charCodeAt(i)-64);return r-1}async function zi(e){try{if(e&&e.length>cn)return{error:"File exceeds 50MB limit"};const{default:n}=await kt(async()=>{const{default:s}=await Promise.resolve().then(()=>Wr);return{default:s}},void 0,import.meta.url),t=await n.loadAsync(e),r=[],i=t.files["xl/sharedStrings.xml"];if(i){const s=await i.async("string"),d=new DOMParser().parseFromString(s,"text/xml").getElementsByTagName("si");for(let u=0;u<d.length;u++){const g=d[u].getElementsByTagName("t");let p="";for(let m=0;m<g.length;m++)p+=g[m].textContent;r.push(p)}}const o=[],a=Object.keys(t.files).filter(s=>/^xl\/worksheets\/sheet\d+\.xml$/.test(s)).sort((s,c)=>{const l=parseInt(s.match(/sheet(\d+)/)[1]),d=parseInt(c.match(/sheet(\d+)/)[1]);return l-d});for(const s of a){const l=await t.files[s].async("string"),g=new DOMParser().parseFromString(l,"text/xml").getElementsByTagName("row"),p=[];for(let m=0;m<g.length;m++){const f=g[m].getElementsByTagName("c"),h=[];for(let _=0;_<f.length;_++){const v=f[_].getAttribute("r"),k=v?qi(v):_,E=f[_].getAttribute("t"),D=f[_].getElementsByTagName("v")[0];let P=D?D.textContent:"";if(E==="s"&&D){const S=parseInt(D.textContent);!isNaN(S)&&S<r.length&&(P=r[S])}const x=Math.min(k,500);for(;h.length<=x;)h.push("");h[x]=P}p.push(h)}o.push(p)}return{sheets:o,sharedStrings:r}}catch(n){return{error:"XLSX extraction failed: "+n.message}}}function ss(e,n,t){try{const r=[],i=[],o=[],a=/(?:processor|component|step|task|job)\s*[:\-]?\s*([A-Z]\w+(?:\s+\w+)*)/gi;let s;const c=new Set;for(;(s=a.exec(e))!==null;){const l=s[1].trim();!c.has(l)&&l.length>2&&l.length<60&&(c.add(l),r.push({name:l,type:"Document.Reference",state:"DOCUMENTED",group:"Document Extract",properties:{Source:t,Context:e.substring(Math.max(0,s.index-50),s.index+100).trim()}}))}for(const l of n){if(l.length<2)continue;const d=l[0].map(p=>p.toLowerCase()),u=d.findIndex(p=>p.includes("name")||p.includes("processor")||p.includes("component")),g=d.findIndex(p=>p.includes("type")||p.includes("class"));if(u>=0)for(let p=1;p<l.length;p++){const m=l[p][u];if(m&&!c.has(m)){c.add(m);const f={};d.forEach((h,_)=>{l[p][_]&&(f[h]=l[p][_])}),r.push({name:m,type:g>=0&&l[p][g]||"Document.TableEntry",state:"DOCUMENTED",group:"Document Table",properties:f})}}}return r.length===0&&(o.push("No processor/component references found in document. Content extracted for analysis only."),r.push({name:"Document_Content",type:"Document.FullText",state:"DOCUMENTED",group:"Document",properties:{"Content Preview":e.substring(0,2e3),"Total Length":String(e.length)}})),{source_name:t,source_type:"document",_nifi:{processors:r,connections:i,controllerServices:[],processGroups:[{name:"Document Extract",parentGroup:""}],idToName:{},clouderaTools:[],deepPropertyInventory:{filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set},sqlTables:[],sqlTableMeta:{}},tables:[],parse_warnings:o.length?o:["Parsed from document â€” content represents extracted references, not NiFi processors."],_deferredProcessorWork:null}}catch(r){return{error:"Document flow building failed: "+r.message}}}function Gi(e){for(var n=[],t="",r=0,i=!1,o="",a=0;a<e.length;a++){var s=e[a];i?(t+=s,s===o&&(i=!1)):s==="'"||s==='"'?(i=!0,o=s,t+=s):s==="("?(r++,t+=s):s===")"?(r--,t+=s):s===":"&&r===0?(n.push(t),t=""):t+=s}return t&&n.push(t),n}function Eo(e){const n=e.toLowerCase();return/password|token|secret|credential|api[_.]?key|private[_.]?key|passphrase/i.test(n)?{type:"secret",code:`dbutils.secrets.get(scope="${n.includes("kafka")?"kafka":n.includes("jdbc")?"jdbc":n.includes("s3")||n.includes("aws")?"aws":n.includes("azure")?"azure":n.includes("es")?"es":"app"}", key="${e}")`}:/^(s3|kafka|jdbc|nifi|aws|azure|gcp|http|ftp|smtp|ssl|sasl)\./i.test(e)||/\.url$|\.host$|\.port$|\.path$|\.bucket$|\.region$/i.test(n)?{type:"config",code:`dbutils.widgets.get("${e}")`}:{type:"column",code:`col("${e}")`}}function De(e,n){if(e=e.trim(),/^'([^']*)'$/.test(e)){var t=e.slice(1,-1).replace(/\\/g,"\\\\").replace(/"/g,'\\"').replace(/\n/g,"\\n");return n==="col"?'lit("'+t+'")':'"'+t+'"'}if(/^\d+$/.test(e)||/^\d+\.\d+$/.test(e))return n==="col"?"lit("+e+")":e;if(e.includes(":"))return xo(e,n);var r=Eo(e);return n==="col"?r.type==="column"?'col("'+e+'")':r.code:r.type==="column"?'_attrs.get("'+e+'", "")':r.code}function Hi(e){var n=e.match(/^([\w:]+)\((.*)\)$/s);if(!n)return{name:e,args:[]};var t=n[1],r=n[2].trim();if(!r)return{name:t,args:[]};for(var i=[],o="",a=0,s=!1,c="",l=0;l<r.length;l++){var d=r[l];s?(o+=d,d===c&&(s=!1)):d==="'"||d==='"'?(s=!0,c=d,o+=d):d==="("?(a++,o+=d):d===")"?(a--,o+=d):d===","&&a===0?(i.push(o.trim()),o=""):o+=d}return o.trim()&&i.push(o.trim()),{name:t,args:i}}function me(e){return e.length>=2&&(e[0]==="'"&&e[e.length-1]==="'"||e[0]==='"'&&e[e.length-1]==='"')?e.slice(1,-1):e}function os(e){return e.replace(/yyyy/g,"%Y").replace(/MM/g,"%m").replace(/dd/g,"%d").replace(/HH/g,"%H").replace(/mm/g,"%M").replace(/SSS/g,"%f").replace(/ss/g,"%S").replace(/EEEE/g,"%A").replace(/E{1,3}/g,"%a").replace(/Z/g,"%z")}function re(e){return String(e).replace(/\\/g,"\\\\").replace(/"/g,'\\"').replace(/\n/g,"\\n").replace(/\r/g,"\\r").replace(/\t/g,"\\t")}function ct(e){return String(e).replace(/[.*+?^${}()|[\]\\]/g,"\\$&")}const as={COMBINEDAPACHELOG:'(?<client>\\S+) \\S+ (?<user>\\S+) \\[(?<timestamp>[^\\]]+)\\] "(?<method>\\S+) (?<request>[^"]+) HTTP/\\S+" (?<status>\\d+) (?<size>\\S+) "(?<referrer>[^"]*)" "(?<agent>[^"]*)"',COMMONAPACHELOG:'(?<client>\\S+) \\S+ (?<user>\\S+) \\[(?<timestamp>[^\\]]+)\\] "(?<method>\\S+) (?<request>[^"]+) HTTP/\\S+" (?<status>\\d+) (?<size>\\S+)',SYSLOGLINE:"(?<timestamp>\\w{3}\\s+\\d{1,2} \\d{2}:\\d{2}:\\d{2}) (?<host>\\S+) (?<program>[^\\[]+)(?:\\[(?<pid>\\d+)\\])?: (?<message>.*)",TIMESTAMP_ISO8601:"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?(?:Z|[+-]\\d{2}:?\\d{2})?",IP:"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}",NUMBER:"(?:-?\\d+(?:\\.\\d+)?)",WORD:"\\w+",DATA:".*?",GREEDYDATA:".*"};function Wi(e){return as[e]||as[e.toUpperCase()]||e}function Vi(e,n,t){var r=Hi(n),i=r.name.toLowerCase(),o=r.args;if(i==="toupper")return t==="col"?"upper("+e+")":e+".upper()";if(i==="tolower")return t==="col"?"lower("+e+")":e+".lower()";if(i==="trim")return t==="col"?"trim("+e+")":e+".strip()";if(i==="length")return t==="col"?"length("+e+")":"len("+e+")";if(i==="substring"){var a=o[0]||"0",s=o[1];return t==="col"?s?"substring("+e+", "+(parseInt(a)+1)+", "+(parseInt(s)-parseInt(a))+")":"substring("+e+", "+(parseInt(a)+1)+", 9999)":s?e+"["+a+":"+s+"]":e+"["+a+":]"}if(i==="replace"){var c=me(o[0]||""),l=me(o[1]||"");return t==="col"?"regexp_replace("+e+', "'+re(ct(c))+'", "'+re(l)+'")':e+'.replace("'+re(c)+'", "'+re(l)+'")'}if(i==="replaceall"){var d=me(o[0]||""),u=me(o[1]||"");return t==="col"?"regexp_replace("+e+', "'+re(d)+'", "'+re(u)+'")':'re.sub(r"'+re(d)+'", "'+re(u)+'", '+e+")"}if(i==="replacefirst"){var g=me(o[0]||""),p=me(o[1]||"");return t==="col"?"regexp_replace("+e+', "'+re(ct(g))+'", "'+re(p)+'")':'re.sub(r"'+re(ct(g))+'", "'+re(p)+'", str('+e+"), count=1)"}if(i==="startswith"){var m=me(o[0]||"");return e+'.startswith("'+re(m)+'")'}if(i==="endswith"){var f=me(o[0]||"");return e+'.endswith("'+re(f)+'")'}if(i==="contains"){var h=me(o[0]||"");return t==="col"?e+'.contains("'+re(h)+'")':'"'+re(h)+'" in '+e}if(i==="matches"){var _=me(o[0]||"");return t==="col"?e+'.rlike("'+re(_)+'")':'re.match(r"'+re(_)+'", '+e+")"}if(i==="find"){var v=me(o[0]||"");return t==="col"?"regexp_extract("+e+', "'+re(v)+'", 0)':'(lambda m: m.group(0) if m else "")(re.search(r"'+re(v)+'", '+e+"))"}if(i==="split"){var k=me(o[0]||",");return t==="col"?"split("+e+', "'+re(ct(k))+'")':e+'.split("'+re(k)+'")'}if(i==="substringbefore"){var E=me(o[0]||"");return t==="col"?"substring_index("+e+', "'+re(E)+'", 1)':e+'.split("'+re(E)+'")[0]'}if(i==="substringafter"){var D=me(o[0]||""),P=D.length;return t==="col"?'when(locate("'+re(D)+'", '+e+") > 0, substring("+e+', locate("'+re(D)+'", '+e+") + "+P+', 9999)).otherwise(lit(""))':'"'+re(D)+'".join('+e+'.split("'+re(D)+'")[1:])'}if(i==="substringbeforelast"){var $=me(o[0]||"");return t==="col"?'when(locate("'+re($)+'", '+e+") > 0, substring_index("+e+', "'+re($)+'", -1 + size(split('+e+', "'+re(ct($))+'")))).otherwise('+e+")":'("'+re($)+'".join('+e+'.rsplit("'+re($)+'")[:-1]) if "'+re($)+'" in '+e+" else "+e+")"}if(i==="substringafterlast"){var x=me(o[0]||"");return t==="col"?'when(locate("'+re(x)+'", '+e+") > 0, element_at(split("+e+', "'+re(ct(x))+'"), -1)).otherwise(lit(""))':"("+e+'.rsplit("'+re(x)+'", 1)[-1] if "'+re(x)+'" in '+e+' else "")'}if(i==="padleft"||i==="leftpad"){var S=o[0]||"10",C=me(o[1]||" ");return t==="col"?"lpad("+e+", "+S+', "'+re(C)+'")':e+".rjust("+S+', "'+re(C)+'")'}if(i==="padright"||i==="rightpad"){var A=o[0]||"10",w=me(o[1]||" ");return t==="col"?"rpad("+e+", "+A+', "'+re(w)+'")':e+".ljust("+A+', "'+re(w)+'")'}if(i==="indexof"){var R=me(o[0]||"");return t==="col"?'(locate("'+re(R)+'", '+e+") - 1)":e+'.find("'+re(R)+'")'}if(i==="getdelimitedfield"){var y=o[0]||"1",O=me(o[1]||",");return t==="col"?"split("+e+', "'+re(ct(O))+'")['+(parseInt(y)-1)+"]":e+'.split("'+re(O)+'")['+(parseInt(y)-1)+"]"}if(i==="append"){var J=me(o[0]||"");return t==="col"?"concat("+e+', lit("'+re(J)+'"))':e+' + "'+re(J)+'"'}if(i==="prepend"){var M=me(o[0]||"");return t==="col"?'concat(lit("'+re(M)+'"), '+e+")":'"'+re(M)+'" + '+e}if(i==="equals"){var z=me(o[0]||"");return t==="col"?"("+e+' == lit("'+re(z)+'"))':"("+e+' == "'+re(z)+'")'}if(i==="equalsignorecase"){var U=me(o[0]||"");return t==="col"?"(lower("+e+') == lit("'+re(U.toLowerCase())+'"))':"("+e+'.lower() == "'+re(U.toLowerCase())+'")'}if(i==="plus"){var X=o[0]||"0";return t==="col"?"("+e+" + lit("+X+"))":"("+e+" + "+X+")"}if(i==="minus"){var F=o[0]||"0";return t==="col"?"("+e+" - lit("+F+"))":"("+e+" - "+F+")"}if(i==="multiply"){var N=o[0]||"1";return t==="col"?"("+e+" * lit("+N+"))":"("+e+" * "+N+")"}if(i==="divide"){var H=o[0]||"1";return t==="col"?"("+e+" / lit("+H+"))":"("+e+" / "+H+")"}if(i==="mod"){var Q=o[0]||"1";return t==="col"?"("+e+" % lit("+Q+"))":"("+e+" % "+Q+")"}if(i==="gt"){var V=o[0]||"0";return t==="col"?"("+e+" > lit("+V+"))":"("+e+" > "+V+")"}if(i==="lt"){var ee=o[0]||"0";return t==="col"?"("+e+" < lit("+ee+"))":"("+e+" < "+ee+")"}if(i==="ge"){var se=o[0]||"0";return t==="col"?"("+e+" >= lit("+se+"))":"("+e+" >= "+se+")"}if(i==="le"){var te=o[0]||"0";return t==="col"?"("+e+" <= lit("+te+"))":"("+e+" <= "+te+")"}if(i==="isempty")return t==="col"?"("+e+".isNull() | ("+e+' == lit("")))':"(not "+e+")";if(i==="isnull")return t==="col"?e+".isNull()":"("+e+" is None)";if(i==="notnull")return t==="col"?e+".isNotNull()":"("+e+" is not None)";if(i==="not")return t==="col"?"(~"+e+")":"(not "+e+")";if(i==="and"){var ae=o[0]?De(o[0],t):"lit(True)";return t==="col"?"("+e+" & "+ae+")":"("+e+" and "+ae+")"}if(i==="or"){var ie=o[0]?De(o[0],t):"lit(False)";return t==="col"?"("+e+" | "+ie+")":"("+e+" or "+ie+")"}if(i==="ifelse"){var fe=o[0]?De(o[0],t):"lit(True)",_e=o[1]?De(o[1],t):"lit(None)";return t==="col"?"when("+e+", "+fe+").otherwise("+_e+")":"("+fe+" if "+e+" else "+_e+")"}if(i==="format"){var ye=me(o[0]||"yyyy-MM-dd");return t==="col"?"date_format("+e+', "'+ye+'")':e+'.strftime("'+os(ye)+'")'}if(i==="todate"){var b=me(o[0]||"yyyy-MM-dd");return t==="col"?"to_timestamp("+e+', "'+b+'")':"datetime.strptime("+e+', "'+os(b)+'")'}if(i==="tonumber")return t==="col"?e+'.cast("long")':"int("+e+")";if(i==="tostring")return t==="col"?e+'.cast("string")':"str("+e+")";if(i==="grok"){var K=me(o[0]||""),q=Wi(K);return t==="col"?"regexp_extract("+e+', "'+re(q)+'", 0)':'re.search(r"'+re(q)+'", '+e+")"}if(i==="evaluateelstring")return t==="col"?e:e+"  # evaluateELString: use f-string or .format()";if(i==="unescapejson"){var L=o[0]?me(o[0]):"STRING";return t==="col"?"from_json("+e+', "'+L+'")':"json.loads("+e+")"}if(i==="unescapexml")return t==="col"?"regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace("+e+`, '&amp;', '&'), '&lt;', '<'), '&gt;', '>'), '&quot;', '"'), '&apos;', "'")`:e+`.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '"').replace("&apos;", "'")`;if(i==="escapexml")return t==="col"?"regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace("+e+`, '&', '&amp;'), '<', '&lt;'), '>', '&gt;'), '"', '&quot;'), "'", '&apos;')`:e+`.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('"', '&quot;').replace("'", '&apos;')`;if(i==="unescapecsv")return t==="col"?"when("+e+`.startsWith('"') & `+e+`.endsWith('"'), regexp_replace(substring(`+e+", 2, length("+e+`) - 2), '""', '"')).otherwise(`+e+")":e+`[1:-1].replace('"' * 2, '"') if `+e+`.startswith('"') and `+e+`.endswith('"') else `+e;if(i==="escapejson")return t==="col"?"to_json(struct("+e+"))":"json.dumps(str("+e+"))[1:-1]";if(i==="urlencode")return t==="col"?"regexp_replace(regexp_replace("+e+", ' ', '%20'), '[^A-Za-z0-9_.~-]', '')":"urllib.parse.quote(str("+e+"), safe='')";if(i==="urldecode")return t==="col"?"regexp_replace("+e+", '%20', ' ')":"urllib.parse.unquote(str("+e+"))";if(i==="base64encode")return t==="col"?"base64(encode("+e+", 'UTF-8'))":"base64.b64encode(str("+e+").encode()).decode()";if(i==="base64decode")return t==="col"?"unbase64("+e+").cast('string')":"base64.b64decode(str("+e+")).decode()";if(i==="escapecsv")return t==="col"?"when("+e+'.contains(",") | '+e+`.contains('"') | `+e+`.contains("\\n"), concat(lit('"'), regexp_replace(`+e+`, '"', '""'), lit('"'))).otherwise(`+e+")":`'"' + `+e+`.replace('"', '""') + '"' if "," in `+e+` or '"' in `+e+' or "\\n" in '+e+" else "+e;if(i==="in"){var I=o.map(function(Z){return me(Z)});return t==="col"?e+".isin("+I.map(function(Z){return'"'+re(Z)+'"'}).join(", ")+")":e+" in ["+I.map(function(Z){return'"'+re(Z)+'"'}).join(", ")+"]"}if(i==="math:tonumber")return t==="col"?e+'.cast("double")':"float("+e+")";if(i==="math:floor")return t==="col"?"floor("+e+")":"math.floor("+e+")";if(i==="math:ceil"||i==="math:ceiling")return t==="col"?"ceil("+e+")":"math.ceil("+e+")";if(i==="math:abs")return"abs("+e+")";if(i==="math:mod"){var T=o[0]||"1";return t==="col"?"("+e+" % lit("+T+"))":"("+e+" % "+T+")"}if(i==="math:multiply"){var G=o[0]||"1";return t==="col"?"("+e+" * lit("+G+"))":"("+e+" * "+G+")"}if(i==="math:max"){var W=o[0]?De(o[0],t):"0";return t==="col"?"greatest("+e+", "+W+")":"max("+e+", "+W+")"}if(i==="math:min"){var j=o[0]?De(o[0],t):"0";return t==="col"?"least("+e+", "+j+")":"min("+e+", "+j+")"}return e+"  /* NEL: "+n+" */"}function xo(e,n){var t=Gi(e);if(t.length===0)return'""';var r=t[0].trim(),i=t.slice(1),o;if(/^now\(\)$/i.test(r))o=n==="col"?"current_timestamp()":"datetime.now()";else if(/^UUID\(\)$/i.test(r))o=n==="col"?'expr("uuid()")':"str(uuid.uuid4())";else if(/^hostname\(\)$/i.test(r))o=n==="col"?'lit(spark.conf.get("spark.databricks.clusterUsageTags.clusterName", "unknown"))':"socket.gethostname()";else if(/^nextInt\(\)$/i.test(r)||/^random\(\)$/i.test(r))o=n==="col"?'(rand() * 2147483647).cast("int")':"random.randint(0, 2147483647)";else if(/^literal\('((?:[^'\\]|\\.)*)'\)$/i.test(r)){var a=r.match(/^literal\('((?:[^'\\]|\\.)*)'\)$/i)[1],s=a.replace(/\\/g,"\\\\").replace(/"/g,'\\"').replace(/\n/g,"\\n");o=n==="col"?'lit("'+s+'")':'"'+s+'"'}else if(/^literal\((\d+)\)$/i.test(r)){var c=r.match(/^literal\((\d+)\)$/i)[1];o=n==="col"?"lit("+c+")":c}else if(/^math:abs\((.+)\)$/i.test(r)){var l=r.match(/^math:abs\((.+)\)$/i)[1];o="abs("+De(l,n)+")"}else if(/^math:ceil\((.+)\)$/i.test(r))o=n==="col"?"ceil("+De(r.match(/^math:ceil\((.+)\)$/i)[1],n)+")":"math.ceil("+De(r.match(/^math:ceil\((.+)\)$/i)[1],n)+")";else if(/^math:floor\((.+)\)$/i.test(r))o=n==="col"?"floor("+De(r.match(/^math:floor\((.+)\)$/i)[1],n)+")":"math.floor("+De(r.match(/^math:floor\((.+)\)$/i)[1],n)+")";else if(/^math:round\((.+)\)$/i.test(r))o=n==="col"?"round("+De(r.match(/^math:round\((.+)\)$/i)[1],n)+")":"round("+De(r.match(/^math:round\((.+)\)$/i)[1],n)+")";else{var d=Eo(r);n==="col"?o=d.type==="column"?'col("'+r+'")':d.code:o=d.type==="column"?'_attrs.get("'+r+'", "")':(d.type==="secret",d.code)}for(var u=0;u<i.length;u++)o=Vi(o,i[u].trim(),n);return o}function Ki(e){const n={};return e._rawXml&&((e._rawXml.match(/<variable\s+(?:name="[^"]+"\s+value="[^"]*"|value="[^"]*"\s+name="[^"]+")\s*\/>/g)||[]).forEach(i=>{const o=i.match(/name="([^"]+)"/),a=i.match(/value="([^"]*)"/);o&&a&&(n[o[1]]=a[1])}),(e._rawXml.match(/<parameter>\s*<name>([^<]+)<\/name>\s*<value>([^<]*)<\/value>/g)||[]).forEach(i=>{const o=i.match(/<name>([^<]+)<\/name>/),a=i.match(/<value>([^<]*)<\/value>/);!o||!a||(n[o[1]]=a[1])})),n}function Qi(e,n){if(!e||!n||Object.keys(n).length===0)return e;let t=e;for(const[r,i]of Object.entries(n)){const o=String(i).replace(/\$/g,"$$$$");t=t.replace(new RegExp("\\$\\{"+r.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")+"\\}","g"),o),t=t.replace(new RegExp("#\\{"+r.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")+"\\}","g"),o)}return t}function Ji(e,n){return!e||typeof e!="string"||(n=n||"col",!e.includes("${"))?e:e.replace(/\$\{([^}]+)\}/g,function(t,r){return xo(r.trim(),n)})}const Zi=50*1024*1024;async function dt(e,n,t={}){if((typeof e=="string"?e.length:e?.byteLength??0)>Zi)return{processors:[],_nifi:{processors:[]},parse_warnings:["File exceeds 50MB parse limit"]};if(!e||e instanceof ArrayBuffer&&e.byteLength===0||typeof e=="string"&&e.trim().length===0)return{processors:[],_nifi:{processors:[]},parse_warnings:["Empty file"]};let i=(n||"").toLowerCase();const o=t.bytes||null;{const c=new Uint8Array(typeof e=="string"?new TextEncoder().encode(e.slice(0,4)):e instanceof ArrayBuffer?new Uint8Array(e,0,Math.min(4,e.byteLength)):o?o.slice(0,4):new Uint8Array(0));if(c.length>=2){if(c[0]===31&&c[1]===139){const l=o||new Uint8Array(typeof e=="string"?new TextEncoder().encode(e):e),d=await rs(l);return d&&d.error?{processors:[],_nifi:{processors:[]},parse_warnings:[d.error]}:dt(d,(n||"").replace(/\.gz$/i,"")||"flow")}if(c[0]===80&&c[1]===75){let l="zip";i.endsWith(".docx")?l="docx":i.endsWith(".xlsx")&&(l="xlsx"),l==="zip"&&!i.endsWith(".zip")&&!i.endsWith(".nar")&&!i.endsWith(".jar")&&(i=n.toLowerCase().replace(/\.[^.]+$/,"")+".zip")}}}if(i.endsWith(".tar.gz")||i.endsWith(".tgz")){if(!o)throw new Error("Binary data required for .tar.gz/.tgz files");const c=await Mi(o);if(c.error)return{processors:[],_nifi:{processors:[]},parse_warnings:[c.error]};if(c.length===0)throw new Error("No parseable text files found in tar.gz archive");const l=c[0];return dt(l.content,l.filename)}if(i.endsWith(".tar")){const c=e instanceof ArrayBuffer?new Uint8Array(e):new TextEncoder().encode(e),l=wo(c);if(l.error)return{processors:[],_nifi:{processors:[]},parse_warnings:[l.error]};if(!l.length)return{processors:[],_nifi:{processors:[]},parse_warnings:["No parseable files found in tar archive"]};const d=l.sort((u,g)=>{const p=m=>/flow\.xml/i.test(m)?0:/flow\.json/i.test(m)?1:/template/i.test(m)?2:/\.xml$/i.test(m)?3:/\.json$/i.test(m)?4:5;return p(u.filename)-p(g.filename)})[0];return dt(d.content,d.filename)}if(i.endsWith(".gz")){if(!o)throw new Error("Binary data required for .gz files");const c=await rs(o);if(c&&c.error)return{processors:[],_nifi:{processors:[]},parse_warnings:[c.error]};const l=(n||"").replace(/\.gz$/i,"")||n||"flow";return dt(c,l)}if(i.endsWith(".zip")||i.endsWith(".nar")||i.endsWith(".jar")){if(!o)throw new Error("Binary data required for .zip/.nar/.jar files");const c=await Ni(o);if(c.error)return{processors:[],_nifi:{processors:[]},parse_warnings:[c.error]};if(c.length===0)throw new Error("No parseable text files found in archive");const l=c[0];return dt(l.content,l.filename)}if(i.endsWith(".docx")){if(!o)throw new Error("Binary data required for .docx files");const c=await Ui(o);if(c.error)return{processors:[],_nifi:{processors:[]},parse_warnings:[c.error]};const{text:l,tables:d}=c;if(!l&&d.length===0)throw new Error("No content found in DOCX file");const u=ss(l,d,n);return u.error?{processors:[],_nifi:{processors:[]},parse_warnings:[u.error]}:u}if(i.endsWith(".xlsx")){if(!o)throw new Error("Binary data required for .xlsx files");const c=await zi(o);if(c.error)return{processors:[],_nifi:{processors:[]},parse_warnings:[c.error]};const{sheets:l}=c,d=l.flat(),u=d.map(m=>m.join("	")).join(`
`),g=d.length>0?[d]:[],p=ss(u,g,n);return p.error?{processors:[],_nifi:{processors:[]},parse_warnings:[p.error]}:p}if(i.endsWith(".sql")){const c=e||"";if(!c.trim())throw new Error("Empty SQL file");const l=ji(c,n);return l.error?{processors:[],_nifi:{processors:[]},parse_warnings:[l.error]}:l}if(/\.(csv|tsv)$/i.test(i))return{processors:[],_nifi:{processors:[]},parse_warnings:["CSV/TSV files are not NiFi flow definitions. Please upload a NiFi template (.xml), flow definition (.json), or flow.xml.gz archive."]};if(/\.(ya?ml)$/i.test(i))return{processors:[],_nifi:{processors:[]},parse_warnings:["YAML files are not directly supported as NiFi flow definitions. If this is a NiFi Registry export, try converting to JSON first."]};if(/\.(avro)$/i.test(i))return{processors:[],_nifi:{processors:[]},parse_warnings:["Avro files are data files, not NiFi flow definitions. Please upload a NiFi template (.xml) or flow definition (.json)."]};if(/\.(parquet)$/i.test(i))return{processors:[],_nifi:{processors:[]},parse_warnings:["Parquet files are data files, not NiFi flow definitions. Please upload a NiFi template (.xml) or flow definition (.json)."]};const a=Oi(e||"");if(!a)throw new Error("Empty or invalid file content");const s=a.trimStart();if(s.startsWith("<")||s.startsWith("<?xml")){const l=new DOMParser().parseFromString(a,"text/xml"),d=l.querySelector("parsererror");if(d)throw new Error("XML parse error: "+d.textContent.substring(0,200));const u=ts(l);return{source_name:n,source_type:"nifi_xml",_nifi:{processors:u.processors,connections:u.connections,controllerServices:u.controllerServices,processGroups:u.processGroups,idToName:u.idToName,clouderaTools:[],deepPropertyInventory:{filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set},sqlTables:[],sqlTableMeta:{}},tables:u.tables,parse_warnings:[],_deferredProcessorWork:null,_rawXml:a}}if(s.startsWith("{")||s.startsWith("[")){let c;try{c=JSON.parse(a)}catch(u){throw new Error("JSON parse error: "+u.message)}c.versionedFlowSnapshot&&(c=c.versionedFlowSnapshot);const l=c.flowContents||c.flow?.flowContents||c.processGroupFlow?.flow||c,d=ns(l,n);return d._rawContent=typeof e=="string"?e:new TextDecoder().decode(e),d}try{const l=new DOMParser().parseFromString(a,"text/xml");if(!l.querySelector("parsererror")){const d=ts(l,n);return{source_name:n,source_type:"nifi_xml",_nifi:{processors:d.processors,connections:d.connections,controllerServices:d.controllerServices,processGroups:d.processGroups,idToName:d.idToName,clouderaTools:[],deepPropertyInventory:{filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set},sqlTables:[],sqlTableMeta:{}},tables:d.tables,parse_warnings:[],_deferredProcessorWork:null,_rawXml:a}}}catch{}try{let c=JSON.parse(a);c.versionedFlowSnapshot&&(c=c.versionedFlowSnapshot);const l=c.flowContents||c.flow?.flowContents||c.processGroupFlow?.flow||c,d=ns(l,n);return d._rawContent=typeof e=="string"?e:new TextDecoder().decode(e),d}catch{throw new Error("Unable to parse file as XML or JSON. Ensure the file is a valid NiFi template, flow definition, or registry export.")}}const Ue={filePath:/(?:\/[\w${}._-]+){2,}/g,urlPattern:/https?:\/\/[^\s"',;]+/gi,jdbcUrl:/jdbc:[a-z0-9]+:[^\s"',;]+/gi,nifiEL:/\$\{[^}]+\}/g,cronExpr:/(?:^|\s)((?:\*|[0-9]+(?:[-/,][0-9]+)*)\s+){4,5}(?:\*|[0-9]+(?:[-/,][0-9]+)*)/,hostPort:/([a-zA-Z0-9][-a-zA-Z0-9.]+\.[a-zA-Z]{2,})(?::(\d{2,5}))?/g,credentialKey:/password|secret|token|api[_-]?key|credential|private[_-]?key|auth/i,dataFormat:/avro|parquet|orc|json|csv|tsv|xml|protobuf|thrift|msgpack|yaml|excel|xlsx/i},is=new Set(["now","nextInt","UUID","hostname","IP","literal","thread","format","toDate","substring","substringBefore","substringAfter","replace","replaceAll","replaceFirst","replaceEmpty","replaceNull","toUpper","toLower","trim","length","isEmpty","equals","equalsIgnoreCase","contains","startsWith","endsWith","append","prepend","plus","minus","multiply","divide","mod","gt","ge","lt","le","and","or","not","ifElse","toString","toNumber","math","getStateValue","count","padLeft","padRight","escapeJson","escapeXml","escapeCsv","unescapeJson","unescapeXml","urlEncode","urlDecode","base64Encode","base64Decode","toRadix","jsonPath","jsonPathDelete","jsonPathAdd","jsonPathSet","jsonPathPut"]);function Co(e){const n={},t={},r={},i={},o={},a=[],s=[],c=[],l=[],d=[],u=[],g=new Set;function p(P,$,x){!P||P.includes("${")||(n[P]||(n[P]={type:$,processors:[]}),n[P].processors.push(x))}function m(P,$,x,S){P&&(t[P]||(t[P]={producers:[],consumers:[],format:S||"unknown"}),$==="read"?t[P].consumers.push(x):t[P].producers.push(x))}function f(P,$,x){if(!P||P==="dual"||P.length<2)return;const S=P.replace(/^["'`]+|["'`]+$/g,"").trim();!S||/^(waiting|because|account|log|Dates|LeadLag|startGrouping|grps|Temptation)$/i.test(S)||(r[S]||(r[S]={readers:[],writers:[]}),$==="read"?r[S].readers.push(x):r[S].writers.push(x))}function h(P,$){if(!P||typeof P!="string")return;const x=P.match(/\$\{([^}]+)\}/g);x&&x.forEach(S=>{const A=S.slice(2,-1).split(":")[0].trim(),w=A.replace(/\(.*$/,"");!is.has(A)&&!is.has(w)&&!A.includes(".")&&!/^(filename|path|absolute\.path|uuid|fileSize|file\.size|entryDate|lineageStartDate|flowfile)/.test(A)&&d.push({expr:S,processor:$,attrName:A,resolved:!1})})}(e.connections||[]).forEach(P=>{P.sourceType==="PROCESSOR"&&g.add(P.sourceId||P.sourceName),P.destinationType==="PROCESSOR"&&g.add(P.destinationId||P.destinationName),g.add(P.sourceName),g.add(P.destinationName)}),(e.processors||[]).forEach(P=>{const $=P.properties||{},x=P.name,S=P.type;if(Object.entries($).forEach(([C,A])=>h(A,x)),S==="GetFile"){const C=$["Input Directory"];p(C,"input",x),C&&m(C+"/*.csv","read",x,"csv")}else if(S==="ListFile"){const C=$["Input Directory"];p(C,"input",x)}else if(S==="FetchFile"){const C=$["File to Fetch"]||$.Filename;m(C||"(dynamic)","read",x)}else if(S==="PutFile"){const C=$.Directory;p(C,"output",x)}else if(S==="PutSFTP"){const C=$.Hostname||"sftp-host",A=$["Remote Path"]||"/";p(`sftp://${C}${A}`,"output",x),m(`sftp://${C}${A}/(dynamic)`,"write",x)}else if(S==="PutHDFS"||S==="PutParquet"){const C=$.Directory||$.directory;p(C,"output",x)}else if(S==="ExecuteSQL"||S==="ExecuteSQLRecord"){const A=($["SQL select query"]||$["sql-select-query"]||"").match(/(?:FROM|JOIN)\s+([\w$.{}"]+)/gi);A&&A.forEach(R=>{const y=R.replace(/^(FROM|JOIN)\s+/i,"").trim();f(y,"read",x)});const w=$["Database Connection Pooling Service"]||$["dbcp-service"];w&&c.push({name:w,processor:x,type:"read"})}else if(S==="PutDatabaseRecord"||S==="PutSQL"){const C=$["Table Name"]||$["table-name"]||$["put-db-record-table-name"];f(C,"write",x);const A=$["Database Connection Pooling Service"]||$["dbcp-service"];A&&c.push({name:A,processor:x,type:"write"})}else if(S==="ExecuteStreamCommand"){const C=$.Command||"",A=$["Command Arguments"]||"";C&&l.push({path:C,args:A,processor:x,_cloudera:null});const w=(C+" "+A).toLowerCase();if(/hdfs\s+dfs|dfs;/.test(w)||/^dfs$/.test(C.trim())){const R=A.match(/(?:-cp|-mv|-put|-get|-ls|-rm|-mkdir|-cat|-chmod|-chown|-touchz)\s*;?\s*([^\s;]+)/);R&&p(R[1],/(-ls|-cat|-get)/.test(A)?"input":"output",x);const y=A.match(/(?:-put|-cp)\s*;?\s*[^\s;]+\s*;?\s*([^\s;]+)/);y&&p(y[1],"output",x)}if(/impala-shell|impala/.test(w)){const R=A.match(/(?:refresh|invalidate\s+metadata|compute\s+stats|from|join|into|insert\s+into|insert\s+overwrite)\s+[;]?\s*([\w.${}]+)/gi);R&&R.forEach(y=>{const O=y.replace(/^(refresh|invalidate\s+metadata|compute\s+stats|from|join|into|insert\s+into|insert\s+overwrite)\s+;?\s*/i,"").trim().replace(/[";]/g,"");O&&O.length>2&&f(O,/insert|into/i.test(y)?"write":"read",x)})}if(/hive|beeline/.test(w)){const R=A.match(/(?:from|join|into|table)\s+([\w.]+)/gi);R&&R.forEach(y=>{const O=y.replace(/^(from|join|into|table)\s+/i,"").trim();O&&O.length>2&&f(O,/into/i.test(y)?"write":"read",x)})}if(/kinit|keytab|kerberos|klist/.test(w)&&l.length&&C&&(l[l.length-1]._cloudera="kerberos"),/sqoop/.test(w)){const R=A.match(/--table\s+(\S+)/);R&&f(R[1],/export/.test(w)?"write":"read",x)}if(/sqlline\.py|phoenix/.test(w)){const R=A.match(/(?:from|into|table|upsert\s+into)\s+([\w.]+)/gi);R&&R.forEach(y=>{const O=y.replace(/^(from|into|table|upsert\s+into)\s+/i,"").trim();O&&O.length>2&&f(O,/into|upsert/i.test(y)?"write":"read",x)})}if(/presto|trino/.test(w)){const R=A.match(/(?:from|join|into)\s+([\w.]+)/gi);R&&R.forEach(y=>{const O=y.replace(/^(from|join|into)\s+/i,"").trim();O&&O.length>2&&f(O,/into/i.test(y)?"write":"read",x)})}}else if(S==="InvokeHTTP"){const C=$["Remote URL"]||$["remote-url"]||"",A=$["HTTP Method"]||$["http-method"]||"GET";C&&a.push({url:C,method:A,processor:x})}else if(S==="ConsumeKafka_2_6"||S==="ConsumeKafka"||S==="ConsumeKafkaRecord_2_6"){const C=$["Topic Name(s)"]||$.topic||"",A=$["Kafka Brokers"]||$["bootstrap.servers"]||"";C&&s.push({topic:C,brokers:A,processor:x,direction:"consume"})}else if(S==="PublishKafka_2_6"||S==="PublishKafka"||S==="PublishKafkaRecord_2_6"){const C=$["Topic Name(s)"]||$.topic||"",A=$["Kafka Brokers"]||$["bootstrap.servers"]||"";C&&s.push({topic:C,brokers:A,processor:x,direction:"produce"})}else if(S==="Wait"){const C=$["Signal Counter Name"]||"",A=parseInt($["Target Signal Count"])||1;C&&(o[C]||(o[C]={senders:[],waiters:[],target:A}),o[C].waiters.push(x))}else if(S==="Notify"){const C=$["Signal Counter Name"]||"";C&&(o[C]||(o[C]={senders:[],waiters:[],target:1}),o[C].senders.push(x))}else if(S==="ControlRate"){const C="rate_"+x.replace(/\s+/g,"_");i[C]={acquirers:[x],releasers:[x]}}else S==="LogMessage"?u.push({name:"log_"+x,processor:x}):(S==="LookupAttribute"||S==="LookupRecord")&&f("lookup_"+x.replace(/\s+/g,"_").toLowerCase(),"read",x)}),(e.controllerServices||[]).forEach(P=>{const $=P.properties||{},x=$["Database Connection URL"]||$["database-connection-url"]||"";(x||P.type.includes("DBCP")||P.type.includes("ConnectionPool"))&&c.push({name:P.name,url:x?x.replace(/password=[^&;]+/gi,"password=***"):"",processor:"(controller service)",type:"service"})});const _=(e.processors||[]).filter(P=>!g.has(P.name)&&!g.has(P.id)),v=[],k=new Set;d.forEach(P=>{const $=P.expr+"|"+P.processor;k.has($)||(k.add($),v.push(P))});const E=Object.keys(n).length+Object.keys(t).length+Object.keys(r).length+Object.keys(i).length+Object.keys(o).length+a.length+s.length+l.length+c.length+u.length,D=e.clouderaTools||[];return{directories:n,files:t,sqlTables:r,tokens:i,signals:o,httpEndpoints:a,kafkaTopics:s,dbConnections:c,scripts:l,clouderaTools:D,parameters:v,counters:u,disconnected:_,disconnectedProcessors:_.map(P=>({name:P.name,type:P.type,group:P.group||"(root)",noInbound:!0,noOutbound:!0})),totalResources:E,warnings:[..._.map(P=>`Processor "${P.name}" (${P.type}) has no connections â€” may never execute`),...v.filter(P=>!P.resolved).slice(0,10).map(P=>`Unresolved expression ${P.expr} in "${P.processor}"`),...c.filter(P=>P.type!=="service").length>0&&c.filter(P=>P.type==="service").length===0?["Processors reference DB connections but no DBCP controller service found"]:[],...D.length?[`${D.length} external system references detected â€” see External Systems & Tools inventory`]:[]]}}function Xi(e,n){const t=e.data_type.toLowerCase(),r={null_ratio:e.nullable?.05:0};if(e.check_constraints&&e.check_constraints.length){const i=e.check_constraints.length;return r.top_values=e.check_constraints.map(o=>({value:o,frequency:Math.round(1/i*1e4)/1e4})),r.distinct_count=i,r}if(["int","integer","smallint","tinyint"].includes(t))e.is_primary_key?Object.assign(r,{min:1,max:n,mean:n/2,stddev:n/6,distinct_count:n}):Object.assign(r,{min:1,max:1e3,mean:500,stddev:300,distinct_count:Math.min(500,n)});else if(["bigint","long"].includes(t))Object.assign(r,{min:1,max:1e5,mean:5e4,stddev:3e4,distinct_count:Math.min(1e4,n)});else if(["float","double"].includes(t))Object.assign(r,{min:0,max:1e4,mean:100,stddev:50,distinct_count:n});else if(["decimal","numeric"].includes(t)){const i=Math.pow(10,(e.precision||10)-(e.scale||2))-1;Object.assign(r,{min:0,max:i,mean:i/10,stddev:i/20,distinct_count:n})}else["varchar","char","text","string"].includes(t)?Object.assign(r,{min_length:3,max_length:Math.min(e.max_length||50,100),distinct_count:n}):t==="date"?Object.assign(r,{min:"2020-01-01",max:"2025-12-31",distinct_count:Math.min(n,2e3)}):["timestamp","datetime"].includes(t)?Object.assign(r,{min:"2020-01-01",max:"2025-12-31",distinct_count:n}):["boolean","bool"].includes(t)?(r.top_values=[{value:!0,frequency:.5},{value:!1,frequency:.5}],r.distinct_count=2):Object.assign(r,{min_length:5,max_length:20,distinct_count:n});return r}function Lr(e){const n=typeof crypto<"u"&&crypto.randomUUID?crypto.randomUUID():"bp-"+Math.random().toString(36).substring(2,10),t=e.tables.map(i=>{const o=i.row_count||1e3,a=i.columns.map(c=>({name:c.name,data_type:c.data_type,nullable:c.nullable,is_primary_key:c.is_primary_key,stats:Xi(c,o)})),s=i.foreign_keys.map(c=>({column:c.fk_column,references_table:c.referenced_table,references_column:c.referenced_column}));return{name:i.name,schema:i.schema,row_count:o,columns:a,foreign_keys:s}}),r=[];return e.tables.forEach(i=>i.foreign_keys.forEach(o=>r.push({from_table:`${i.schema}.${i.name}`,to_table:`${i.schema}.${o.referenced_table}`,relationship_type:"one_to_many",join_columns:[{from_column:o.fk_column,to_column:o.referenced_column}]}))),{blueprint_id:n,source_system:{name:e.source_name,type:e.source_type},tables:t,relationships:r}}function Or(e){const n=e.processors||[],t=e.connections||[],r={},i={};n.forEach(c=>{r[c.name]=[],i[c.name]=[]}),t.forEach(c=>{const l=c.sourceName,d=c.destinationName;l&&d&&(r[l]||(r[l]=[]),i[d]||(i[d]=[]),r[l].includes(d)||r[l].push(d),i[d].includes(l)||i[d].push(l))});function o(c,l){const d=[],u=new Set,g=[...c[l]||[]];for(;g.length;){const p=g.shift();u.has(p)||(u.add(p),d.push(p),(c[p]||[]).forEach(m=>{u.has(m)||g.push(m)}))}return d}const a={},s={};return n.forEach(c=>{s[c.name]=o(i,c.name),a[c.name]=o(r,c.name)}),{downstream:r,upstream:i,fullDownstream:a,fullUpstream:s}}const Fn={kafka:{pip:["confluent-kafka"],dbx:"Pre-installed on DBR",desc:"Kafka"},oracle:{pip:["oracledb"],dbx:"JDBC driver via cluster library",desc:"Oracle Database"},mysql:{pip:["mysql-connector-python"],dbx:"JDBC driver via cluster library",desc:"MySQL"},postgresql:{pip:["psycopg2-binary"],dbx:"JDBC driver via cluster library",desc:"PostgreSQL"},sqlserver:{pip:["pymssql"],dbx:"JDBC driver via cluster library",desc:"SQL Server"},mongodb:{pip:["pymongo"],dbx:"MongoDB Spark Connector",desc:"MongoDB"},elasticsearch:{pip:["elasticsearch"],dbx:"Elasticsearch Spark library",desc:"Elasticsearch"},cassandra:{pip:["cassandra-driver"],dbx:"Spark Cassandra Connector",desc:"Cassandra"},redis:{pip:["redis"],dbx:"Custom library install",desc:"Redis"},hbase:{pip:[],dbx:"Delta Lake migration",desc:"HBase"},kudu:{pip:[],dbx:"Delta Lake (direct replacement)",desc:"Kudu"},s3:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS S3"},azure_blob:{pip:["azure-storage-blob"],dbx:"Pre-installed on DBR",desc:"Azure Blob"},azure_adls:{pip:["azure-storage-file-datalake"],dbx:"Pre-installed on DBR",desc:"Azure Data Lake"},gcs:{pip:["google-cloud-storage"],dbx:"GCS Spark connector",desc:"GCS"},hdfs:{pip:[],dbx:"dbutils.fs (pre-installed)",desc:"HDFS"},sftp:{pip:["paramiko"],dbx:"Volumes-based staging",desc:"SFTP/FTP"},http:{pip:["requests"],dbx:"Pre-installed on DBR",desc:"HTTP/REST"},email:{pip:["sendgrid"],dbx:"Webhook notification",desc:"Email"},mqtt:{pip:["paho-mqtt"],dbx:"Custom library",desc:"MQTT"},jms:{pip:["stomp.py"],dbx:"Custom library",desc:"JMS/AMQP"},snowflake:{pip:["snowflake-connector-python"],dbx:"Snowflake Spark Connector",desc:"Snowflake"},neo4j:{pip:["neo4j"],dbx:"Neo4j Spark Connector",desc:"Neo4j"},splunk:{pip:["splunklib"],dbx:"Splunk Spark Add-on",desc:"Splunk"},influxdb:{pip:["influxdb-client"],dbx:"Custom library",desc:"InfluxDB"},solr:{pip:["pysolr"],dbx:"Solr Spark library",desc:"Solr"},hive:{pip:[],dbx:"Pre-installed (Spark SQL)",desc:"Hive"},iceberg:{pip:[],dbx:"Pre-installed on DBR 13+",desc:"Iceberg"},teradata:{pip:["teradatasql"],dbx:"JDBC driver",desc:"Teradata"},slack:{pip:["slack-sdk"],dbx:"Webhook integration",desc:"Slack"},kerberos:{pip:[],dbx:"Unity Catalog identity federation",desc:"Kerberos"},azure_eventhub:{pip:["azure-eventhub"],dbx:"Pre-installed on DBR",desc:"Azure Event Hubs"},azure_servicebus:{pip:["azure-servicebus"],dbx:"Custom library install",desc:"Azure Service Bus"},azure_cosmos:{pip:[],dbx:"Pre-installed (Cosmos Spark connector)",desc:"Azure Cosmos DB"},azure_queue:{pip:["azure-storage-queue"],dbx:"Custom library install",desc:"Azure Queue Storage"},gcp_pubsub:{pip:["google-cloud-pubsub"],dbx:"Custom library install",desc:"GCP Pub/Sub"},gcp_bigquery:{pip:["google-cloud-bigquery"],dbx:"BigQuery Spark connector",desc:"GCP BigQuery"},clickhouse:{pip:["clickhouse-driver"],dbx:"ClickHouse JDBC driver",desc:"ClickHouse"},druid:{pip:[],dbx:"Druid JDBC driver",desc:"Apache Druid"},hudi:{pip:[],dbx:"Pre-installed on DBR 13+",desc:"Apache Hudi"},kinesis:{pip:["boto3"],dbx:"Kinesis Spark connector",desc:"AWS Kinesis"},cloudwatch:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS CloudWatch"},sqs:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS SQS"},sns:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS SNS"},dynamodb:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS DynamoDB"},lambda_aws:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS Lambda"},pagerduty:{pip:["pdpyras"],dbx:"Custom library install",desc:"PagerDuty"},opsgenie:{pip:["opsgenie-sdk"],dbx:"Custom library install",desc:"OpsGenie"},telegram:{pip:[],dbx:"HTTP API (no SDK needed)",desc:"Telegram"},geoip:{pip:["geoip2"],dbx:"Custom library install",desc:"GeoIP"},exchange:{pip:["exchangelib"],dbx:"Custom library install",desc:"Microsoft Exchange"},whois:{pip:["python-whois"],dbx:"Custom library install",desc:"WHOIS"},snmp:{pip:["pysnmp"],dbx:"Custom library install",desc:"SNMP"},datadog:{pip:["datadog-api-client"],dbx:"Datadog integration",desc:"Datadog"},prometheus:{pip:["prometheus-client"],dbx:"Custom library install",desc:"Prometheus"},grafana:{pip:[],dbx:"Grafana REST API",desc:"Grafana"},phoenix:{pip:[],dbx:"Phoenix JDBC driver",desc:"Apache Phoenix"},cockroachdb:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"CockroachDB"},timescaledb:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"TimescaleDB"},greenplum:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"Greenplum"},vertica:{pip:["vertica-python"],dbx:"Vertica JDBC driver",desc:"Vertica"},saphana:{pip:["hdbcli"],dbx:"SAP HANA JDBC driver",desc:"SAP HANA"},presto:{pip:[],dbx:"Presto JDBC driver",desc:"Presto"},trino:{pip:["trino"],dbx:"Trino JDBC driver",desc:"Trino"}},Yi={ClickHouse:"clickhouse",Druid:"druid",Hudi:"hudi",Kinesis:"kinesis",CloudWatch:"cloudwatch",SQS:"sqs",SNS:"sns",DynamoDB:"dynamodb",Lambda:"lambda_aws",PagerDuty:"pagerduty",OpsGenie:"opsgenie",GeoIP:"geoip",Exchange:"exchange",SNMP:"snmp",Datadog:"datadog",Prometheus:"prometheus",Grafana:"grafana",Phoenix:"phoenix",CockroachDB:"cockroachdb",TimescaleDB:"timescaledb",Greenplum:"greenplum",Vertica:"vertica",SAPHANA:"saphana",Presto:"presto",Trino:"trino",CosmosDB:"azure_cosmos",ServiceBus:"azure_servicebus",EventHub:"azure_eventhub",PubSub:"gcp_pubsub",BigQuery:"gcp_bigquery",Kafka:"kafka",Oracle:"oracle",MySQL:"mysql",Postgres:"postgresql",Mongo:"mongodb",Elastic:"elasticsearch",Cassandra:"cassandra",Redis:"redis",HBase:"hbase",Kudu:"kudu",S3:"s3",AzureBlob:"azure_blob",ADLS:"azure_adls",GCS:"gcs",HDFS:"hdfs",SFTP:"sftp",FTP:"sftp",HTTP:"http",InvokeHTTP:"http",Email:"email",MQTT:"mqtt",JMS:"jms",AMQP:"jms",Snowflake:"snowflake",Neo4j:"neo4j",Splunk:"splunk",InfluxDB:"influxdb",Solr:"solr",Hive:"hive",Iceberg:"iceberg",Teradata:"teradata",Syslog:"syslog",Slack:"slack"};function Nr(e){const n=new Set;for(const[t,r]of Object.entries(Yi))e.includes(t)&&n.add(r);return[...n].map(t=>Fn[t]).filter(Boolean)}function Hn(e){const n={},t=e.processors||[],r=e.deepPropertyInventory||{},i=[[/Kafka/i,"Apache Kafka","kafka"],[/Oracle/i,"Oracle Database","oracle"],[/MySQL/i,"MySQL","mysql"],[/Postgres/i,"PostgreSQL","postgresql"],[/Mongo/i,"MongoDB","mongodb"],[/Elastic/i,"Elasticsearch","elasticsearch"],[/Cassandra/i,"Cassandra","cassandra"],[/HBase/i,"HBase","hbase"],[/Kudu/i,"Apache Kudu","kudu"],[/Hive/i,"Hive","hive"],[/HDFS|Hadoop/i,"HDFS","hdfs"],[/S3/i,"AWS S3","s3"],[/Azure.*Blob/i,"Azure Blob","azure_blob"],[/Azure.*Lake|ADLS/i,"Azure Data Lake","azure_adls"],[/Azure.*Event/i,"Azure Event Hubs","azure_eventhub"],[/GCS|BigQuery/i,"Google Cloud","gcs"],[/Snowflake/i,"Snowflake","snowflake"],[/Redis/i,"Redis","redis"],[/Solr/i,"Solr","solr"],[/SFTP|FTP/i,"SFTP/FTP","sftp"],[/HTTP|REST/i,"HTTP/REST","http"],[/JMS/i,"JMS","jms"],[/AMQP/i,"AMQP","amqp"],[/MQTT/i,"MQTT","mqtt"],[/Email|SMTP/i,"Email","email"],[/Syslog/i,"Syslog","syslog"],[/Slack/i,"Slack","slack"],[/Splunk/i,"Splunk","splunk"],[/InfluxDB/i,"InfluxDB","influxdb"],[/Neo4j/i,"Neo4j","neo4j"],[/Teradata/i,"Teradata","teradata"],[/Iceberg/i,"Iceberg","iceberg"],[/SQL|Database|JDBC/i,"SQL/JDBC","sql_jdbc"]];return t.forEach(o=>{const a=/^(Get|List|Consume|Listen|Fetch|Query|Scan|Select)/i.test(o.type)?"READ":/^(Put|Publish|Send|Post|Insert|Write|Delete)/i.test(o.type)?"WRITE":"READ/WRITE";i.forEach(([s,c,l])=>{s.test(o.type)&&(n[l]||(n[l]={name:c,key:l,processors:[],jdbcUrls:[],credentials:[],packages:[]}),n[l].processors.push({name:o.name,type:o.type,direction:a,group:o.group}))})}),r.jdbcUrls&&Object.keys(r.jdbcUrls).forEach(o=>{const a=o.match(/jdbc:(\w+):/);if(a){const s=a[1].toLowerCase(),c={oracle:"oracle",mysql:"mysql",postgresql:"postgresql",sqlserver:"sqlserver",hive2:"hive",teradata:"teradata",snowflake:"snowflake"}[s]||"sql_jdbc";n[c]||(n[c]={name:Fn[c]?Fn[c].desc:c,key:c,processors:[],jdbcUrls:[],credentials:[],packages:[]}),n[c].jdbcUrls.push(o)}}),r.credentialRefs&&Object.keys(r.credentialRefs).forEach(o=>{Object.values(n).forEach(a=>{const s=a.processors.map(l=>l.name),c=r.credentialRefs[o];Array.isArray(c)&&c.some(l=>s.includes(l))&&a.credentials.push(o)})}),Object.values(n).forEach(o=>{const a=Fn[o.key];o.dbxApproach=a?a.dbx:"Custom implementation",o.packages=a?a.pip:[]}),n}function ve(e){return e?e.replace(/[^a-zA-Z0-9_]/g,"_").replace(/^(\d)/,"_$1").toLowerCase().substring(0,40):"_empty"}function ec(e,n,t,r,i,o,a,s){let c=e.tpl.replace(/\{v\}/g,n).replace(/\{in\}/g,t).replace(/\{in1\}/g,t).replace(/\{in2\}/g,r[1]?ve(r[1]):"input2");return Object.entries(i).forEach(([l,d])=>{const u=l.replace(/\s+/g,"_").toLowerCase();c=c.replace(new RegExp("\\{"+u+"\\}","gi"),d)}),c.includes("${")&&(c=o(c,"python")),c.includes("${")&&(c=a(c,s)),{code:c,conf:e.conf}}const tc=/^(ConsumeKafka|ConsumeKafkaRecord|ListenHTTP|ListenTCP|ListenUDP|ListenSyslog|ListenRELP|ListenSMTP|ListenGRPC|ListenWebSocket|ConsumeJMS|ConsumeMQTT|ConsumeAMQP|ConsumeGCPubSub|ConsumeAzureEventHub|ConsumeAzureServiceBus|ConsumeKinesisStream|TailFile|GetHTTP|GenerateFlowFile)/,nc=/\.toPandas\(\)|for\s+row\s+in\s+df_\w+\.(?:limit\(\d+\)\.)?collect\(\)|df_\w+\.limit\(\d+\)\.toPandas|df_\w+\.count\(\)|\.write\.format\(|\.saveAsTable\(|\.save\(|\.show\(|\.display\(/,rc=/^(ExecuteSQL|PutDatabaseRecord|PutFile|PutFTP|PutSFTP|PutHDFS|FetchFile|QueryDatabaseTable|ListDatabaseTables|GenerateTableFetch)$/;function sc(e,n,t,r,i){if(!i||!nc.test(e))return e;let o=e;if(/for\s+row\s+in\s+df_\w+/.test(o)){const a=o.indexOf("for row");if(a>=0){const s=o.substring(0,a),l=o.substring(a).split(`
`).map(d=>"    "+d).join(`
`);o=s+`# Streaming-safe sink using foreachBatch
def _process_batch_`+t+`(batch_df, batch_id):
    """Process each micro-batch (streaming-safe)"""
`+l.replace(/df_\w+\.(?:limit\(\d+\)\.)?collect\(\)/g,"batch_df.collect()")+`

(df_`+r+`.writeStream
    .foreachBatch(_process_batch_`+t+`)
    .option("checkpointLocation", "/Volumes/<catalog>/<schema>/data/checkpoints/`+t+`")
    .trigger(processingTime="10 seconds")
    .start()
)`}}return/\.toPandas\(\)\.to_dict/.test(o)&&(o=o.replace(/df_(\w+)(?:\.limit\(\d+\))?\.toPandas\(\)\.to_dict\(orient="records"\)/g,"None  # Resolved in foreachBatch below"),o=`# Streaming-safe: collect via foreachBatch, not .toPandas()
def _process_batch_`+t+`(batch_df, batch_id):
    _records = batch_df.toPandas().to_dict(orient="records")
    # Process _records here
    return _records

`+o),o=o.replace(/df_(\w+)\.count\(\)/g,"0  # Cannot call .count() on streaming DataFrame; use watermark/window aggregation"),o=o.replace(/df_(\w+)\.show\([^)]*\)/g,"# Cannot call .show() on streaming DataFrame â€” use display() in notebook"),o=o.replace(/display\(df_(\w+)\)/g,"# display() streams automatically in Databricks notebooks"),/\.write\.format\(/.test(o)&&!/\.writeStream/.test(o)&&(o=o.replace(/(\w+)\.write\.format\(([^)]+)\)((?:\s*\.option\([^)]+\))*)\s*\.(?:save|mode)\(/g,function(a,s,c,l){return s+".writeStream.format("+c+")"+l+`
    .option("checkpointLocation", "/Volumes/<catalog>/<schema>/checkpoints/`+t+`")
    .trigger(availableNow=True)
    .start(`})),/\.write\.(?:mode\([^)]+\)\.)?saveAsTable\(/.test(o)&&!/\.writeStream/.test(o)&&(o=o.replace(/\.write\.(?:mode\([^)]+\)\.)?saveAsTable\(([^)]+)\)/g,`.writeStream
    .option("checkpointLocation", "/Volumes/<catalog>/<schema>/checkpoints/`+t+`")
    .trigger(availableNow=True)
    .toTable($1)`)),/\.writeStream/.test(o)&&!/checkpointLocation/.test(o)&&(o=o.replace(/\.writeStream/g,`.writeStream
    .option("checkpointLocation", "/Volumes/<catalog>/<schema>/checkpoints/`+t+'")')),o}function oc(e,n){const t=new Set;e.forEach(o=>{tc.test(o.type)&&t.add(o.name)});const r={};e.forEach(o=>{r[o.name]=o.type});let i=!0;for(;i;)i=!1,n.forEach(o=>{if(t.has(o.sourceName)&&!t.has(o.destinationName)){const a=r[o.destinationName]||"";if(rc.test(a))return;t.add(o.destinationName),i=!0}});return t}function ac(e,n){if(!n||!e)return null;const t=n.find(o=>o.name===e||o.type.includes(e));if(!t)return null;const r=t.properties||{},i={name:t.name,type:t.type,props:{}};return/DBCP|ConnectionPool/i.test(t.type)&&(i.jdbcUrl=r["Database Connection URL"]||"",i.driver=r["Database Driver Class Name"]||"",i.user=r["Database User"]||"",i.maxConns=r["Max Total Connections"]||"10",/oracle/i.test(i.jdbcUrl)?i.dbType="oracle":/postgresql/i.test(i.jdbcUrl)?i.dbType="postgresql":/mysql/i.test(i.jdbcUrl)?i.dbType="mysql":/sqlserver/i.test(i.jdbcUrl)?i.dbType="sqlserver":/hive/i.test(i.jdbcUrl)?i.dbType="hive":i.dbType="jdbc"),/SSL/i.test(t.type)&&(i.keystore=r["Keystore Filename"]||"",i.truststore=r["Truststore Filename"]||"",i.protocol=r["SSL Protocol"]||"TLS"),/Cache/i.test(t.type)&&(i.cacheHost=r["Server Hostname"]||"<cache_host>",i.cachePort=r["Server Port"]||"4557",i.cacheHostSecret='dbutils.secrets.get(scope="cache", key="host")'),/Reader|Writer/i.test(t.type)&&(i.schemaStrategy=r["Schema Access Strategy"]||"Infer Schema",/CSV/i.test(t.type)?i.format="csv":/Json/i.test(t.type)?i.format="json":/Avro/i.test(t.type)?i.format="avro":/Parquet/i.test(t.type)&&(i.format="parquet")),i}function ic(e,n,t){if(!e||!e.includes("${"))return e;var r=e.split(`
`),i=r.map(function(a){if(a.trim().charAt(0)==="#"||!a.includes("${"))return a;var s=t(a,"python");return s}),o=i.join(`
`);return o.indexOf("_attrs.get(")>=0&&o.indexOf("_attrs =")<0&&o.indexOf("_attrs=")<0&&(o=`# Resolve NiFi FlowFile attributes from upstream DataFrame
_attrs = {}  # Populated at runtime from flowfile attribute columns

`+o),o}function cc(e,n,t,r,i,o){let a=i,s=o;if(e.type==="ListenHTTP"||e.type==="HandleHttpRequest"){const c=n["Listening Port"]||n.Port||"8080",l=n["Base Path"]||"/api/v1";return a="# HTTP Endpoint: "+e.name+`
# NiFi HTTP listener on port `+c+", path: "+l+`
#
# DO NOT run a blocking web server (Flask/FastAPI) in a notebook cell.
# It will hang indefinitely and block all downstream execution.
#
# OPTION 1 (RECOMMENDED): Databricks Model Serving Endpoint
# Deploy as an MLflow model serving endpoint that writes to Delta table.
# See: https://docs.databricks.com/machine-learning/model-serving/
#
# OPTION 2: Databricks Apps (Gradio/Streamlit on port 8080)
# See: databricks.yml app deployment
#
# OPTION 3: External API Gateway -> Databricks Job trigger
# AWS API Gateway / Azure APIM -> triggers Databricks Job via REST API
#
# Implementation: Read from Delta landing table (populated by serving endpoint)
df_`+t+` = (spark.readStream
    .format("delta")
    .table("`+t+`_incoming")
)
print(f"[HTTP] Endpoint: streaming from `+t+'_incoming Delta table")',s=.92,{code:a,conf:s}}if(/^Consume(Kafka|KafkaRecord)/.test(e.type)){const c=n["Kafka Brokers"]||n["bootstrap.servers"]||"kafka:9092",l=n["Topic Name(s)"]||n.topic||"default_topic",d=n["Group ID"]||n["group.id"]||"consumer_group",u=n["Offset Reset"]||n["auto.offset.reset"]||"earliest",g=n["Security Protocol"]||"";let p="";return g.includes("SASL")&&(p=`
  .option("kafka.security.protocol", "${g}")
  .option("kafka.sasl.mechanism", "PLAIN")
  .option("kafka.sasl.jaas.config", f"org.apache.kafka.common.security.plain.PlainLoginModule required username='{_kafka_user}' password='{_kafka_pass}';")
`),a=`# Kafka Consumer: ${e.name}
# Topic: ${l} | Group: ${d} | Brokers: ${c}
_kafka_user = dbutils.secrets.get(scope='kafka', key='user')
_kafka_pass = dbutils.secrets.get(scope='kafka', key='pass')
df_${t} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "${c}")
  .option("subscribe", "${l}")
  .option("kafka.group.id", "${d}")
  .option("startingOffsets", "${u}")
  .option("maxOffsetsPerTrigger", 10000)${p}
  .load()
  .selectExpr("CAST(key AS STRING) as key", "CAST(value AS STRING) as value", "topic", "partition", "offset", "timestamp")
)
print(f"[KAFKA] Consuming from ${l} with group ${d}")`,s=.95,{code:a,conf:s}}if(/^(Get|Fetch|List)(SFTP|FTP)$/.test(e.type)){const c=n.Hostname||"sftp.example.com",l=n.Port||"22",d=n.Username||"sftp_user",u=n["Remote Path"]||"/",g=n["File Filter Regex"]||n["File Filter"]||".*";return a=`# ${e.type}: ${e.name}
# Host: ${c}:${l} | Path: ${u} | Filter: ${g}
import paramiko
_ssh = paramiko.SSHClient()
_ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
_ssh.connect("${c}", port=int("${l}"), username="${d}",
    password=dbutils.secrets.get(scope="sftp", key="password"))
_sftp = _ssh.open_sftp()

import re as _re
_files = [f for f in _sftp.listdir("${u}") if _re.match(r"${g}", f)]
_data = []
for _fname in _files:
    with _sftp.open(f"${u}/{_fname}") as _f:
        _data.append({"filename": _fname, "content": _f.read().decode("utf-8", errors="replace")})

df_${t} = spark.createDataFrame(_data) if _data else spark.createDataFrame([], "filename STRING, content STRING")
_sftp.close()
_ssh.close()
print(f"[SFTP] Fetched {len(_files)} files from ${c}:${u}")`,s=.9,{code:a,conf:s}}if(/^(ExecuteSQL|QueryDatabase|GenerateTableFetch)/.test(e.type)){const c=n["Database Connection Pooling Service"]||n["JDBC Connection Pool"]||"",l=n["SQL select query"]||n["SQL Statement"]||"",d=n["Table Name"]||"",u=n["Max Rows Per Flow File"]||"0";let g="jdbc:database://host:port/db",p="com.database.Driver";if(/oracle/i.test(c)?(g="jdbc:oracle:thin:@db_host:1521:db_sid",p="oracle.jdbc.driver.OracleDriver"):/postgres/i.test(c)?(g="jdbc:postgresql://pg_host:5432/pg_db",p="org.postgresql.Driver"):/mysql/i.test(c)?(g="jdbc:mysql://mysql_host:3306/mysql_db",p="com.mysql.cj.jdbc.Driver"):/hive/i.test(c)&&(g="",p=""),g){const m=l?'"('+l.replace(/"/g,'\\"').substring(0,200)+') AS subq"':`"${d}"`;a=`# SQL Query: ${e.name}
# Pool: ${c} | Table: ${d||"(custom query)"}
df_${t} = (spark.read
  .format("jdbc")
  .option("url", "${g}")
  .option("dbtable", ${m})
  .option("driver", "${p}")
  .option("user", dbutils.secrets.get(scope="db", key="user"))
  .option("password", dbutils.secrets.get(scope="db", key="pass"))`+(u!=="0"?`
  .option("fetchsize", "${u}")`:"")+`
  .load()
)
print(f"[SQL] Read from ${d||"query"}")`}else{const f=(l||"SELECT * FROM "+d).replace(/\\/g,"\\\\").replace(/"/g,'\\"').replace(/\n/g," ");a=`# SQL Query: ${e.name} (via Hive/Spark SQL)
df_${t} = spark.sql("${f}")
print("[SQL] Read from Hive/Spark SQL")`}return s=.92,{code:a,conf:s}}if(e.type==="GenerateFlowFile"){const c=n["Batch Size"]||"1";return a=`# Generate Test Data: ${e.name}
from pyspark.sql.functions import current_timestamp, lit
df_${t} = spark.range(${c}).toDF("id")
df_${t} = df_${t}.withColumn("_generated_at", current_timestamp())
print(f"[GEN] Generated ${c} test records")`,s=.95,{code:a,conf:s}}if(e.type==="TailFile"){const c=n["File(s) to Tail"]||n["File to Tail"]||"/var/log/app.log";return a=`# TailFile: ${e.name}
# File: ${c}
# In Databricks, use Auto Loader for continuous file ingestion
df_${t} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "text")
  .load("${c.replace(/[^/]*$/,"")}")
)
print(f"[TAIL] Streaming from ${c}")`,s=.92,{code:a,conf:s}}if(e.type==="ConsumeKinesisStream"){const c=n["Kinesis Stream Name"]||n["Amazon Kinesis Stream Name"]||"stream",l=n.Region||"us-east-1";return a=`# Kinesis: ${e.name}
df_${t} = (spark.readStream
  .format("kinesis")
  .option("streamName", "${c}")
  .option("region", "${l}")
  .option("initialPosition", "TRIM_HORIZON")
  .load())`,s=.92,{code:a,conf:s}}if(e.type==="CaptureChangeMySQL"){const c=n["MySQL Hostname"]||n.Hosts||"mysql_host",l=n["MySQL Port"]||n.Port||"3306",d=n["Database/Schema"]||n.Database||"source_db",u=n.Table||n["Table Name Pattern"]||"source_table";return n["Server ID"],a=`# CaptureChangeMySQL (CDC): ${e.name}
# MySQL Host: ${c}:${l} | DB: ${d} | Table: ${u}
#
# OPTION 1 (RECOMMENDED): Use Auto Loader to ingest CDC events from a landing zone
# Assumes MySQL CDC events (e.g., from Debezium) land as JSON files in a Volume
df_${t} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", "/Volumes/<catalog>/<schema>/tmp/cdc_schema/${d}_${u}")
  .option("cloudFiles.inferColumnTypes", "true")
  .load("/Volumes/<catalog>/<schema>/cdc/${d}/${u}/")
)
print(f"[CDC] Auto Loader ingesting MySQL CDC events for ${d}.${u}")

# OPTION 2: Read directly from MySQL via JDBC (batch, not streaming)
# df_${t} = (spark.read
#   .format("jdbc")
#   .option("url", f"jdbc:mysql://{dbutils.secrets.get(scope='mysql', key='host')}:${l}/${d}")
#   .option("dbtable", "${u}")
#   .option("driver", "com.mysql.cj.jdbc.Driver")
#   .option("user", dbutils.secrets.get(scope="mysql", key="user"))
#   .option("password", dbutils.secrets.get(scope="mysql", key="pass"))
#   .load()
# )`,s=.85,{code:a,conf:s}}return null}function lc(e,n,t,r,i,o,a){let s=i,c=o;if(e.type==="UpdateAttribute"){const l=new Set(["Delete Attributes Expression","Store State","Stateful Variables Initial Value","canonical-value-lookup-cache-size"]),d=Object.entries(n).filter(([u])=>!l.has(u));if(d.length){const u=["from pyspark.sql.functions import col, lit, upper, lower, trim, length, substring, regexp_replace, concat, when, current_timestamp, date_format, to_timestamp, expr, rand, substring_index, lpad, rpad, locate, split, regexp_extract, round, abs, ceil, floor",`# UpdateAttribute: ${e.name} â€” set DataFrame columns via NEL expressions`,`df_${t} = df_${r}`];return d.forEach(([g,p])=>{const m=g.replace(/[^a-zA-Z0-9_]/g,"_").toLowerCase();if(p.includes("${")){const f=a(p,"col");u.push(`df_${t} = df_${t}.withColumn("${m}", ${f})  # NEL: ${p.substring(0,80).replace(/"/g,"'")}`)}else u.push(`df_${t} = df_${t}.withColumn("${m}", lit("${p.replace(/"/g,'\\"')}"))  # ${g}`)}),s=u.join(`
`),c=.92,{code:s,conf:c}}}if(e.type==="EvaluateJsonPath"){n.Destination;const l=Object.entries(n).filter(([d])=>!["Destination","Return Type","Null Value Representation","Path Not Found Behavior"].includes(d));if(l.length){const d=[`# JSON Path Evaluation: ${e.name}`,"from pyspark.sql.functions import col, get_json_object"];return d.push(`df_${t} = df_${r}`),l.forEach(([u,g])=>{const p=u.replace(/[^a-zA-Z0-9_]/g,"_"),m=g.replace(/^\$/,"$");d.push(`df_${t} = df_${t}.withColumn("${p}", get_json_object(col("value"), "${m}"))`)}),d.push(`print(f"[JSON] Extracted ${l.length} fields from JSON")`),s=d.join(`
`),c=.93,{code:s,conf:c}}}if(e.type==="JoltTransformJSON"){const l=n["Jolt Specification"]||"[]",d=n["Jolt Transformation DSL"]||"Chain";return s=`# Jolt Transform: ${e.name}
# DSL: ${d}
# Spec: ${l.substring(0,100)}...
from pyspark.sql.functions import col, from_json, to_json, struct, lit
import json

_jolt_spec = json.loads('${l.replace(/\\/g,"\\\\").replace(/'/g,"\\'").replace(/\r?\n/g," ")}')
df_${t} = df_${r}
for op in (_jolt_spec if isinstance(_jolt_spec, list) else [_jolt_spec]):
    _operation = op.get("operation", "")
    _spec = op.get("spec", {})
    if _operation == "shift":
        for src, dst in _spec.items():
            if src != "*" and isinstance(dst, str):
                df_${t} = df_${t}.withColumnRenamed(src, dst)
            elif src == "*" and isinstance(dst, str):
                # Wildcard shift: rename all to nested path
                for c in df_${t}.columns:
                    df_${t} = df_${t}.withColumnRenamed(c, f"{dst}.{c}")
    elif _operation == "default":
        for k, v in _spec.items():
            if isinstance(v, (str, int, float)):
                df_${t} = df_${t}.withColumn(k, lit(v))
    elif _operation == "remove":
        for k in _spec.keys():
            if k in df_${t}.columns:
                df_${t} = df_${t}.drop(k)
    elif _operation == "modify-overwrite-beta":
        for k, v in _spec.items():
            if isinstance(v, str) and v.startswith("="):
                df_${t} = df_${t}.withColumn(k, lit(v[1:]))
print(f"[JOLT] Applied {len(_jolt_spec) if isinstance(_jolt_spec, list) else 1} transformation(s)")`,c=.9,{code:s,conf:c}}if(e.type==="JoltTransformRecord")return s=`# Jolt Record: ${e.name}
df_${t} = df_${r}
# Apply Jolt-equivalent column renames/transforms
print(f"[JOLT] Record transformation applied")`,c=.9,{code:s,conf:c};if(e.type==="ConvertRecord"){const l=n["Record Reader"]||"CSVReader",d=n["Record Writer"]||"JsonRecordSetWriter",u=/CSV/i.test(l)?"csv":/Avro/i.test(l)?"avro":/Json/i.test(l)?"json":"csv",g=/CSV/i.test(d)?"csv":/Avro/i.test(d)?"avro":(/Json/i.test(d),"json");return s=`# Format Conversion: ${e.name}
# ${l} -> ${d} (${u} -> ${g})
df_${t} = df_${r}  # Spark DataFrames are format-agnostic
# Write example: df_${t}.write.format("${g}").save("/path/to/output")
print(f"[CONVERT] ${u} -> ${g}")`,c=.93,{code:s,conf:c}}if(/^Merge(Content|Record)$/.test(e.type)){const l=n["Merge Strategy"]||"Bin-Packing",d=n["Minimum Number of Entries"]||"1",u=n["Maximum Number of Entries"]||"1000",g=n["Merge Format"]||"Binary Concatenation";return s=`# Merge: ${e.name}
# Strategy: ${l} | Entries: ${d}-${u} | Format: ${g}
# Enable Delta write optimizations for small file compaction
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.minNumFiles", ${d})

# Coalesce partitions to reduce file count
_num_parts = max(1, df_${r}.rdd.getNumPartitions() // 4)
df_${t} = df_${r}.coalesce(_num_parts)
# Post-write: run OPTIMIZE on target table for best performance
# spark.sql("OPTIMIZE <catalog>.<schema>.<table> ZORDER BY (<key_column>)")
print(f"[MERGE] Coalesced to {_num_parts} partitions â€” Delta Auto Optimize + Auto Compaction enabled")`,c=.93,{code:s,conf:c}}if(e.type==="SplitJson"){const l=n["JsonPath Expression"]||"$.*",d=l.replace(/^\$\.?\*?/,"$"),u=["from pyspark.sql.functions import explode, col, from_json, get_json_object","from pyspark.sql.types import ArrayType, StringType",`# SplitJson: ${e.name}`,`# JsonPath: ${l}`];return l==="$"||l==="$.*"||l==="$[*]"?(u.push("# Top-level array â€” explode directly"),u.push(`df_${t} = df_${r}.withColumn("_items", from_json(col("value"), ArrayType(StringType())))`),u.push(`df_${t} = df_${t}.withColumn("_item", explode(col("_items"))).drop("_items")`)):(u.push("# Extract nested array then explode"),u.push(`df_${t} = df_${r}.withColumn("_nested", get_json_object(col("value"), "${d||"$"}"))`),u.push(`df_${t} = df_${t}.withColumn("_items", from_json(col("_nested"), ArrayType(StringType())))`),u.push(`df_${t} = df_${t}.withColumn("value", explode(col("_items"))).drop("_nested", "_items")`)),u.push("# Note: For complex nested structures, define explicit schema instead of StringType()"),u.push('print(f"[SPLIT] JSON array exploded into individual rows")'),s=u.join(`
`),c=.92,{code:s,conf:c}}if(e.type==="SplitContent"){const l=n["Byte Sequence"]||n["Line Split Count"]||"",d=n["Byte Sequence Format"]||"UTF-8",u=["from pyspark.sql.functions import explode, split, col, lit",`# SplitContent: ${e.name}`,`# Byte Sequence: ${l||"(newline)"} | Format: ${d}`],p=(l||"\\n").replace(/\\/g,"\\\\").replace(/"/g,'\\"');return u.push(`df_${t} = df_${r}.withColumn("_parts", split(col("value"), "${p}"))`),u.push(`df_${t} = df_${t}.withColumn("value", explode(col("_parts"))).drop("_parts")`),u.push(`df_${t} = df_${t}.filter(col("value") != lit(""))  # Remove empty splits`),u.push('print(f"[SPLIT] Content split into individual rows by delimiter")'),s=u.join(`
`),c=.92,{code:s,conf:c}}if(/^Split(Text|Xml|Record|Avro)$/.test(e.type)){const l=e.type.replace("Split","").toLowerCase();return s=`# Split: ${e.name}
# In Databricks, Spark reads entire ${l} datasets as DataFrames.
df_${t} = df_${r}  # Already partitioned across Spark executors
from pyspark.sql.functions import explode, col
print(f"[SPLIT] ${l} data already distributed across partitions")`,c=.92,{code:s,conf:c}}if(/^(Compress|Unpack)Content$/.test(e.type)){const l=n.Mode||(e.type==="CompressContent"?"compress":"decompress"),d=n["Compression Format"]||n["Compression Level"]||"gzip",g={gzip:"gzip",bzip2:"bzip2",snappy:"snappy",lz4:"lz4",zstd:"zstd",deflate:"deflate",lzo:"lzo",none:"none"}[d.toLowerCase()]||"snappy";return e.type==="CompressContent"?s=`# CompressContent: ${e.name}
# Mode: ${l} | Format: ${d}
# Set Spark compression codec for Parquet/Delta writes
spark.conf.set("spark.sql.parquet.compression.codec", "${g}")
spark.conf.set("spark.sql.orc.compression.codec", "${g==="gzip"?"zlib":g}")
df_${t} = df_${r}
print(f"[COMPRESS] Codec set to ${g} for downstream writes")`:s=`# UnpackContent: ${e.name}
# Mode: ${l} | Format: ${d}
# Spark auto-detects compression when reading (gzip, snappy, bzip2, etc.)
df_${t} = df_${r}
print(f"[DECOMPRESS] Spark auto-detects ${d} compression on read")`,c=.95,{code:s,conf:c}}if(e.type==="EncryptContent"){const l=n["Encryption Algorithm"]||"AES/GCM/NoPadding";return/AES/i.test(l)?s=`# Encryption: ${e.name}
# Algorithm: ${l}
from pyspark.sql.functions import col, lit, base64, aes_encrypt

# Retrieve encryption key from Databricks secret scope (never hardcode keys)
_enc_key = dbutils.secrets.get(scope="encryption", key="aes-key")

df_${t} = df_${r}
for _col in df_${r}.columns:
    if _col not in ["id", "key", "timestamp"]:
        df_${t} = df_${t}.withColumn(_col,
            base64(aes_encrypt(col(_col).cast("string"), lit(_enc_key), lit("GCM"), lit("DEFAULT"))))
print(f"[ENCRYPT] AES-GCM encryption applied via aes_encrypt() with secret scope key")`:s=`# Encryption: ${e.name}
# Algorithm: ${l}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType

_key_str = dbutils.secrets.get(scope="encryption", key="fernet-key")

@udf(StringType())
def encrypt_value(val):
    if val is None: return None
    from cryptography.fernet import Fernet as _F
    return _F(_key_str.encode()).encrypt(val.encode()).decode()

df_${t} = df_${r}
for _col in df_${r}.columns:
    if _col not in ["id", "key", "timestamp"]:
        df_${t} = df_${t}.withColumn(_col, encrypt_value(col(_col)))
print(f"[ENCRYPT] ${l} encryption applied")`,c=.9,{code:s,conf:c}}if(e.type==="ExecuteScript"){const l=n["Script Engine"]||"python",d=n["Script Body"]||"",u=n["Script File"]||"",g=d.length||0,p=e.name.replace(/"/g,"'"),m=d.split(/\r?\n/),f=m.slice(0,20).map(v=>`# ${v}`);m.length>20&&f.push(`# ... (${m.length-20} more lines)`);const h=f.join(`
`),_=d?`Script Body (${g} chars)`:`Script File: ${u}`;return s=`# ExecuteScript: ${p}
# Engine: ${l} | ${_}
# Original ${l} script (${g} chars):
${h}

raise NotImplementedError(
    f"ExecuteScript '${p}' contains ${l} logic that requires manual porting to PySpark. "
    f"See the commented script above (${g} chars). "
    f"Consider using a PySpark UDF or pandas_udf for the equivalent logic."
)`,c=.15,{code:s,conf:c}}if(e.type==="ExecuteGroovyScript"){const l=n["Script Body"]||"",d=n["Script File"]||"",u=l.length||0,g=e.name.replace(/"/g,"'"),p=l.split(/\r?\n/),m=p.slice(0,20).map(_=>`# ${_}`);p.length>20&&m.push(`# ... (${p.length-20} more lines)`);const f=m.join(`
`),h=l?`Script Body (${u} chars)`:`Script File: ${d}`;return s=`# ExecuteGroovyScript: ${g}
# Engine: Groovy | ${h}
# Original Groovy script (${u} chars):
${f}

raise NotImplementedError(
    f"ExecuteGroovyScript '${g}' contains Groovy logic that requires manual porting to PySpark. "
    f"See the commented script above (${u} chars). "
    f"Consider using a PySpark UDF or pandas_udf for the equivalent logic."
)`,c=.15,{code:s,conf:c}}if(e.type==="ReplaceText"&&!s.includes("regexp_replace")){const l=n["Search Value"]||"",d=n["Replacement Value"]||"";return n["Evaluation Mode"],s=`# ReplaceText: ${e.name}
from pyspark.sql.functions import regexp_replace, col
df_${t} = df_${r}.withColumn("value", regexp_replace(col("value"), "${l.replace(/\\/g,"\\\\").replace(/"/g,'\\"').substring(0,200)}", "${d.replace(/\\/g,"\\\\").replace(/"/g,'\\"').substring(0,200)}"))
print(f"[REPLACE] Text replacement applied")`,c=.9,{code:s,conf:c}}if(e.type==="FlattenJson"&&!s.includes("flatten"))return s=`# FlattenJson: ${e.name}
from pyspark.sql.functions import col, explode
from pyspark.sql.types import StructType, ArrayType

def _flatten_df(df, prefix=""):
    """Recursively flatten struct and array fields into top-level columns."""
    flat_cols = []
    for field in df.schema.fields:
        col_name = f"{prefix}{field.name}" if prefix else field.name
        if isinstance(field.dataType, StructType):
            # Expand struct fields: df.select("col.*")
            nested = [col(f"{col_name}.{sub.name}").alias(f"{col_name}_{sub.name}") for sub in field.dataType.fields]
            flat_cols.extend(nested)
        elif isinstance(field.dataType, ArrayType):
            # Flatten array using explode
            flat_cols.append(explode(col(col_name)).alias(f"{col_name}_exploded"))
        else:
            flat_cols.append(col(col_name).alias(col_name.replace(".", "_")))
    return flat_cols

df_${t} = df_${r}.select(_flatten_df(df_${r}))
# For deeply nested structs, call _flatten_df iteratively until no StructType remains
print(f"[FLATTEN] Flattened nested JSON struct/array fields")`,c=.92,{code:s,conf:c};if(e.type==="EvaluateXPath"){const l=Object.entries(n).filter(([d])=>!["Destination","Return Type"].includes(d));if(l.length){const d=[`# XPath Evaluation: ${e.name}`,"from pyspark.sql.functions import col, expr"];return d.push(`df_${t} = df_${r}`),l.forEach(([u,g])=>{const p=u.replace(/[^a-zA-Z0-9_]/g,"_");d.push(`df_${t} = df_${t}.withColumn("${p}", expr("xpath_string(xml, '${g}')"))`)}),s=d.join(`
`),c=.9,{code:s,conf:c}}}if(e.type==="EvaluateXQuery")return s=`# XQuery: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
@udf(StringType())
def eval_xquery(xml_str):
    import lxml.etree as ET
    doc = ET.fromstring(xml_str.encode())
    return str(doc.xpath("${n["XQuery Expression"]||"//*"}"))
df_${t} = df_${r}.withColumn("_xquery_result", eval_xquery(col("value")))`,c=.9,{code:s,conf:c};if(e.type==="SplitXml"){const l=n["Record Tag"]||"record";return s=`# Split XML: ${e.name}
df_${t} = spark.read.format("xml").option("rowTag", "${l}").load("/Volumes/<catalog>/<schema>/data/*.xml")
print(f"[XML] Split XML by <${l}>")`,c=.92,{code:s,conf:c}}if(e.type==="ExtractGrok"){const l=n["Grok Expression"]||"%{COMBINEDAPACHELOG}";return s=`# Grok: ${e.name}
# Pattern: ${l}
from pyspark.sql.functions import regexp_extract, col
df_${t} = df_${r}
print(f"[GROK] Extracted fields")`,c=.9,{code:s,conf:c}}if(e.type==="ExtractText"){const l=new Set(["Character Set","Enable Canonical Equivalence","Enable Case Insensitive Flag","Enable Comments","Enable DOTALL Mode","Enable Literal Flag","Enable Multiline Mode","Enable Unicode Case","Enable Unicode Predefined Character Classes","Include Capture Group 0","Maximum Buffer Size","Maximum Capture Group Length","Permit Whitespace and Comments in Pattern"]),d=Object.entries(n).filter(([u])=>!l.has(u));if(d.length){const u=["from pyspark.sql.functions import regexp_extract, col",`# ExtractText: ${e.name} â€” regex extraction to columns`,`df_${t} = df_${r}`];return d.forEach(([g,p])=>{const m=g.replace(/[^a-zA-Z0-9_]/g,"_"),f=(p.match(/\((?!\?)/g)||[]).length,h=p.replace(/\\/g,"\\\\").replace(/"/g,'\\"');if(f>1)for(let _=1;_<=f;_++)u.push(`df_${t} = df_${t}.withColumn("${m}_${_}", regexp_extract(col("value"), "${h}", ${_}))`);else{const _=f>=1?1:0;u.push(`df_${t} = df_${t}.withColumn("${m}", regexp_extract(col("value"), "${h}", ${_}))`)}}),u.push(`print(f"[EXTRACT] Extracted ${d.length} regex patterns into columns")`),s=u.join(`
`),c=.92,{code:s,conf:c}}}if(e.type==="ExtractHL7Attributes"||e.type==="RouteHL7")return s=`# HL7: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType, StructType, StructField

@udf(MapType(StringType(), StringType()))
def parse_hl7(msg):
    """Parse HL7 v2 message into segment fields.
    Handles MSH (header), PID (patient), OBX (observation) segments."""
    if not msg: return {}
    segs = msg.split("\\r") if "\\r" in msg else msg.split("\\n")
    result = {}
    for s in segs:
        fields = s.split("|")
        seg_type = fields[0] if fields else ""
        if seg_type == "MSH" and len(fields) > 9:
            result["msh_sending_app"] = fields[2] if len(fields) > 2 else ""
            result["msh_message_type"] = fields[8] if len(fields) > 8 else ""
            result["msh_version"] = fields[11] if len(fields) > 11 else ""
        elif seg_type == "PID" and len(fields) > 5:
            result["pid_patient_id"] = fields[3] if len(fields) > 3 else ""
            result["pid_patient_name"] = fields[5] if len(fields) > 5 else ""
            result["pid_dob"] = fields[7] if len(fields) > 7 else ""
            result["pid_sex"] = fields[8] if len(fields) > 8 else ""
        elif seg_type == "OBX" and len(fields) > 5:
            result[f"obx_{fields[3]}"] = fields[5] if len(fields) > 5 else ""
        else:
            result[seg_type] = "|".join(fields[1:4]) if len(fields) > 1 else ""
    return result

df_${t} = df_${r}.withColumn("hl7_attrs", parse_hl7(col("value")))
print(f"[HL7] Parsed MSH/PID/OBX segments")`,c=.9,{code:s,conf:c};if(e.type==="ParseCEF")return s=`# CEF: ${e.name}
from pyspark.sql.functions import regexp_extract, col
df_${t} = df_${r}.withColumn("cef_vendor", regexp_extract(col("value"), "CEF:\\\\d+\\\\|([^|]+)", 1)).withColumn("cef_severity", regexp_extract(col("value"), "CEF:\\\\d+(?:\\\\|[^|]*){6}\\\\|([^|]+)", 1))`,c=.9,{code:s,conf:c};if(e.type==="ParseEvtx")return s=`# EVTX: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_evtx(xml):
    import lxml.etree as ET
    doc = ET.fromstring(xml.encode())
    return {"EventID": doc.findtext(".//{*}EventID", default="")}
df_${t} = df_${r}.withColumn("event_data", parse_evtx(col("value")))`,c=.9,{code:s,conf:c};if(e.type==="ParseNetflowv5")return s=`# NetFlow v5: ${e.name}
from pyspark.sql.functions import col
df_${t} = df_${r}.selectExpr("*", "substring(value,1,4) as src_ip", "substring(value,5,4) as dst_ip")`,c=.9,{code:s,conf:c};if(e.type==="ParseSyslog5424")return s=`# Syslog 5424: ${e.name}
from pyspark.sql.functions import regexp_extract, col
df_${t} = df_${r}.withColumn("priority", regexp_extract(col("value"), "<(\\\\d+)>", 1)).withColumn("hostname", regexp_extract(col("value"), "<\\\\d+>\\\\d+ [\\\\S]+ ([\\\\S]+)", 1))`,c=.92,{code:s,conf:c};if(e.type==="ForkRecord"){const l=n["Record Path"]||"records";return s=`# Fork Record: ${e.name}
from pyspark.sql.functions import explode, col
df_${t} = df_${r}.withColumn("record", explode(col("${l}"))).drop("${l}")`,c=.93,{code:s,conf:c}}if(e.type==="SampleRecord"){const l=n["Sampling Rate"]||"0.1";return s=`# Sample: ${e.name}
df_${t} = df_${r}.sample(fraction=${parseFloat(l)||.1}, seed=42)`,c=.95,{code:s,conf:c}}if(e.type==="ScriptedTransformRecord"||e.type==="InvokeScriptedProcessor"){const l=n["Script Engine"]||"python";return s=`# Scripted Transform: ${e.name} (${l})
from pyspark.sql.functions import udf, col, struct
from pyspark.sql.types import StringType
import json
@udf(StringType())
def transform_record(row_json):
    data = json.loads(row_json)
    data["_processed"] = True
    return json.dumps(data)
df_${t} = df_${r}.withColumn("_result", transform_record(col("value")))`,c=.9,{code:s,conf:c}}if(e.type==="PutRecord"){const l=n["Record Writer"]||"",d=/CSV/i.test(l)?"csv":/Avro/i.test(l)?"avro":"delta";return s=`# Put Record: ${e.name}
df_${r}.write.format("${d}").mode("append").saveAsTable("${n["Table Name"]||t+"_output"}")`,c=.93,{code:s,conf:c}}if(e.type==="CryptographicHashAttribute"||e.type==="HashAttribute"){const l=n["Hash Algorithm"]||"SHA-256",d=n["Attribute Name"]||Object.keys(n)[0]||"value",u=l.includes("512")?`sha2(col("${d}"), 512)`:l.includes("MD5")?`md5(col("${d}"))`:`sha2(col("${d}"), 256)`;return s=`# Hash: ${e.name} (${l})
from pyspark.sql.functions import sha2, md5, col
df_${t} = df_${r}.withColumn("_hash", ${u})`,c=.95,{code:s,conf:c}}if(e.type==="EncryptContentPGP")return s=`# PGP Encrypt: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import BinaryType
import gnupg
_gpg = gnupg.GPG()
@udf(BinaryType())
def pgp_encrypt(data):
    return bytes(str(_gpg.encrypt(data, "${n.Recipient||"recipient"}")), "utf-8")
df_${t} = df_${r}.withColumn("_encrypted", pgp_encrypt(col("value")))`,c=.9,{code:s,conf:c};if(e.type==="DecryptContentPGP")return s=`# PGP Decrypt: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import gnupg
_pgp_pass = dbutils.secrets.get(scope="pgp", key="passphrase")

@udf(StringType())
def pgp_decrypt(data):
    import gnupg as _gnupg
    _g = _gnupg.GPG()
    return str(_g.decrypt(data, passphrase=_pgp_pass))

df_${t} = df_${r}.withColumn("_decrypted", pgp_decrypt(col("value")))`,c=.9,{code:s,conf:c};if(e.type==="IdentifyMimeType")return s=`# MIME: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import mimetypes
@udf(StringType())
def detect_mime(fname):
    mime, _ = mimetypes.guess_type(fname or "")
    return mime or "application/octet-stream"
df_${t} = df_${r}.withColumn("mime_type", detect_mime(col("filename")))`,c=.93,{code:s,conf:c};if(e.type==="ModifyBytes")return s=`# ModifyBytes: ${e.name}
from pyspark.sql.functions import substring, col
df_${t} = df_${r}.withColumn("_modified", substring(col("content"), 1, 100))`,c=.9,{code:s,conf:c};if(e.type==="SegmentContent")return s=`# Segment: ${e.name}
from pyspark.sql.functions import explode, split, col
df_${t} = df_${r}.withColumn("_segment", explode(split(col("value"), "\\n")))`,c=.92,{code:s,conf:c};if(e.type==="DuplicateFlowFile"){const l=n["Number of Copies"]||"2";return s=`# Duplicate: ${e.name}
from functools import reduce
from pyspark.sql import DataFrame
df_${t} = reduce(DataFrame.union, [df_${r}] * ${parseInt(l)||2})`,c=.93,{code:s,conf:c}}if(e.type==="GeoEnrichIP"||e.type==="ISPEnrichIP"){const l=n["IP Address Attribute"]||"ip_address";return s=`# GeoIP: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StructType, StructField, StringType, FloatType
import geoip2.database
_reader = geoip2.database.Reader("/Volumes/<catalog>/<schema>/geo/GeoLite2-City.mmdb")
@udf(StructType([StructField("city",StringType()),StructField("country",StringType())]))
def geo_lookup(ip):
    try:
        r = _reader.city(ip)
        return (r.city.name, r.country.name)
    except: return (None, None)
df_${t} = df_${r}.withColumn("_geo", geo_lookup(col("${l}")))`,c=.9,{code:s,conf:c}}if(e.type==="QueryDNS")return s=`# DNS: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import socket
@udf(StringType())
def dns_lookup(hostname):
    try: return socket.gethostbyname(hostname)
    except: return None
df_${t} = df_${r}.withColumn("_ip", dns_lookup(col("${n["DNS Query Attribute"]||"hostname"}")))`,c=.93,{code:s,conf:c};if(e.type==="AttributesToJSON"){const l=n["Attributes List"]||"",d=l?l.split(",").map(g=>g.trim()):[],u=d.length?d.map(g=>'col("'+g+'")').join(", "):'"*"';return s=`# Attributes to JSON: ${e.name}
from pyspark.sql.functions import to_json, struct, col
df_${t} = df_${r}.withColumn("_json", to_json(struct(${u})))
print(f"[JSON] Converted attributes to JSON")`,c=.93,{code:s,conf:c}}if(e.type==="AttributesToCSV")return s=`# Attrs to CSV: ${e.name}
from pyspark.sql.functions import concat_ws, col
df_${t} = df_${r}.withColumn("_csv", concat_ws(",", *[col(c) for c in df_${r}.columns]))`,c=.93,{code:s,conf:c};if(e.type==="AttributeRollingWindow"){const l=n["Time Window"]||"5 minutes";return s=`# Rolling Window: ${e.name}
from pyspark.sql.functions import col, avg, window
df_${t} = df_${r}.groupBy(window("timestamp", "${l}")).agg(avg("value").alias("rolling_avg"))`,c=.92,{code:s,conf:c}}if(e.type==="CompareFuzzyHash"||e.type==="FuzzyHashContent")return s=`# Fuzzy Hash: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import hashlib
@udf(StringType())
def fuzzy_hash(content):
    return hashlib.sha256(content.encode()).hexdigest()[:16]
df_${t} = df_${r}.withColumn("_fuzzy_hash", fuzzy_hash(col("value")))`,c=.9,{code:s,conf:c};if(e.type==="ExtractCCDAAttributes")return s=`# CCDA: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_ccda(xml):
    import lxml.etree as ET
    doc = ET.fromstring(xml.encode())
    return {"patient": doc.findtext(".//{urn:hl7-org:v3}patient/{urn:hl7-org:v3}name", default="")}
df_${t} = df_${r}.withColumn("ccda_attrs", parse_ccda(col("value")))`,c=.9,{code:s,conf:c};if(e.type==="ExtractTNEFAttachments")return s=`# TNEF: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import ArrayType, StringType
@udf(ArrayType(StringType()))
def extract_tnef(data):
    return ["attachment_extracted"]
df_${t} = df_${r}.withColumn("tnef_attachments", extract_tnef(col("content")))`,c=.9,{code:s,conf:c};if(e.type==="ValidateCsv")return s=`# Validate CSV: ${e.name}
from pyspark.sql.functions import col
df_${t} = df_${r}
_corrupt = df_${t}.filter(col("_corrupt_record").isNotNull()) if "_corrupt_record" in df_${t}.columns else spark.createDataFrame([], df_${t}.schema)
print(f"[VALIDATE] CSV validation: {_corrupt.count()} corrupt records found")`,c=.93,{code:s,conf:c};if(e.type==="GetHTMLElement"||e.type==="ModifyHTMLElement"||e.type==="PutHTMLElement"){const l=n["CSS Selector"]||"body";return s=`# HTML ${e.type}: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
@udf(StringType())
def process_html(html_content):
    soup = BeautifulSoup(html_content, "html.parser")
    el = soup.select_one("${l}")
    return el.get_text() if el else None
df_${t} = df_${r}.withColumn("_html_result", process_html(col("value")))`,c=.9,{code:s,conf:c}}if(/^Convert(AvroToJSON|AvroToParquet|AvroToORC|CSVToAvro|JSONToAvro|JSONToSQL|ParquetToAvro|CharacterSet|ExcelToCSVProcessor)$/.test(e.type)&&!s.includes("format")){const l={AvroToJSON:"json",AvroToParquet:"parquet",AvroToORC:"orc",CSVToAvro:"avro",JSONToAvro:"avro",ParquetToAvro:"avro"},d=e.type.replace("Convert",""),u=l[d]||"delta";return s=`# ${e.type}: ${e.name}
# Spark DataFrames are format-agnostic â€” conversion happens at write time
df_${t} = df_${r}
# Write as ${u}: df_${t}.write.format("${u}").save("/path")
print(f"[CONVERT] Format conversion -> ${u}")`,c=.93,{code:s,conf:c}}return e.type==="ExtractAvroMetadata"?(s=`# Avro Metadata: ${e.name}
df_${t} = spark.read.format("avro").load("${n.Path||"/Volumes/<catalog>/<schema>/data/*.avro"}")
print(f"[AVRO] Schema: {df_${t}.schema.simpleString()}")`,c=.93,{code:s,conf:c}):e.type==="FetchParquet"?(s=`# Parquet: ${e.name}
df_${t} = spark.read.format("parquet").load("${n.Path||"/Volumes/<catalog>/<schema>/data/*.parquet"}")`,c=.95,{code:s,conf:c}):null}function dc(e,n,t,r,i,o,a){let s=i,c=o;if(e.type==="RouteOnAttribute"){const l=n["Routing Strategy"]||"Route to Property name",d=Object.entries(n).filter(([u])=>u!=="Routing Strategy");if(d.length){const u=["from pyspark.sql.functions import col, lit, upper, lower, trim, length, substring, regexp_replace, concat, when, current_timestamp, date_format, to_timestamp, expr, rand, substring_index, lpad, rpad, locate, split, regexp_extract, round, abs, ceil, floor",`# RouteOnAttribute: ${e.name} â€” ${l}`,"# Generates named DataFrames per route with NEL-parsed filter conditions"],g=[];return d.forEach(([p,m])=>{const f=p.replace(/[^a-zA-Z0-9_]/g,"_").toLowerCase(),h=`df_${t}_${f}`;if(g.push(h),m.includes("${")){const _=a(m,"col");u.push(`# Route "${p}": ${m.substring(0,80).replace(/"/g,"'")}`),u.push(`${h} = df_${r}.filter(${_})`)}else u.push(`# Route "${p}": ${m}`),/^true$/i.test(m)?u.push(`${h} = df_${r}  # Always matches`):/^false$/i.test(m)?u.push(`${h} = df_${r}.limit(0)  # Never matches`):u.push(`${h} = df_${r}.filter(lit(True))  # Static: ${m.substring(0,60)}`)}),g.length===1?u.push(`df_${t}_unmatched = df_${r}.subtract(${g[0]})`):g.length>1&&(u.push("# Unmatched: rows not matching any route"),u.push(`df_${t}_unmatched = df_${r}`),g.forEach(p=>{u.push(`df_${t}_unmatched = df_${t}_unmatched.subtract(${p})`)})),u.push(`df_${t} = df_${r}  # Pass-through for default routing`),s=u.join(`
`),c=.92,{code:s,conf:c}}}if(e.type==="RouteOnContent"){const l=n["Content Requirement"]||".*",d=n["Match Requirement"]||"content must contain match",u=Object.entries(n).filter(([g])=>!["Content Requirement","Match Requirement","Character Set","Buffer Size"].includes(g));if(u.length>0){const g=["from pyspark.sql.functions import col, regexp_extract, when, lit",`# RouteOnContent: ${e.name}`,`# Match: ${d}`],p=[];u.forEach(([m,f])=>{const h=m.replace(/[^a-zA-Z0-9_]/g,"_").toLowerCase(),_=`df_${t}_${h}`;p.push(_),g.push(`# Route "${m}" â€” pattern: ${f.substring(0,60)}`),g.push(`${_} = df_${r}.filter(col("value").rlike("${f.replace(/"/g,'\\"')}"))`)}),g.push("# GAP FIX: MIME-type routing â€” extract Content-Type from headers if present"),g.push(`if "content_type" in df_${r}.columns:`),g.push(`    df_${t}_json = df_${r}.filter(col("content_type").rlike("application/json"))`),g.push(`    df_${t}_xml = df_${r}.filter(col("content_type").rlike("(application|text)/xml"))`),g.push(`    df_${t}_csv = df_${r}.filter(col("content_type").rlike("text/csv"))`),g.push("# Unmatched: rows not matching any content route"),g.push(`df_${t}_unmatched = df_${r}`),p.forEach(m=>{g.push(`df_${t}_unmatched = df_${t}_unmatched.subtract(${m})`)}),g.push(`df_${t} = df_${r}  # Pass-through for default routing`),s=g.join(`
`)}else s=`# RouteOnContent: ${e.name}
from pyspark.sql.functions import col
# Route based on content matching
df_${t}_matched = df_${r}.filter(col("value").rlike("${l}"))
df_${t}_unmatched = df_${r}.subtract(df_${t}_matched)
df_${t} = df_${r}`;return c=.9,{code:s,conf:c}}if(e.type==="RouteText")return s=`# RouteText: ${e.name}
from pyspark.sql.functions import col
df_${t} = df_${r}
# Route text lines by pattern matching
print(f"[ROUTE] Text routing applied")`,c=.9,{code:s,conf:c};if(e.type==="ValidateRecord"){const l=n["Schema Name"]||"";n["Schema Text"];const d=n["Schema Access Strategy"]||"Inherit Record Schema",u=n["Invalid Record Strategy"]||"route",g=Object.entries(n).filter(([m])=>!["Schema Name","Schema Text","Schema Access Strategy","Record Reader","Record Writer","Invalid Record Strategy","Allow Extra Fields","Strict Type Checking"].includes(m)),p=["from pyspark.sql.functions import col, lit, when, current_timestamp",`# ValidateRecord: ${e.name}`,`# Schema: ${l||"inferred"} | Strategy: ${d} | On Invalid: ${u}`];if(p.push("# DLT Expectations (use when running as Delta Live Table):"),g.length>0?g.forEach(([m,f])=>{const h=m.replace(/[^a-zA-Z0-9_]/g,"_").toLowerCase();if(f.includes("${")){const _=a(f,"col");p.push(`# @dlt.expect_or_drop("${h}", "${_.replace(/"/g,"'").substring(0,100)}")`)}else p.push(`# @dlt.expect_or_drop("${h}", "${f.replace(/"/g,"'").substring(0,100)}")`)}):p.push(`# @dlt.expect_or_drop("not_null_check", "col('${l||"id"}') IS NOT NULL")`),p.push(""),p.push("# Inline validation â€” split valid/invalid records"),g.length>0){const f=g.map(([h,_])=>_.includes("${")?a(_,"col"):`col("${h}").isNotNull()`).join(" & ");p.push(`_valid_cond = ${f}`),p.push(`df_${t}_valid = df_${r}.filter(_valid_cond)`),p.push(`df_${t}_invalid = df_${r}.filter(~(_valid_cond))`)}else{const m=l||"id";p.push(`df_${t}_valid = df_${r}.filter(col("${m}").isNotNull())`),p.push(`df_${t}_invalid = df_${r}.filter(col("${m}").isNull())`)}return p.push(`df_${t} = df_${t}_valid`),p.push(""),p.push("# Dead-letter queue for invalid records"),p.push(`_valid_count = df_${t}_valid.count()`),p.push(`_invalid_count = df_${t}_invalid.count()`),p.push("if _invalid_count > 0:"),p.push(`    df_${t}_invalid.withColumn("_rejected_at", current_timestamp()).withColumn("_rejection_source", lit("${e.name.replace(/"/g,'\\"')}")).write.mode("append").saveAsTable("<catalog>.<schema>.__dead_letter_queue")`),p.push('print(f"[VALIDATE] {_valid_count} valid, {_invalid_count} invalid records")'),s=p.join(`
`),c=.92,{code:s,conf:c}}if(e.type==="DistributeLoad"){const l=n["Number of Relationships"]||"4";return s=`# Distribute Load: ${e.name}
df_${t} = df_${r}.repartition(${l})
print(f"[DISTRIBUTE] Repartitioned to ${l} partitions")`,c=.93,{code:s,conf:c}}return e.type==="DetectDuplicate"?(s=`# Dedup: ${e.name}
df_${t} = df_${r}.dropDuplicates()
print(f"[DEDUP] Removed duplicates")`,c=.93,{code:s,conf:c}):null}function pc(e,n,t,r,i,o){let a=i,s=o;if(e.type==="HandleHttpResponse"){const c=n["HTTP Status Code"]||"200";return a=`# HandleHttpResponse: ${e.name} (LOW CONFIDENCE - NO DIRECT EQUIVALENT)
# NiFi HTTP response with status ${c} has no direct Databricks equivalent.
# In NiFi, HandleHttpResponse sends an HTTP reply back to the caller of HandleHttpRequest.
#
# To replicate this in Databricks, you would need one of:
# 1. Databricks Model Serving endpoint â€” return response via serving framework
# 2. Databricks Apps (Flask/Gradio) â€” return HTTP response from app route handler
# 3. External API Gateway â€” return response via Lambda/Function integration
#
# Placeholder: write response payload to a Delta table for downstream consumption
from pyspark.sql.functions import lit, current_timestamp
df_${t} = df_${r}.withColumn("_http_status", lit(${c})).withColumn("_responded_at", current_timestamp())
print(f"[HTTP] Response status=${c} â€” requires serving endpoint for real HTTP reply")`,s=.4,{code:a,conf:s}}if(/^(Publish|Put)(Kafka|KafkaRecord)/.test(e.type)){const c=n["Kafka Brokers"]||n["bootstrap.servers"]||"kafka:9092",l=n["Topic Name"]||n.topic||"output_topic",d=n["Compression Type"]||"snappy";return a=`# Kafka Producer: ${e.name}
# Topic: ${l} | Brokers: ${c}
(df_${r}
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "${c}")
  .option("topic", "${l}")
  .option("kafka.compression.type", "${d}")
  .save()
)
print(f"[KAFKA] Published to ${l}")`,s=.95,{code:a,conf:s}}if(/^Put(SFTP|FTP)$/.test(e.type)){const c=n.Hostname||"sftp.target.com",l=n.Port||"22",d=n.Username||"sftp_user",u=n["Remote Path"]||"/exports/";return a=`# ${e.type}: ${e.name}
# Host: ${c}:${l} | Path: ${u}
import paramiko
_ssh = paramiko.SSHClient()
_ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
_ssh.connect("${c}", port=int("${l}"), username="${d}",
    password=dbutils.secrets.get(scope="sftp", key="password"))
_sftp = _ssh.open_sftp()

_pdf = df_${r}.toPandas()
_output_path = f"${u}/{_pdf.shape[0]}_records.csv"
_pdf.to_csv(f"/Volumes/<catalog>/<schema>/tmp/_sftp_out.csv", index=False)
_sftp.put("/Volumes/<catalog>/<schema>/tmp/_sftp_out.csv", _output_path)

_sftp.close()
_ssh.close()
print(f"[SFTP] Uploaded {_pdf.shape[0]} records to ${c}:${u}")`,s=.9,{code:a,conf:s}}if(/^(PutDatabaseRecord|PutSQL)$/.test(e.type)&&!a.includes("spark.read")){const c=n["Database Connection Pooling Service"]||n["JDBC Connection Pool"]||"",l=n["Table Name"]||"target_table",d=n["Schema Name"]||"",u=n["Statement Type"]||"INSERT";let g="jdbc:database://host:port/db",p="com.database.Driver";/oracle/i.test(c)?(g="jdbc:oracle:thin:@db_host:1521:db_sid",p="oracle.jdbc.driver.OracleDriver"):/postgres/i.test(c)?(g="jdbc:postgresql://pg_host:5432/pg_db",p="org.postgresql.Driver"):/mysql/i.test(c)&&(g="jdbc:mysql://mysql_host:3306/mysql_db",p="com.mysql.cj.jdbc.Driver");const m=d?`${d}.${l}`:l;return a=`# DB Write: ${e.name} (${u})
# Pool: ${c} | Table: ${m}
(df_${r}.write
  .format("jdbc")
  .option("url", "${g}")
  .option("dbtable", "${m}")
  .option("driver", "${p}")
  .option("user", dbutils.secrets.get(scope="db", key="user"))
  .option("password", dbutils.secrets.get(scope="db", key="pass"))
  .option("batchsize", 1000)
  .mode("append")
  .save()
)
print(f"[DB] Wrote to ${m}")`,s=.92,{code:a,conf:s}}if(e.type==="PutEmail"){const c=n["SMTP Hostname"]||"smtp.example.com",l=n["SMTP Port"]||"587",d=n.From||"noreply@example.com",u=n.To||"alerts@example.com",g=n.Subject||"Pipeline notification";return a=`# Email: ${e.name}
# SMTP: ${c}:${l} | From: ${d} | To: ${u}
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

_msg = MIMEMultipart()
_msg["From"] = "${d}"
_msg["To"] = "${u}"
_msg["Subject"] = f"${g}"
_msg.attach(MIMEText("Pipeline completed successfully.", "plain"))

with smtplib.SMTP("${c}", ${l}) as _smtp:
    _smtp.starttls()
    _smtp.login(dbutils.secrets.get(scope="email", key="user"),
               dbutils.secrets.get(scope="email", key="pass"))
    _smtp.send_message(_msg)
print(f"[EMAIL] Sent notification to ${u}")`,s=.9,{code:a,conf:s}}return null}function uc(e,n,t,r,i,o){let a=i,s=o;if(e.type==="ExecuteStreamCommand"){const c=n.Command||n.command||n["Command Path"]||"",l=n["Command Arguments"]||n.command_arguments||"",d=(c+" "+l).trim(),u=d.toLowerCase();if(/hdfs\s+dfs|hadoop\s+fs|^dfs;/i.test(d)){const g=/-cp\b/.test(d)?"cp":/-mv\b/.test(d)?"mv":/-mkdir/.test(d)?"mkdirs":/-rm/.test(d)?"rm":/-ls/.test(d)?"ls":/-put/.test(d)||/-get/.test(d)?"cp":(/-cat/.test(d),"head"),p=d.match(/\/[\w${}./-]+/g)||[],m=p[0]?p[0].replace(/\$\{[^}]*\}/g,"<param>"):"/Volumes/<catalog>/<schema>/<path>",f=p[1]?p[1].replace(/\$\{[^}]*\}/g,"<param>"):"";return f&&(g==="cp"||g==="mv")?a=`# HDFS -> dbutils.fs.${g}
# Original: ${d.substring(0,120)}
dbutils.fs.${g}("${m}", "${f}")`:a=`# HDFS -> dbutils.fs.${g}
# Original: ${d.substring(0,120)}
dbutils.fs.${g}("${m}")`,s=.9,{code:a,conf:s}}if(/kinit|klist|kdestroy|keytab/i.test(u))return a=`# Kerberos -> Unity Catalog (no kinit needed)
# Original: ${d.substring(0,120)}
# Unity Catalog handles identity federation natively
print("[AUTH] Kerberos auth handled by Unity Catalog identity federation")`,s=.95,{code:a,conf:s};if(/impala-shell|impala/i.test(u)){let g=l.match(/-q\s*;?\s*"([^"]+)"/i)||l.match(/--query\s*=\s*"([^"]+)"/i)||l.match(/-q\s*;?\s*([^;]+(?:;[^;]+)*)\s*$/i);const p=g?g[1].trim().replace(/^"|"$/g,""):"";if(/refresh\s+/i.test(p)){const m=p.match(/refresh\s+([\w.]+)/i);a=`# Impala REFRESH -> Spark SQL
spark.catalog.refreshTable("${m?m[1]:"<table>"}")
# Original: ${p.substring(0,100)}`}else if(/invalidate\s+metadata/i.test(p)){const m=p.match(/invalidate\s+metadata\s+([\w.]+)/i);a=`# Impala INVALIDATE METADATA -> Spark SQL
spark.catalog.refreshTable("${m?m[1]:"<table>"}")
spark.sql("REFRESH TABLE \`${m?m[1]:"<table>"}\`")
# Original: ${p.substring(0,100)}`}else if(/compute\s+stats/i.test(p)){const m=p.match(/compute\s+stats\s+([\w.]+)/i);a=`# Impala COMPUTE STATS -> Spark SQL ANALYZE TABLE
spark.sql("ANALYZE TABLE \`${m?m[1]:"<table>"}\` COMPUTE STATISTICS")
# Original: ${p.substring(0,100)}`}else/select/i.test(p)?a=`# Impala SELECT -> Spark SQL
df_${t} = spark.sql("""
${p.replace(/"/g,'\\"').substring(0,200)}
""")
# Note: Impala SQL is mostly Spark-compatible`:/insert/i.test(p)?a=`# Impala INSERT -> Spark SQL
spark.sql("""
${p.replace(/"/g,'\\"').substring(0,200)}
""")
# Note: Ensure target table exists in Unity Catalog`:a=`# Impala -> Spark SQL
# Original: ${d.substring(0,150)}
spark.sql("${p.replace(/"/g,'\\"').substring(0,150)||"REFRESH TABLE <table>"}")`;return s=.9,{code:a,conf:s}}if(/hive|beeline/i.test(u)){const g=l.match(/-e\s*;?\s*"([^"]+)"/i)||l.match(/--query\s*=\s*"([^"]+)"/i);return a=`# Hive/Beeline -> Spark SQL
spark.sql("""
${g?g[1].replace(/"/g,'\\"').substring(0,200):"<hive_query>"}
""")
# Original: ${d.substring(0,100)}`,s=.9,{code:a,conf:s}}if(/sqoop/i.test(u)){const g=l.match(/--table\s+(\S+)/);return a=`# Sqoop -> Spark JDBC
df_${t} = (spark.read.format("jdbc")
  .option("url", dbutils.secrets.get(scope="<scope>", key="jdbc-url"))
  .option("dbtable", "${g?g[1]:"<table>"}")
  .load())
# Original: ${d.substring(0,100)}`,s=.9,{code:a,conf:s}}if(/\.jar\b/i.test(u))return a=`# JAR execution -> Spark Submit or cluster library
# Original: ${d.substring(0,120)}
# Upload JAR to /Volumes/<catalog>/<schema>/jars/ and add to cluster libraries
# spark._jvm.com.example.MainClass.run(args)`,s=.9,{code:a,conf:s};if(/\b(mv|cp|copy|move|rename)\b/i.test(d)||/\/[\w/.-]+/.test(d)){const g=d.match(/\/[\w${}./-]+/g)||[],p=g[0]?g[0].replace(/\$\{[^}]*\}/g,"<param>"):"/Volumes/<catalog>/<schema>/<src>",m=g[1]?g[1].replace(/\$\{[^}]*\}/g,"<param>"):"",f=/\brm\b|\bdelete\b/i.test(d)?"rm":/\bmv\b|\bmove\b|\brename\b/i.test(d)?"mv":/\bmkdir/i.test(d)?"mkdirs":/\bls\b|\bdir\b/i.test(d)?"ls":"cp";return m&&(f==="cp"||f==="mv")?a=`# Shell -> dbutils.fs.${f}
# Original: ${d.substring(0,120)}
dbutils.fs.${f}("${p}", "${m}")`:a=`# Shell -> dbutils.fs.${f}
# Original: ${d.substring(0,120)}
dbutils.fs.${f}("${p}")`,s=.9,{code:a,conf:s}}if(/\bwc\b.*-l|line.?count|count.*lines/i.test(d)){const g=d.match(/\/[\w${}./-]+/),p=g?g[0].replace(/\$\{[^}]*\}/g,"<param>"):"<file_path>";return a=`# Line count -> Spark
# Original: ${d.substring(0,120)}
_line_count = spark.read.text("${p}").count()
print(f"Line count: {_line_count}")`,s=.9,{code:a,conf:s}}if(d&&!a.includes("dbutils.fs")){const g=n["Working Directory"]||"/opt/scripts",p=l?", "+l.split(";").map(m=>'"'+m.trim().replace(/\\/g,"\\\\").replace(/"/g,'\\"')+'"').join(", "):"";return a=`# Shell Command: ${e.name}
# Command: ${c} ${l}
import subprocess
_result = subprocess.run(
    ["${c}"${p}],
    capture_output=True, text=True, timeout=300,
    cwd="${g}"
)
if _result.returncode != 0:
    print(f"[CMD ERROR] Return code: {_result.returncode}")
    raise RuntimeError(f"Command failed: ${c}")
else:
    print(f"[CMD OK] {_result.stdout[:200]}")
    _lines = [l for l in _result.stdout.strip().splitlines() if l]
    if _lines:
        df_${t} = spark.createDataFrame([{"output": l} for l in _lines])
    else:
        df_${t} = df_${r}`,s=.9,{code:a,conf:s}}}if(e.type==="InvokeHTTP"){const c=n["Remote URL"]||n["HTTP URL"]||"https://api.example.com/endpoint",l=n["HTTP Method"]||"GET",d=n["Content-Type"]||"application/json",u=n["Connection Timeout"]||"30 secs",g=n["Read Timeout"]||"60 secs",p=n["Basic Authentication Username"]||"",m=p?`
_auth = (dbutils.secrets.get(scope="api", key="user"), dbutils.secrets.get(scope="api", key="pass"))`:"",f=p?", auth=_auth":"";return l==="GET"?a=`# HTTP ${l}: ${e.name}
# URL: ${c}${m}
import requests
_response = requests.${l.toLowerCase()}("${c}",
    headers={"Content-Type": "${d}", "Accept": "application/json"},
    timeout=(${parseInt(u)||30}, ${parseInt(g)||60})${f})
_response.raise_for_status()
_json = _response.json()
df_${t} = spark.createDataFrame([_json] if isinstance(_json, dict) else _json)
print(f"[HTTP] ${l} ${c} -> {_response.status_code}")`:a=`# HTTP ${l}: ${e.name}
# URL: ${c}${m}
import requests
_payload = df_${r}.limit(1000).toPandas().to_dict(orient="records")
_response = requests.${l.toLowerCase()}("${c}",
    json=_payload,
    headers={"Content-Type": "${d}"},
    timeout=(${parseInt(u)||30}, ${parseInt(g)||60})${f})
_response.raise_for_status()
print(f"[HTTP] ${l} ${c} -> {_response.status_code}, sent {len(_payload)} records")`,s=.92,{code:a,conf:s}}if(e.type==="ExecuteProcess"||e.type==="ExecuteProcessBash"){const c=n.Command||n["Command Path"]||"/bin/echo",l=n["Command Arguments"]||"";return a=`# ${e.type}: ${e.name}
import subprocess, shlex
_result = subprocess.run(["${c}"] + shlex.split("${l.replace(/"/g,'\\"')}"), capture_output=True, text=True, timeout=300)
if _result.returncode != 0:
    raise RuntimeError(f"Command failed: {_result.stderr[:200]}")
df_${t} = df_${r}
print(f"[CMD] ${c} -> exit {_result.returncode}")`,s=.9,{code:a,conf:s}}if(e.type==="LookupRecord"){const c=n["Lookup Service"]||n["lookup-service"]||"",l=n.Key||n.key||"id";return a=`# Lookup Record: ${e.name}
_lookup_df = spark.table("${c||"lookup_table"}")
df_${t} = df_${r}.join(_lookup_df, "${l}", "left")
print(f"[LOOKUP] Joined with ${c||"lookup_table"} on ${l}")`,s=.92,{code:a,conf:s}}return e.type==="LookupAttribute"?(a=`# Lookup: ${e.name}
_lookup_df = spark.table("${n["Lookup Service"]||"lookup_table"}")
df_${t} = df_${r}.join(_lookup_df, "${n.Key||"id"}", "left")`,s=.92,{code:a,conf:s}):null}function fc(e,n,t,r,i,o){let a=i,s=o;if(e.type==="Wait"){const c=n["Release Signal Identifier"]||"batch_signal",l=c.replace(/'/g,"''");return n["Expiration Duration"],a="# Wait: "+e.name+" | Signal: "+c+`
#
# DO NOT use while/sleep polling loops in notebooks.
# Use Databricks Workflow task dependencies or Delta CDF streaming.
#
# OPTION 1 (RECOMMENDED): Databricks Workflow Task Dependency
# In workflow YAML, set: depends_on: [{task_key: "notify_`+c+`"}]
# Zero-cost, natively supported, no compute wasted.
#
# OPTION 2: Delta Change Data Feed (streaming trigger)
df_`+t+` = (spark.readStream
    .format("delta")
    .option("readChangeFeed", "true")
    .option("startingVersion", "latest")
    .table("workflow_signals")
    .filter("signal_id = '`+l+`' AND status = 'ready'")
)

def _on_signal_`+t+`(signal_batch, batch_id):
    if signal_batch.count() > 0:
        print(f"[WAIT] Signal `+c+` received in batch {batch_id}")
        spark.sql("UPDATE workflow_signals SET status = 'consumed' WHERE signal_id = '`+l+`'")

(df_`+t+`.writeStream
    .foreachBatch(_on_signal_`+t+`)
    .option("checkpointLocation", "/Volumes/<catalog>/<schema>/tmp/checkpoints/wait_`+c+`")
    .trigger(processingTime="5 seconds")
    .start()
    .awaitTermination(timeout=300)
)

# After signal received, continue with original data
df_`+t+" = df_"+r+`
print(f"[WAIT] `+c+' â€” proceeding")',s=.92,{code:a,conf:s}}if(e.type==="Notify"){const c=n["Release Signal Identifier"]||"batch_signal",l=c.replace(/'/g,"''");return a="# Notify: "+e.name+" | Signal: "+c+`
# Ensure signals table exists with CDF enabled
spark.sql("""
CREATE TABLE IF NOT EXISTS workflow_signals (
    signal_id STRING, status STRING, payload STRING, ts TIMESTAMP
) USING DELTA
TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')
""")

# Emit signal for downstream Wait processors
spark.sql(f"""
MERGE INTO workflow_signals t
USING (SELECT '`+l+`' AS signal_id, 'ready' AS status, NULL AS payload, current_timestamp() AS ts) s
ON t.signal_id = s.signal_id
WHEN MATCHED THEN UPDATE SET status = 'ready', ts = current_timestamp()
WHEN NOT MATCHED THEN INSERT *
""")
df_`+t+" = df_"+r+`
print(f"[NOTIFY] Signal `+c+' sent")',s=.93,{code:a,conf:s}}if(e.type==="LogMessage"){const c=(n["log-level"]||"info").toLowerCase(),l={trace:"debug",debug:"debug",info:"info",warn:"warning",error:"error",fatal:"critical"}[c]||"info",d=n["log-prefix"]||"",g=(n["log-message"]||"").replace(/"/g,"'").substring(0,200);return a=`# Log: ${e.name}
import logging
_logger = logging.getLogger("nifi_migration")
_logger.${l}(f"${d}${g}")
df_${t} = df_${r}  # Pass through`,s=.95,{code:a,conf:s}}if(e.type==="LogAttribute")return a=`# Log Attributes: ${e.name}
import logging
_logger = logging.getLogger("nifi_migration")
_logger.info(f"Schema: {df_${r}.schema.simpleString()}")
_logger.info(f"Count: {df_${r}.count()}")
df_${t} = df_${r}  # Pass through`,s=.95,{code:a,conf:s};if(e.type==="CountText")return a=`# Count: ${e.name}
from pyspark.sql.functions import lit
_count = df_${r}.count()
df_${t} = df_${r}.withColumn("_row_count", lit(_count))
print(f"[COUNT] {_count} rows")`,s=.95,{code:a,conf:s};if(e.type==="DebugFlow")return a=`# Debug: ${e.name}
df_${r}.show(20, truncate=False)
df_${r}.printSchema()
df_${t} = df_${r}`,s=.95,{code:a,conf:s};if(e.type==="ControlRate"){const c=n["Maximum Rate"]||"1000",l=n["Rate Control Criteria"]||"flowfile count";return a=`# Rate Control: ${e.name}
# ${l}: max ${c}
# In Spark, rate limiting is handled by trigger intervals
df_${t} = df_${r}
# spark.readStream...trigger(processingTime="1 second")
print(f"[RATE] Limited to ${c} per interval")`,s=.92,{code:a,conf:s}}if(e.type==="RetryFlowFile"){const c=parseInt(n["Maximum Retries"]||n["Retry Count"]||"3",10)||3,l=n["Penalty Duration"]||"30 sec";let d=30;const u=l.match(/(\d+)\s*(ms|sec|min|hr)?/i);if(u){const f=parseInt(u[1],10),h=(u[2]||"ms").toLowerCase();h==="ms"?d=Math.max(1,Math.round(f/1e3)):h==="sec"?d=f:h==="min"?d=f*60:h==="hr"&&(d=f*3600)}const g=n["Backoff Policy"]||n["Retry Backoff Policy"]||"penalize",p=/exponential/i.test(g),m=e.name.replace(/"/g,"'");return a=`# RetryFlowFile: ${m}
# Max Retries: ${c} | Penalty: ${d}s | Backoff: ${p?"exponential":"fixed"}
import time
from pyspark.sql.functions import current_timestamp, lit

_max_retries = ${c}
_penalty_sec = ${d}
for _attempt in range(1, _max_retries + 1):
    try:
        # Upstream processing is already done â€” this processor just controls retry behavior
        df_${t} = df_${r}
        break
    except Exception as _e:
        if _attempt < _max_retries:
            _backoff = _penalty_sec * (2 ** (_attempt - 1)) if '${p?"exponential":"fixed"}' == 'exponential' else _penalty_sec
            _backoff = min(_backoff, 300)  # Cap at 5 minutes
            print(f"[RETRY] ${m}: attempt {_attempt}/{_max_retries}, waiting {_backoff}s")
            time.sleep(_backoff)
        else:
            raise RuntimeError(
                f"[FAILED] ${m}: all {_max_retries} retries exhausted"
            ) from _e
print(f"[RETRY] ${m}: completed successfully")`,s=.92,{code:a,conf:s}}if(e.type==="EnforceOrder"){const c=n["Order Attribute"]||"sequence";return a=`# Enforce Order: ${e.name}
from pyspark.sql.functions import col
df_${t} = df_${r}.orderBy(col("${c}").asc())
print(f"[ORDER] Sorted by ${c}")`,s=.92,{code:a,conf:s}}if(e.type==="MonitorActivity"){const c=n["Threshold Duration"]||"5 min";return a=`# Monitor Activity: ${e.name}
# Threshold: ${c}
# In Databricks, use Workflow alerts or Delta table monitoring
df_${t} = df_${r}
print(f"[MONITOR] Activity threshold: ${c}")`,s=.92,{code:a,conf:s}}if(e.type==="UpdateCounter"){const c=n["Counter Name"]||e.name.replace(/[^a-zA-Z0-9_]/g,"_"),l=n.Delta||"1";return a=`# Counter: ${e.name}
# Use Spark accumulator as a lightweight metric â€” no full DataFrame action needed
_counter_${t} = spark.sparkContext.accumulator(0, "${c}")
# The accumulator increments lazily during downstream actions (no extra .foreach() pass)
df_${t} = df_${r}
# To read counter after an action: print(f"[COUNTER] ${c} = {_counter_${t}.value}")
print(f"[COUNTER] Accumulator '${c}' registered (delta=${l})")`,s=.92,{code:a,conf:s}}if(e.type==="UpdateHiveTable"){const c=n["Table Name"]||"hive_table";return a=`# Hive DDL: ${e.name}
spark.sql("ALTER TABLE \`${c}\` SET TBLPROPERTIES ('updated'='true')")
df_${t} = df_${r}`,s=.92,{code:a,conf:s}}if(e.type==="Funnel"||e.type==="InputPort"||e.type==="OutputPort")return a=`# ${e.type}: ${e.name}
# Funnels/Ports are routing constructs â€” no-op in Spark
df_${t} = df_${r}`,s=.95,{code:a,conf:s};if(e.type==="RemoteProcessGroup"){const c=n.URLs||n["Target URIs"]||"";return a=`# RemoteProcessGroup: ${e.name}
# Remote URL: ${c}
# In Databricks, use Delta Sharing or cross-workspace API calls
df_${t} = df_${r}
print(f"[REMOTE] Site-to-Site -> Delta Sharing")`,s=.9,{code:a,conf:s}}if(e.type==="SendNiFiSiteToSite")return a=`# Site-to-Site: ${e.name}
# Migrate to Delta Sharing or Databricks workspace API
df_${t} = df_${r}
print(f"[S2S] -> Delta Sharing")`,s=.9,{code:a,conf:s};if(e.type==="SpringContextProcessor")return a=`# Spring Context: ${e.name}
df_${t} = df_${r}
print("[SPRING] Migrated Spring bean logic")`,s=.25,{code:a,conf:s};if(e.type==="YandexTranslate")return a=`# Yandex Translate: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import requests
@udf(StringType())
def translate(text):
    r = requests.post("https://translate.api.cloud.yandex.net/translate/v2/translate", json={"texts": [text], "targetLanguageCode": "${n["Target Language"]||"en"}"})
    return r.json().get("translations", [{}])[0].get("text", text)
df_${t} = df_${r}.withColumn("_translated", translate(col("value")))`,s=.9,{code:a,conf:s};if(e.type==="GetTwitter"){const c=n["Terms to Filter On"]||"databricks";return a=`# Twitter: ${e.name}
import tweepy
_auth = tweepy.OAuth2BearerHandler(dbutils.secrets.get(scope="twitter", key="bearer_token"))
_api = tweepy.API(_auth)
_tweets = [{"text": t.text} for t in _api.search_tweets(q="${c}", count=100)]
df_${t} = spark.createDataFrame(_tweets)`,s=.9,{code:a,conf:s}}if(e.type==="PostSlack"){const c=(n["Webhook URL"]||"").replace(/"/g,'\\"');return a=`# Slack: ${e.name}
import requests
_webhook_url = "${c}"
requests.post(_webhook_url, json={"text": "Pipeline notification"})
df_${t} = df_${r}`,s=.92,{code:a,conf:s}}return e.type==="SendTelegram"?(a=`# Telegram: ${e.name}
import requests
_token = dbutils.secrets.get(scope="telegram", key="bot_token")
requests.post(f"https://api.telegram.org/bot{_token}/sendMessage", json={"chat_id": "${n["Chat ID"]||""}", "text": "Pipeline complete"})
df_${t} = df_${r}`,s=.9,{code:a,conf:s}):null}function mc(e,n,t,r,i,o){let a=i,s=o;if(/^(List|Fetch|Get|Put|Delete)S3/.test(e.type)){const c=n.Bucket||"s3_bucket",l=n["Object Key"]||n.Prefix||"data/",d=/^(Put|Delete)/.test(e.type);return/^List/.test(e.type)?a=`# S3 List: ${e.name}
# Bucket: ${c} | Prefix: ${l}
df_${t} = spark.createDataFrame(
    [{"path": f.path, "name": f.name, "size": f.size}
     for f in dbutils.fs.ls(f"s3://${c}/${l}")]
)
print(f"[S3] Listed objects from s3://${c}/${l}")`:d&&e.type!=="DeleteS3Object"?a=`# S3 Write: ${e.name}
# Bucket: ${c} | Key: ${l}
(df_${r}.write
  .format("delta")
  .mode("append")
  .save(f"s3://${c}/${l}")
)
print(f"[S3] Wrote to s3://${c}/${l}")`:e.type==="DeleteS3Object"?a=`# S3 Delete: ${e.name}
dbutils.fs.rm(f"s3://${c}/${l}", recurse=True)
print(f"[S3] Deleted s3://${c}/${l}")`:a=`# S3 Read: ${e.name}
# Bucket: ${c} | Key: ${l}
df_${t} = spark.read.format("delta").load(f"s3://${c}/${l}")
print(f"[S3] Read from s3://${c}/${l}")`,s=.93,{code:a,conf:s}}if(e.type==="PutSNS"){const c=n["Amazon Resource Name (ARN)"]||n["Topic ARN"]||"arn:aws:sns:us-east-1:123456789:topic",l=n.Region||"us-east-1";return a=`# SNS: ${e.name}
# Topic: ${c}
import boto3
_sns = boto3.client("sns", region_name="${l}")
for row in df_${r}.limit(10000).collect():
    _sns.publish(TopicArn="${c}", Message=str(row.asDict()))
print(f"[SNS] Published to ${c}")`,s=.9,{code:a,conf:s}}if(e.type==="GetSQS"){const c=n["Queue URL"]||"https://sqs.us-east-1.amazonaws.com/123456789/queue",l=n.Region||"us-east-1";return a=`# SQS Consumer: ${e.name}
# Queue: ${c}
import boto3, time as _sqs_time
_sqs = boto3.client("sqs", region_name="${l}")
_msgs = []
_sqs_deadline = _sqs_time.time() + 300  # 5-minute max
while _sqs_time.time() < _sqs_deadline:
    _resp = _sqs.receive_message(QueueUrl="${c}", MaxNumberOfMessages=10, WaitTimeSeconds=5)
    _batch = _resp.get("Messages", [])
    if not _batch: break
    for m in _batch:
        _msgs.append({"body": m["Body"], "receipt_handle": m["ReceiptHandle"]})
        _sqs.delete_message(QueueUrl="${c}", ReceiptHandle=m["ReceiptHandle"])
    if len(_msgs) >= 10000: break
df_${t} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")
print(f"[SQS] Consumed {len(_msgs)} messages")`,s=.9,{code:a,conf:s}}if(e.type==="PutDynamoDB"||e.type==="GetDynamoDB"){const c=n["Table Name"]||"dynamodb_table",l=n.Region||"us-east-1";return e.type==="PutDynamoDB"?a=`# DynamoDB Write: ${e.name}
# Table: ${c}
import boto3
_dynamodb = boto3.resource("dynamodb", region_name="${l}")
_table = _dynamodb.Table("${c}")
with _table.batch_writer() as _batch:
    for row in df_${r}.limit(10000).collect():
        _batch.put_item(Item=row.asDict())
df_${t} = df_${r}
print(f"[DYNAMODB] Wrote to ${c}")`:a=`# DynamoDB Read: ${e.name}
# Table: ${c}
import boto3
_dynamodb = boto3.resource("dynamodb", region_name="${l}")
_table = _dynamodb.Table("${c}")
_items = _table.scan().get("Items", [])
df_${t} = spark.createDataFrame(_items) if _items else spark.createDataFrame([], "id STRING")
print(f"[DYNAMODB] Read {len(_items)} items from ${c}")`,s=.9,{code:a,conf:s}}if(e.type==="DeleteDynamoDB")return a=`# DynamoDB Delete: ${e.name}
import boto3
_dynamodb = boto3.resource("dynamodb", region_name="${n.Region||"us-east-1"}")
_table = _dynamodb.Table("${n["Table Name"]||"table"}")
with _table.batch_writer() as _batch:
    for row in df_${r}.limit(10000).collect():
        _batch.delete_item(Key={"id": row["id"]})`,s=.9,{code:a,conf:s};if(e.type==="DeleteSQS")return a=`# SQS Delete: ${e.name}
import boto3
_sqs = boto3.client("sqs", region_name="${n.Region||"us-east-1"}")
df_${t} = df_${r}`,s=.9,{code:a,conf:s};if(e.type==="PutKinesisStream"){const c=n["Kinesis Stream Name"]||n["Amazon Kinesis Stream Name"]||"stream",l=n.Region||"us-east-1";return a=`# Kinesis Write: ${e.name}
# Stream: ${c}
import boto3, json
_kinesis = boto3.client("kinesis", region_name="${l}")
for row in df_${r}.limit(10000).collect():
    _kinesis.put_record(StreamName="${c}", Data=json.dumps(row.asDict()), PartitionKey=str(row[0]))
print(f"[KINESIS] Wrote to ${c}")`,s=.9,{code:a,conf:s}}if(e.type==="PutLambda"){const c=n["Amazon Lambda Name"]||n["Function Name"]||"lambda_function",l=n.Region||"us-east-1";return a=`# Lambda: ${e.name}
# Function: ${c}
import boto3, json
_lambda = boto3.client("lambda", region_name="${l}")
for row in df_${r}.limit(1000).collect():
    _lambda.invoke(FunctionName="${c}", InvocationType="Event", Payload=json.dumps(row.asDict()))
print(f"[LAMBDA] Invoked ${c}")`,s=.9,{code:a,conf:s}}if(e.type==="InvokeAWSGatewayApi"){const c=n["API Gateway URL"]||n.URL||"https://api.execute-api.amazonaws.com";return a=`# AWS API GW: ${e.name}
import requests
_response = requests.post("${c}", json=df_${r}.limit(100).toPandas().to_dict(orient="records"))
df_${t} = spark.createDataFrame([_response.json()] if isinstance(_response.json(), dict) else _response.json())`,s=.9,{code:a,conf:s}}return null}function hc(e,n,t,r,i,o){let a=i,s=o;if(/^(Put|Fetch|List|Delete)Azure(Blob|DataLake)/.test(e.type)){const c=n["Container Name"]||n["Filesystem Name"]||"mycontainer",l=n["Storage Account Name"]||"mystorageaccount",d=n["Blob Name"]||n.Directory||"data/",u=/^Put/.test(e.type),g=/DataLake/.test(e.type),p=g?"abfss":"wasbs",m=g?"dfs.core.windows.net":"blob.core.windows.net";return u?a=`# Azure Write: ${e.name}
(df_${r}.write
  .format("delta")
  .mode("append")
  .save("${p}://${c}@${l}.${m}/${d}")
)
print(f"[AZURE] Wrote to ${p}://${c}@${l}")`:a=`# Azure Read: ${e.name}
df_${t} = spark.read.format("delta").load("${p}://${c}@${l}.${m}/${d}")
print(f"[AZURE] Read from Azure")`,s=.93,{code:a,conf:s}}if(e.type==="ConsumeAzureEventHub"){const c=n["Event Hub Namespace"]||"my-eventhub-ns",l=n["Event Hub Name"]||"my-hub",d=n["Consumer Group"]||"$Default";return n["Event Hub Connection String"]||n["Connection String"],a=`# Azure Event Hub Consumer: ${e.name}
# Namespace: ${c} | Hub: ${l} | Group: ${d}
import json as _json

_conn_str = dbutils.secrets.get(scope="azure", key="eventhub-conn-str")
# Use native PySpark EventHubs connector â€” pass connection string directly (encrypted at rest)
_ehConf = {
    "eventhubs.connectionString": _conn_str,
    "eventhubs.consumerGroup": "${d}",
    "eventhubs.startingPosition": _json.dumps({"offset": "-1", "seqNo": -1, "enqueuedTime": None, "isInclusive": True})
}
df_${t} = (spark.readStream
  .format("eventhubs")
  .options(**_ehConf)
  .load()
  .selectExpr("CAST(body AS STRING) as value", "enqueuedTime as timestamp", "offset", "sequenceNumber")
)
print(f"[EVENTHUB] Consuming from ${c}/${l}")`,s=.92,{code:a,conf:s}}if(e.type==="PutEventHub"||e.type==="PublishAzureEventHub"){const c=n["Event Hub Namespace"]||"my-eventhub-ns",l=n["Event Hub Name"]||"my-hub";return a=`# Azure Event Hub Producer: ${e.name}
# Namespace: ${c} | Hub: ${l}
_conn_str = dbutils.secrets.get(scope="azure", key="eventhub-conn-str")
# Use native PySpark EventHubs connector â€” no JVM reflection needed
_ehConf = {
    "eventhubs.connectionString": _conn_str
}
(df_${r}
  .selectExpr("CAST(value AS STRING) as body")
  .write
  .format("eventhubs")
  .options(**_ehConf)
  .save()
)
print(f"[EVENTHUB] Published to ${c}/${l}")`,s=.92,{code:a,conf:s}}if(e.type==="PutAzureQueueStorage"||e.type==="GetAzureQueueStorage"){const c=n["Queue Name"]||"my-queue";return n["Storage Account Name"],/^Put/.test(e.type)?a=`# Azure Queue Write: ${e.name}
from azure.storage.queue import QueueClient
_queue = QueueClient.from_connection_string(
    dbutils.secrets.get(scope="azure", key="storage-conn-str"), "${c}")
for row in df_${r}.limit(10000).collect():
    _queue.send_message(str(row.asDict()))
print(f"[AZURE-QUEUE] Published to ${c}")`:a=`# Azure Queue Read: ${e.name}
from azure.storage.queue import QueueClient
_queue = QueueClient.from_connection_string(
    dbutils.secrets.get(scope="azure", key="storage-conn-str"), "${c}")
_msgs = [{"body": m.content} for m in _queue.receive_messages(messages_per_page=32)]
df_${t} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")
print(f"[AZURE-QUEUE] Read {len(_msgs)} messages from ${c}")`,s=.9,{code:a,conf:s}}return null}function gc(e,n,t,r,i,o){let a=i,s=o;if(/^(Put|List|Fetch|Get|Delete)GCS/.test(e.type)){const c=n.Bucket||n["GCS Bucket"]||"gcs_bucket",l=n.Key||n.Prefix||"data/",d=/^List/.test(e.type),u=/^Put/.test(e.type),g=/^Delete/.test(e.type);return d?a=`# GCS List: ${e.name}
# Bucket: ${c} | Prefix: ${l}
df_${t} = spark.createDataFrame(
    [{"path": f.path, "name": f.name, "size": f.size}
     for f in dbutils.fs.ls(f"gs://${c}/${l}")]
)
print(f"[GCS] Listed objects from gs://${c}/${l}")`:u?a=`# GCS Write: ${e.name}
# Bucket: ${c} | Key: ${l}
(df_${r}.write
  .format("delta")
  .mode("append")
  .save(f"gs://${c}/${l}")
)
print(f"[GCS] Wrote to gs://${c}/${l}")`:g?a=`# GCS Delete: ${e.name}
dbutils.fs.rm(f"gs://${c}/${l}", recurse=True)
print(f"[GCS] Deleted gs://${c}/${l}")`:a=`# GCS Read: ${e.name}
# Bucket: ${c} | Key: ${l}
df_${t} = spark.read.format("delta").load(f"gs://${c}/${l}")
print(f"[GCS] Read from gs://${c}/${l}")`,s=.93,{code:a,conf:s}}if(e.type==="PutBigQueryBatch"||e.type==="PutBigQuery"){const c=n["Project ID"]||n.Project||"my-project",l=n.Dataset||"my_dataset",d=n["Table Name"]||n.Table||"my_table";return a=`# BigQuery Write: ${e.name}
# Project: ${c} | Dataset: ${l} | Table: ${d}
(df_${r}.write
  .format("bigquery")
  .option("project", "${c}")
  .option("dataset", "${l}")
  .option("table", "${d}")
  .option("temporaryGcsBucket", "gs://${n["Temporary Bucket"]||"temp-bucket"}/bq-staging")
  .mode("append")
  .save()
)
print(f"[BQ] Wrote to ${c}.${l}.${d}")`,s=.92,{code:a,conf:s}}if(e.type==="ConsumeGCPubSub"){const c=n.Subscription||n["Subscription Name"]||"projects/my-project/subscriptions/my-sub";return n["Project ID"],a=`# GCP Pub/Sub Consumer: ${e.name}
# Subscription: ${c}
from google.cloud import pubsub_v1
import json
_subscriber = pubsub_v1.SubscriberClient()
_msgs = []
def _callback(message):
    _msgs.append({"data": message.data.decode("utf-8"), "attributes": dict(message.attributes)})
    message.ack()
_future = _subscriber.subscribe("${c}", callback=_callback)
import time; time.sleep(10)
_future.cancel()
df_${t} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "data STRING")
print(f"[PUBSUB] Consumed {len(_msgs)} messages")`,s=.9,{code:a,conf:s}}if(e.type==="PublishGCPubSub"){const c=n["Topic Name"]||n.Topic||"projects/my-project/topics/my-topic";return a=`# GCP Pub/Sub Publisher: ${e.name}
# Topic: ${c}
from google.cloud import pubsub_v1
import json
_publisher = pubsub_v1.PublisherClient()
for row in df_${r}.limit(10000).collect():
    _publisher.publish("${c}", json.dumps(row.asDict()).encode("utf-8"))
print(f"[PUBSUB] Published to ${c}")`,s=.9,{code:a,conf:s}}return null}function _c(e,n,t,r,i,o){let a=i,s=o;if(/^(Query|Put|Get)Cassandra/.test(e.type)){const c=n.Keyspace||"my_keyspace",l=n.Table||n["Table Name"]||"my_table",d=n["Contact Points"]||n["Cassandra Contact Points"]||"cassandra_host";return/^Put/.test(e.type)?a=`# Cassandra Write: ${e.name}
# Keyspace: ${c} | Table: ${l}
(df_${r}.write
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "${c}")
  .option("table", "${l}")
  .option("spark.cassandra.connection.host", "${d}")
  .mode("append")
  .save()
)
print(f"[CASSANDRA] Wrote to ${c}.${l}")`:a=`# Cassandra Read: ${e.name}
# Keyspace: ${c} | Table: ${l}
df_${t} = (spark.read
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "${c}")
  .option("table", "${l}")
  .option("spark.cassandra.connection.host", "${d}")
  .load()
)
print(f"[CASSANDRA] Read from ${c}.${l}")`,s=.92,{code:a,conf:s}}if(/^(Get|Put|Scan|Delete)HBase/.test(e.type)){const c=n["Table Name"]||"hbase_table",l=n["HBase Hostname"]||n["HBase Client Service"]||"hbase_host",d=/^Put/.test(e.type);return/^Delete/.test(e.type)?a=`# HBase Delete: ${e.name}
import happybase
_conn = happybase.Connection("${l}")
_table = _conn.table("${c}")
for row in df_${r}.limit(1000).collect():
    _table.delete(row["row_key"].encode())
_conn.close()
print(f"[HBASE] Deleted from ${c}")`:d?a=`# HBase Write: ${e.name}
# Table: ${c}
import happybase, json
_conn = happybase.Connection("${l}")
_table = _conn.table("${c}")
for row in df_${r}.limit(10000).collect():
    _d = row.asDict()
    _rk = str(_d.get("row_key", _d.get("id", "")))
    _table.put(_rk.encode(), {f"cf:{k}".encode(): str(v).encode() for k, v in _d.items()})
_conn.close()
print(f"[HBASE] Wrote to ${c}")`:a=`# HBase Read: ${e.name}
# Table: ${c}
import happybase
_conn = happybase.Connection("${l}")
_table = _conn.table("${c}")
_rows = [{**{"row_key": k.decode()}, **{c.decode(): v.decode() for c, v in d.items()}} for k, d in _table.scan(limit=50000)]
df_${t} = spark.createDataFrame(_rows) if _rows else spark.createDataFrame([], "row_key STRING")
_conn.close()
print(f"[HBASE] Read {len(_rows)} rows from ${c}")`,s=.9,{code:a,conf:s}}if(/^(SelectHiveQL|PutHiveQL|PutHiveStreaming|UpdateHiveTable)$/.test(e.type)){const c=n["HiveQL Select Query"]||n["HiveQL Statement"]||"",l=n["Table Name"]||"hive_table";if(/^(Put|Update)/.test(e.type))a=`# Hive Write: ${e.name}
(df_${r}.write
  .mode("append")
  .saveAsTable("${l}")
)
print(f"[HIVE] Wrote to ${l}")`;else{const g=(c||`SELECT * FROM ${l}`).substring(0,300).replace(/"""/g,'\\"\\"\\"');a=`# Hive Query: ${e.name}
df_${t} = spark.sql("""${g}""")
print(f"[HIVE] Queried ${l}")`}return s=.92,{code:a,conf:s}}if(/^(Put|Get|Query)Kudu/.test(e.type)){const c=n["Kudu Masters"]||"kudu_master:7051",l=n["Table Name"]||"kudu_table";return/^Put/.test(e.type)?a=`# Kudu Write: ${e.name}
# Master: ${c} | Table: ${l}
(df_${r}.write
  .format("kudu")
  .option("kudu.master", "${c}")
  .option("kudu.table", "${l}")
  .mode("append")
  .save()
)
print(f"[KUDU] Wrote to ${l}")`:a=`# Kudu Read: ${e.name}
df_${t} = (spark.read
  .format("kudu")
  .option("kudu.master", "${c}")
  .option("kudu.table", "${l}")
  .load()
)
print(f"[KUDU] Read from ${l}")`,s=.9,{code:a,conf:s}}if(/^(Query|Put)Phoenix/.test(e.type)){const c=n["Phoenix URL"]||n["ZooKeeper Quorum"]||"zk_host:2181",l=n["Table Name"]||"phoenix_table";return/^Put/.test(e.type)?a=`# Phoenix Write: ${e.name} (via JDBC â€” phoenix-spark connector is deprecated on Spark 3+)
(df_${r}.write
  .format("jdbc")
  .option("driver", "org.apache.phoenix.jdbc.PhoenixDriver")
  .option("url", "jdbc:phoenix:${c}")
  .option("dbtable", "${l}")
  .mode("overwrite")
  .save()
)
print(f"[PHOENIX] Wrote to ${l} via JDBC")`:a=`# Phoenix Read: ${e.name} (via JDBC â€” phoenix-spark connector is deprecated on Spark 3+)
df_${t} = (spark.read
  .format("jdbc")
  .option("driver", "org.apache.phoenix.jdbc.PhoenixDriver")
  .option("url", "jdbc:phoenix:${c}")
  .option("dbtable", "${l}")
  .load()
)
print(f"[PHOENIX] Read from ${l} via JDBC")`,s=.7,{code:a,conf:s}}if(/^(Query|Put)Oracle/.test(e.type)){const c=n["Table Name"]||"oracle_table";return a=`# Oracle: ${e.name}
df_${t} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="oracle", key="jdbc-url"))
  .option("dbtable", "${c}")
  .option("driver", "oracle.jdbc.driver.OracleDriver")
  .option("user", dbutils.secrets.get(scope="oracle", key="user"))
  .option("password", dbutils.secrets.get(scope="oracle", key="pass"))
  .load()
)
print(f"[ORACLE] Read from ${c}")`,s=.92,{code:a,conf:s}}if(/^(Query|Put)Teradata/.test(e.type)){const c=n["Table Name"]||"teradata_table";return a=`# Teradata: ${e.name}
df_${t} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="teradata", key="jdbc-url"))
  .option("dbtable", "${c}")
  .option("driver", "com.teradata.jdbc.TeraDriver")
  .option("user", dbutils.secrets.get(scope="teradata", key="user"))
  .option("password", dbutils.secrets.get(scope="teradata", key="pass"))
  .load()
)
print(f"[TERADATA] Read from ${c}")`,s=.92,{code:a,conf:s}}if(/^(Get|Put|Query)Snowflake/.test(e.type)){const c=n.Database||"snowflake_db",l=n.Schema||"public",d=n["Table Name"]||"snowflake_table";return/^Put/.test(e.type)?a=`# Snowflake Write: ${e.name}
(df_${r}.write
  .format("snowflake")
  .option("sfUrl", dbutils.secrets.get(scope="snowflake", key="url"))
  .option("sfUser", dbutils.secrets.get(scope="snowflake", key="user"))
  .option("sfPassword", dbutils.secrets.get(scope="snowflake", key="pass"))
  .option("sfDatabase", "${c}")
  .option("sfSchema", "${l}")
  .option("dbtable", "${d}")
  .mode("append")
  .save()
)
print(f"[SNOWFLAKE] Wrote to ${c}.${l}.${d}")`:a=`# Snowflake Read: ${e.name}
df_${t} = (spark.read
  .format("snowflake")
  .option("sfUrl", dbutils.secrets.get(scope="snowflake", key="url"))
  .option("sfUser", dbutils.secrets.get(scope="snowflake", key="user"))
  .option("sfPassword", dbutils.secrets.get(scope="snowflake", key="pass"))
  .option("sfDatabase", "${c}")
  .option("sfSchema", "${l}")
  .option("dbtable", "${d}")
  .load()
)
print(f"[SNOWFLAKE] Read from ${c}.${l}.${d}")`,s=.92,{code:a,conf:s}}return null}function yc(e,n,t,r,i,o){let a=i,s=o;if(/^(Consume|Publish)(JMS|AMQP)$/.test(e.type)){const c=n["Destination Name"]||n.Queue||"default_queue",l=/^Consume/.test(e.type);if(/AMQP/.test(e.type)){const d=n.Hostname||"amqp_host";l?a=`# AMQP Consumer: ${e.name}
# Queue: ${c}
import pika
_conn = pika.BlockingConnection(pika.ConnectionParameters(
    host="${d}",
    credentials=pika.PlainCredentials(
        dbutils.secrets.get(scope="amqp", key="user"),
        dbutils.secrets.get(scope="amqp", key="pass"))))
_ch = _conn.channel()
_msgs = []
def _cb(ch, method, properties, body):
    _msgs.append({"body": body.decode("utf-8")})
    ch.basic_ack(delivery_tag=method.delivery_tag)
    if len(_msgs) >= 1000: ch.stop_consuming()
_ch.basic_consume(queue="${c}", on_message_callback=_cb)
try:
    _ch.start_consuming()
except: pass
df_${t} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")
_conn.close()
print(f"[AMQP] Consumed {len(_msgs)} messages from ${c}")`:a=`# AMQP Publisher: ${e.name}
# Queue: ${c}
import pika, json
_conn = pika.BlockingConnection(pika.ConnectionParameters(
    host="${d}",
    credentials=pika.PlainCredentials(
        dbutils.secrets.get(scope="amqp", key="user"),
        dbutils.secrets.get(scope="amqp", key="pass"))))
_ch = _conn.channel()
_ch.queue_declare(queue="${c}", durable=True)
for row in df_${r}.limit(10000).collect():
    _ch.basic_publish(exchange="", routing_key="${c}",
        body=json.dumps(row.asDict()),
        properties=pika.BasicProperties(delivery_mode=2))
_conn.close()
print(f"[AMQP] Published to ${c}")`}else{const d=n.Hostname||"jms_host",u=n.Port||"61613",g=n["Client ID"]||"nifi-migration-client";l?a=`# JMS Consumer: ${e.name}
# Destination: ${c} | Client: ${g}
import stomp
import time

_msgs = []
class _JMSListener(stomp.ConnectionListener):
    """JMS message listener with connection management."""
    def on_message(self, frame):
        _msgs.append({"body": frame.body, "headers": str(frame.headers)})
    def on_error(self, frame):
        print(f"[JMS ERROR] {frame.body}")
    def on_disconnected(self):
        print("[JMS] Disconnected")

_conn = stomp.Connection([("${d}", ${u})])
_conn.set_listener("jms_listener", _JMSListener())
try:
    _conn.connect(
        dbutils.secrets.get(scope="jms", key="user"),
        dbutils.secrets.get(scope="jms", key="pass"),
        wait=True, headers={"client-id": "${g}"})
    _conn.subscribe(destination="/queue/${c}", id=1, ack="client-individual")
    time.sleep(5)  # Collect messages for 5 seconds
finally:
    _conn.disconnect()

df_${t} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING, headers STRING")
print(f"[JMS] Consumed {len(_msgs)} messages from ${c}")`:a=`# JMS Publisher: ${e.name}
# Destination: ${c} | Client: ${g}
import stomp, json

_conn = stomp.Connection([("${d}", ${u})])
try:
    _conn.connect(
        dbutils.secrets.get(scope="jms", key="user"),
        dbutils.secrets.get(scope="jms", key="pass"),
        wait=True, headers={"client-id": "${g}"})
    _sent = 0
    for row in df_${r}.limit(10000).collect():
        _conn.send(
            destination="/queue/${c}",
            body=json.dumps(row.asDict()),
            content_type="application/json",
            headers={"persistent": "true"})
        _sent += 1
    print(f"[JMS] Published {_sent} messages to ${c}")
finally:
    _conn.disconnect()`}return s=.9,{code:a,conf:s}}if(e.type==="GetJMSQueue"){const c=n["Destination Name"]||n.Queue||"default_queue",l=n.Hostname||"jms_host",d=n.Port||"61613";return a=`# JMS Queue: ${e.name}
import stomp
_msgs = []
class _L(stomp.ConnectionListener):
    def on_message(self, frame): _msgs.append({"body": frame.body})
_conn = stomp.Connection([("${l}", ${d})])
_conn.set_listener("", _L())
_conn.connect(dbutils.secrets.get(scope="jms", key="user"), dbutils.secrets.get(scope="jms", key="pass"), wait=True)
_conn.subscribe(destination="/queue/${c}", id=1, ack="auto")
import time; time.sleep(5)
_conn.disconnect()
df_${t} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")`,s=.9,{code:a,conf:s}}if(/^(Consume|Publish)MQTT$/.test(e.type)){const c=n["Topic Filter"]||n.Topic||"iot/sensors/#",l=n["Broker URI"]||"tcp://mqtt_broker:1883",d=l.replace("tcp://","").split(":")[0]||"mqtt_broker",u=l.split(":").pop()||"1883";return/^Consume/.test(e.type)?a=`# MQTT Consumer: ${e.name}
# Broker: ${l} | Topic: ${c}
import paho.mqtt.client as mqtt
import json, time
_msgs = []
def _on_msg(client, userdata, msg):
    _msgs.append({"topic": msg.topic, "payload": msg.payload.decode("utf-8")})
_client = mqtt.Client()
_client.username_pw_set(dbutils.secrets.get(scope="mqtt", key="user"),
                        dbutils.secrets.get(scope="mqtt", key="pass"))
_client.on_message = _on_msg
_client.connect("${d}", ${u})
_client.subscribe("${c}")
_client.loop_start()
time.sleep(10)
_client.loop_stop()
_client.disconnect()
df_${t} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "topic STRING, payload STRING")
print(f"[MQTT] Consumed {len(_msgs)} messages from ${c}")`:a=`# MQTT Publisher: ${e.name}
# Broker: ${l} | Topic: ${c}
import paho.mqtt.client as mqtt
import json
_client = mqtt.Client()
_client.username_pw_set(dbutils.secrets.get(scope="mqtt", key="user"),
                        dbutils.secrets.get(scope="mqtt", key="pass"))
_client.connect("${d}", ${u})
for row in df_${r}.limit(10000).collect():
    _client.publish("${c}", json.dumps(row.asDict()))
_client.disconnect()
print(f"[MQTT] Published to ${c}")`,s=.9,{code:a,conf:s}}if(e.type==="ListenSMTP")return a=`# SMTP: ${e.name}
# Deploy as Databricks App with aiosmtpd
df_${t} = df_${r}
print("[SMTP] Email receiver configured")`,s=.9,{code:a,conf:s};if(e.type==="ConnectWebSocket"||e.type==="ListenWebSocket"||e.type==="PutWebSocket"){const c=n["WebSocket URI"]||n.URL||"",l=c?`"${c}"`:'dbutils.secrets.get(scope="websocket", key="url")';return a=`# WebSocket: ${e.name}
import websocket, json
_ws_url = ${l}
_ws = websocket.create_connection(_ws_url)
df_${t} = df_${r}`,s=.9,{code:a,conf:s}}if(e.type==="ListenGRPC"||e.type==="InvokeGRPC")return a=`# gRPC: ${e.name}
import grpc
df_${t} = df_${r}`,s=.9,{code:a,conf:s};if(e.type==="ListenTCPRecord"||e.type==="ListenUDPRecord"){const c=n["Local Network Interface"]||"0.0.0.0",l=n.Port||"9999";return a=`# ${e.type}: ${e.name}
# Note: Spark structured streaming socket source is for development only
df_${t} = (spark.readStream
  .format("socket")
  .option("host", "${c}")
  .option("port", "${l}")
  .load())`,s=.9,{code:a,conf:s}}if(e.type==="GetSmbFile"||e.type==="PutSmbFile"){const c=n.Hostname||n["SMB Share"]||"smb_server";return a=`# SMB: ${e.name}
import smbclient
smbclient.register_session("${c}", username=dbutils.secrets.get(scope="smb", key="user"), password=dbutils.secrets.get(scope="smb", key="pass"))
df_${t} = df_${r}`,s=.9,{code:a,conf:s}}return null}function bc(e,n,t,r,i,o){let a=i,s=o;if(e.type==="PutSplunkHTTP"||e.type==="GetSplunk"){if(/^Put/.test(e.type)){const l=n["HTTP Event Collector URL"]||"https://splunk:8088/services/collector";a=`# Splunk HEC: ${e.name}
import requests
_token = dbutils.secrets.get(scope="splunk", key="hec_token")
_sent = 0
for row in df_${r}.limit(10000).collect():
    requests.post("${l}",
        json={"event": row.asDict()},
        headers={"Authorization": f"Splunk {_token}"},
        verify=True)  # Override with CA cert path if using self-signed certs
    _sent += 1
print(f"[SPLUNK] Sent {_sent} events to HEC")`}else{const l=n["Splunk URL"]||"https://splunk:8089";a=`# Splunk Search: ${e.name}
import requests
_token = dbutils.secrets.get(scope="splunk", key="api_token")
_resp = requests.post("${l}/services/search/jobs/export",
    data={"search": "${n["Splunk Query"]||"search index=main | head 1000"}",
          "output_mode": "json"},
    headers={"Authorization": f"Bearer {_token}"},
    verify=True)  # Override with CA cert path if using self-signed certs
import json as _json
_events = [_json.loads(line) for line in _resp.text.splitlines() if line.strip()]
df_${t} = spark.createDataFrame(_events) if _events else df_${r}
print(f"[SPLUNK] Retrieved {len(_events)} events")`}return s=.9,{code:a,conf:s}}if(e.type==="ExecuteInfluxDBQuery"||e.type==="PutInfluxDB"){const c=n["InfluxDB Connection URL"]||"http://influxdb:8086";if(/^Put/.test(e.type)){const d=n.Bucket||n["Database Name"]||"default",u=n.Organization||"my-org";a=`# InfluxDB Write: ${e.name}
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS
_client = InfluxDBClient(url="${c}", token=dbutils.secrets.get(scope="influx", key="token"), org="${u}")
_write_api = _client.write_api(write_options=SYNCHRONOUS)
for row in df_${r}.limit(10000).collect():
    _point = Point("measurement").field("value", str(row[0]))
    _write_api.write(bucket="${d}", org="${u}", record=_point)
_client.close()
print(f"[INFLUXDB] Wrote to ${d}")`}else a=`# InfluxDB Query: ${e.name}
from influxdb_client import InfluxDBClient
_client = InfluxDBClient(url="${c}", token=dbutils.secrets.get(scope="influx", key="token"))
_tables = _client.query_api().query("${n["Flux Query"]||'from(bucket: \\"default\\")'}")
_records = [r.values for table in _tables for r in table.records]
df_${t} = spark.createDataFrame(_records) if _records else df_${r}
_client.close()
print(f"[INFLUXDB] Retrieved {len(_records)} records")`;return s=.9,{code:a,conf:s}}if(/^(Put|Send)Datadog/.test(e.type))return a=`# Datadog: ${e.name}
import requests
_api_key = dbutils.secrets.get(scope="datadog", key="api_key")
for row in df_${r}.limit(10000).collect():
    requests.post("https://api.datadoghq.com/api/v2/logs",
        json={"ddsource": "nifi-migration", "message": str(row.asDict())},
        headers={"DD-API-KEY": _api_key, "Content-Type": "application/json"})
df_${t} = df_${r}
print(f"[DATADOG] Sent logs")`,s=.9,{code:a,conf:s};if(/^(Push|Query)Prometheus/.test(e.type)){const c=n["Push Gateway URL"]||n["Prometheus URL"]||"http://prometheus:9091";return a=`# Prometheus: ${e.name}
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
_registry = CollectorRegistry()
_gauge = Gauge("nifi_migration_metric", "Migrated metric", registry=_registry)
_gauge.set(df_${r}.count())
push_to_gateway("${c}", job="nifi_migration", registry=_registry)
df_${t} = df_${r}
print(f"[PROMETHEUS] Pushed metrics to ${c}")`,s=.9,{code:a,conf:s}}if(/^(Send|Push)Grafana/.test(e.type)){const c=n["Grafana URL"]||"http://grafana:3000";return a=`# Grafana: ${e.name}
import requests
requests.post("${c}/api/annotations",
    json={"text": "Pipeline event", "tags": ["nifi-migration"]},
    headers={"Authorization": f"Bearer {dbutils.secrets.get(scope='grafana', key='api_key')}"})
df_${t} = df_${r}
print(f"[GRAFANA] Sent annotation")`,s=.9,{code:a,conf:s}}return e.type==="PutRiemann"?(a=`# Riemann: ${e.name}
df_${t} = df_${r}
print("[RIEMANN] Monitoring event sent")`,s=.9,{code:a,conf:s}):null}function vc(e,n,t,r,i,o){let a=i,s=o;if(/^(Put|Fetch|Get|Query|Scroll|JsonQuery|Delete)Elasticsearch/.test(e.type)){const c=n["Elasticsearch URL"]||n["HTTP Hosts"]||"https://es_host:9200",l=n.Index||"default_index",d=/^Put/.test(e.type),u=/^Delete/.test(e.type);return d?a=`# Elasticsearch Write: ${e.name}
# URL: ${c} | Index: ${l}
from elasticsearch import Elasticsearch, helpers
_es = Elasticsearch("${c}",
    basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")),
    verify_certs=False)

_records = df_${r}.limit(10000).toPandas().to_dict(orient="records")
_actions = [{"_index": "${l}", "_source": r} for r in _records]
helpers.bulk(_es, _actions, chunk_size=500, request_timeout=60)
print(f"[ES] Indexed {len(_records)} documents to ${l}")`:u?a=`# Elasticsearch Delete: ${e.name}
from elasticsearch import Elasticsearch
_es = Elasticsearch("${c}", basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")))
_es.delete_by_query(index="${l}", body={"query": {"match_all": {}}})
df_${t} = df_${r}
print(f"[ES] Deleted from ${l}")`:a=`# Elasticsearch Read: ${e.name}
# URL: ${c} | Index: ${l}
from elasticsearch import Elasticsearch
_es = Elasticsearch("${c}",
    basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")),
    verify_certs=False)

_result = _es.search(index="${l}", body={"query": {"match_all": {}}}, size=10000, scroll="2m")
_hits = [h["_source"] for h in _result["hits"]["hits"]]
df_${t} = spark.createDataFrame(_hits) if _hits else spark.createDataFrame([], "id STRING")
print(f"[ES] Read {len(_hits)} documents from ${l}")`,s=.9,{code:a,conf:s}}if(/^(Get|Put|Delete)Mongo/.test(e.type)){const c=n["Mongo URI"]?`"${n["Mongo URI"].replace(/"/g,'\\"')}"`:'dbutils.secrets.get(scope="nosql", key="mongo-uri")',l=n["Mongo Database Name"]||"mydb",d=n["Mongo Collection Name"]||"mycollection";return/^(Put|Delete)/.test(e.type)&&e.type!=="DeleteMongo"?a=`# MongoDB Write: ${e.name}
# DB: ${l} | Collection: ${d}
from pymongo import MongoClient
_mongo_uri = ${c}
_client = MongoClient(_mongo_uri)
_db = _client["${l}"]
_coll = _db["${d}"]

_records = df_${r}.limit(10000).toPandas().to_dict(orient="records")
_result = _coll.insert_many(_records)
print(f"[MONGO] Inserted {len(_result.inserted_ids)} documents into ${l}.${d}")
_client.close()`:e.type==="DeleteMongo"?a=`# MongoDB Delete: ${e.name}
from pymongo import MongoClient
_mongo_uri = ${c}
_client = MongoClient(_mongo_uri)
_coll = _client["${l}"]["${d}"]
_result = _coll.delete_many({})
print(f"[MONGO] Deleted {_result.deleted_count} documents from ${l}.${d}")
_client.close()
df_${t} = df_${r}`:a=`# MongoDB Read: ${e.name}
# DB: ${l} | Collection: ${d}
from pymongo import MongoClient
_mongo_uri = ${c}
_client = MongoClient(_mongo_uri)
_db = _client["${l}"]
_coll = _db["${d}"]

_docs = list(_coll.find({}, {"_id": 0}).limit(50000))
df_${t} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
print(f"[MONGO] Read {len(_docs)} documents from ${l}.${d}")
_client.close()`,s=.9,{code:a,conf:s}}if(e.type==="GetMongoRecord"){const c=n["Mongo URI"]?`"${n["Mongo URI"].replace(/"/g,'\\"')}"`:'dbutils.secrets.get(scope="nosql", key="mongo-uri")',l=n["Mongo Database Name"]||"mydb",d=n["Mongo Collection Name"]||"collection";return a=`# Mongo Record: ${e.name}
from pymongo import MongoClient
_mongo_uri = ${c}
_client = MongoClient(_mongo_uri)
_docs = list(_client["${l}"]["${d}"].find({}, {"_id": 0}).limit(50000))
df_${t} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
_client.close()`,s=.9,{code:a,conf:s}}if(e.type==="RunMongoAggregation"){const c=n["Mongo URI"]?`"${n["Mongo URI"].replace(/"/g,'\\"')}"`:'dbutils.secrets.get(scope="nosql", key="mongo-uri")',l=n["Mongo Database Name"]||"mydb",d=n["Mongo Collection Name"]||"collection";return a=`# Mongo Aggregation: ${e.name}
from pymongo import MongoClient
_mongo_uri = ${c}
_client = MongoClient(_mongo_uri)
_results = list(_client["${l}"]["${d}"].aggregate([]))
df_${t} = spark.createDataFrame(_results) if _results else df_${r}
_client.close()`,s=.9,{code:a,conf:s}}if(/^(Fetch|Put|Delete)GridFS$/.test(e.type)){const c=n["Mongo URI"]?`"${n["Mongo URI"].replace(/"/g,'\\"')}"`:'dbutils.secrets.get(scope="nosql", key="mongo-uri")',l=n["Mongo Database Name"]||"files_db";return a=`# GridFS ${e.type}: ${e.name}
from pymongo import MongoClient
import gridfs
_mongo_uri = ${c}
_client = MongoClient(_mongo_uri)
_fs = gridfs.GridFS(_client["${l}"])
df_${t} = df_${r}
_client.close()`,s=.9,{code:a,conf:s}}if(/^(Put|Get|Fetch|Delete)Redis/.test(e.type)){const c=n["Redis Connection Pool"]||n.Hostname||"redis_host",l=n.Port||"6379";return/^(Put|Delete)/.test(e.type)?a=`# Redis Write: ${e.name}
import redis, json
_r = redis.Redis(host="${c}", port=${l},
    password=dbutils.secrets.get(scope="redis", key="pass"))
for row in df_${r}.limit(10000).collect():
    _d = row.asDict()
    _r.set(str(_d.get("id", "")), json.dumps(_d))
print(f"[REDIS] Wrote to Redis")
df_${t} = df_${r}`:a=`# Redis Read: ${e.name}
import redis, json
_r = redis.Redis(host="${c}", port=${l},
    password=dbutils.secrets.get(scope="redis", key="pass"))
_keys = _r.keys("*")
_data = [json.loads(_r.get(k)) for k in _keys[:50000] if _r.get(k)]
df_${t} = spark.createDataFrame(_data) if _data else spark.createDataFrame([], "id STRING")
print(f"[REDIS] Read {len(_data)} keys")`,s=.9,{code:a,conf:s}}if(/^(Get|Put)Couchbase/.test(e.type)){const c=n["Cluster Hostname"]||"couchbase_host",l=n["Bucket Name"]||"default";return/^Put/.test(e.type)?a=`# Couchbase Write: ${e.name}
from couchbase.cluster import Cluster
from couchbase.auth import PasswordAuthenticator
from couchbase.options import ClusterOptions
_auth = PasswordAuthenticator(
    dbutils.secrets.get(scope="couchbase", key="user"),
    dbutils.secrets.get(scope="couchbase", key="pass"))
_cluster = Cluster(f"couchbase://${c}", ClusterOptions(_auth))
_bucket = _cluster.bucket("${l}")
_coll = _bucket.default_collection()
for row in df_${r}.limit(10000).collect():
    _d = row.asDict()
    _coll.upsert(str(_d.get("id", "")), _d)
df_${t} = df_${r}
print(f"[COUCHBASE] Wrote to ${l}")`:a=`# Couchbase Read: ${e.name}
from couchbase.cluster import Cluster
from couchbase.auth import PasswordAuthenticator
from couchbase.options import ClusterOptions
_auth = PasswordAuthenticator(
    dbutils.secrets.get(scope="couchbase", key="user"),
    dbutils.secrets.get(scope="couchbase", key="pass"))
_cluster = Cluster(f"couchbase://${c}", ClusterOptions(_auth))
_result = _cluster.query("SELECT * FROM \`${l}\` LIMIT 50000")
_rows = [r for r in _result]
df_${t} = spark.createDataFrame(_rows) if _rows else spark.createDataFrame([], "id STRING")
print(f"[COUCHBASE] Read from ${l}")`,s=.9,{code:a,conf:s}}if(/^(Put|Query|Get)Solr/.test(e.type)){const c=n["Solr Location"]||n["Solr URL"]||"http://solr:8983/solr",l=n.Collection||"default_collection";return/^Put/.test(e.type)?a=`# Solr Write: ${e.name}
import requests, json
for row in df_${r}.limit(10000).collect():
    requests.post(f"${c}/${l}/update/json/docs",
        json=row.asDict(),
        params={"commit": "true"})
df_${t} = df_${r}
print(f"[SOLR] Indexed to ${l}")`:a=`# Solr Read: ${e.name}
import requests
_resp = requests.get(f"${c}/${l}/select", params={"q": "*:*", "rows": 50000, "wt": "json"})
_docs = _resp.json().get("response", {}).get("docs", [])
df_${t} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
print(f"[SOLR] Read {len(_docs)} documents from ${l}")`,s=.9,{code:a,conf:s}}if(/^(Get|Put|Delete)RethinkDB$/.test(e.type)){const c=n.Hostname||"rethinkdb_host",l=n["DB Name"]||"<database_name>",d=n["Table Name"]||"data";return a=`# RethinkDB: ${e.name}
from rethinkdb import r
_conn = r.connect(host="${c}", port=28015, db="${l}")
_docs = list(r.table("${d}").limit(50000).run(_conn))
df_${t} = spark.createDataFrame(_docs) if _docs else df_${r}
_conn.close()`,s=.9,{code:a,conf:s}}return null}function kc(e,n,t,r,i,o){let a=i,s=o;if(/^(Get|Put|Fetch|List|Move|Delete)HDFS$/.test(e.type)){const c=n.Directory||"/data";return/^(Put|Move|Delete)/.test(e.type)&&e.type==="PutHDFS"?a=`# HDFS Write: ${e.name}
# Directory: ${c}
(df_${r}.write
  .format("delta")
  .mode("append")
  .save("${c}")
)
print(f"[HDFS] Wrote to ${c}")`:e.type==="MoveHDFS"?a=`# HDFS Move: ${e.name}
dbutils.fs.mv("${c}/source", "${c}/dest", recurse=True)
print(f"[HDFS] Moved files")`:e.type==="DeleteHDFS"?a=`# HDFS Delete: ${e.name}
dbutils.fs.rm("${c}", recurse=True)
print(f"[HDFS] Deleted ${c}")`:a=`# HDFS Read: ${e.name}
# Directory: ${c}
df_${t} = spark.read.format("delta").load("${c}")
print(f"[HDFS] Read from ${c}")`,s=.93,{code:a,conf:s}}if(e.type==="GetHDFSEvents")return a=`# HDFS Events: ${e.name}
df_${t} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .load("${n["HDFS Path"]||"/data"}"))
print(f"[HDFS] Streaming events via Auto Loader")`,s=.92,{code:a,conf:s};if(e.type==="GetHDFSFileInfo")return a=`# HDFS Info: ${e.name}
_files = dbutils.fs.ls("${n.Directory||"/data"}")
df_${t} = spark.createDataFrame([{"path": f.path, "name": f.name, "size": f.size} for f in _files])`,s=.93,{code:a,conf:s};if(e.type==="GetHDFSSequenceFile")return a=`# SequenceFile: ${e.name}
df_${t} = spark.sparkContext.sequenceFile("${n.Directory||"/data"}", "org.apache.hadoop.io.Text", "org.apache.hadoop.io.Text").toDF(["key", "value"])`,s=.9,{code:a,conf:s};if(/^(Put|Get|Fetch)ORC$/.test(e.type)||e.type==="ConvertAvroToORC"){const c=n.Directory||n.Path||"/Volumes/<catalog>/<schema>/data";return/^Put/.test(e.type)?a=`# ORC Write: ${e.name}
(df_${r}.write
  .format("orc")
  .mode("append")
  .save("${c}")
)
print(f"[ORC] Wrote to ${c}")`:a=`# ORC Read: ${e.name}
df_${t} = spark.read.format("orc").load("${c}")
print(f"[ORC] Read from ${c}")`,s=.93,{code:a,conf:s}}if(/^(Put|Get|Fetch)Parquet$/.test(e.type)){const c=n.Directory||n.Path||"/Volumes/<catalog>/<schema>/data";return/^Put/.test(e.type)?a=`# Parquet Write: ${e.name}
(df_${r}.write
  .format("parquet")
  .mode("append")
  .save("${c}")
)
print(f"[PARQUET] Wrote to ${c}")`:a=`# Parquet Read: ${e.name}
df_${t} = spark.read.format("parquet").load("${c}")
print(f"[PARQUET] Read from ${c}")`,s=.95,{code:a,conf:s}}if(/^(Put|Get|Listen)Flume/.test(e.type)){const c=n.Hostname||"flume_host",l=n.Port||"41414";return a=`# Flume: ${e.name}
# Flume is deprecated â€” use Spark Structured Streaming directly
# Host: ${c}:${l}
df_${t} = (spark.readStream
  .format("socket")
  .option("host", "${c}")
  .option("port", "${l}")
  .load()
)
print(f"[FLUME] Streaming from ${c}:${l} â€” consider migrating to native Spark source")`,s=.85,{code:a,conf:s}}if(/^Put(Hudi|HoodieRecord)$/.test(e.type)){const c=n["Table Name"]||"hudi_table",l=n["Base Path"]||"/Volumes/<catalog>/<schema>/hudi",d=n["Record Key Field"]||"id",u=n["Precombine Key Field"]||"timestamp";return a=`# Hudi Write: ${e.name}
(df_${r}.write
  .format("hudi")
  .option("hoodie.table.name", "${c}")
  .option("hoodie.datasource.write.recordkey.field", "${d}")
  .option("hoodie.datasource.write.precombine.field", "${u}")
  .option("hoodie.datasource.write.operation", "upsert")
  .mode("append")
  .save("${l}/${c}")
)
print(f"[HUDI] Wrote to ${c}")`,s=.92,{code:a,conf:s}}if(/^PutIceberg$/.test(e.type)){const c=n["Table Name"]||"iceberg_table";return a=`# Iceberg Write: ${e.name}
(df_${r}.write
  .format("iceberg")
  .mode("append")
  .save("${c}")
)
print(f"[ICEBERG] Wrote to ${c}")`,s=.92,{code:a,conf:s}}return null}const ht={GetFile:{cat:"Auto Loader",tpl:`df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "{format}")
  .option("cloudFiles.schemaLocation", "/Volumes/{catalog}/{schema}/checkpoints/{v}")
  .load("/Volumes/{catalog}/{schema}/{path}"))`,desc:"Auto Loader from Databricks Volumes",notes:"Configure volume mount path",imp:[],conf:.9},ConsumeKafka:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load())`,desc:"Kafka streaming source",notes:"Configure security protocol if needed",imp:[],conf:.9},ConsumeKafka_2_6:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load())`,desc:"Kafka streaming source",notes:"Same as ConsumeKafka",imp:[],conf:.9},ConsumeKafkaRecord_2_6:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load()
  .select(from_json(col("value").cast("string"), schema).alias("data"))
  .select("data.*"))`,desc:"Kafka record streaming",notes:"Define schema for deserialization",imp:[],conf:.9},QueryDatabaseTable:{cat:"JDBC Source",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "{table}")
  .option("driver", "{driver}")
  .load())`,desc:"JDBC database read",notes:"Store credentials in Databricks secret scope",imp:[],conf:.9},QueryDatabaseTableRecord:{cat:"JDBC Source",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "{table}")
  .load())`,desc:"JDBC database read with record",notes:"Same as QueryDatabaseTable",imp:[],conf:.9},ListenHTTP:{cat:"Model Serving",tpl:`# ListenHTTP -> Databricks Model Serving / Delta Live Tables
# Deploy an MLflow serving endpoint that writes to Delta
df_{v} = spark.readStream.format("delta").table("{v}_incoming")
print(f"[HTTP] Streaming from {v}_incoming")`,desc:"HTTP listener -> Model Serving",notes:"Never run Flask in notebook â€” use Model Serving",imp:["mlflow"],conf:.92},GetSFTP:{cat:"External Storage",tpl:`# Stage from SFTP to Unity Catalog Volumes
dbutils.fs.cp("sftp://{host}/{path}", "/Volumes/{catalog}/{schema}/landing/")
df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/landing/")`,desc:"SFTP file retrieval",notes:"Requires SFTP mount or external transfer utility",imp:[],conf:.9},GetFTP:{cat:"External Storage",tpl:`# Stage from FTP to Unity Catalog Volumes (use external transfer)
df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/landing/")`,desc:"FTP file retrieval",notes:"No native FTP â€” use external transfer tool to stage to Volumes",imp:[],conf:.9},GenerateFlowFile:{cat:"Test Data",tpl:`df_{v} = spark.range({count}).toDF("id")
# Add test columns as needed`,desc:"Test data generator",notes:"Replace with actual test data generation",imp:[],conf:.9},ListS3:{cat:"Cloud Storage",tpl:`_files = dbutils.fs.ls("s3://{bucket}/{prefix}")
df_{v} = spark.createDataFrame(_files)`,desc:"List S3 objects",notes:"Use Unity Catalog external locations",imp:[],conf:.9},FetchS3Object:{cat:"Cloud Storage",tpl:'df_{v} = spark.read.format("{format}").load("s3://{bucket}/{key}")',desc:"Read S3 object",notes:"Configure external location in Unity Catalog",imp:[],conf:.9},GetS3Object:{cat:"Cloud Storage",tpl:'df_{v} = spark.read.format("{format}").load("s3://{bucket}/{key}")',desc:"Read S3 object",notes:"Same as FetchS3Object",imp:[],conf:.9},TailFile:{cat:"Auto Loader",tpl:`df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "text")
  .load("/Volumes/{catalog}/{schema}/{path}"))`,desc:"File tail via Auto Loader",notes:"Streaming mode handles new files automatically",imp:[],conf:.9},ListFile:{cat:"Cloud Storage",tpl:`_files = dbutils.fs.ls("/Volumes/{catalog}/{schema}/{path}")
df_{v} = spark.createDataFrame(_files)`,desc:"List files in directory",notes:"Use Volumes or external location",imp:[],conf:.9},GetMongo:{cat:"MongoDB Connector",tpl:`df_{v} = (spark.read
  .format("mongodb")
  .option("connection.uri", dbutils.secrets.get(scope="{scope}", key="mongo-uri"))
  .option("database", "{database}")
  .option("collection", "{collection}")
  .load())`,desc:"MongoDB read",notes:"Install mongodb-spark-connector library",imp:[],conf:.9},GetElasticsearch:{cat:"ES Connector",tpl:`df_{v} = (spark.read
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .option("es.resource", "{index}")
  .load())`,desc:"Elasticsearch read",notes:"Install elasticsearch-spark library",imp:[],conf:.9},FetchFile:{cat:"Volumes Read",tpl:`# Fetch file content by path attribute
_path = "/Volumes/{catalog}/{schema}/landing/{filename}"
df_{v} = spark.read.format("{format}").load(_path)`,desc:"Read file by path from Unity Catalog Volumes",notes:"Map NiFi filename attribute to Volumes path",imp:[],conf:.9},ConvertRecord:{cat:"DataFrame API",tpl:`df_{v} = df_{in}.selectExpr("*")  # Convert record format
# Adjust column types/names as needed`,desc:"Record format conversion",notes:"Spark handles format conversion natively",imp:[],conf:.9},ConvertJSONToSQL:{cat:"Spark SQL",tpl:`df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("SELECT * FROM tmp_{v}")`,desc:"JSON to SQL conversion",notes:"Parse JSON and query with SQL",imp:[],conf:.9},SplitRecord:{cat:"DataFrame API",tpl:`df_{v} = df_{in}.withColumn("item", explode(col("{array_field}")))
  .select("item.*")`,desc:"Split records by array field",notes:"Identify the array field to explode",imp:[],conf:.9},MergeRecord:{cat:"DataFrame API",tpl:"df_{v} = df_{in1}.unionByName(df_{in2}, allowMissingColumns=True)",desc:"Merge/union records",notes:"Ensure compatible schemas",imp:[],conf:.9},MergeContent:{cat:"DataFrame API",tpl:"df_{v} = df_{in1}.unionByName(df_{in2}, allowMissingColumns=True)",desc:"Merge content streams",notes:"Same as MergeRecord for DataFrames",imp:[],conf:.9},ReplaceText:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{col}", regexp_replace(col("{col}"), "{pattern}", "{replacement}"))',desc:"Regex text replacement",notes:"Apply to specific text columns",imp:[],conf:.9},UpdateAttribute:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{attr}", lit("{value}"))',desc:"Set/update attributes as columns",notes:"Map NiFi attributes to DataFrame columns",imp:[],conf:.9},JoltTransformJSON:{cat:"JSON Processing",tpl:`# Define target schema for JSON transform
_schema = "..."
df_{v} = df_{in}.withColumn("parsed", from_json(col("value"), _schema))
  .select("parsed.*")`,desc:"Complex JSON transformation",notes:"Jolt specs must be manually translated to Spark JSON ops",imp:[],conf:.9},EvaluateJsonPath:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{field}", get_json_object(col("value"), "$.{path}"))',desc:"Extract JSON paths",notes:"Map each JsonPath expression to get_json_object",imp:[],conf:.9},ExtractText:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{field}", regexp_extract(col("{col}"), "{pattern}", {group}))',desc:"Regex text extraction",notes:"Translate NiFi regex groups to Spark",imp:[],conf:.9},SplitJson:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("items", explode(from_json(col("value"), ArrayType(StringType()))))',desc:"Split JSON array",notes:"Define element schema",imp:[],conf:.9},SplitText:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("lines", explode(split(col("value"), "\\\\n")))',desc:"Split text by delimiter",notes:"Adjust delimiter as needed",imp:[],conf:.9},SplitContent:{cat:"DataFrame API",tpl:`# Split content by byte boundary or delimiter
df_{v} = df_{in}.withColumn("parts", split(col("value"), "{byte_sequence}"))
df_{v} = df_{v}.withColumn("part", explode(col("parts"))).drop("parts")`,desc:"Split content by delimiter/boundary",notes:"Adjust byte_sequence pattern for content splitting",imp:[],conf:.9},CompressContent:{cat:"Native",tpl:`# Delta Lake handles compression natively (snappy/zstd)
# No explicit compression step needed
df_{v} = df_{in}`,desc:"Compression",notes:"Delta Lake auto-compresses",imp:[],conf:.95},EncryptContent:{cat:"Security",tpl:`# NiFi EncryptContent â€” PySpark column-level AES encryption
# Algorithm: {Algorithm} | Key derivation: PBKDF2
from pyspark.sql.functions import col, lit, base64, aes_encrypt, aes_decrypt

# Encryption key from Databricks Secret Scope (16/24/32 bytes for AES-128/192/256)
_enc_key = dbutils.secrets.get(scope="{scope}", key="encryption-key")

# Encrypt sensitive columns using AES-GCM
df_{v} = df_{in}.withColumn(
    "{col}_encrypted",
    base64(aes_encrypt(col("{col}").cast("string"), lit(_enc_key), lit("GCM"), lit("DEFAULT")))
)
# To decrypt later:
# df_decrypted = df_{v}.withColumn("{col}_plain",
#     aes_decrypt(unbase64(col("{col}_encrypted")), lit(_enc_key), lit("GCM"), lit("DEFAULT")).cast("string"))`,desc:"AES column-level encryption via aes_encrypt/aes_decrypt",notes:"Store encryption key in Databricks Secret Scope. AES-GCM recommended; for CBC mode change the mode parameter. Supports AES-128/192/256 based on key length.",imp:["pyspark.sql.functions"],conf:.9},HashContent:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{col}_hash", sha2(col("{col}").cast("string"), 256))',desc:"SHA-256 hashing",notes:"Apply to specific columns",imp:[],conf:.9},TransformXml:{cat:"XML Processing",tpl:`# Use spark-xml library
df_{v} = spark.read.format("com.databricks.spark.xml").option("rowTag", "{tag}").load("{path}")`,desc:"XML transformation",notes:"Install spark-xml; define row tag",imp:[],conf:.9},ExecuteScript:{cat:"PySpark UDF",tpl:`# NiFi ExecuteScript migration â€” engine: {engine}
# If Groovy: manually port logic to PySpark (no direct Groovy support)
# If Python/Jython: wrap in pandas_udf for distributed execution
from pyspark.sql.functions import pandas_udf, col
import pandas as pd

@pandas_udf("string")
def _exec_script_{v}(values: pd.Series) -> pd.Series:
    """Ported from NiFi ExecuteScript ({engine}).
    Review and adapt the original script logic below."""
    def _process(val):
        # --- Paste original script logic here ---
        result = val  # placeholder: transform val as needed
        return str(result) if result is not None else None
    return values.apply(_process)

df_{v} = df_{in}.withColumn("_scripted", _exec_script_{v}(col("value")))`,desc:"ExecuteScript ported to pandas_udf for distributed execution",notes:"Groovy scripts require manual port to Python; Python/Jython scripts can often be adapted directly inside the pandas_udf body. Review original script logic and replace the placeholder.",imp:["pyspark.sql.functions"],conf:.85},ExecuteStreamCommand:{cat:"dbutils Shell",tpl:`# Execute shell command via %sh magic or dbutils
# %sh {command}
# Or use dbutils.notebook.run() to call a helper notebook
dbutils.fs.put("/Volumes/{catalog}/{schema}/tmp/{v}_cmd.sh", "{command}", True)`,desc:"External command via dbutils",notes:"Use %sh cell magic or dbutils.notebook.run()",imp:[],conf:.9},AttributesToJSON:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.select(to_json(struct("*")).alias("json_value"))',desc:"Columns to JSON",notes:"Converts all columns to single JSON string",imp:[],conf:.9},RouteOnAttribute:{cat:"DataFrame Filter",tpl:`# Route based on attribute conditions
df_{v}_matched = df_{in}.filter("{condition}")
df_{v}_unmatched = df_{in}.filter("NOT ({condition})")`,desc:"Conditional routing",notes:"Map NiFi routing rules to filter conditions",imp:[],conf:.9},RouteOnContent:{cat:"DataFrame Filter",tpl:'df_{v} = df_{in}.filter(col("value").rlike("{pattern}"))',desc:"Content-based routing",notes:"Translate content match patterns to regex",imp:[],conf:.9},DistributeLoad:{cat:"Spark Partitioning",tpl:"df_{v} = df_{in}.repartition({partitions})",desc:"Load distribution",notes:"Spark handles distribution automatically; repartition if needed",imp:[],conf:.9},DetectDuplicate:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.dropDuplicates(["{key}"])',desc:"Duplicate detection/removal",notes:"Specify dedup key columns",imp:[],conf:.9},ExecuteSQL:{cat:"Spark SQL",tpl:`df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("""
{sql}
""")`,desc:"SQL execution",notes:"Register input as temp view first",imp:[],conf:.9},ExecuteSQLRecord:{cat:"Spark SQL",tpl:`df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("""
{sql}
""")`,desc:"SQL execution with records",notes:"Same as ExecuteSQL",imp:[],conf:.9},InvokeHTTP:{cat:"Spark UDF",tpl:`# HTTP call via PySpark pandas UDF (Databricks-compatible)
from pyspark.sql.functions import pandas_udf
import pandas as pd
@pandas_udf("string")
def _call_api(urls: pd.Series) -> pd.Series:
  import urllib.request, json
  def _get(u):
    with urllib.request.urlopen(u) as r: return r.read().decode()
  return urls.apply(_get)
df_{v} = df_{in}.withColumn("api_response", _call_api(col("url")))`,desc:"HTTP API call via PySpark UDF",notes:"Uses pandas UDF for distributed execution; add error handling",imp:[],conf:.9},HandleHttpRequest:{cat:"Model Serving",tpl:`# HandleHttpRequest -> Databricks Model Serving Endpoint
# Incoming data lands in Delta table for downstream processing
df_{v} = spark.readStream.format("delta").table("{v}_incoming")`,desc:"HTTP server -> Model Serving",notes:"Never run blocking web server in notebook",imp:["mlflow"],conf:.92},HandleHttpResponse:{cat:"Model Serving",tpl:`# NiFi HandleHttpResponse â€” Databricks Model Serving endpoint
# Deploy a scoring endpoint that returns responses via MLflow
import mlflow
from mlflow.deployments import get_deploy_client

# Register the model for serving
_client = get_deploy_client("databricks")

# Create or update serving endpoint
_endpoint_config = {
    "served_entities": [{
        "entity_name": "{catalog}.{schema}.{model_name}",
        "entity_version": "1",
        "workload_size": "Small",
        "scale_to_zero_enabled": True
    }],
    "traffic_config": {
        "routes": [{"served_model_name": "{model_name}-1", "traffic_percentage": 100}]
    }
}
try:
    _client.create_endpoint(name="{v}_endpoint", config=_endpoint_config)
except Exception as e:
    if "already exists" in str(e):
        _client.update_endpoint(endpoint="{v}_endpoint", config=_endpoint_config)
    else:
        raise

print(f"[HTTP] Model Serving endpoint {v}_endpoint deployed")
print(f"[HTTP] Invoke at: https://<workspace>.databricks.com/serving-endpoints/{v}_endpoint/invocations")`,desc:"HTTP response via Databricks Model Serving endpoint",notes:"Deploys an MLflow Model Serving endpoint. Request/response handled by the serving infrastructure. Pair with HandleHttpRequest which writes incoming data to Delta.",imp:["mlflow"],conf:.88},PutFile:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"Write to Delta Lake table",notes:"Uses Unity Catalog managed table",imp:[],conf:.9},PutKafka:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Write to Kafka topic",notes:"Configure security protocol",imp:[],conf:.9},PublishKafka:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Publish to Kafka",notes:"Same as PutKafka",imp:[],conf:.9},PublishKafka_2_6:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Publish to Kafka 2.6",notes:"Same as PutKafka",imp:[],conf:.9},PublishKafkaRecord_2_6:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Publish Kafka records",notes:"Same as PutKafka",imp:[],conf:.9},PutS3Object:{cat:"Cloud Storage Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .save("s3a://{bucket}/{path}"))`,desc:"Write to S3",notes:"Use external location in Unity Catalog",imp:[],conf:.9},PutHDFS:{cat:"Cloud Storage Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .save("{path}"))`,desc:"Write to cloud storage",notes:"Map HDFS path to cloud storage / Volumes",imp:[],conf:.9},PutSFTP:{cat:"External Storage Write",tpl:`# NiFi PutSFTP â†’ paramiko SFTP transfer
import paramiko
_sftp_host = dbutils.secrets.get(scope="{scope}", key="sftp-host") if "{hostname}" == "<hostname>" else "{hostname}"
_sftp_user = dbutils.secrets.get(scope="{scope}", key="sftp-user")
_sftp_key = dbutils.secrets.get(scope="{scope}", key="sftp-private-key")
# Stage data to local temp file
_local_path = "/Volumes/{catalog}/{schema}/tmp/{v}_export"
df_{in}.toPandas().to_csv(_local_path, index=False)
# Transfer via SFTP
_pkey = paramiko.RSAKey.from_private_key_file(_sftp_key) if _sftp_key.startswith("/") else paramiko.RSAKey(data=_sftp_key.encode())
_transport = paramiko.Transport((_sftp_host, 22))
_transport.connect(username=_sftp_user, pkey=_pkey)
_sftp = paramiko.SFTPClient.from_transport(_transport)
_sftp.put(_local_path, "{remote_path}/{v}.csv")
_sftp.close()
_transport.close()
print(f"[SFTP] Uploaded {_local_path} â†’ {_sftp_host}:{remote_path}/{v}.csv")`,desc:"SFTP upload via paramiko",notes:"Install paramiko; store credentials in Secret Scopes",imp:["import paramiko"],conf:.9},PutEmail:{cat:"Workflow Notification",tpl:`# Use Databricks workflow email notifications
# Configure in Job settings: email_notifications.on_success / on_failure
# Or use dbutils.notebook.exit() with downstream webhook task
dbutils.notebook.exit("NOTIFY: {subject}")`,desc:"Email via workflow notification",notes:"Configure email notifications in Databricks Job settings",imp:[],conf:.9},PutMongo:{cat:"MongoDB Connector",tpl:`(df_{in}.write
  .format("mongodb")
  .option("connection.uri", dbutils.secrets.get(scope="{scope}", key="mongo-uri"))
  .option("database", "{database}")
  .option("collection", "{collection}")
  .mode("append")
  .save())`,desc:"MongoDB write",notes:"Install mongodb-spark-connector",imp:[],conf:.9},PutElasticsearch:{cat:"ES Connector",tpl:`(df_{in}.write
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .save("{index}"))`,desc:"Elasticsearch write",notes:"Install elasticsearch-spark",imp:[],conf:.9},PutDatabaseRecord:{cat:"JDBC Write",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "{table}")
  .mode("append")
  .save())`,desc:"Database record write",notes:"Store credentials in secret scope",imp:[],conf:.9},LogMessage:{cat:"Spark Logging",tpl:`# Databricks notebook logging
print(f"[INFO] {v}: Processing complete")
spark.sparkContext.setLocalProperty("callSite.short", "{v}")`,desc:"Spark-native logging",notes:"Captured in Spark driver logs and notebook output",imp:[],conf:.9},LogAttribute:{cat:"Spark Display",tpl:`# Inspect schema and sample data
display(df_{in})
df_{in}.printSchema()`,desc:"Display attributes and preview",notes:"display() renders interactive table in Databricks",imp:[],conf:.9},Wait:{cat:"Workflow Dependency",tpl:`# Wait -> Databricks Workflow task dependency or Delta CDF streaming
# DO NOT use while/sleep polling loops
df_{v} = spark.readStream.format("delta").option("readChangeFeed","true").table("workflow_signals").filter("signal_id = '{v}_signal' AND status = 'ready'")`,desc:"Workflow task dependency / Delta CDF wait",notes:"Use Workflow depends_on or Delta CDF â€” never poll in a loop",imp:[],conf:.92},Notify:{cat:"Workflow Signal",tpl:`# NiFi Notify â†’ Delta table signal write (notify downstream tasks)
_notify_key = "{v}_signal"
spark.sql(f"MERGE INTO __workflow_signals AS t USING (SELECT '{{_notify_key}}' AS signal_key, 'READY' AS status, current_timestamp() AS updated_at) AS s ON t.signal_key = s.signal_key WHEN MATCHED THEN UPDATE SET status = s.status, updated_at = s.updated_at WHEN NOT MATCHED THEN INSERT *")
print(f"[NOTIFY] Signal {{_notify_key}} set to READY")`,desc:"Write signal to Delta table for downstream",notes:"Downstream Wait processors poll for this signal",imp:[],conf:.9},DebugFlow:{cat:"Spark Display",tpl:`display(df_{in})
df_{in}.printSchema()`,desc:"Debug/inspect data",notes:"display() renders interactive table in Databricks",imp:[],conf:.9},CountText:{cat:"DataFrame API",tpl:`_count = df_{in}.count()
displayHTML(f"<h3>Row count: {_count}</h3>")`,desc:"Count rows",notes:"displayHTML renders formatted output in Databricks",imp:[],conf:.95},ControlRate:{cat:"Streaming Trigger",tpl:`# NiFi ControlRate â†’ Python rate limiter
import time as _time
_rate_interval = 10  # seconds between batches
print(f"[RATE] Throttling: {_rate_interval}s between executions")
_time.sleep(_rate_interval)`,desc:"Rate limiting via sleep",notes:"Adjust _rate_interval; for streaming use .trigger(processingTime=...)",imp:[],conf:.9},PutHiveQL:{cat:"Spark SQL",tpl:`spark.sql("""
{sql}
""")`,desc:"HiveQL write",notes:"HiveQL â†’ Spark SQL direct",imp:[],conf:.9},SelectHiveQL:{cat:"Spark SQL",tpl:`df_{v} = spark.sql("""
{sql}
""")`,desc:"HiveQL read",notes:"HiveQL â†’ Spark SQL direct",imp:[],conf:.9},PutHiveStreaming:{cat:"Delta Streaming",tpl:`(df_{in}.writeStream
  .format("delta")
  .outputMode("append")
  .toTable("{catalog}.{schema}.{table}"))`,desc:"Hive streaming â†’ Delta streaming",notes:"Replace Hive ACID streaming with Delta Lake streaming",imp:[],conf:.9},PutORC:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"ORC â†’ Delta Lake",notes:"Delta provides ACID on top of Parquet; better than ORC",imp:[],conf:.9},PutParquet:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"Parquet â†’ Delta Lake",notes:"Delta adds ACID, time travel, Z-order to Parquet",imp:[],conf:.9},PutKudu:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"Kudu â†’ Delta Lake",notes:"Replace Kudu upsert with Delta MERGE",imp:[],conf:.9},PutHBaseCell:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"HBase cell write â†’ Delta",notes:"Replace HBase cells with Delta columns",imp:[],conf:.9},PutHBaseJSON:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"HBase JSON write â†’ Delta",notes:"JSON â†’ Delta with auto-inferred schema",imp:[],conf:.9},PutHBaseRecord:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"HBase record write â†’ Delta",notes:"Record-based HBase â†’ Delta table append",imp:[],conf:.9},GetHBase:{cat:"Delta Lake Read",tpl:'df_{v} = spark.table("{catalog}.{schema}.{table}")',desc:"HBase read â†’ Delta table",notes:"Replace HBase scan with Delta read + filter",imp:[],conf:.9},ScanHBase:{cat:"Delta Lake Read",tpl:'df_{v} = spark.table("{catalog}.{schema}.{table}").filter("{condition}")',desc:"HBase scan â†’ Delta filter",notes:"HBase row key scan â†’ Delta point lookup or range filter",imp:[],conf:.9},FetchHBaseRow:{cat:"Delta Lake Read",tpl:'df_{v} = spark.table("{catalog}.{schema}.{table}").filter(col("rowkey") == "{key}")',desc:"HBase row fetch â†’ Delta point lookup",notes:"Use Z-ORDER BY rowkey for fast lookups",imp:[],conf:.9},GetHDFS:{cat:"Volumes Read",tpl:'df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/{path}")',desc:"HDFS read â†’ Volumes",notes:"Replace HDFS paths with Unity Catalog Volumes",imp:[],conf:.9},ListHDFS:{cat:"Volumes List",tpl:`_files = dbutils.fs.ls("/Volumes/{catalog}/{schema}/{path}")
df_{v} = spark.createDataFrame(_files)`,desc:"HDFS list â†’ Volumes list",notes:"dbutils.fs.ls for file listing",imp:[],conf:.9},FetchHDFS:{cat:"Volumes Read",tpl:'df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/{path}")',desc:"HDFS fetch â†’ Volumes read",notes:"Direct file read from Volumes",imp:[],conf:.9},MoveHDFS:{cat:"dbutils.fs",tpl:'dbutils.fs.mv("/Volumes/{catalog}/{schema}/{source}", "/Volumes/{catalog}/{schema}/{dest}")',desc:"HDFS move â†’ dbutils.fs.mv",notes:"File move between Volumes paths",imp:[],conf:.9},DeleteHDFS:{cat:"dbutils.fs",tpl:'dbutils.fs.rm("/Volumes/{catalog}/{schema}/{path}", recurse=True)',desc:"HDFS delete â†’ dbutils.fs.rm",notes:"File deletion in Volumes",imp:[],conf:.9},CreateHadoopSequenceFile:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"Sequence file â†’ Delta",notes:"Replace Hadoop SequenceFile with Delta Lake",imp:[],conf:.9},PutCassandraQL:{cat:"Cassandra Connector",tpl:`(df_{in}.write
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "{keyspace}")
  .option("table", "{table}")
  .mode("append")
  .save())`,desc:"Cassandra write via Spark connector",notes:"Install spark-cassandra-connector library on cluster",imp:[],conf:.9},QueryCassandra:{cat:"Cassandra Connector",tpl:`df_{v} = (spark.read
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "{keyspace}")
  .option("table", "{table}")
  .load())`,desc:"Cassandra read via Spark connector",notes:"Install spark-cassandra-connector library",imp:[],conf:.9},ConsumeJMS:{cat:"Custom Source",tpl:`# NiFi ConsumeJMS â€” stomp.py STOMP consumer
import stomp
import time

_msgs = []
class _JMSListener_{v}(stomp.ConnectionListener):
    """Collects messages from JMS queue via STOMP protocol."""
    def on_message(self, frame):
        _msgs.append({"body": frame.body, "msg_id": frame.headers.get("message-id", ""), "timestamp": frame.headers.get("timestamp", "")})
    def on_error(self, frame):
        print(f"[JMS ERROR] {frame.body}")

_conn = stomp.Connection([("{host}", {port})])
_conn.set_listener("jms_{v}", _JMSListener_{v}())
try:
    _conn.connect(
        dbutils.secrets.get(scope="jms", key="user"),
        dbutils.secrets.get(scope="jms", key="pass"),
        wait=True, headers={"client-id": "{client_id}"})
    _conn.subscribe(destination="/queue/{queue}", id=1, ack="client-individual")
    time.sleep(5)  # Collect messages for 5s; adjust as needed
finally:
    _conn.disconnect()

df_{v} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING, msg_id STRING, timestamp STRING")
print(f"[JMS] Consumed {len(_msgs)} messages from {queue}")`,desc:"JMS queue consumer via stomp.py with connection management",notes:"Install stomp.py on cluster. Store JMS credentials in Secret Scope. Adjust sleep duration for message collection window.",imp:["stomp.py"],conf:.88},PublishJMS:{cat:"Custom Sink",tpl:`# NiFi PublishJMS â€” stomp.py STOMP publisher
import stomp, json

def _jms_publish_{v}(batch_df, batch_id=0):
    """Publish DataFrame rows to JMS queue. Streaming-safe via foreachBatch."""
    _conn = stomp.Connection([("{host}", {port})])
    try:
        _conn.connect(
            dbutils.secrets.get(scope="jms", key="user"),
            dbutils.secrets.get(scope="jms", key="pass"),
            wait=True, headers={"client-id": "{client_id}"})
        _sent = 0
        for row in batch_df.limit(10000).collect():
            _conn.send(
                destination="/queue/{queue}",
                body=json.dumps(row.asDict(), default=str),
                content_type="application/json",
                headers={"persistent": "true"})
            _sent += 1
        print(f"[JMS] Batch {batch_id}: published {_sent} messages to {queue}")
    finally:
        _conn.disconnect()

# For streaming: df_{in}.writeStream.foreachBatch(_jms_publish_{v}).start()
# For batch:
_jms_publish_{v}(df_{in})`,desc:"JMS queue publisher via stomp.py with foreachBatch support",notes:"Install stomp.py on cluster. Streaming-safe: use foreachBatch for writeStream. Store credentials in Secret Scope.",imp:["stomp.py"],conf:.88},ConsumeAMQP:{cat:"Custom Source",tpl:`# AMQP/RabbitMQ â†’ pika library
import pika
# connection = pika.BlockingConnection(pika.ConnectionParameters("{host}"))`,desc:"AMQP consumer",notes:"Use pika library for RabbitMQ",imp:[],conf:.9},PublishAMQP:{cat:"Custom Sink",tpl:`# AMQP/RabbitMQ â†’ pika library
import pika
# channel.basic_publish(exchange="", routing_key="{queue}", body=msg)`,desc:"AMQP publisher",notes:"Use pika library for RabbitMQ",imp:[],conf:.9},ConsumeMQTT:{cat:"Custom Source",tpl:`# MQTT â†’ paho-mqtt
import paho.mqtt.client as mqtt
# client.connect("{host}", {port})
# client.subscribe("{topic}")`,desc:"MQTT subscriber",notes:"Use paho-mqtt library",imp:[],conf:.9},PublishMQTT:{cat:"Custom Sink",tpl:`# MQTT â†’ paho-mqtt
import paho.mqtt.client as mqtt
# client.publish("{topic}", payload=msg)`,desc:"MQTT publisher",notes:"Use paho-mqtt library",imp:[],conf:.9},PutSNS:{cat:"AWS boto3",tpl:`import boto3
sns = boto3.client("sns")
sns.publish(TopicArn="{topic_arn}", Message=str(msg))`,desc:"AWS SNS publish",notes:"Use boto3; store AWS credentials in Secret Scopes",imp:[],conf:.9},GetSQS:{cat:"AWS boto3",tpl:`import boto3
sqs = boto3.client("sqs")
msgs = sqs.receive_message(QueueUrl="{queue_url}")`,desc:"AWS SQS receive",notes:"Use boto3; for streaming use Kinesis instead",imp:[],conf:.9},PutSQS:{cat:"AWS boto3",tpl:`import boto3
sqs = boto3.client("sqs")
sqs.send_message(QueueUrl="{queue_url}", MessageBody=str(msg))`,desc:"AWS SQS send",notes:"Use boto3; store credentials in Secret Scopes",imp:[],conf:.9},PutDynamoDB:{cat:"DynamoDB Connector",tpl:`(df_{in}.write
  .format("dynamodb")
  .option("tableName", "{table}")
  .option("region", "{region}")
  .save())`,desc:"DynamoDB write",notes:"Install emr-dynamodb-connector library",imp:[],conf:.9},GetDynamoDB:{cat:"DynamoDB Connector",tpl:`df_{v} = (spark.read
  .format("dynamodb")
  .option("tableName", "{table}")
  .option("region", "{region}")
  .load())`,desc:"DynamoDB read",notes:"Install emr-dynamodb-connector library",imp:[],conf:.9},PutKinesisFirehose:{cat:"Kinesis Connector",tpl:`(df_{in}.writeStream
  .format("kinesis")
  .option("streamName", "{stream}")
  .option("region", "{region}")
  .start())`,desc:"Kinesis Firehose write",notes:"Use kinesis-spark connector",imp:[],conf:.9},PutLambda:{cat:"AWS boto3",tpl:`import boto3
lam = boto3.client("lambda")
lam.invoke(FunctionName="{function}", Payload=json.dumps(payload))`,desc:"Lambda invocation",notes:"Use boto3 for Lambda; or replace with Databricks Job",imp:[],conf:.9},PutAzureBlobStorage:{cat:"Azure Storage",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .save("wasbs://{container}@{account}.blob.core.windows.net/{path}"))`,desc:"Azure Blob write",notes:"Use Unity Catalog external location",imp:[],conf:.9},FetchAzureBlobStorage:{cat:"Azure Storage",tpl:'df_{v} = spark.read.format("{format}").load("wasbs://{container}@{account}.blob.core.windows.net/{path}")',desc:"Azure Blob read",notes:"Unity Catalog external location",imp:[],conf:.9},ListAzureBlobStorage:{cat:"Azure Storage",tpl:`_files = dbutils.fs.ls("wasbs://{container}@{account}.blob.core.windows.net/{path}")
df_{v} = spark.createDataFrame(_files)`,desc:"Azure Blob list",notes:"Use dbutils.fs.ls",imp:[],conf:.9},PutAzureDataLakeStorage:{cat:"Azure ADLS",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .save("abfss://{container}@{account}.dfs.core.windows.net/{path}"))`,desc:"ADLS Gen2 write",notes:"Unity Catalog external location",imp:[],conf:.9},PutAzureEventHub:{cat:"Event Hubs Connector",tpl:`(df_{in}.writeStream
  .format("eventhubs")
  .option("eventhubs.connectionString", dbutils.secrets.get(scope="{scope}", key="eh-conn-string"))
  .start())`,desc:"Event Hubs write",notes:"Install azure-eventhubs-spark library",imp:[],conf:.9},ConsumeAzureEventHub:{cat:"Event Hubs Connector",tpl:`df_{v} = (spark.readStream
  .format("eventhubs")
  .option("eventhubs.connectionString", dbutils.secrets.get(scope="{scope}", key="eh-conn-string"))
  .load())`,desc:"Event Hubs read",notes:"Install azure-eventhubs-spark library",imp:[],conf:.9},ListGCSBucket:{cat:"GCS",tpl:`_files = dbutils.fs.ls("gs://{bucket}/{prefix}")
df_{v} = spark.createDataFrame(_files)`,desc:"GCS list",notes:"Unity Catalog external location for GCS",imp:[],conf:.9},FetchGCSObject:{cat:"GCS",tpl:'df_{v} = spark.read.format("{format}").load("gs://{bucket}/{key}")',desc:"GCS read",notes:"Unity Catalog external location",imp:[],conf:.9},PutGCSObject:{cat:"GCS",tpl:`(df_{in}.write
  .format("delta")
  .save("gs://{bucket}/{key}"))`,desc:"GCS write",notes:"Unity Catalog external location",imp:[],conf:.9},PutBigQueryBatch:{cat:"BigQuery Connector",tpl:`(df_{in}.write
  .format("bigquery")
  .option("table", "{project}.{dataset}.{table}")
  .save())`,desc:"BigQuery write",notes:"Install spark-bigquery-connector",imp:[],conf:.9},PutSlack:{cat:"Webhook",tpl:`import requests
requests.post("{webhook_url}", json={"text": f"Pipeline update: {msg}"})`,desc:"Slack notification",notes:"Use Slack webhook URL; store in Secret Scopes",imp:[],conf:.9},ListenSyslog:{cat:"Streaming Socket",tpl:`df_{v} = (spark.readStream
  .format("socket")
  .option("host", "{host}")
  .option("port", "{port}")
  .load())`,desc:"Syslog listener",notes:"Use socket source or external syslog collector",imp:[],conf:.9},ParseSyslog:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("parsed", regexp_extract(col("value"), "{pattern}", 0))',desc:"Syslog parsing",notes:"Use regex to parse syslog format",imp:[],conf:.9},PutSplunk:{cat:"Splunk HEC",tpl:`import requests
requests.post("{hec_url}", headers={"Authorization":"Splunk {token}"}, json={"event":data})`,desc:"Splunk HEC write",notes:"Use Splunk HTTP Event Collector",imp:[],conf:.9},PutInfluxDB:{cat:"InfluxDB Client",tpl:`from influxdb_client import InfluxDBClient
client = InfluxDBClient(url="{url}", token="{token}", org="{org}")
client.write_api().write(bucket="{bucket}", record=data)`,desc:"InfluxDB write",notes:"Install influxdb-client-python",imp:[],conf:.9},PutTCP:{cat:"Python Socket",tpl:`import socket
s = socket.socket()
s.connect(("{host}", {port}))
s.send(data.encode())`,desc:"TCP send",notes:"Use Python socket module",imp:[],conf:.9},ListenTCP:{cat:"Streaming Socket",tpl:`df_{v} = (spark.readStream
  .format("socket")
  .option("host", "0.0.0.0")
  .option("port", "{port}")
  .load())`,desc:"TCP listener",notes:"Use Spark socket source",imp:[],conf:.9},UpdateRecord:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{field}", expr("{expression}"))',desc:"Record field update",notes:"Map UpdateRecord field expressions to withColumn",imp:[],conf:.9},LookupRecord:{cat:"DataFrame Join",tpl:`df_lookup = spark.table("{catalog}.{schema}.{lookup_table}").cache()
df_{v} = df_{in}.join(df_lookup, on="{key}", how="left")`,desc:"Record lookup via cached join",notes:"Cache lookup table for performance",imp:[],conf:.9},ValidateRecord:{cat:"DLT Expectations",tpl:`# Data quality validation
# @dlt.expect_or_drop("{rule}", "{expression}")
df_{v} = df_{in}.filter("{expression}")`,desc:"Record validation via DLT expectations",notes:"Best implemented as DLT expectations",imp:[],conf:.9},PartitionRecord:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.repartition("{partition_field}")',desc:"Record partitioning",notes:"Spark handles partitioning natively",imp:[],conf:.9},QueryRecord:{cat:"Spark SQL",tpl:`df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("{sql}")`,desc:"SQL query on records",notes:"Register DataFrame as temp view then query",imp:[],conf:.9},ConvertAvroToJSON:{cat:"DataFrame API",tpl:`from pyspark.sql.avro.functions import from_avro
df_{v} = df_{in}.select(from_avro("value", _schema).alias("data")).select("data.*")`,desc:"Avro to JSON",notes:"Spark handles Avro natively",imp:[],conf:.9},GenerateTableFetch:{cat:"JDBC Incremental",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "(SELECT * FROM {table} WHERE {column} > ?) subq")
  .load())`,desc:"Incremental JDBC fetch",notes:"Use query pushdown for incremental reads",imp:[],conf:.9},PutDistributedMapCache:{cat:"Delta Lookup",tpl:`# DistributedMapCache -> Delta lookup table (persistent, shared, versioned)
df_{in}.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("{catalog}.{schema}.cache_{v}")
print(f"[CACHE] Persisted to {catalog}.{schema}.cache_{v}")`,desc:"Delta lookup table write",notes:"Replace NiFi cache with Delta table â€” shared across clusters",imp:[],conf:.93},FetchDistributedMapCache:{cat:"Delta Lookup",tpl:`# FetchDistributedMapCache -> Cached Delta lookup join
df_lookup = spark.table("{catalog}.{schema}.cache_{v}").cache()
df_{v} = df_{in}.join(df_lookup, on="{key}", how="left")
print(f"[CACHE] Joined with cached Delta lookup table")`,desc:"Delta lookup table read + join",notes:"Delta table cached in memory â€” fast lookups",imp:[],conf:.93},PutCouchbaseKey:{cat:"Couchbase Connector",tpl:`# Install couchbase-spark-connector
(df_{in}.write
  .format("couchbase.kv")
  .option("couchbase.bucket", "{bucket}")
  .save())`,desc:"Couchbase write",notes:"Install couchbase-spark connector",imp:[],conf:.9},GetCouchbaseKey:{cat:"Couchbase Connector",tpl:`df_{v} = (spark.read
  .format("couchbase.kv")
  .option("couchbase.bucket", "{bucket}")
  .load())`,desc:"Couchbase read",notes:"Install couchbase-spark connector",imp:[],conf:.9},PutSolrContentStream:{cat:"Solr Connector",tpl:`# Install solr-spark connector or use pysolr
import pysolr
solr = pysolr.Solr("{url}")
solr.add([row.asDict() for row in df_{in}.collect()])`,desc:"Solr write",notes:"Use pysolr or solr-spark connector",imp:[],conf:.9},QuerySolr:{cat:"Solr Connector",tpl:`# Use pysolr or solr-spark connector
import pysolr
solr = pysolr.Solr("{url}")
results = solr.search("{query}")`,desc:"Solr query",notes:"Use pysolr for queries",imp:[],conf:.9},ExecuteGroovyScript:{cat:"PySpark Cell",tpl:`# NiFi Groovy script â†’ PySpark equivalent
# Original engine: Groovy
# Translate Groovy logic to PySpark DataFrame operations
df_{v} = df_{in}`,desc:"Groovy script execution",notes:"Manual translation from Groovy to PySpark required",imp:[],conf:.25},ExecuteProcess:{cat:"subprocess",tpl:`# Execute external process
import subprocess as _sp
_result = _sp.run(["{command}"], capture_output=True, text=True, timeout=300)
if _result.returncode != 0:
    print(f"[ERROR] Process failed: {_result.stderr[:200]}")
else:
    print(f"[OK] {_result.stdout[:200]}")`,desc:"External process execution via subprocess",notes:"Review command for Databricks compatibility",imp:[],conf:.9},UnpackContent:{cat:"DataFrame API",tpl:`# Unpack/decompress content â€” Spark handles gzip/snappy/lz4 natively
df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/{path}")`,desc:"Content decompression",notes:"Spark auto-decompresses gzip, snappy, lz4, zstd",imp:[],conf:.9},GetJMSTopic:{cat:"Custom Source",tpl:`# NiFi GetJMSTopic â€” stomp.py STOMP topic subscriber
import stomp
import time

_msgs = []
class _TopicListener_{v}(stomp.ConnectionListener):
    """Subscribes to JMS topic via STOMP and collects messages."""
    def on_message(self, frame):
        _msgs.append({"body": frame.body, "topic": frame.headers.get("destination", ""), "msg_id": frame.headers.get("message-id", "")})
    def on_error(self, frame):
        print(f"[JMS TOPIC ERROR] {frame.body}")

_conn = stomp.Connection([("{host}", {port})])
_conn.set_listener("topic_{v}", _TopicListener_{v}())
try:
    _conn.connect(
        dbutils.secrets.get(scope="jms", key="user"),
        dbutils.secrets.get(scope="jms", key="pass"),
        wait=True, headers={"client-id": "{client_id}"})
    _conn.subscribe(destination="/topic/{topic}", id=1, ack="auto",
        headers={"activemq.subscriptionName": "{v}_durable"})
    time.sleep(5)  # Collect messages for 5s
finally:
    _conn.disconnect()

df_{v} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING, topic STRING, msg_id STRING")
print(f"[JMS] Subscribed to topic {topic}: {len(_msgs)} messages")`,desc:"JMS topic subscriber via stomp.py with durable subscription",notes:"Install stomp.py on cluster. Uses durable subscription for reliable delivery. Adjust sleep for collection window.",imp:["stomp.py"],conf:.88},PutJMS:{cat:"Custom Sink",tpl:`# JMS publish â€” streaming-safe with foreachBatch
import stomp

def _jms_batch_{v}(batch_df, batch_id):
    _conn = stomp.Connection([("{host}", {port})])
    _conn.connect(wait=True)
    for row in batch_df.collect():
        _conn.send(destination="{queue}", body=str(row.asDict()))
    _conn.disconnect()
    print(f"[JMS] Batch {{batch_id}}: {{batch_df.count()}} messages")

# For streaming: df_{in}.writeStream.foreachBatch(_jms_batch_{v}).start()
# For batch:
_jms_batch_{v}(df_{in}, 0)`,desc:"JMS with foreachBatch",notes:"Streaming-safe",imp:["stomp.py"],conf:.92},PutFTP:{cat:"External Storage Write",tpl:`# FTP upload via ftplib
import ftplib
_ftp = ftplib.FTP("{hostname}")
_ftp.login(dbutils.secrets.get(scope="{scope}", key="ftp-user"), dbutils.secrets.get(scope="{scope}", key="ftp-pass"))
# Stage to local then upload
_local = "/Volumes/{catalog}/{schema}/tmp/{v}_export"
df_{in}.toPandas().to_csv(_local, index=False)
with open(_local, "rb") as f:
    _ftp.storbinary(f"STOR {remote_path}/{v}.csv", f)
_ftp.quit()`,desc:"FTP upload via ftplib",notes:"Stage to temp then upload; store creds in Secret Scopes",imp:[],conf:.9},RouteText:{cat:"DataFrame Filter",tpl:`# Route text content by pattern matching
df_{v}_matched = df_{in}.filter(col("value").rlike("{pattern}"))
df_{v}_unmatched = df_{in}.filter(~col("value").rlike("{pattern}"))`,desc:"Text-based routing",notes:"Translate NiFi text routing rules to Spark regex filters",imp:[],conf:.9},PutElasticsearchHttp:{cat:"ES Connector",tpl:`(df_{in}.write
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .save("{index}"))`,desc:"Elasticsearch HTTP write",notes:"Install elasticsearch-spark",imp:[],conf:.9},PutElasticsearchHttpRecord:{cat:"ES Connector",tpl:`(df_{in}.write
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .save("{index}"))`,desc:"Elasticsearch HTTP record write",notes:"Install elasticsearch-spark",imp:[],conf:.9},PutElasticsearchRecord:{cat:"ES Connector",tpl:`(df_{in}.write
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .save("{index}"))`,desc:"Elasticsearch record write",notes:"Install elasticsearch-spark",imp:[],conf:.9},FetchElasticsearchHttp:{cat:"ES Connector",tpl:`df_{v} = (spark.read
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .option("es.resource", "{index}")
  .load())`,desc:"Elasticsearch HTTP fetch",notes:"Install elasticsearch-spark",imp:[],conf:.9},JsonQueryElasticsearch:{cat:"ES Connector",tpl:`df_{v} = (spark.read
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .option("es.query", "{query}")
  .load("{index}"))`,desc:"Elasticsearch JSON query",notes:"Install elasticsearch-spark; pass query DSL",imp:[],conf:.9},ScrollElasticsearchHttp:{cat:"ES Connector",tpl:`df_{v} = (spark.read
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .option("es.scroll.size", "1000")
  .load("{index}"))`,desc:"Elasticsearch scroll read",notes:"Install elasticsearch-spark; Spark handles pagination",imp:[],conf:.9},PutMongoRecord:{cat:"MongoDB Connector",tpl:`(df_{in}.write
  .format("mongodb")
  .option("connection.uri", dbutils.secrets.get(scope="{scope}", key="mongo-uri"))
  .option("database", "{database}")
  .option("collection", "{collection}")
  .mode("append")
  .save())`,desc:"MongoDB record write",notes:"Install mongodb-spark-connector",imp:[],conf:.9},DeleteMongo:{cat:"MongoDB Connector",tpl:`# MongoDB delete via pymongo
from pymongo import MongoClient
_client = MongoClient(dbutils.secrets.get(scope="{scope}", key="mongo-uri"))
_db = _client["{database}"]
_result = _db["{collection}"].delete_many({filter})
print(f"[MONGO] Deleted {_result.deleted_count} documents")`,desc:"MongoDB delete",notes:"Install pymongo; use for targeted deletes",imp:[],conf:.9},PutCassandraRecord:{cat:"Cassandra Connector",tpl:`(df_{in}.write
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "{keyspace}")
  .option("table", "{table}")
  .mode("append")
  .save())`,desc:"Cassandra record write",notes:"Install spark-cassandra-connector",imp:[],conf:.9},PutSolrRecord:{cat:"Solr Connector",tpl:`# Solr record write via pysolr or solr-spark connector
import pysolr
solr = pysolr.Solr("{url}")
solr.add([row.asDict() for row in df_{in}.collect()])`,desc:"Solr record write",notes:"Install pysolr or solr-spark connector",imp:[],conf:.9},GetSolr:{cat:"Solr Connector",tpl:`# Solr read via pysolr
import pysolr
solr = pysolr.Solr("{url}")
results = solr.search("*:*", rows=10000)`,desc:"Solr read",notes:"Use pysolr or solr-spark connector",imp:[],conf:.9},ListSFTP:{cat:"External Storage",tpl:`# SFTP directory listing via paramiko
import paramiko
_transport = paramiko.Transport(("{host}", 22))
_transport.connect(username=dbutils.secrets.get(scope="{scope}", key="sftp-user"))
_sftp = paramiko.SFTPClient.from_transport(_transport)
_files = _sftp.listdir("{remote_path}")
_sftp.close(); _transport.close()
df_{v} = spark.createDataFrame([(f,) for f in _files], ["filename"])`,desc:"SFTP directory listing",notes:"Install paramiko; store credentials in Secret Scopes",imp:["import paramiko"],conf:.9},FetchSFTP:{cat:"External Storage",tpl:`# SFTP file fetch via paramiko
import paramiko
_transport = paramiko.Transport(("{host}", 22))
_transport.connect(username=dbutils.secrets.get(scope="{scope}", key="sftp-user"))
_sftp = paramiko.SFTPClient.from_transport(_transport)
_sftp.get("{remote_path}/{filename}", "/Volumes/{catalog}/{schema}/tmp/{v}_download")
_sftp.close(); _transport.close()
df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/tmp/{v}_download")`,desc:"SFTP file fetch",notes:"Install paramiko; downloads to local then reads",imp:["import paramiko"],conf:.9},ListFTP:{cat:"External Storage",tpl:`# FTP directory listing via ftplib
import ftplib
_ftp = ftplib.FTP("{hostname}")
_ftp.login(dbutils.secrets.get(scope="{scope}", key="ftp-user"), dbutils.secrets.get(scope="{scope}", key="ftp-pass"))
_files = _ftp.nlst("{remote_path}")
_ftp.quit()
df_{v} = spark.createDataFrame([(f,) for f in _files], ["filename"])`,desc:"FTP directory listing",notes:"Use ftplib; store credentials in Secret Scopes",imp:[],conf:.9},FetchFTP:{cat:"External Storage",tpl:`# FTP file fetch via ftplib
import ftplib
_ftp = ftplib.FTP("{hostname}")
_ftp.login(dbutils.secrets.get(scope="{scope}", key="ftp-user"), dbutils.secrets.get(scope="{scope}", key="ftp-pass"))
with open("/Volumes/{catalog}/{schema}/tmp/{v}_download", "wb") as f:
    _ftp.retrbinary("RETR {remote_path}/{filename}", f.write)
_ftp.quit()
df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/tmp/{v}_download")`,desc:"FTP file fetch",notes:"Use ftplib; downloads to local then reads",imp:[],conf:.9},GetPOP3:{cat:"Email",tpl:`# POP3 email retrieval
import poplib
_pop = poplib.POP3_SSL("{host}")
_pop.user(dbutils.secrets.get(scope="{scope}", key="pop3-user"))
_pop.pass_(dbutils.secrets.get(scope="{scope}", key="pop3-pass"))
_count = len(_pop.list()[1])
print(f"[POP3] {_count} messages available")
_pop.quit()`,desc:"POP3 email retrieval",notes:"Use poplib; store credentials in Secret Scopes",imp:[],conf:.9},GetIMAP:{cat:"Email",tpl:`# IMAP email retrieval
import imaplib
_mail = imaplib.IMAP4_SSL("{host}")
_mail.login(dbutils.secrets.get(scope="{scope}", key="imap-user"), dbutils.secrets.get(scope="{scope}", key="imap-pass"))
_mail.select("INBOX")
_status, _msgs = _mail.search(None, "ALL")
print(f"[IMAP] {len(_msgs[0].split())} messages")
_mail.logout()`,desc:"IMAP email retrieval",notes:"Use imaplib; store credentials in Secret Scopes",imp:[],conf:.9},ListenUDP:{cat:"Python Socket",tpl:`# UDP listener â€” not ideal for Databricks
import socket
s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
s.bind(("0.0.0.0", {port}))
data, addr = s.recvfrom(4096)`,desc:"UDP listener",notes:"Use external UDP collector; ingest to Delta",imp:[],conf:.9},GetSNMP:{cat:"SNMP",tpl:`# SNMP get via pysnmp
from pysnmp.hlapi import *
_errorIndication, _errorStatus, _errorIndex, _varBinds = next(
    getCmd(SnmpEngine(), CommunityData("{community}"), UdpTransportTarget(("{host}", 161)), ContextData(), ObjectType(ObjectIdentity("{oid}"))))
if _errorIndication: print(f"[SNMP ERROR] {_errorIndication}")
else: print(f"[SNMP] {_varBinds}")`,desc:"SNMP get request",notes:"Install pysnmp library",imp:[],conf:.9},PutPrometheusRemoteWrite:{cat:"Monitoring",tpl:`# Prometheus remote write â€” use Databricks monitoring instead
# Databricks provides built-in metrics via Ganglia and Spark UI
print(f"[PROMETHEUS] Use Databricks built-in monitoring or configure Prometheus remote write endpoint")
# For custom metrics: import prometheus_client`,desc:"Prometheus remote write",notes:"Use Databricks built-in monitoring; or prometheus_client",imp:[],conf:.9},SendNiFiSiteToSite:{cat:"Deprecated",tpl:`# NiFi Site-to-Site â€” NOT needed in Databricks.
# Data flows are handled within the Databricks workspace.
# If cross-workspace transfer is needed, use Unity Catalog sharing.
print("[MIGRATION] NiFi Site-to-Site replaced by Unity Catalog cross-workspace sharing")`,desc:"NiFi Site-to-Site sender",notes:"Not needed â€” use UC sharing for cross-workspace data flow",imp:[],conf:.9},ConfluentSchemaRegistry:{cat:"Schema Registry",tpl:`# Confluent Schema Registry integration via Spark
from confluent_kafka.schema_registry import SchemaRegistryClient
_sr_client = SchemaRegistryClient({"url": "{schema_registry_url}"})
_schema = _sr_client.get_latest_version("{subject}").schema
print(f"[SCHEMA] Retrieved schema for {subject}: version {_schema.version}")`,desc:"Confluent Schema Registry",notes:"Install confluent-kafka; use for Kafka schema evolution",imp:[],conf:.9},HortonworksSchemaRegistry:{cat:"Schema Registry",tpl:`# Hortonworks/Cloudera Schema Registry â†’ Confluent Schema Registry or Unity Catalog
# Unity Catalog provides schema governance natively.
print("[MIGRATION] Hortonworks Schema Registry â†’ Unity Catalog schema management")`,desc:"Hortonworks Schema Registry",notes:"Migrate to Unity Catalog or Confluent Schema Registry",imp:[],conf:.9},PutRedis:{cat:"Redis",tpl:`# Redis write via redis-py
import redis
_r = redis.Redis(host="{host}", port={port}, password=dbutils.secrets.get(scope="{scope}", key="redis-pass"))
for row in df_{in}.limit(10000).collect():
    _r.set(row["{key_field}"], str(row.asDict()))`,desc:"Redis write",notes:"Install redis library; for large datasets use RDD mapPartitions",imp:[],conf:.9},GetRedis:{cat:"Redis",tpl:`# Redis read via redis-py
import redis
_r = redis.Redis(host="{host}", port={port}, password=dbutils.secrets.get(scope="{scope}", key="redis-pass"))
_keys = _r.keys("*")
_data = [{k.decode(): _r.get(k).decode()} for k in _keys[:10000]]
df_{v} = spark.createDataFrame(_data)`,desc:"Redis read",notes:"Install redis library; for large datasets use scan_iter",imp:[],conf:.9},PutPhoenix:{cat:"Phoenix/JDBC",tpl:`# DEPRECATED: Phoenix connector has limited Spark 3+ support
(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:phoenix:{zookeeper_quorum}")
  .option("dbtable", "{table}")
  .option("driver", "org.apache.phoenix.jdbc.PhoenixDriver")
  .mode("append")
  .save())`,desc:"Phoenix write via JDBC (deprecated)",notes:"Phoenix connector barely works on Spark 3+; strongly consider migrating to Delta Lake",imp:[],conf:.3},QueryPhoenix:{cat:"Phoenix/JDBC",tpl:`# DEPRECATED: Phoenix connector has limited Spark 3+ support
df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:phoenix:{zookeeper_quorum}")
  .option("dbtable", "{table}")
  .option("driver", "org.apache.phoenix.jdbc.PhoenixDriver")
  .load())`,desc:"Phoenix read via JDBC (deprecated)",notes:"Phoenix connector barely works on Spark 3+; strongly consider migrating to Delta Lake",imp:[],conf:.3},PutTeradata:{cat:"Teradata JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:teradata://{host}/DATABASE={database}")
  .option("dbtable", "{table}")
  .option("driver", "com.teradata.jdbc.TeraDriver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="td-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="td-pass"))
  .mode("append")
  .save())`,desc:"Teradata write",notes:"Install Teradata JDBC driver; store credentials in Secret Scopes",imp:[],conf:.9},QueryTeradata:{cat:"Teradata JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:teradata://{host}/DATABASE={database}")
  .option("dbtable", "{table}")
  .option("driver", "com.teradata.jdbc.TeraDriver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="td-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="td-pass"))
  .load())`,desc:"Teradata read",notes:"Install Teradata JDBC driver",imp:[],conf:.9},PutOracle:{cat:"Oracle JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:oracle:thin:@{host}:{port}:{sid}")
  .option("dbtable", "{table}")
  .option("driver", "oracle.jdbc.driver.OracleDriver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="ora-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="ora-pass"))
  .mode("append")
  .save())`,desc:"Oracle database write",notes:"Install Oracle JDBC driver (ojdbc8.jar)",imp:[],conf:.9},QueryOracle:{cat:"Oracle JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:oracle:thin:@{host}:{port}:{sid}")
  .option("dbtable", "{table}")
  .option("driver", "oracle.jdbc.driver.OracleDriver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="ora-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="ora-pass"))
  .load())`,desc:"Oracle database read",notes:"Install Oracle JDBC driver",imp:[],conf:.9},PutSAPHANA:{cat:"SAP HANA JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:sap://{host}:{port}")
  .option("dbtable", "{table}")
  .option("driver", "com.sap.db.jdbc.Driver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="sap-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="sap-pass"))
  .mode("append")
  .save())`,desc:"SAP HANA write",notes:"Install SAP HANA JDBC driver (ngdbc.jar)",imp:[],conf:.9},PutVertica:{cat:"Vertica JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:vertica://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "com.vertica.jdbc.Driver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="vertica-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="vertica-pass"))
  .mode("append")
  .save())`,desc:"Vertica write",notes:"Install Vertica JDBC driver",imp:[],conf:.9},QueryPresto:{cat:"Presto/Trino JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:presto://{host}:{port}/{catalog}")
  .option("dbtable", "{table}")
  .option("driver", "com.facebook.presto.jdbc.PrestoDriver")
  .load())`,desc:"Presto query via JDBC",notes:"Consider migrating Presto queries to Spark SQL",imp:[],conf:.9},QueryTrino:{cat:"Presto/Trino JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:trino://{host}:{port}/{catalog}")
  .option("dbtable", "{table}")
  .option("driver", "io.trino.jdbc.TrinoDriver")
  .load())`,desc:"Trino query via JDBC",notes:"Consider migrating Trino queries to Spark SQL",imp:[],conf:.9},PutGreenplum:{cat:"Greenplum JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:postgresql://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "org.postgresql.Driver")
  .mode("append")
  .save())`,desc:"Greenplum write via JDBC",notes:"Greenplum uses PostgreSQL JDBC driver",imp:[],conf:.9},PutCockroachDB:{cat:"CockroachDB JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:postgresql://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "org.postgresql.Driver")
  .mode("append")
  .save())`,desc:"CockroachDB write via JDBC",notes:"CockroachDB uses PostgreSQL wire protocol",imp:[],conf:.9},PutTimescaleDB:{cat:"TimescaleDB JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:postgresql://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "org.postgresql.Driver")
  .mode("append")
  .save())`,desc:"TimescaleDB write via JDBC",notes:"TimescaleDB uses PostgreSQL JDBC driver",imp:[],conf:.9},PutDatadog:{cat:"Monitoring",tpl:`# Datadog metrics via API
import requests
requests.post("https://api.datadoghq.com/api/v2/series", headers={"DD-API-KEY": dbutils.secrets.get(scope="{scope}", key="dd-api-key")}, json={"series":[{"metric":"{metric_name}","points":[[int(__import__("time").time()), {value}]]}]})`,desc:"Datadog metrics",notes:"Use Datadog API; or configure Databricks Datadog integration",imp:[],conf:.9},PutGrafanaAnnotation:{cat:"Monitoring",tpl:`# Grafana annotation via API
import requests
requests.post("{grafana_url}/api/annotations", headers={"Authorization":f"Bearer {dbutils.secrets.get(scope=\\"{scope}\\", key=\\"grafana-token\\")}"}, json={"text":"{annotation_text}","tags":["{tag}"]})`,desc:"Grafana annotation",notes:"Use Grafana REST API",imp:[],conf:.9},ExecuteFlinkSQL:{cat:"Spark SQL",tpl:`# Flink SQL â†’ Spark SQL (mostly compatible)
df_{v} = spark.sql("""
{sql}
""")`,desc:"Flink SQL â†’ Spark SQL",notes:"Most Flink SQL syntax is compatible with Spark SQL",imp:[],conf:.9},TriggerAirflowDag:{cat:"Workflows",tpl:`# Airflow DAG trigger â†’ Databricks Workflows
# Use Databricks Workflows for orchestration instead of Airflow
print("[MIGRATION] Airflow DAG trigger â†’ Databricks Workflows job trigger")
# To trigger a Databricks Job programmatically:
# from databricks.sdk import WorkspaceClient
# w = WorkspaceClient()
# w.jobs.run_now(job_id=<job_id>)`,desc:"Airflow DAG trigger",notes:"Replace Airflow with Databricks Workflows",imp:[],conf:.9},ExecuteProcessBash:{cat:"subprocess",tpl:`# Execute bash process
import subprocess as _sp
_result = _sp.run(["/bin/bash", "-c", "{command}"], capture_output=True, text=True, timeout=300)
if _result.returncode != 0:
    print(f"[ERROR] {_result.stderr[:200]}")
else:
    print(f"[OK] {_result.stdout[:200]}")`,desc:"Bash process execution",notes:"Review script for Databricks compatibility",imp:[],conf:.9},ConvertCSVToAvro:{cat:"DataFrame API",tpl:`from pyspark.sql.avro.functions import to_avro
df_{v} = df_{in}.select(to_avro(struct("*")).alias("value"))`,desc:"CSV to Avro conversion",notes:"Spark handles format conversion natively via DataFrame API",imp:[],conf:.9},ConvertExcelToCSVProcessor:{cat:"DataFrame API",tpl:`# Excel to CSV conversion
df_{v} = spark.read.format("com.crealytics.spark.excel")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("/Volumes/{catalog}/{schema}/{path}")`,desc:"Excel to DataFrame",notes:"Install spark-excel library (com.crealytics)",imp:[],conf:.9},EnforceOrder:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.orderBy("{order_column}")',desc:"Enforce ordering",notes:"Use orderBy for deterministic ordering",imp:[],conf:.9},GenerateRecord:{cat:"Test Data",tpl:`df_{v} = spark.range({count}).toDF("id")
# Add test columns as needed`,desc:"Generate test records",notes:"Replace with actual test data generation",imp:[],conf:.9},ListenFTP:{cat:"External Storage",tpl:`# FTP listener â†’ poll-based approach
# No native FTP listener in Databricks
# Use Auto Loader on a staged landing zone instead
df_{v} = spark.readStream.format("cloudFiles").option("cloudFiles.format", "{format}").load("/Volumes/{catalog}/{schema}/ftp_landing/")`,desc:"FTP listener â†’ Auto Loader",notes:"Stage FTP files to Volumes; use Auto Loader for pickup",imp:[],conf:.9},ValidateCSV:{cat:"DLT Expectations",tpl:`# CSV validation via DLT expectations
# @dlt.expect_or_drop("valid_csv", "col1 IS NOT NULL")
df_{v} = df_{in}.filter(col("{validation_col}").isNotNull())`,desc:"CSV validation",notes:"Use DLT expectations for data quality",imp:[],conf:.9},ValidateXml:{cat:"DataFrame API",tpl:`# XML validation â€” check structure
from pyspark.sql.functions import length
df_{v} = df_{in}.filter(length(col("value")) > 0)`,desc:"XML validation",notes:"Use spark-xml for structured parsing",imp:[],conf:.9},DeleteS3Object:{cat:"AWS S3",tpl:`# Delete S3 object
import boto3
_s3 = boto3.client("s3")
_s3.delete_object(Bucket="{bucket}", Key="{key}")
print(f"[S3] Deleted s3://{bucket}/{key}")`,desc:"S3 object deletion",notes:"Use boto3; or dbutils.fs.rm for DBFS-mounted paths",imp:[],conf:.9},TagS3Object:{cat:"AWS S3",tpl:`# Tag S3 object
import boto3
_s3 = boto3.client("s3")
_s3.put_object_tagging(Bucket="{bucket}", Key="{key}", Tagging={"TagSet":[{"Key":"{tag_key}","Value":"{tag_value}"}]})`,desc:"S3 object tagging",notes:"Use boto3 for S3 tagging operations",imp:[],conf:.9},PutKinesisStream:{cat:"AWS Kinesis",tpl:`# Kinesis â€” streaming-safe with foreachBatch
import boto3, json

def _kinesis_batch_{v}(batch_df, batch_id):
    _kinesis = boto3.client("kinesis", region_name="{region}")
    _records = [{{"Data": json.dumps(row.asDict()), "PartitionKey": str(row["{partition_key}"])}} for row in batch_df.collect()]
    for i in range(0, len(_records), 500):
        _kinesis.put_records(StreamName="{stream}", Records=_records[i:i+500])
    print(f"[KINESIS] Batch {{batch_id}}: {{len(_records)}} records")

# For streaming: df_{in}.writeStream.foreachBatch(_kinesis_batch_{v}).start()
# For batch:
_kinesis_batch_{v}(df_{in}, 0)`,desc:"Kinesis with foreachBatch",notes:"Streaming-safe; batch API",imp:["boto3"],conf:.92},GetKinesisStream:{cat:"AWS Kinesis",tpl:`# Kinesis stream read via Spark Structured Streaming
df_{v} = (spark.readStream
  .format("kinesis")
  .option("streamName", "{stream}")
  .option("region", "{region}")
  .option("initialPosition", "TRIM_HORIZON")
  .load())`,desc:"Kinesis stream read",notes:"Use Spark Kinesis connector; requires kinesis-asl library",imp:[],conf:.9},PutCloudWatchMetric:{cat:"AWS CloudWatch",tpl:`# CloudWatch metrics
import boto3
_cw = boto3.client("cloudwatch", region_name="{region}")
_cw.put_metric_data(Namespace="{namespace}", MetricData=[{"MetricName":"{metric}","Value":{value},"Unit":"{unit}"}])`,desc:"CloudWatch metric publish",notes:"Use boto3 for CloudWatch integration",imp:[],conf:.9},AmazonGlacierUpload:{cat:"AWS Glacier",tpl:`# Glacier upload via boto3
import boto3
_glacier = boto3.client("glacier", region_name="{region}")
# For archival storage, consider using S3 Glacier storage class instead
print("[AWS] Use S3 with Glacier storage class for archival")`,desc:"Glacier upload",notes:"Use S3 Intelligent-Tiering or Glacier storage class",imp:[],conf:.9},PutCloudWatchLogs:{cat:"AWS CloudWatch",tpl:`# CloudWatch Logs
import boto3
_logs = boto3.client("logs", region_name="{region}")
_logs.put_log_events(logGroupName="{log_group}", logStreamName="{log_stream}", logEvents=[{"timestamp":int(__import__("time").time()*1000),"message":"{message}"}])`,desc:"CloudWatch Logs publish",notes:"Use boto3; or configure Databricks log delivery",imp:[],conf:.9},FetchAzureDataLakeStorage:{cat:"Azure ADLS",tpl:'df_{v} = spark.read.format("{format}").load("abfss://{container}@{account}.dfs.core.windows.net/{path}")',desc:"ADLS Gen2 read",notes:"Use ABFSS paths with Unity Catalog external locations",imp:[],conf:.9},ListAzureDataLakeStorage:{cat:"Azure ADLS",tpl:`_files = dbutils.fs.ls("abfss://{container}@{account}.dfs.core.windows.net/{path}")
for f in _files:
    print(f.name, f.size)`,desc:"ADLS Gen2 list",notes:"Use dbutils.fs.ls or Auto Loader for continuous listing",imp:[],conf:.9},DeleteAzureDataLakeStorage:{cat:"Azure ADLS",tpl:'dbutils.fs.rm("abfss://{container}@{account}.dfs.core.windows.net/{path}", recurse=True)',desc:"ADLS Gen2 delete",notes:"Use dbutils.fs.rm for ADLS operations",imp:[],conf:.9},DeleteAzureBlobStorage:{cat:"Azure Blob",tpl:'dbutils.fs.rm("wasbs://{container}@{account}.blob.core.windows.net/{path}")',desc:"Azure Blob delete",notes:"Use dbutils.fs.rm",imp:[],conf:.9},PutAzureCosmosDB:{cat:"Azure Cosmos",tpl:`(df_{in}.write
  .format("cosmos.oltp")
  .option("spark.cosmos.accountEndpoint", "{endpoint}")
  .option("spark.cosmos.accountKey", dbutils.secrets.get(scope="{scope}", key="cosmos-key"))
  .option("spark.cosmos.database", "{database}")
  .option("spark.cosmos.container", "{container}")
  .mode("append")
  .save())`,desc:"Cosmos DB write",notes:"Use Azure Cosmos DB Spark connector (pre-installed on DBR)",imp:[],conf:.9},PutAzureCosmosDBRecord:{cat:"Azure Cosmos",tpl:`(df_{in}.write
  .format("cosmos.oltp")
  .option("spark.cosmos.accountEndpoint", "{endpoint}")
  .option("spark.cosmos.accountKey", dbutils.secrets.get(scope="{scope}", key="cosmos-key"))
  .option("spark.cosmos.database", "{database}")
  .option("spark.cosmos.container", "{container}")
  .option("spark.cosmos.write.strategy", "ItemOverwrite")
  .mode("append")
  .save())`,desc:"Cosmos DB record write",notes:"Use Cosmos DB Spark connector with record-level writes",imp:[],conf:.9},GetAzureEventHub:{cat:"Azure Event Hubs",tpl:`df_{v} = (spark.readStream
  .format("eventhubs")
  .options(**{"eventhubs.connectionString": dbutils.secrets.get(scope="{scope}", key="eh-connstr")})
  .load())`,desc:"Azure Event Hubs read",notes:"Use Azure Event Hubs Spark connector",imp:[],conf:.9},PutAzureQueueStorage:{cat:"Azure Queue",tpl:`# Azure Queue Storage write
from azure.storage.queue import QueueClient
_q = QueueClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="azure-storage-conn"), "{queue_name}")
for row in df_{in}.limit(1000).collect():
    _q.send_message(str(row.asDict()))`,desc:"Azure Queue write",notes:"Use azure-storage-queue SDK",imp:[],conf:.9},GetAzureQueueStorage:{cat:"Azure Queue",tpl:`# Azure Queue Storage read
from azure.storage.queue import QueueClient
_q = QueueClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="azure-storage-conn"), "{queue_name}")
_msgs = [m.content for m in _q.receive_messages(max_messages=32)]
df_{v} = spark.createDataFrame([{"message": m} for m in _msgs])`,desc:"Azure Queue read",notes:"Use azure-storage-queue SDK",imp:[],conf:.9},PutAzureServiceBus:{cat:"Azure Service Bus",tpl:`# Azure Service Bus write
from azure.servicebus import ServiceBusClient, ServiceBusMessage
_sb = ServiceBusClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="sb-connstr"))
with _sb.get_topic_sender("{topic}") as sender:
    for row in df_{in}.limit(1000).collect():
        sender.send_messages(ServiceBusMessage(str(row.asDict())))`,desc:"Azure Service Bus write",notes:"Use azure-servicebus SDK",imp:[],conf:.9},ConsumeAzureServiceBus:{cat:"Azure Service Bus",tpl:`# Azure Service Bus read
from azure.servicebus import ServiceBusClient
_sb = ServiceBusClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="sb-connstr"))
with _sb.get_subscription_receiver("{topic}", "{subscription}") as receiver:
    _msgs = [{"body": str(m)} for m in receiver.receive_messages(max_message_count=100)]
df_{v} = spark.createDataFrame(_msgs)`,desc:"Azure Service Bus read",notes:"Use azure-servicebus SDK",imp:[],conf:.9},DeleteGCSObject:{cat:"GCP GCS",tpl:'dbutils.fs.rm("gs://{bucket}/{key}")',desc:"GCS object delete",notes:"Use dbutils.fs.rm for GCS paths",imp:[],conf:.9},PutBigQueryStreaming:{cat:"GCP BigQuery",tpl:`(df_{in}.write
  .format("bigquery")
  .option("table", "{project}.{dataset}.{table}")
  .option("temporaryGcsBucket", "{temp_bucket}")
  .option("writeMethod", "direct")
  .mode("append")
  .save())`,desc:"BigQuery streaming write",notes:"Use Spark BigQuery connector with direct write method",imp:[],conf:.9},PublishGCPubSub:{cat:"GCP Pub/Sub",tpl:`# GCP Pub/Sub publish
from google.cloud import pubsub_v1
_publisher = pubsub_v1.PublisherClient()
_topic = _publisher.topic_path("{project}", "{topic}")
for row in df_{in}.limit(1000).collect():
    _publisher.publish(_topic, str(row.asDict()).encode("utf-8"))`,desc:"GCP Pub/Sub publish",notes:"Use google-cloud-pubsub SDK",imp:[],conf:.9},ConsumeGCPubSub:{cat:"GCP Pub/Sub",tpl:`# GCP Pub/Sub consume
from google.cloud import pubsub_v1
_subscriber = pubsub_v1.SubscriberClient()
_sub = _subscriber.subscription_path("{project}", "{subscription}")
_response = _subscriber.pull(subscription=_sub, max_messages=100)
_msgs = [{"data": m.message.data.decode("utf-8")} for m in _response.received_messages]
df_{v} = spark.createDataFrame(_msgs)`,desc:"GCP Pub/Sub consume",notes:"Use google-cloud-pubsub SDK; or Spark Pub/Sub connector",imp:[],conf:.9},PutGCPDataflow:{cat:"GCP Dataflow",tpl:`# GCP Dataflow trigger
# Migrate Dataflow pipelines to Databricks Structured Streaming
print("[MIGRATION] GCP Dataflow pipeline â†’ Databricks Structured Streaming")`,desc:"GCP Dataflow trigger",notes:"Replace with Databricks Structured Streaming",imp:[],conf:.9},PutSnowflake:{cat:"Snowflake",tpl:`(df_{in}.write
  .format("snowflake")
  .option("sfUrl", "{account}.snowflakecomputing.com")
  .option("sfUser", dbutils.secrets.get(scope="{scope}", key="sf-user"))
  .option("sfPassword", dbutils.secrets.get(scope="{scope}", key="sf-pass"))
  .option("sfDatabase", "{database}")
  .option("sfSchema", "{schema}")
  .option("dbtable", "{table}")
  .mode("append")
  .save())`,desc:"Snowflake write",notes:"Use Databricks Snowflake connector",imp:[],conf:.9},GetSnowflake:{cat:"Snowflake",tpl:`df_{v} = (spark.read
  .format("snowflake")
  .option("sfUrl", "{account}.snowflakecomputing.com")
  .option("sfUser", dbutils.secrets.get(scope="{scope}", key="sf-user"))
  .option("sfPassword", dbutils.secrets.get(scope="{scope}", key="sf-pass"))
  .option("sfDatabase", "{database}")
  .option("sfSchema", "{schema}")
  .option("dbtable", "{table}")
  .load())`,desc:"Snowflake read",notes:"Use Databricks Snowflake connector",imp:[],conf:.9},PutCypher:{cat:"Neo4j",tpl:`# Neo4j write via neo4j-driver
from neo4j import GraphDatabase
_driver = GraphDatabase.driver("{uri}", auth=(dbutils.secrets.get(scope="{scope}", key="neo4j-user"), dbutils.secrets.get(scope="{scope}", key="neo4j-pass")))
with _driver.session() as session:
    for row in df_{in}.limit(1000).collect():
        session.run("{cypher_query}", **row.asDict())`,desc:"Neo4j Cypher write",notes:"Use neo4j-driver; or Neo4j Spark connector for large datasets",imp:[],conf:.9},GetCypher:{cat:"Neo4j",tpl:`# Neo4j read via Spark connector
df_{v} = (spark.read
  .format("org.neo4j.spark.DataSource")
  .option("url", "{uri}")
  .option("authentication.basic.username", dbutils.secrets.get(scope="{scope}", key="neo4j-user"))
  .option("authentication.basic.password", dbutils.secrets.get(scope="{scope}", key="neo4j-pass"))
  .option("query", "{cypher_query}")
  .load())`,desc:"Neo4j Cypher read",notes:"Use Neo4j Spark Connector for distributed reads",imp:[],conf:.9},PutDruidRecord:{cat:"Druid",tpl:`# Apache Druid ingestion via JDBC
(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:avatica:remote:url={druid_url}/druid/v2/sql/avatica/")
  .option("dbtable", "{datasource}")
  .option("driver", "org.apache.calcite.avatica.remote.Driver")
  .mode("append")
  .save())
# Alternative: use Druid indexer REST API for native ingestion`,desc:"Druid record ingestion via JDBC",notes:"Use JDBC with Avatica driver for Druid SQL; consider migrating to Delta Lake + Photon",imp:[],conf:.7},QueryDruid:{cat:"Druid",tpl:`# Druid query via JDBC
df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:avatica:remote:url={druid_url}/druid/v2/sql/avatica/")
  .option("dbtable", "({sql}) AS druid_query")
  .option("driver", "org.apache.calcite.avatica.remote.Driver")
  .load())`,desc:"Druid SQL query via JDBC",notes:"Use JDBC with Avatica driver; consider migrating to Spark SQL on Delta Lake",imp:[],conf:.7},PutClickHouse:{cat:"ClickHouse",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:clickhouse://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
  .mode("append")
  .save())`,desc:"ClickHouse write",notes:"Use ClickHouse JDBC driver",imp:[],conf:.9},QueryClickHouse:{cat:"ClickHouse",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:clickhouse://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
  .load())`,desc:"ClickHouse read",notes:"Use ClickHouse JDBC driver; consider migrating to Delta Lake + Photon",imp:[],conf:.9},PutIceberg:{cat:"Iceberg",tpl:`(df_{in}.writeTo("spark_catalog.{database}.{table}")
  .using("iceberg")
  .tableProperty("format-version", "2")
  .createOrReplace())`,desc:"Iceberg table write",notes:"Databricks supports Iceberg natively with UniForm; consider Delta Lake",imp:[],conf:.9},PutHudi:{cat:"Hudi",tpl:`(df_{in}.write
  .format("hudi")
  .option("hoodie.table.name", "{table}")
  .option("hoodie.datasource.write.operation", "upsert")
  .option("hoodie.datasource.write.recordkey.field", "{record_key}")
  .mode("append")
  .save("{path}"))`,desc:"Hudi table write",notes:"Consider migrating to Delta Lake for native Databricks support",imp:[],conf:.9},GetSplunk:{cat:"Splunk",tpl:`# Splunk search via SDK
import splunklib.client as client
import splunklib.results as results
_svc = client.connect(host="{host}", port={port}, username=dbutils.secrets.get(scope="{scope}", key="splunk-user"), password=dbutils.secrets.get(scope="{scope}", key="splunk-pass"))
_job = _svc.jobs.create("{search_query}", **{"earliest_time":"-1h"})
while not _job.is_done(): __import__("time").sleep(1)
_reader = results.JSONResultsReader(_job.results(output_mode="json"))
df_{v} = spark.createDataFrame([dict(r) for r in _reader if isinstance(r, dict)])`,desc:"Splunk search read",notes:"Use splunklib SDK; or Splunk Spark connector for large datasets",imp:[],conf:.9},QuerySplunkIndexingStatus:{cat:"Splunk",tpl:`# Splunk indexing status query
import splunklib.client as client
_svc = client.connect(host="{host}", port={port})
print(f"[SPLUNK] Indexing status: {_svc.indexes.list()}")`,desc:"Splunk indexing status",notes:"Use splunklib SDK for status monitoring",imp:[],conf:.9},QueryInfluxDB:{cat:"InfluxDB",tpl:`# InfluxDB query via client
from influxdb_client import InfluxDBClient
_client = InfluxDBClient(url="{url}", token=dbutils.secrets.get(scope="{scope}", key="influx-token"), org="{org}")
_query_api = _client.query_api()
_tables = _query_api.query("{flux_query}")
_records = []
for table in _tables:
    for record in table.records:
        _records.append(record.values)
df_{v} = spark.createDataFrame(_records)`,desc:"InfluxDB query",notes:"Use influxdb-client SDK; consider migrating time-series to Delta Lake",imp:[],conf:.9},ConvertAvroToParquet:{cat:"DataFrame API",tpl:`df_{v} = spark.read.format("avro").load("{input_path}")
df_{v}.write.format("parquet").save("{output_path}")`,desc:"Avro to Parquet conversion",notes:"Spark handles format conversion natively",imp:[],conf:.9},ConvertParquetToAvro:{cat:"DataFrame API",tpl:`df_{v} = spark.read.format("parquet").load("{input_path}")
df_{v}.write.format("avro").save("{output_path}")`,desc:"Parquet to Avro conversion",notes:"Spark handles format conversion natively",imp:[],conf:.9},SplitAvro:{cat:"DataFrame API",tpl:`# Avro split â€” Spark reads Avro files as DataFrames natively
df_{v} = spark.read.format("avro").load("{input_path}")
# Process each record individually if needed
for row in df_{v}.collect():
    pass  # Process row`,desc:"Avro split",notes:"Not needed in Spark â€” read entire Avro dataset as DataFrame",imp:[],conf:.9},ConvertJSONToAvro:{cat:"DataFrame API",tpl:`from pyspark.sql.avro.functions import to_avro
from pyspark.sql.functions import struct
df_{v} = df_{in}.select(to_avro(struct("*")).alias("value"))`,desc:"JSON to Avro conversion",notes:"Use Spark built-in avro functions",imp:[],conf:.9},FlattenJson:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import col, explode_outer
# Flatten nested JSON structure
df_{v} = df_{in}
for field in [f for f in df_{in}.schema.fields if str(f.dataType).startswith("Struct")]:
    for nested in field.dataType.fields:
        df_{v} = df_{v}.withColumn(f"{field.name}_{nested.name}", col(f"{field.name}.{nested.name}"))
    df_{v} = df_{v}.drop(field.name)`,desc:"JSON flattening",notes:"Use PySpark struct navigation; or explode for arrays",imp:[],conf:.9},Base64EncodeContent:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import base64, unbase64
df_{v} = df_{in}.withColumn("{column}", base64(col("{column}")))`,desc:"Base64 encode/decode",notes:"Use PySpark base64/unbase64 functions",imp:[],conf:.9},ConvertCharacterSet:{cat:"DataFrame API",tpl:`# Character set conversion â€” Spark handles encoding via options
df_{v} = spark.read.option("encoding", "{target_encoding}").format("{format}").load("{path}")`,desc:"Character set conversion",notes:"Use Spark read options for encoding; or Python encode/decode",imp:[],conf:.9},ExtractEmailHeaders:{cat:"Email",tpl:`# Extract email headers
import email
from pyspark.sql.functions import udf
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def extract_headers(raw):
    msg = email.message_from_string(raw)
    return dict(msg.items())
df_{v} = df_{in}.withColumn("headers", extract_headers(col("{content_column}")))`,desc:"Email header extraction",notes:"Use Python email module as UDF",imp:[],conf:.9},ExtractEmailAttachments:{cat:"Email",tpl:`# Extract email attachments
import email, base64
def extract_attachments(raw):
    msg = email.message_from_string(raw)
    attachments = []
    for part in msg.walk():
        if part.get_content_disposition() == "attachment":
            attachments.append({"filename": part.get_filename(), "size": len(part.get_payload())})
    return attachments
# Apply as UDF on email content column`,desc:"Email attachment extraction",notes:"Use Python email module; store attachments in Volumes",imp:[],conf:.9},ConsumeIMAP:{cat:"Email",tpl:`# IMAP email consumption
import imaplib, email
_mail = imaplib.IMAP4_SSL("{host}")
_mail.login(dbutils.secrets.get(scope="{scope}", key="email-user"), dbutils.secrets.get(scope="{scope}", key="email-pass"))
_mail.select("INBOX")
_, _nums = _mail.search(None, "UNSEEN")
_emails = []
for num in _nums[0].split():
    _, data = _mail.fetch(num, "(RFC822)")
    _emails.append({"raw": data[0][1].decode("utf-8", errors="replace")})
df_{v} = spark.createDataFrame(_emails)`,desc:"IMAP email consumer",notes:"Use imaplib; schedule as periodic job",imp:[],conf:.9},ConsumePOP3:{cat:"Email",tpl:`# POP3 email consumption
import poplib, email
_pop = poplib.POP3_SSL("{host}")
_pop.user(dbutils.secrets.get(scope="{scope}", key="email-user"))
_pop.pass_(dbutils.secrets.get(scope="{scope}", key="email-pass"))
_count = len(_pop.list()[1])
_emails = []
for i in range(1, min(_count+1, 100)):
    _, lines, _ = _pop.retr(i)
    _emails.append({"raw": b"\\n".join(lines).decode("utf-8", errors="replace")})
df_{v} = spark.createDataFrame(_emails)
_pop.quit()`,desc:"POP3 email consumer",notes:"Use poplib; schedule as periodic job",imp:[],conf:.9},SetSNMP:{cat:"SNMP",tpl:`# SNMP set operation
from pysnmp.hlapi import setCmd, SnmpEngine, CommunityData, UdpTransportTarget, ContextData, ObjectType, ObjectIdentity
_errorIndication, _, _, _ = next(setCmd(
    SnmpEngine(), CommunityData("{community}"),
    UdpTransportTarget(("{host}", {port})), ContextData(),
    ObjectType(ObjectIdentity("{oid}"), "{value}")
))
if _errorIndication: print(f"[SNMP ERROR] {_errorIndication}")`,desc:"SNMP set operation",notes:"Use pysnmp library; install via pip",imp:[],conf:.9},ListenSNMP:{cat:"SNMP",tpl:`# SNMP trap listener â€” use scheduled polling instead
from pysnmp.hlapi import getCmd, SnmpEngine, CommunityData, UdpTransportTarget, ContextData, ObjectType, ObjectIdentity
print("[MIGRATION] SNMP trap listener â†’ scheduled SNMP polling job")`,desc:"SNMP trap listener",notes:"Replace with scheduled SNMP polling job",imp:[],conf:.9},PutSyslog:{cat:"Syslog",tpl:`# Syslog send
import socket
_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
_sock.sendto("{message}".encode(), ("{host}", {port}))
_sock.close()`,desc:"Syslog sender",notes:"Use socket for UDP syslog; or configure log forwarding",imp:[],conf:.9},PutUDP:{cat:"Network",tpl:`# UDP send
import socket
_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
for row in df_{in}.limit(1000).collect():
    _sock.sendto(str(row.asDict()).encode(), ("{host}", {port}))
_sock.close()`,desc:"UDP sender",notes:"Use Python socket for UDP operations",imp:[],conf:.9},GetTCP:{cat:"Network",tpl:`# TCP read â€” use structured streaming or scheduled batch instead
print("[MIGRATION] TCP listener â†’ Databricks Structured Streaming or scheduled batch job")
# For TCP sources, stage data to cloud storage and use Auto Loader`,desc:"TCP reader",notes:"Replace with Auto Loader on staged data",imp:[],conf:.9},PutRELP:{cat:"Network",tpl:`# RELP output â€” use standard syslog or HTTP endpoint
import requests
for row in df_{in}.limit(1000).collect():
    requests.post("{endpoint}", json=row.asDict())`,desc:"RELP output",notes:"Replace with HTTP endpoint or syslog",imp:[],conf:.9},ListenRELP:{cat:"Network",tpl:`# RELP listener â†’ use HTTP endpoint or cloud-based log collection
print("[MIGRATION] RELP listener â†’ HTTP endpoint or cloud logging service")`,desc:"RELP listener",notes:"Replace with cloud-native log collection",imp:[],conf:.9},ExecuteFlumeSink:{cat:"Spark Streaming",tpl:`# Flume sink â†’ Spark Structured Streaming
# Apache Flume is deprecated â€” migrate to Spark Streaming
df_{v} = (spark.readStream
  .format("{format}")
  .load("{path}"))
print("[MIGRATION] Flume sink replaced by Spark Structured Streaming")`,desc:"Flume sink â†’ Structured Streaming",notes:"Flume is deprecated; migrate to Structured Streaming",imp:[],conf:.9},ExecuteFlumeSource:{cat:"Spark Streaming",tpl:`# Flume source â†’ Spark Structured Streaming
# Apache Flume is deprecated â€” migrate to Spark Streaming
df_{v} = (spark.readStream
  .format("{format}")
  .load("{path}"))
print("[MIGRATION] Flume source replaced by Spark Structured Streaming")`,desc:"Flume source â†’ Structured Streaming",notes:"Flume is deprecated; migrate to Structured Streaming",imp:[],conf:.9},ConsumeWindowsEventLog:{cat:"Windows",tpl:`# Windows Event Log â†’ schedule a collector script
# In Databricks, collect Windows events via:
# 1. Forward events to a log aggregator (Splunk, ELK)
# 2. Write to cloud storage
# 3. Read with Auto Loader
print("[MIGRATION] Windows Event Log â†’ forward to cloud storage + Auto Loader")`,desc:"Windows Event Log consumer",notes:"Forward events to cloud storage; use Auto Loader",imp:[],conf:.9},ConsumeEWS:{cat:"Email/Exchange",tpl:`# Exchange Web Services consumer
# Use Microsoft Graph API instead of EWS
import requests
_token = dbutils.secrets.get(scope="{scope}", key="graph-token")
_r = requests.get("https://graph.microsoft.com/v1.0/me/messages", headers={"Authorization":f"Bearer {_token}"})
df_{v} = spark.createDataFrame(_r.json().get("value", []))`,desc:"Exchange Web Services consumer",notes:"Migrate to Microsoft Graph API",imp:[],conf:.9},RetryFlowFile:{cat:"Utility",tpl:`# Retry logic â€” use try/except with backoff
import time
for _attempt in range(3):
    try:
        # <retry logic here>
        break
    except Exception as e:
        if _attempt < 2: time.sleep(2 ** _attempt)
        else: raise`,desc:"Retry logic",notes:"Use Python try/except with exponential backoff",imp:[],conf:.9},ReplaceTextWithMapping:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import regexp_replace, col
df_{v} = df_{in}
# Apply text replacements
_mappings = {"{pattern}": "{replacement}"}
for pat, rep in _mappings.items():
    df_{v} = df_{v}.withColumn("{column}", regexp_replace(col("{column}"), pat, rep))`,desc:"Text replacement with mapping",notes:"Use PySpark regexp_replace for pattern-based replacements",imp:[],conf:.9},MonitorActivity:{cat:"Utility",tpl:`# Monitor activity â€” track flow metrics
from datetime import datetime
_now = datetime.now()
print(f"[MONITOR] Flow activity check at {_now}")
# In Databricks, use Workflows monitoring and Spark UI`,desc:"Activity monitor",notes:"Use Databricks Workflows monitoring and alerting",imp:[],conf:.9},ScanAttribute:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import col
# Scan/filter based on attribute patterns
df_{v} = df_{in}.filter(col("{attribute}").rlike("{pattern}"))`,desc:"Attribute scanning",notes:"Use PySpark rlike for regex-based attribute scanning",imp:[],conf:.9},ScanContent:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import col
# Scan content for pattern matches
df_{v} = df_{in}.filter(col("{content_column}").rlike("{pattern}"))`,desc:"Content scanning",notes:"Use PySpark rlike for content pattern matching",imp:[],conf:.9},QueryWhois:{cat:"Network",tpl:`# WHOIS query
import subprocess
_result = subprocess.run(["whois", "{domain}"], capture_output=True, text=True, timeout=30)
print(_result.stdout[:500])`,desc:"WHOIS lookup",notes:"Use whois command or python-whois library",imp:[],conf:.9},GeoEnrichIPRecord:{cat:"DataFrame API",tpl:`# IP geolocation enrichment
# Install: pip install geoip2
import geoip2.database
_reader = geoip2.database.Reader("{geoip_db_path}")
def enrich_ip(ip):
    try:
        r = _reader.city(ip)
        return {"country": r.country.name, "city": r.city.name, "lat": r.location.latitude, "lon": r.location.longitude}
    except: return None
# Register as UDF and apply to IP column`,desc:"IP geolocation enrichment",notes:"Use geoip2 library with MaxMind database",imp:[],conf:.9},PublishKafka_1_0:{cat:"Kafka",tpl:`(df_{in}.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{bootstrap_servers}")
  .option("topic", "{topic}")
  .save())`,desc:"Kafka 1.0 producer",notes:"Use Spark Kafka connector (version-agnostic)",imp:[],conf:.9},ConsumeKafka_1_0:{cat:"Kafka",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{bootstrap_servers}")
  .option("subscribe", "{topic}")
  .option("startingOffsets", "earliest")
  .load())`,desc:"Kafka 1.0 consumer",notes:"Use Spark Kafka connector (version-agnostic)",imp:[],conf:.9},ConsumeKafkaRecord:{cat:"Kafka",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{bootstrap_servers}")
  .option("subscribe", "{topic}")
  .option("startingOffsets", "earliest")
  .load()
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)"))`,desc:"Kafka record consumer",notes:"Use Spark Kafka connector with schema-aware deserialization",imp:[],conf:.9},ListDatabaseTables:{cat:"JDBC",tpl:`# List database tables
df_{v} = spark.sql("SHOW TABLES IN {database}")
df_{v}.show()`,desc:"List database tables",notes:"Use Spark SQL SHOW TABLES; or JDBC metadata query",imp:[],conf:.9},PutSQL:{cat:"JDBC",tpl:`# Execute SQL statement
(df_{in}.write
  .format("jdbc")
  .option("url", "{jdbc_url}")
  .option("dbtable", "{table}")
  .option("driver", "{driver}")
  .mode("append")
  .save())`,desc:"SQL write via JDBC",notes:"Use Spark JDBC writer",imp:[],conf:.9},AvroSchemaRegistry:{cat:"Schema Registry",tpl:`# Avro Schema Registry integration
from confluent_kafka.schema_registry import SchemaRegistryClient
_sr = SchemaRegistryClient({"url": "{schema_registry_url}"})
_schema = _sr.get_latest_version("{subject}").schema
print(f"[SCHEMA] Latest schema version retrieved")`,desc:"Avro Schema Registry",notes:"Use Confluent Schema Registry client",imp:[],conf:.9},RemoteProcessGroup:{cat:"Deprecated",tpl:`# NiFi Remote Process Group â€” NOT needed in Databricks
# Cross-workspace data sharing handled by Unity Catalog
print("[MIGRATION] NiFi Remote Process Group â†’ Unity Catalog cross-workspace sharing")`,desc:"NiFi RPG",notes:"Not needed â€” use Unity Catalog for data sharing",imp:[],conf:.9},InputPort:{cat:"Deprecated",tpl:`# NiFi Input Port â€” NOT needed in Databricks
# Data ingress handled by Auto Loader or readStream
print("[MIGRATION] NiFi Input Port â†’ Auto Loader / readStream")`,desc:"NiFi Input Port",notes:"Not needed â€” use Auto Loader for ingestion",imp:[],conf:.9},OutputPort:{cat:"Deprecated",tpl:`# NiFi Output Port â€” NOT needed in Databricks
# Data egress handled by writeStream or Delta sharing
print("[MIGRATION] NiFi Output Port â†’ writeStream / Delta Sharing")`,desc:"NiFi Output Port",notes:"Not needed â€” use writeStream or Delta Sharing",imp:[],conf:.9},Funnel:{cat:"Deprecated",tpl:`# NiFi Funnel â€” NOT needed in Databricks
# DataFrame operations naturally merge data flows via union()
print("[MIGRATION] NiFi Funnel â†’ DataFrame union()")`,desc:"NiFi Funnel",notes:"Not needed â€” use DataFrame union()",imp:[],conf:.9},PutSlackMessage:{cat:"Slack",tpl:`# Slack message via webhook
import requests
requests.post("{webhook_url}", json={"text": "{message}"})`,desc:"Slack message",notes:"Use Slack webhook URL; store in secrets",imp:[],conf:.9},SendTelegram:{cat:"Notification",tpl:`# Telegram notification
import requests
requests.post(f"https://api.telegram.org/bot{dbutils.secrets.get(scope=\\"{scope}\\", key=\\"telegram-token\\")}/sendMessage", json={"chat_id":"{chat_id}","text":"{message}"})`,desc:"Telegram notification",notes:"Use Telegram Bot API",imp:[],conf:.9},PutPagerDuty:{cat:"Notification",tpl:`# PagerDuty alert
import requests
requests.post("https://events.pagerduty.com/v2/enqueue", json={"routing_key":dbutils.secrets.get(scope="{scope}",key="pd-key"),"event_action":"trigger","payload":{"summary":"{summary}","severity":"{severity}","source":"{source}"}})`,desc:"PagerDuty alert",notes:"Use PagerDuty Events API v2",imp:[],conf:.9},PutOpsGenie:{cat:"Notification",tpl:`# OpsGenie alert
import requests
requests.post("https://api.opsgenie.com/v2/alerts", headers={"Authorization":"GenieKey "+dbutils.secrets.get(scope="{scope}",key="opsgenie-key")}, json={"message":"{message}","priority":"{priority}"})`,desc:"OpsGenie alert",notes:"Use OpsGenie REST API",imp:[],conf:.9},PostHTTP:{cat:"HTTP",tpl:`# HTTP POST â€” streaming-safe with foreachBatch
import requests

def _post_batch_{v}(batch_df, batch_id):
    _records = batch_df.toPandas().to_dict(orient="records")
    _response = requests.post("{url}", json=_records, headers={"{header_key}":"{header_value}"}, timeout=30)
    print(f"[HTTP] Batch {batch_id}: {len(_records)} records -> {_response.status_code}")

# For streaming: df_{in}.writeStream.foreachBatch(_post_batch_{v}).start()
# For batch:
_post_batch_{v}(df_{in}, 0)`,desc:"HTTP POST with foreachBatch",notes:"Streaming-safe",imp:["requests"],conf:.92},GetHTTP:{cat:"HTTP",tpl:`# DEPRECATED NiFi processor â€” use InvokeHTTP instead
import requests
_response = requests.get("{url}", headers={"{header_key}":"{header_value}"}, timeout=30)
df_{v} = spark.createDataFrame([_response.json()] if isinstance(_response.json(), dict) else _response.json())`,desc:"HTTP GET (deprecated NiFi processor)",notes:"GetHTTP is deprecated in NiFi; use InvokeHTTP. Mapping uses requests library.",imp:[],conf:.5},CryptographicHashContent:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import sha2, col
df_{v} = df_{in}.withColumn("{hash_column}", sha2(col("{content_column}"), 256))`,desc:"Cryptographic hash",notes:"Use PySpark sha2, md5, or sha1 functions",imp:[],conf:.9},SignContent:{cat:"Security",tpl:`# Digital signature â€” use Python cryptography library
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import padding
print("[CRYPTO] Use cryptography library for digital signatures")`,desc:"Digital signature",notes:"Use Python cryptography library",imp:[],conf:.9},VerifyContentMAC:{cat:"Security",tpl:`# MAC verification â€” use Python hmac module
import hmac, hashlib
_mac = hmac.new(key=b"{key}", msg=b"{message}", digestmod=hashlib.sha256)
print(f"[CRYPTO] MAC: {_mac.hexdigest()}")`,desc:"MAC verification",notes:"Use Python hmac module",imp:[],conf:.9},ValidateJSON:{cat:"DLT Expectations",tpl:`from pyspark.sql.functions import col, from_json, schema_of_json
# Validate JSON structure
_sample = df_{in}.select("{json_column}").first()[0]
_schema = schema_of_json(_sample)
df_{v} = df_{in}.withColumn("_parsed", from_json(col("{json_column}"), _schema)).filter(col("_parsed").isNotNull())`,desc:"JSON validation",notes:"Use DLT expectations for production data quality",imp:[],conf:.9},SchemaValidation:{cat:"DLT Expectations",tpl:`# Schema validation via DLT expectations
# @dlt.expect_all_or_drop({{"valid_schema": "col1 IS NOT NULL AND col2 IS NOT NULL"}})
df_{v} = df_{in}.filter(col("{required_col}").isNotNull())`,desc:"Schema validation",notes:"Use DLT expectations for declarative data quality",imp:[],conf:.9},ConsumeKafka_2_0:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .option("startingOffsets", "earliest")
  .load()
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)", "topic", "partition", "offset", "timestamp"))`,desc:"Kafka 2.0 consumer via Structured Streaming",notes:"Update broker config",imp:["pyspark.sql.functions"],conf:.95},ConsumeKafkaRecord_1_0:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load()
  .selectExpr("CAST(value AS STRING) as json_str"))`,desc:"Kafka Record 1.0 consumer",notes:"Schema handled by Spark",imp:["pyspark.sql.functions"],conf:.95},ConsumeKafkaRecord_2_0:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load()
  .selectExpr("CAST(value AS STRING) as json_str"))`,desc:"Kafka Record 2.0 consumer",notes:"Schema handled by Spark",imp:["pyspark.sql.functions"],conf:.95},PublishKafka_2_0:{cat:"Structured Streaming",tpl:`(df_{v}
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .write.format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Kafka 2.0 producer",notes:"Update broker config",imp:[],conf:.95},PublishKafkaRecord_1_0:{cat:"Structured Streaming",tpl:`(df_{v}
  .selectExpr("CAST(key AS STRING)", "to_json(struct(*)) AS value")
  .write.format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Kafka Record 1.0 producer",notes:"Update broker config",imp:["pyspark.sql.functions"],conf:.95},PublishKafkaRecord_2_0:{cat:"Structured Streaming",tpl:`(df_{v}
  .selectExpr("CAST(key AS STRING)", "to_json(struct(*)) AS value")
  .write.format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Kafka Record 2.0 producer",notes:"Update broker config",imp:["pyspark.sql.functions"],conf:.95},ConsumeKinesisStream:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kinesis")
  .option("streamName", "{stream}")
  .option("region", "{region}")
  .option("initialPosition", "TRIM_HORIZON")
  .load())`,desc:"Kinesis consumer via Structured Streaming",notes:"Use Databricks Kinesis connector",imp:[],conf:.92},ConvertAvroToORC:{cat:"Spark DataFrame",tpl:`df_{v} = spark.read.format("avro").load("{path}")
df_{v}.write.format("orc").save("{output_path}")`,desc:"Avro to ORC via Spark",notes:"Format-agnostic DataFrames",imp:[],conf:.95},FetchParquet:{cat:"Spark DataFrame",tpl:'df_{v} = spark.read.format("parquet").load("{path}")',desc:"Read Parquet natively",notes:"Parquet is Spark native",imp:[],conf:.95},JoltTransformRecord:{cat:"Spark DataFrame",tpl:`df_{v} = df_{input}
for src, dst in _jolt_mappings.items():
    df_{v} = df_{v}.withColumnRenamed(src, dst)`,desc:"Jolt record transform via DataFrame ops",notes:"Translate Jolt spec to PySpark",imp:["pyspark.sql.functions"],conf:.9},EvaluateXPath:{cat:"Spark XML",tpl:`from pyspark.sql.functions import xpath, col
# xpath() returns array; take first element
df_{v} = df_{input}.withColumn("_xpath_result", xpath(col("xml_content"), lit("{xpath_expr}"))[0])`,desc:"XPath evaluation via PySpark xpath()",notes:"xpath() returns ArrayType; use [0] to get first result. Install spark-xml for complex XML.",imp:["pyspark.sql.functions"],conf:.85},EvaluateXQuery:{cat:"Spark XML",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import lxml.etree as ET
@udf(StringType())
def eval_xquery(xml_str):
    doc = ET.fromstring(xml_str.encode())
    return str(doc.xpath("{xquery_expr}"))
df_{v} = df_{input}.withColumn("_xq", eval_xquery(col("value")))`,desc:"XQuery via lxml UDF",notes:"Install lxml on cluster",imp:["lxml"],conf:.9},SplitXml:{cat:"Spark XML",tpl:'df_{v} = spark.read.format("xml").option("rowTag", "{tag}").load("{path}")',desc:"Split XML via spark-xml rowTag",notes:"Install spark-xml",imp:[],conf:.92},ExtractGrok:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import regexp_extract, col
df_{v} = df_{input}.withColumn("_extracted", regexp_extract(col("value"), r"{regex_pattern}", 1))`,desc:"Grok extraction via regexp_extract",notes:"Translate Grok to regex",imp:["pyspark.sql.functions"],conf:.9},ExtractAvroMetadata:{cat:"Spark DataFrame",tpl:`df_{v} = spark.read.format("avro").load("{path}")
print(f"Schema: {df_{v}.schema.simpleString()}")`,desc:"Extract Avro schema metadata",notes:"Schema auto-detected by Spark",imp:[],conf:.93},ExtractHL7Attributes:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_hl7(msg):
    segs = msg.split("\\r")
    return {s.split("|")[0]: "|".join(s.split("|")[1:4]) for s in segs if "|" in s}
df_{v} = df_{input}.withColumn("hl7_attrs", parse_hl7(col("value")))`,desc:"HL7 message parsing via UDF",notes:"Use hl7apy for production",imp:["pyspark.sql.functions"],conf:.9},ExtractCCDAAttributes:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_ccda(xml):
    import lxml.etree as ET
    doc = ET.fromstring(xml.encode())
    return {"patient": doc.findtext(".//{urn:hl7-org:v3}patient/{urn:hl7-org:v3}name", default="")}
df_{v} = df_{input}.withColumn("ccda_attrs", parse_ccda(col("value")))`,desc:"CCDA clinical document parsing",notes:"Install lxml",imp:["lxml"],conf:.9},RouteHL7:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import col
df_{v}_adt = df_{input}.filter(col("value").contains("ADT"))
df_{v}_orm = df_{input}.filter(col("value").contains("ORM"))
df_{v} = df_{input}`,desc:"HL7 message routing by type",notes:"Filter by MSH segment",imp:["pyspark.sql.functions"],conf:.9},ExtractTNEFAttachments:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import ArrayType, StringType
@udf(ArrayType(StringType()))
def extract_tnef(data):
    return ["attachment_extracted"]
df_{v} = df_{input}.withColumn("tnef_attachments", extract_tnef(col("content")))`,desc:"TNEF attachment extraction",notes:"Install tnefparse",imp:["tnefparse"],conf:.9},ParseCEF:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import regexp_extract, col
df_{v} = df_{input}.withColumn("cef_vendor", regexp_extract(col("value"), r"CEF:\\d+\\|([^|]+)", 1)).withColumn("cef_severity", regexp_extract(col("value"), r"CEF:\\d+(?:\\|[^|]*){6}\\|([^|]+)", 1))`,desc:"CEF security log parsing",notes:"Regex-based",imp:["pyspark.sql.functions"],conf:.9},ParseEvtx:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_evtx(xml):
    import lxml.etree as ET
    doc = ET.fromstring(xml.encode())
    return {"EventID": doc.findtext(".//{*}EventID", default="")}
df_{v} = df_{input}.withColumn("event_data", parse_evtx(col("value")))`,desc:"Windows Event Log XML parsing",notes:"Parse EVTX XML",imp:["lxml"],conf:.9},ParseNetflowv5:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import col
df_{v} = df_{input}.selectExpr("*", "substring(value,1,4) as src_ip", "substring(value,5,4) as dst_ip")`,desc:"NetFlow v5 packet parsing",notes:"Binary format needs UDF",imp:["pyspark.sql.functions"],conf:.9},ParseSyslog5424:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import regexp_extract, col
df_{v} = df_{input}.withColumn("priority", regexp_extract(col("value"), r"<(\\d+)>", 1)).withColumn("hostname", regexp_extract(col("value"), r"<\\d+>\\d+ [\\S]+ ([\\S]+)", 1))`,desc:"RFC 5424 Syslog parsing",notes:"Regex extraction",imp:["pyspark.sql.functions"],conf:.92},ListenGRPC:{cat:"Databricks Serving",tpl:`import grpc
from concurrent import futures
print(f"[gRPC] Server endpoint configured")`,desc:"gRPC listener via Databricks Serving",notes:"Deploy as Databricks App",imp:["grpcio"],conf:.9},InvokeGRPC:{cat:"PySpark UDF",tpl:`import grpc
_channel = grpc.insecure_channel("{host}:{port}")
df_{v} = df_{input}`,desc:"gRPC client invocation",notes:"Generate stubs from .proto",imp:["grpcio"],conf:.9},ConnectWebSocket:{cat:"Structured Streaming",tpl:`import websocket, json
_ws = websocket.create_connection("{ws_url}")
_msgs = [{"data": _ws.recv()} for _ in range(100)]
_ws.close()
df_{v} = spark.createDataFrame(_msgs)`,desc:"WebSocket client",notes:"Use websocket-client",imp:["websocket-client"],conf:.9},ListenWebSocket:{cat:"Structured Streaming",tpl:`import asyncio, websockets
print("[WS] WebSocket listener configured")`,desc:"WebSocket server via Databricks App",notes:"Deploy as Databricks App",imp:["websockets"],conf:.9},PutWebSocket:{cat:"PySpark UDF",tpl:`import websocket, json
_ws = websocket.create_connection("{ws_url}")
for row in df_{input}.limit(1000).collect():
    _ws.send(json.dumps(row.asDict()))
_ws.close()`,desc:"WebSocket message sender",notes:"Use websocket-client",imp:["websocket-client"],conf:.9},ListenSMTP:{cat:"Databricks App",tpl:`from aiosmtpd.controller import Controller
print("[SMTP] Email receiver configured")`,desc:"SMTP receiver via Databricks App",notes:"Deploy as Databricks App",imp:["aiosmtpd"],conf:.9},ForkRecord:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import explode, col
df_{v} = df_{input}.select(explode(col("{array_field}")).alias("record"), "*")`,desc:"Fork/explode nested records",notes:"Use explode",imp:["pyspark.sql.functions"],conf:.93},SampleRecord:{cat:"Spark DataFrame",tpl:"df_{v} = df_{input}.sample(fraction={sample_rate}, seed=42)",desc:"Sample records",notes:"Adjust fraction",imp:[],conf:.95},ScriptedTransformRecord:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col, struct
from pyspark.sql.types import StringType
import json
@udf(StringType())
def transform_record(row_json):
    data = json.loads(row_json)
    data["_processed"] = True
    return json.dumps(data)
df_{v} = df_{input}.withColumn("_transformed", transform_record(col("value")))`,desc:"Scripted record transform via UDF",notes:"Migrate NiFi script",imp:["pyspark.sql.functions"],conf:.9},PutRecord:{cat:"Delta Lake",tpl:'df_{input}.write.format("delta").mode("append").saveAsTable("{table_name}")',desc:"Generic record writer via Delta",notes:"Use Delta for persistence",imp:[],conf:.93},InvokeScriptedProcessor:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
@udf(StringType())
def scripted_logic(value):
    return value
df_{v} = df_{input}.withColumn("_result", scripted_logic(col("value")))`,desc:"Scripted processor via UDF",notes:"Translate NiFi script to Python",imp:["pyspark.sql.functions"],conf:.9},CryptographicHashAttribute:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import sha2, col
df_{v} = df_{input}.withColumn("_hash", sha2(col("{attr_name}"), 256))`,desc:"Hash attribute with SHA-256",notes:"Built-in sha2",imp:["pyspark.sql.functions"],conf:.95},HashAttribute:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import sha2, col
df_{v} = df_{input}.withColumn("_hash", sha2(col("{attr_name}"), 256))`,desc:"Hash attribute value",notes:"Built-in sha2/md5",imp:["pyspark.sql.functions"],conf:.95},EncryptContentPGP:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import BinaryType
import gnupg
_gpg = gnupg.GPG()
@udf(BinaryType())
def pgp_encrypt(data):
    return bytes(str(_gpg.encrypt(data, "{recipient_key}")), "utf-8")
df_{v} = df_{input}.withColumn("_encrypted", pgp_encrypt(col("value")))`,desc:"PGP encryption",notes:"Install python-gnupg",imp:["gnupg"],conf:.9},DecryptContentPGP:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import gnupg
_gpg = gnupg.GPG()
@udf(StringType())
def pgp_decrypt(data):
    return str(_gpg.decrypt(data, passphrase=dbutils.secrets.get(scope="pgp", key="passphrase")))
df_{v} = df_{input}.withColumn("_decrypted", pgp_decrypt(col("value")))`,desc:"PGP decryption",notes:"Install python-gnupg",imp:["gnupg"],conf:.9},AttributeRollingWindow:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import col, avg, window
df_{v} = df_{input}.groupBy(window("timestamp", "{window_duration}")).agg(avg("{metric_col}").alias("rolling_avg"))`,desc:"Rolling window aggregation",notes:"Use Spark window functions",imp:["pyspark.sql.functions"],conf:.92},AttributesToCSV:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import concat_ws, col
df_{v} = df_{input}.withColumn("_csv", concat_ws(",", *[col(c) for c in df_{input}.columns]))`,desc:"Attributes to CSV string",notes:"Use concat_ws",imp:["pyspark.sql.functions"],conf:.93},LookupAttribute:{cat:"Spark DataFrame",tpl:`_lookup_df = spark.table("{lookup_table}")
df_{v} = df_{input}.join(_lookup_df, df_{input}["{key_col}"] == _lookup_df["{lookup_key}"], "left")`,desc:"Attribute lookup via join",notes:"Broadcast join for small lookups",imp:["pyspark.sql.functions"],conf:.92},CaptureChangeMySQL:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("delta")
  .option("readChangeFeed", "true")
  .table("{source_table}"))`,desc:"MySQL CDC via DLT Change Data Feed",notes:"Or use Debezium + Kafka",imp:[],conf:.92},CompareFuzzyHash:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import FloatType
@udf(FloatType())
def fuzzy_sim(a, b):
    if not a or not b: return 0.0
    sa, sb = set(a), set(b)
    return float(len(sa & sb)) / float(len(sa | sb))
df_{v} = df_{input}.withColumn("_similarity", fuzzy_sim(col("hash1"), col("hash2")))`,desc:"Fuzzy hash comparison",notes:"Use ssdeep for production",imp:[],conf:.9},FuzzyHashContent:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import hashlib
@udf(StringType())
def fuzzy_hash(content):
    return hashlib.sha256(content.encode()).hexdigest()[:16]
df_{v} = df_{input}.withColumn("_fuzzy_hash", fuzzy_hash(col("value")))`,desc:"Fuzzy hash generation",notes:"Use ssdeep for true fuzzy hashing",imp:[],conf:.9},GeoEnrichIP:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StructType, StructField, StringType, FloatType
import geoip2.database
_reader = geoip2.database.Reader("/Volumes/<catalog>/<schema>/geo/GeoLite2-City.mmdb")
@udf(StructType([StructField("city",StringType()),StructField("country",StringType())]))
def geo_lookup(ip):
    try:
        r = _reader.city(ip)
        return (r.city.name, r.country.name)
    except: return (None, None)
df_{v} = df_{input}.withColumn("_geo", geo_lookup(col("ip_address")))`,desc:"IP geolocation enrichment",notes:"Download GeoLite2 DB",imp:["geoip2"],conf:.9},ISPEnrichIP:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import geoip2.database
_reader = geoip2.database.Reader("/Volumes/<catalog>/<schema>/geo/GeoLite2-ASN.mmdb")
@udf(StringType())
def isp_lookup(ip):
    try: return _reader.asn(ip).autonomous_system_organization
    except: return None
df_{v} = df_{input}.withColumn("_isp", isp_lookup(col("ip_address")))`,desc:"ISP/ASN enrichment",notes:"Download GeoLite2-ASN DB",imp:["geoip2"],conf:.9},IdentifyMimeType:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import mimetypes
@udf(StringType())
def detect_mime(fname):
    mime, _ = mimetypes.guess_type(fname or "")
    return mime or "application/octet-stream"
df_{v} = df_{input}.withColumn("mime_type", detect_mime(col("filename")))`,desc:"MIME type detection",notes:"Use python-magic for binary",imp:["mimetypes"],conf:.93},ModifyBytes:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import substring, col
df_{v} = df_{input}.withColumn("_modified", substring(col("content"), 1, 100))`,desc:"Byte-level content modification",notes:"Use substring",imp:["pyspark.sql.functions"],conf:.9},SegmentContent:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import explode, split, col
df_{v} = df_{input}.withColumn("_segment", explode(split(col("value"), "{delimiter}")))`,desc:"Segment content by delimiter",notes:"Use split + explode",imp:["pyspark.sql.functions"],conf:.92},DuplicateFlowFile:{cat:"Spark DataFrame",tpl:`from functools import reduce
from pyspark.sql import DataFrame
df_{v} = reduce(DataFrame.union, [df_{input}] * {num_copies})`,desc:"Duplicate records N times",notes:"Use union",imp:[],conf:.93},GetHTMLElement:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
@udf(StringType())
def extract_html(html):
    soup = BeautifulSoup(html, "html.parser")
    el = soup.select_one("{css_selector}")
    return el.get_text() if el else None
df_{v} = df_{input}.withColumn("_extracted", extract_html(col("html")))`,desc:"HTML element extraction",notes:"Install beautifulsoup4",imp:["beautifulsoup4"],conf:.9},ModifyHTMLElement:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
@udf(StringType())
def modify_html(html):
    soup = BeautifulSoup(html, "html.parser")
    el = soup.select_one("{css_selector}")
    if el: el.string = "{new_value}"
    return str(soup)
df_{v} = df_{input}.withColumn("_modified_html", modify_html(col("html")))`,desc:"HTML element modification",notes:"Install beautifulsoup4",imp:["beautifulsoup4"],conf:.9},PutHTMLElement:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
@udf(StringType())
def insert_html(html):
    soup = BeautifulSoup(html, "html.parser")
    tag = soup.new_tag("{tag_name}")
    tag.string = "{content}"
    soup.body.append(tag)
    return str(soup)
df_{v} = df_{input}.withColumn("_html", insert_html(col("html")))`,desc:"HTML element insertion",notes:"Install beautifulsoup4",imp:["beautifulsoup4"],conf:.9},GetSmbFile:{cat:"PySpark UDF",tpl:`import smbclient
smbclient.register_session("{server}", username=dbutils.secrets.get(scope="smb", key="user"), password=dbutils.secrets.get(scope="smb", key="pass"))
_data = smbclient.open_file(r"\\\\{server}\\{share}\\{path}", mode="rb").read()
df_{v} = spark.createDataFrame([{"content": _data.decode("utf-8", errors="replace")}])`,desc:"Read from SMB/Windows share",notes:"Install smbprotocol",imp:["smbprotocol"],conf:.9},PutSmbFile:{cat:"PySpark UDF",tpl:`import smbclient
smbclient.register_session("{server}", username=dbutils.secrets.get(scope="smb", key="user"), password=dbutils.secrets.get(scope="smb", key="pass"))
df_{input}.toPandas().to_csv(r"\\\\{server}\\{share}\\output.csv", index=False)`,desc:"Write to SMB/Windows share",notes:"Install smbprotocol",imp:["smbprotocol"],conf:.9},GetTwitter:{cat:"PySpark UDF",tpl:`import tweepy
_auth = tweepy.OAuth2BearerHandler(dbutils.secrets.get(scope="twitter", key="bearer_token"))
_api = tweepy.API(_auth)
_tweets = [{"text": t.text, "created_at": str(t.created_at)} for t in _api.search_tweets(q="{query}", count=100)]
df_{v} = spark.createDataFrame(_tweets)`,desc:"Twitter/X data ingestion",notes:"Use tweepy",imp:["tweepy"],conf:.9},PostSlack:{cat:"PySpark UDF",tpl:`import requests
requests.post("{webhook_url}", json={"text": "Pipeline notification"})
df_{v} = df_{input}`,desc:"Slack via webhook",notes:"Use Slack webhook URL",imp:["requests"],conf:.92},GetRethinkDB:{cat:"PySpark UDF",tpl:`from rethinkdb import r
_conn = r.connect(host="{host}", port=28015, db="{database}")
_docs = list(r.table("{table}").limit(50000).run(_conn))
df_{v} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
_conn.close()`,desc:"RethinkDB reader",notes:"Install rethinkdb",imp:["rethinkdb"],conf:.9},PutRethinkDB:{cat:"PySpark UDF",tpl:`from rethinkdb import r
_conn = r.connect(host="{host}", port=28015, db="{database}")
r.table("{table}").insert(df_{input}.limit(10000).toPandas().to_dict(orient="records")).run(_conn)
_conn.close()`,desc:"RethinkDB writer",notes:"Install rethinkdb",imp:["rethinkdb"],conf:.9},DeleteRethinkDB:{cat:"PySpark UDF",tpl:`from rethinkdb import r
_conn = r.connect(host="{host}", port=28015, db="{database}")
r.table("{table}").delete().run(_conn)
_conn.close()`,desc:"RethinkDB delete",notes:"Install rethinkdb",imp:["rethinkdb"],conf:.9},FetchGridFS:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
import gridfs
_client = MongoClient("{mongo_uri}")
_fs = gridfs.GridFS(_client["{database}"])
_files = [{"filename": f.filename, "length": f.length} for f in _fs.find().limit(1000)]
df_{v} = spark.createDataFrame(_files) if _files else spark.createDataFrame([], "filename STRING, length LONG")
_client.close()`,desc:"GridFS file listing",notes:"Use pymongo gridfs",imp:["pymongo"],conf:.9},PutGridFS:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
import gridfs
_client = MongoClient("{mongo_uri}")
_fs = gridfs.GridFS(_client["{database}"])
for row in df_{input}.limit(1000).collect():
    _fs.put(str(row.asDict()).encode(), filename=f"record")
_client.close()`,desc:"GridFS file upload",notes:"Use pymongo gridfs",imp:["pymongo"],conf:.9},DeleteGridFS:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
import gridfs
_client = MongoClient("{mongo_uri}")
_fs = gridfs.GridFS(_client["{database}"])
for f in _fs.find({"filename": "{pattern}"}):
    _fs.delete(f._id)
_client.close()`,desc:"GridFS file deletion",notes:"Use pymongo gridfs",imp:["pymongo"],conf:.9},FetchElasticsearch:{cat:"PySpark UDF",tpl:`from elasticsearch import Elasticsearch
_es = Elasticsearch("{es_url}", basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")))
_doc = _es.get(index="{index}", id="{doc_id}")
df_{v} = spark.createDataFrame([_doc["_source"]])`,desc:"Fetch single ES document",notes:"Use elasticsearch-py",imp:["elasticsearch"],conf:.9},DeleteByQueryElasticsearch:{cat:"PySpark UDF",tpl:`from elasticsearch import Elasticsearch
_es = Elasticsearch("{es_url}", basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")))
_es.delete_by_query(index="{index}", body={"query": {"match_all": {}}})
df_{v} = df_{input}`,desc:"ES delete by query",notes:"Use elasticsearch-py",imp:["elasticsearch"],conf:.9},QueryElasticsearchHttp:{cat:"PySpark UDF",tpl:`from elasticsearch import Elasticsearch
_es = Elasticsearch("{es_url}", basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")))
_result = _es.search(index="{index}", size=10000)
_hits = [h["_source"] for h in _result["hits"]["hits"]]
df_{v} = spark.createDataFrame(_hits) if _hits else spark.createDataFrame([], "id STRING")`,desc:"ES HTTP query",notes:"Use elasticsearch-py",imp:["elasticsearch"],conf:.9},DeleteHBaseCells:{cat:"PySpark UDF",tpl:`import happybase
_conn = happybase.Connection("{hbase_host}")
_table = _conn.table("{table}")
for row in df_{input}.limit(1000).collect():
    _table.delete(row["row_key"].encode())
_conn.close()`,desc:"HBase cell deletion",notes:"Use happybase",imp:["happybase"],conf:.9},DeleteHBaseRow:{cat:"PySpark UDF",tpl:`import happybase
_conn = happybase.Connection("{hbase_host}")
_table = _conn.table("{table}")
for row in df_{input}.limit(1000).collect():
    _table.delete(row["row_key"].encode())
_conn.close()`,desc:"HBase row deletion",notes:"Use happybase",imp:["happybase"],conf:.9},GetHDFSEvents:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", "/Volumes/{catalog}/{schema}/checkpoints/hdfs_events")
  .load("{hdfs_path}"))`,desc:"HDFS events via Auto Loader",notes:"Use cloudFiles",imp:[],conf:.92},GetHDFSFileInfo:{cat:"Spark DataFrame",tpl:`_files = dbutils.fs.ls("{hdfs_path}")
df_{v} = spark.createDataFrame([{"path": f.path, "name": f.name, "size": f.size} for f in _files])`,desc:"HDFS file info listing",notes:"Use dbutils.fs.ls",imp:[],conf:.93},GetHDFSSequenceFile:{cat:"Spark DataFrame",tpl:'df_{v} = spark.sparkContext.sequenceFile("{hdfs_path}", "org.apache.hadoop.io.Text", "org.apache.hadoop.io.Text").toDF(["key", "value"])',desc:"Hadoop SequenceFile reader",notes:"Use sparkContext.sequenceFile",imp:[],conf:.9},DeleteDynamoDB:{cat:"PySpark UDF",tpl:`import boto3
_dynamodb = boto3.resource("dynamodb", region_name="{region}")
_table = _dynamodb.Table("{table}")
with _table.batch_writer() as _batch:
    for row in df_{input}.limit(10000).collect():
        _batch.delete_item(Key={"id": row["id"]})`,desc:"DynamoDB batch delete",notes:"Configure AWS creds",imp:["boto3"],conf:.9},DeleteSQS:{cat:"PySpark UDF",tpl:`import boto3
_sqs = boto3.client("sqs", region_name="{region}")
_msgs = _sqs.receive_message(QueueUrl="{queue_url}", MaxNumberOfMessages=10)
for msg in _msgs.get("Messages", []):
    _sqs.delete_message(QueueUrl="{queue_url}", ReceiptHandle=msg["ReceiptHandle"])`,desc:"SQS message deletion",notes:"Configure AWS creds",imp:["boto3"],conf:.9},InvokeAWSGatewayApi:{cat:"PySpark UDF",tpl:`import requests
_response = requests.post("{api_gateway_url}", json=df_{input}.limit(100).toPandas().to_dict(orient="records"))
df_{v} = spark.createDataFrame([_response.json()] if isinstance(_response.json(), dict) else _response.json())`,desc:"AWS API Gateway invocation",notes:"Configure AWS creds",imp:["boto3","requests"],conf:.9},PutSplunkHTTP:{cat:"PySpark UDF",tpl:`import requests
_token = dbutils.secrets.get(scope="splunk", key="hec_token")
for row in df_{input}.limit(10000).collect():
    requests.post("{splunk_hec_url}", json={"event": row.asDict()}, headers={"Authorization": f"Splunk {_token}"}, verify=False)`,desc:"Splunk HEC sender",notes:"Configure HEC token",imp:["requests"],conf:.9},PutRiemann:{cat:"PySpark UDF",tpl:`import socket
_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
_sock.connect(("{riemann_host}", 5555))
print("[RIEMANN] Sent monitoring events")
df_{v} = df_{input}`,desc:"Riemann monitoring events",notes:"Use riemann-client",imp:[],conf:.9},ListenTCPRecord:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("socket")
  .option("host", "{host}")
  .option("port", "{port}")
  .load())`,desc:"TCP record stream listener",notes:"Use socket source",imp:[],conf:.9},ListenUDPRecord:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{udp_topic}")
  .load())`,desc:"UDP record stream via Kafka",notes:"Pipe UDP through Kafka",imp:[],conf:.9},UpdateCounter:{cat:"Spark DataFrame",tpl:`_counter = spark.sparkContext.accumulator(0)
def _count(row): _counter.add(1)
df_{input}.foreach(_count)
print(f"[COUNTER] Count: {_counter.value}")
df_{v} = df_{input}`,desc:"Counter via accumulator",notes:"Use accumulators or Delta",imp:[],conf:.92},UpdateHiveTable:{cat:"Delta Lake",tpl:`spark.sql("ALTER TABLE {table_name} SET TBLPROPERTIES ('updated'='true')")
df_{v} = df_{input}`,desc:"Hive table DDL update",notes:"Use Unity Catalog",imp:[],conf:.92},ValidateCsv:{cat:"Spark DataFrame",tpl:`df_{v} = spark.read.option("header", "true").option("mode", "PERMISSIVE").csv("{path}")
_corrupt = df_{v}.filter(col("_corrupt_record").isNotNull())`,desc:"CSV validation",notes:"Use permissive mode",imp:["pyspark.sql.functions"],conf:.93},GetMongoRecord:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
_client = MongoClient("{mongo_uri}")
_docs = list(_client["{database}"]["{collection}"].find({}, {"_id": 0}).limit(50000))
df_{v} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
_client.close()`,desc:"MongoDB record reader",notes:"Use pymongo",imp:["pymongo"],conf:.9},RunMongoAggregation:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
_client = MongoClient("{mongo_uri}")
_results = list(_client["{database}"]["{collection}"].aggregate([{pipeline}]))
df_{v} = spark.createDataFrame(_results) if _results else spark.createDataFrame([], "id STRING")
_client.close()`,desc:"MongoDB aggregation pipeline",notes:"Translate to PySpark",imp:["pymongo"],conf:.9},ExecuteInfluxDBQuery:{cat:"PySpark UDF",tpl:`from influxdb_client import InfluxDBClient
_client = InfluxDBClient(url="{influx_url}", token=dbutils.secrets.get(scope="influx", key="token"), org="{org}")
_tables = _client.query_api().query("{flux_query}")
_records = [r.values for table in _tables for r in table.records]
df_{v} = spark.createDataFrame(_records) if _records else spark.createDataFrame([], "time STRING, value DOUBLE")`,desc:"InfluxDB Flux query",notes:"Use influxdb-client",imp:["influxdb-client"],conf:.9},QueryDNS:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import socket
@udf(StringType())
def dns_lookup(hostname):
    try: return socket.gethostbyname(hostname)
    except: return None
df_{v} = df_{input}.withColumn("_ip", dns_lookup(col("hostname")))`,desc:"DNS lookup via UDF",notes:"Use socket.gethostbyname",imp:[],conf:.93},GetJMSQueue:{cat:"PySpark UDF",tpl:`import stomp
_msgs = []
class _L(stomp.ConnectionListener):
    def on_message(self, frame): _msgs.append({"body": frame.body})
_conn = stomp.Connection([("{jms_host}", {jms_port})])
_conn.set_listener("", _L())
_conn.connect(dbutils.secrets.get(scope="jms", key="user"), dbutils.secrets.get(scope="jms", key="pass"), wait=True)
_conn.subscribe(destination="/queue/{queue}", id=1, ack="auto")
import time; time.sleep(5)
_conn.disconnect()
df_{v} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")`,desc:"JMS Queue consumer",notes:"Use stomp.py",imp:["stomp.py"],conf:.9},SpringContextProcessor:{cat:"PySpark UDF",tpl:`# Spring Context Processor â€” migrate Spring bean logic to Python
df_{v} = df_{input}`,desc:"Spring context migration",notes:"Manual migration required â€” no automated translation",imp:[],conf:.2},YandexTranslate:{cat:"PySpark UDF",tpl:`import requests
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
@udf(StringType())
def translate(text):
    r = requests.post("https://translate.api.cloud.yandex.net/translate/v2/translate", json={"texts": [text], "targetLanguageCode": "{lang}"})
    return r.json()["translations"][0]["text"]
df_{v} = df_{input}.withColumn("_translated", translate(col("value")))`,desc:"Yandex translation",notes:"Configure API key",imp:["requests"],conf:.9}},Sc={source:{tpl:`# Source processor: {type}
# Auto Loader ingestion from landing zone
df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", "/Volumes/{catalog}/{schema}/_schema/{v}")
  .option("cloudFiles.inferColumnTypes", "true")
  .load("/Volumes/{catalog}/{schema}/landing/{v}"))
print(f"[SOURCE] {v}: streaming from landing zone")`,desc:"Source - Auto Loader ingestion",conf:.7},sink:{tpl:`# Sink processor: {type}
# Delta Lake write with merge semantics
(df_{in}.write
  .format("delta")
  .mode("append")
  .option("mergeSchema", "true")
  .saveAsTable("{catalog}.{schema}.{v}"))
print(f"[SINK] {v}: written to Delta table")`,desc:"Sink - Delta Lake write",conf:.7},transform:{tpl:`# Transform processor: {type}
# DataFrame transformation pipeline
from pyspark.sql.functions import col, lit, current_timestamp
df_{v} = (df_{in}
  .withColumn("_processed_at", current_timestamp())
  .withColumn("_source_processor", lit("{type}")))
print(f"[TRANSFORM] {v}: applied transformation")`,desc:"Transform - DataFrame transformation",conf:.7},route:{tpl:`# Route processor: {type}
# Conditional DataFrame routing via filter
from pyspark.sql.functions import col, lit

# Parse routing conditions from processor properties
_route_conditions = {routeConditions}
if _route_conditions:
    for _route_name, _route_expr in _route_conditions.items():
        try:
            globals()[f"df_{v}_{_route_name}"] = df_{in}.filter(_route_expr)
        except Exception as _route_err:
            raise NotImplementedError(
                f"Route condition for {type}/{_route_name} requires manual translation: "
                f"{_route_expr} (error: {_route_err})"
            )
    # Default: rows not matching any named route
    _matched_union = None
    for _rn in _route_conditions:
        _rdf = globals().get(f"df_{v}_{_rn}")
        if _rdf is not None:
            _matched_union = _rdf if _matched_union is None else _matched_union.union(_rdf)
    df_{v}_unmatched = df_{in}.subtract(_matched_union) if _matched_union is not None else df_{in}
    df_{v} = df_{in}  # Pass all through; downstream uses named route DataFrames
else:
    raise NotImplementedError(
        f"Route processor {type} has no translatable routing conditions. "
        f"Review NiFi processor properties and add manual PySpark filter expressions."
    )
print(f"[ROUTE] {v}: routing applied with {{len(_route_conditions)}} condition(s)")`,desc:"Route - DataFrame filter routing with parsed conditions",conf:.65},process:{tpl:`# Process processor: {type}
# Custom processing step
from pyspark.sql.functions import col, expr, current_timestamp
df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("SELECT *, current_timestamp() AS _processed_at FROM tmp_{v}")
print(f"[PROCESS] {v}: processed {{df_{v}.count()}} rows")`,desc:"Process - Custom processing via Spark SQL",conf:.7},utility:{tpl:`# Utility processor: {type}
# Logging and inspection
print(f"[UTILITY] {v}: {{df_{in}.count()}} rows")
df_{in}.printSchema()
display(df_{in}.limit(5))
df_{v} = df_{in}`,desc:"Utility - logging and inspection",conf:.8}};function wc(e,n,t,r,i,o){const a=o||3;if(!/^(InvokeHTTP|PutElasticsearch|PutMongo|PutS3|PutDynamoDB|ExecuteSQL|PutSQL|PutDatabaseRecord|ConsumeKafka|PublishKafka|Fetch|Get(HTTP|SQS|DynamoDB))/.test(n))return t;const c=ve(e),l=n.toLowerCase().replace(/[^a-z0-9]/g,"_")+"_"+c,d=t.split(`
`).map(u=>"    "+u).join(`
`);return`# ${e} â€” with retry logic
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import logging
_logger = logging.getLogger("nifi_migration")

@retry(
    stop=stop_after_attempt(${a}),
    wait=wait_exponential(multiplier=1, min=2, max=30),
    retry=retry_if_exception_type((ConnectionError, TimeoutError, IOError)),
    before_sleep=lambda rs: _logger.warning(f"Retry {rs.attempt_number}/${a} for ${e.replace(/"/g,'\\"')}")
)
def _exec_${l}():
${d}
    return df_${c}

try:
    df_${c} = _exec_${l}()
except Exception as e:
    _logger.error(f"[FAILED] ${e}: {e}")
    raise`}function Ec(e,n,t,r,i){if(!e.includes("subprocess.run")&&!e.includes("_sp.run"))return e;const o=(i||{})["Input Source"]||"";if(o.toLowerCase().includes("flowfile")||o.toLowerCase().includes("content")){const a=e.match(/(?:subprocess\.run|_sp\.run)\(\s*(?:"([^"]+)"|'([^']+)'|\[([^\]]+)\])/),s=a?a[1]||a[2]||a[3]:"echo";return`# ${n} â€” distributed via Pandas UDF (per-row execution)
from pyspark.sql.functions import pandas_udf, col
import pandas as pd
import subprocess

@pandas_udf("string")
def _exec_cmd_${t}(rows: pd.Series) -> pd.Series:
    """Execute command per row â€” runs on workers, not driver"""
    results = []
    for row_val in rows:
        try:
            _r = subprocess.run(${JSON.stringify(s)}.split(), input=str(row_val), capture_output=True, text=True, timeout=30)
            results.append(_r.stdout.strip() if _r.returncode == 0 else f"ERROR: {_r.stderr[:100]}")
        except Exception as e:
            results.append(f"ERROR: {str(e)[:100]}")
    return pd.Series(results)

df_${t} = df_${r}.withColumn("_cmd_result", _exec_cmd_${t}(col("value")))
print(f"[CMD] Distributed execution via Pandas UDF")`}return e}const xc=[{pattern:/\b(ssn|social.?security|ss_num)\b/i,field:"SSN"},{pattern:/\b(mrn|medical.?record|patient.?id|patient_number)\b/i,field:"MRN"},{pattern:/\b(dob|date.?of.?birth|birth.?date|birthdate)\b/i,field:"DOB"},{pattern:/\b(patient|patient.?name|first.?name|last.?name|full.?name)\b/i,field:"Patient Name"},{pattern:/\b(diagnosis|icd.?\d{1,2}|cpt.?code|procedure.?code|drg)\b/i,field:"Diagnosis/Code"},{pattern:/\b(phone|fax|email|address|zip.?code|street)\b/i,field:"Contact Info"},{pattern:/\b(insurance|policy.?number|member.?id|subscriber)\b/i,field:"Insurance"},{pattern:/\b(prescription|medication|rx_|drug.?name|ndc)\b/i,field:"Medication"},{pattern:/\b(lab.?result|test.?result|vital|blood.?pressure|heart.?rate)\b/i,field:"Clinical Data"},{pattern:/\b(account.?number|billing|charge|payment)\b/i,field:"Financial/Billing"}];function $o(e){const n=[],t=JSON.stringify(e||{}).toLowerCase();xc.forEach(i=>{i.pattern.test(t)&&(Object.keys(e||{}).forEach(o=>{i.pattern.test(o)&&n.push({key:o,category:i.field})}),Object.values(e||{}).forEach(o=>{typeof o=="string"&&i.pattern.test(o)&&n.push({key:o.substring(0,50),category:i.field})}))});const r=new Set;return n.filter(i=>r.has(i.key)?!1:(r.add(i.key),!0))}function Cc(e,n){const{NIFI_DATABRICKS_MAP:t,ROLE_FALLBACK_TEMPLATES:r,parseVariableRegistry:i,resolveVariables:o,translateNELtoPySpark:a,generateRetryWrapper:s,wrapSubprocessAsPandasUDF:c,detectPHIFields:l}=n,d=e.controllerServices||[],u=i(e),g=Object.keys(u).length>0,p=e.processors||[],m=e.connections||[],f={};m.forEach(k=>{f[k.destinationName]||(f[k.destinationName]={inputs:[],outputs:[]}),f[k.destinationName].inputs.push(k.sourceName),f[k.sourceName]||(f[k.sourceName]={inputs:[],outputs:[]}),f[k.sourceName].outputs.push(k.destinationName)});const h=oc(p,m),_={};function v(k){if(_[k])return _[k];const E=ac(k,d);return E&&(_[k]=E),E}return p.map(k=>{const E=Pe(k.type),D=t[k.type],P=ve(k.name),$=f[k.name]&&f[k.name].inputs||[],x=$.length?ve($[0]):"input";if(D){const A={...k.properties||{}};if(g)for(const[N,H]of Object.entries(A))typeof H=="string"&&(A[N]=o(H,u));let{code:w,conf:R}=ec(D,P,x,$,A,a,(N,H)=>ic(N,H,a),k.name);w=w.replace(/\{(\w+)\}/g,"<$1>");const y=[k,A,P,x,w,R],O=cc(...y)||pc(...y)||dc(k,A,P,x,w,R,a)||lc(k,A,P,x,w,R,a)||uc(...y)||fc(...y)||mc(...y)||hc(...y)||gc(...y)||_c(...y)||yc(...y)||bc(...y)||vc(...y)||kc(...y);if(O&&(w=O.code,R=O.conf),/^(ExecuteSQL|QueryDatabase|GenerateTableFetch|PutDatabaseRecord|PutSQL|SelectHiveQL)/.test(k.type)){const N=A["Database Connection Pooling Service"]||A["JDBC Connection Pool"]||"",H=v(N);if(H&&H.jdbcUrl){const Q=H.jdbcUrl,V=H.driver||"",ee=H.user||"",se=A["SQL select query"]||A["SQL Statement"]||"",te=A["Table Name"]||"";if(/ExecuteSQL|QueryDatabase|GenerateTableFetch|SelectHiveQL/.test(k.type)){const ae=se?`"(${se.replace(/"/g,'\\"').substring(0,300)}) AS subq"`:`"${te}"`;w=`# SQL: ${k.name} [CS: ${H.name}]
# JDBC: ${Q}
# Driver: ${V}
df_${P} = (spark.read
  .format("jdbc")
  .option("url", "${Q}")
  .option("dbtable", ${ae})
  .option("driver", "${V}")
  .option("user", dbutils.secrets.get(scope="db", key="${ee||"user"}"))
  .option("password", dbutils.secrets.get(scope="db", key="pass"))
  .load()
)
print(f"[SQL] Read from ${te||"query"} via ${H.name}")`,R=.95}else{const ae=A["Schema Name"]?`${A["Schema Name"]}.${te}`:te;w=`# DB Write: ${k.name} [CS: ${H.name}]
# JDBC: ${Q}
(df_${x}.write
  .format("jdbc")
  .option("url", "${Q}")
  .option("dbtable", "${ae}")
  .option("driver", "${V}")
  .option("user", dbutils.secrets.get(scope="db", key="${ee||"user"}"))
  .option("password", dbutils.secrets.get(scope="db", key="pass"))
  .option("batchsize", 1000)
  .mode("append")
  .save()
)
print(f"[DB] Wrote to ${ae} via ${H.name}")`,R=.95}}}if(/Kafka/.test(k.type)){const N=A["Kafka Client Service"]||"",H=v(N);if(H&&H.props){const Q=H.props["bootstrap.servers"]||H.props["Kafka Brokers"]||"";Q&&!w.includes(Q)&&(w=w.replace(/kafka[_.]?[a-z]*:9092/gi,Q))}}if(/ConvertRecord|ValidateRecord|LookupRecord|PutRecord|QueryRecord|PartitionRecord|SplitRecord|ForkRecord|SampleRecord|UpdateRecord/.test(k.type)){const N=A["Record Reader"]||"",H=A["Record Writer"]||"",Q=v(N),V=v(H),ee=Q?Q.format||"json":/CSV/i.test(N)?"csv":/Avro/i.test(N)?"avro":"json",se=V?V.format||"json":/CSV/i.test(H)?"csv":/Avro/i.test(H)?"avro":"json";(!w||w.includes("{"))&&(w=`# ${k.type}: ${k.name}
# Reader: ${N} (${ee}) | Writer: ${H} (${se})
df_${P} = df_${x}
# Input format: ${ee}, Output format: ${se}
# Write: df_${P}.write.format("${se}").save("/path/output")
print(f"[RECORD] ${k.type} â€” ${ee} -> ${se}")`,R=.93)}const J=h.has(k.name);w=sc(w,k.name,P,x,J),J&&!w.includes("readStream")&&!w.includes("writeStream")&&!w.includes("foreachBatch")&&(w=`# âš  STREAMING CONTEXT: upstream is a streaming source
# Ensure all operations are streaming-compatible
`+w);const M=A["Penalty Duration"]||"30 sec",z=A["Yield Duration"]||"1 sec",U=A["Max Retries"]||A["Retry Count"]||"3",X=parseInt(U,10);w=s(k.name,k.type,w,M,z,isNaN(X)?3:X),w=c(w,k.name,P,x,A);const F=l(A);return F.length>0&&(w=`# âš  PHI/HIPAA WARNING: Protected health information detected
# Fields: `+F.map(N=>N.key+" ("+N.category+")").join(", ")+`
# All PHI fields are hashed (SHA-256) in DLQ writes
`+w),{name:k.name,type:k.type,group:k.group,role:E,mapped:!0,confidence:R,category:D.cat,code:w,desc:D.desc,notes:D.notes,imports:D.imp||[],state:k.state}}const S=r[E]||r.process,C=`${S.tpl}
# Original: ${k.name} (${k.type}) in ${k.group||"root"}`;return{name:k.name,type:k.type,group:k.group,role:E,mapped:!1,confidence:S.conf,category:"Manual Migration",code:C,desc:S.desc,notes:"Role-based template. Manual implementation required.",imports:[],state:k.state,gapReason:`No direct mapping - ${E}-based template provided`,fallbackUsed:!0}})}const $c={NIFI_DATABRICKS_MAP:ht,ROLE_FALLBACK_TEMPLATES:Sc,parseVariableRegistry:Ki,resolveVariables:Qi,translateNELtoPySpark:Ji,generateRetryWrapper:wc,wrapSubprocessAsPandasUDF:Ec,detectPHIFields:$o};function Mr(e){return Cc(e,$c)}function Pc(e,n){const t=n.connections||[],r={},i={};(n.processors||[]).forEach(s=>{i[s.id]=s});const o={},a={};return t.forEach(s=>{o[s.sourceName]||(o[s.sourceName]=[]),o[s.sourceName].push({dest:s.destinationName,rel:s.relationships&&s.relationships[0]||"success"}),a[s.destinationName]||(a[s.destinationName]=[]),a[s.destinationName].push({src:s.sourceName,rel:s.relationships&&s.relationships[0]||"success"})}),e.forEach(s=>{const c=ve(s.name),l=(a[s.name]||[]).map(u=>({varName:"df_"+ve(u.src),procName:u.src,relationship:u.rel})),d=(o[s.name]||[]).map(u=>({varName:"df_"+ve(u.dest),procName:u.dest,relationship:u.rel}));r[s.name]={outputVar:"df_"+c,inputVars:l,outputTargets:d,role:s.role,type:s.type}}),r}const Rc=new Set(["pyspark.sql.functions","pyspark.sql.types","pyspark.sql.streaming","pyspark.sql.window","pyspark.sql","pyspark","datetime","json","logging","os","re","hashlib","base64","subprocess","mimetypes","socket","smtplib","email","functools","xml.etree","xml","csv","io","collections","math","sys","time","typing","uuid","pathlib","struct","itertools","urllib"]);function Dc(e,n){const t={pyspark:new Set(["from pyspark.sql.functions import *","from pyspark.sql.types import *"]),python:new Set(["from datetime import datetime, timedelta","import json","import logging"]),databricks:new Set,thirdParty:new Set},r=new Set;e.forEach(l=>{(l.imports||[]).forEach(d=>{d.includes("pyspark")?t.pyspark.add(d):d.includes("dbutils")||d.includes("databricks")?t.databricks.add(d):t.python.add(d);const u=d.replace(/^(?:from|import)\s+/,"").split(/[\s.]/)[0];u&&!Rc.has(u)&&!u.startsWith("pyspark")&&!u.startsWith("dbutils")&&!u.startsWith("databricks")&&r.add(u)}),l.code&&(l.code.includes("requests.")&&(t.thirdParty.add("import requests"),r.add("requests")),l.code.includes("subprocess")&&t.python.add("import subprocess"),l.code.includes("re.")&&t.python.add("import re"),l.code.includes("os.")&&t.python.add("import os"),l.code.includes("hashlib")&&t.python.add("import hashlib"),l.code.includes("base64")&&t.python.add("import base64"),l.code.includes("urllib.parse")&&t.python.add("import urllib.parse"),l.code.includes("xml.etree")&&t.python.add("import xml.etree.ElementTree as ET"),(l.code.includes("readStream")||l.code.includes("writeStream"))&&t.pyspark.add("from pyspark.sql.streaming import StreamingQuery"),l.code.includes("Window")&&t.pyspark.add("from pyspark.sql.window import Window"),l.code.includes("paramiko")&&r.add("paramiko"),l.code.includes("geoip2")&&r.add("geoip2"),l.code.includes("lxml")&&r.add("lxml"),l.code.includes("gnupg")&&r.add("python-gnupg"),l.code.includes("tenacity")&&r.add("tenacity"),l.code.includes("tweepy")&&r.add("tweepy"),(l.code.includes("BeautifulSoup")||l.code.includes("bs4"))&&r.add("beautifulsoup4"),l.code.includes("cryptography")&&r.add("cryptography"))});let i=`# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPORTS â€” Auto-collected from all processors
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

`;i+=`# PySpark
`+[...t.pyspark].filter(l=>!l.startsWith("#")).sort().join(`
`);const o=[...t.python].filter(l=>!l.startsWith("#")).sort();o.length&&(i+=`

# Python Standard Library
`+o.join(`
`));const a=[...t.databricks].filter(l=>!l.startsWith("#")).sort();a.length&&(i+=`

# Databricks
`+a.join(`
`));const s=[...t.thirdParty].filter(l=>!l.startsWith("#")).sort();s.length&&(i+=`

# Third-party
`+s.join(`
`));let c=null;return r.size>0&&(c={type:"code",role:"setup",label:"Install Dependencies",source:`# Install required packages
`+[...r].sort().map(d=>`%pip install ${d}`).join(`
`)+`
dbutils.library.restartPython()`}),{code:i,all:t,pipCell:c}}function Tc(e,n){const t=n.connections||[],r={};e.forEach((d,u)=>{r[d.name]=u});const i={},o={};e.forEach(d=>{i[d.name]=[],o[d.name]=0}),t.forEach(d=>{r[d.sourceName]!==void 0&&r[d.destinationName]!==void 0&&(i[d.sourceName].push(d.destinationName),o[d.destinationName]=(o[d.destinationName]||0)+1)});const a=[];e.forEach(d=>{(o[d.name]||0)===0&&a.push(d.name)});const s=[],c=new Set,l={source:0,route:1,transform:2,process:3,sink:4,utility:5};for(;a.length;){a.sort((u,g)=>{const p=e[r[u]],m=e[r[g]];return(l[p.role]||3)-(l[m.role]||3)});const d=a.shift();c.has(d)||(c.add(d),s.push(e[r[d]]),(i[d]||[]).forEach(u=>{o[u]--,o[u]<=0&&!c.has(u)&&a.push(u)}))}return e.forEach(d=>{c.has(d.name)||s.push(d)}),s}function Ac(e,n){const t=(n.processors||[]).find(s=>s.name===e.name);if(!t)return{used:{},unused:{},all:{}};const r=t.properties||{},i=new Set,o=["Input Directory","File Filter","Output Directory","Directory","Database Connection Pooling Service","SQL select query","Table Name","Record Reader","Record Writer","Kafka Brokers","Topic Name","Group ID","Routing Strategy","JDBC Connection URL","Conflict Resolution Strategy","Log Level","Log Message","Command","Command Arguments"];Object.entries(r).forEach(([s])=>{o.some(c=>s.includes(c))&&i.add(s)});const a={};return Object.entries(r).forEach(([s,c])=>{i.has(s)||(a[s]=c)}),{used:Object.fromEntries([...i].map(s=>[s,r[s]])),unused:a,all:r}}function Ic({flowName:e,totalProcessors:n,mappedCount:t,coveragePct:r,qualifiedSchema:i}){return{type:"md",label:"Header",source:`# NiFi Migration: ${e}
Generated by SEG Demo | ${new Date().toISOString().split("T")[0]}

**Processors:** ${n} | **Mapped:** ${t} | **Coverage:** ${r}%

**Target:** ${i}

**Enhancements:** DataFrame Lineage | Smart Imports | Topo-Sort | Adaptive Code | Error Framework | Auto Recovery | Full Properties | Relationship Routing | Exec Report | E2E Validation`,role:"config"}}function Fc({smartImportsCode:e,catalogName:n,schemaName:t,secretScope:r}){let i=e+`

# Databricks notebook configuration
spark.conf.set("spark.sql.adaptive.enabled", "true")`;n&&(i+=`
spark.sql("USE CATALOG \`${n}\`")`),i+=`
spark.sql("USE SCHEMA \`${t}\`")`,r&&(i+=`

SECRET_SCOPE = "${r}"`);const o=n?`/Volumes/${n}/${t}/checkpoints`:xe.workspacePath+"/checkpoints";return i+=`

# Config-driven paths (no hardcoded /tmp/ or /mnt/)`,i+=`
CHECKPOINT_BASE = "${o}"`,i+=`
VOLUMES_BASE = "${n?`/Volumes/${n}/${t}`:`/Volumes/main/${t}`}"`,i+=`
print(f"Notebook initialized â€” Spark version: {spark.version}")`,{type:"code",label:"Imports & Config",source:i,role:"config"}}function Lc({catalogName:e,schemaName:n,qualifiedSchema:t,tables:r}){if(!r||r.length===0)return null;let i="";return e&&(i+=`CREATE CATALOG IF NOT EXISTS \`${e}\`;
USE CATALOG \`${e}\`;
`),i+=`CREATE SCHEMA IF NOT EXISTS \`${n}\`;
USE SCHEMA \`${n}\`;
`,r.forEach(o=>{i+=`
CREATE TABLE IF NOT EXISTS ${t}.\`${o.name}\` (
`,i+=o.columns.map(a=>`  \`${a.name}\` ${(a.data_type||a.type||"STRING").toUpperCase()}`).join(`,
`),i+=`
) USING DELTA;`}),{type:"sql",label:"Unity Catalog Setup",source:i,role:"config"}}function Oc(e,n,t){const r=ve(e.name);return e.role==="source"&&e.mapped&&e.code?"# Adaptive format detection for "+e.name+`
def _detect_format_`+r+`(path):
    import os
    ext = os.path.splitext(path)[1].lower() if path else ""
    fmt_map = {".csv":"csv",".tsv":"csv",".json":"json",".jsonl":"json",
               ".parquet":"parquet",".avro":"avro",".orc":"orc",".xml":"xml"}
    if ext in fmt_map: return fmt_map[ext]
    try:
        head = dbutils.fs.head(path, 4)
        if head.startswith("PAR1"): return "parquet"
        if head.startswith("{"): return "json"
    except: pass
    return "csv"

`+e.code:e.code}function Nc(e,n,t,r){if(!e.mapped||!e.code||e.code.startsWith("# TODO"))return e.code;const i=ve(e.name),o=r[e.name]||{},a=o.outputVar||"df_"+i,s=o.inputVars&&o.inputVars.length?o.inputVars[0].varName:"df_input",c=e.code.split(`
`).map(h=>"    "+h).join(`
`),l=e.name.replace(/"/g,'\\"').replace(/'/g,"''"),d=e.type.replace(/'/g,"''"),u=e.role.replace(/'/g,"''"),g=(e.desc||"").replace(/'/g,"''"),p=(e.notes||"").replace(/'/g,"''"),m=(o.inputVars||[]).map(h=>h.procName.replace(/'/g,"''")).join(",");Math.round(e.confidence*100);let f="";return e.role==="source"?f=Mc(i,a,l):e.role==="transform"||e.role==="process"?f=Bc(i,a,s,l):e.role==="sink"&&(f=jc(i,s,l,n)),`# [${u.toUpperCase()}] ${e.name}
# ${g}${p?"  |  "+p:""}
import time as _t_${i}
_cell_start_${i} = _t_${i}.time()
_cell_status_${i} = "SUCCESS"
_cell_error_${i} = ""
_cell_rows_${i} = 0
try:
${c}
    try: _cell_rows_${i} = ${a}.count()
    except Exception: pass
    _cell_dur_ms_${i} = int((_t_${i}.time() - _cell_start_${i}) * 1000)
    print(f"[OK] ${l} ({_cell_rows_${i}} rows, {_cell_dur_ms_${i}}ms)")
    spark.sql(f"""INSERT INTO ${n}.__execution_log VALUES (
        _RUN_ID, '${l}', '${d}', '${u}',
        current_timestamp(), '{_cell_status_${i}}',
        '{_cell_error_${i}}', {_cell_rows_${i}},
        {_cell_dur_ms_${i}}, ${(e.confidence||0).toFixed(2)},
        '${m}'
    )""")
except Exception as _e:
    _cell_dur_ms_${i} = int((_t_${i}.time() - _cell_start_${i}) * 1000)
    _cell_error_${i} = str(_e).replace("'", "''").replace("\\\\", "\\\\\\\\")[:500]
    _cell_status_${i} = "FAILED"
    _recovered_${i} = False
${f}
    # Log the outcome (success, recovery, or failure)
    try:
        spark.sql(f"""INSERT INTO ${n}.__execution_log VALUES (
            _RUN_ID, '${l}', '${d}', '${u}',
            current_timestamp(), '{_cell_status_${i}}',
            '{_cell_error_${i}}', {_cell_rows_${i}},
            {_cell_dur_ms_${i}}, ${(e.confidence||0).toFixed(2)},
            '${m}'
        )""")
    except Exception:
        pass  # Do not let logging failure mask original error
    if not _recovered_${i}:
        print(f"[ERROR] ${l}: {_e}")
        raise _e`}function Mc(e,n,t,r){return`
    # RECOVERY: Source-specific error handling
    _err_str_${e} = str(_e).lower()
    if "cannot resolve" in _err_str_${e} or "analysisexception" in _err_str_${e}:
        # Schema mismatch: try PERMISSIVE mode with schema inference
        try:
            _recovery_base = globals().get("VOLUMES_BASE", "/Volumes/main/nifi_migration")
            ${n} = (spark.read
                .option("mode", "PERMISSIVE")
                .option("inferSchema", "true")
                .option("header", "true")
                .option("columnNameOfCorruptRecord", "_corrupt_record")
                .csv(_recovery_base + "/fallback"))
            _cell_status_${e} = "RECOVERED"
            _recovered_${e} = True
            print(f"[RECOVERED] ${t}: re-read with PERMISSIVE mode")
        except Exception as _e2:
            print(f"[RECOVERY FAILED] ${t}: PERMISSIVE read also failed: {_e2}")
    elif "permission" in _err_str_${e} or "access" in _err_str_${e}:
        print(f"[SKIPPED] ${t}: permission denied -- check Unity Catalog grants")
        _cell_status_${e} = "SKIPPED"
        _recovered_${e} = True
    elif "not found" in _err_str_${e} or "does not exist" in _err_str_${e}:
        print(f"[SKIPPED] ${t}: source resource not found -- verify path/table exists")
        _cell_status_${e} = "SKIPPED"
        _recovered_${e} = True`}function Bc(e,n,t,r,i){return`
    # RECOVERY: Transform-specific error handling
    _err_str_${e} = str(_e).lower()
    if "cannot resolve" in _err_str_${e} or "analysisexception" in _err_str_${e}:
        # Column resolution error: try selecting only columns that exist
        try:
            import re as _re_${e}
            _missing_col_match = _re_${e}.search(r"cannot resolve.*['\`]([^'\`]+)['\`]", str(_e))
            _missing_col = _missing_col_match.group(1) if _missing_col_match else None
            if _missing_col and _missing_col in ${t}.columns:
                raise _e  # Column exists but something else is wrong
            _available_cols = [c for c in ${t}.columns if not c.startswith("_")]
            ${n} = ${t}.select(*_available_cols)
            _cell_status_${e} = "RECOVERED"
            _recovered_${e} = True
            print(f"[RECOVERED] ${r}: selected {len(_available_cols)} available columns (dropped missing)")
        except Exception as _e2:
            if _recovered_${e}:
                pass  # Already recovered above
            else:
                print(f"[RECOVERY FAILED] ${r}: column subset also failed: {_e2}")
    elif "parseexception" in _err_str_${e} or "illegal" in _err_str_${e}:
        print(f"[SKIPPED] ${r}: expression parse error requires manual fix")
        _cell_status_${e} = "SKIPPED"
        _recovered_${e} = True`}function jc(e,n,t,r){return`
    # RECOVERY: Sink-specific error handling -- write ALL failed records to DLQ
    _err_str_${e} = str(_e).lower()
    try:
        from pyspark.sql.functions import lit, current_timestamp
        _dlq_df = (${n}
            .withColumn("_dlq_source", lit("${t}"))
            .withColumn("_dlq_error", lit(str(_e)[:500]))
            .withColumn("_dlq_timestamp", current_timestamp())
            .withColumn("_dlq_run_id", lit(_RUN_ID)))
        _dlq_df.write.mode("append").saveAsTable("${r}.__dead_letter_queue")
        _dlq_count = ${n}.count()
        _cell_status_${e} = "DLQ"
        _recovered_${e} = True
        print(f"[DLQ] ${t}: {_dlq_count} records written to dead letter queue")
    except Exception as _dlq_err:
        print(f"[DLQ FAILED] ${t}: could not write to DLQ: {_dlq_err}")
    if "table or view not found" in _err_str_${e}:
        print(f"[HINT] ${t}: target table may not exist -- check DDL or create table first")
    elif "connection" in _err_str_${e} or "timeout" in _err_str_${e}:
        # Connection error: retry with exponential backoff
        import time as _retry_time_${e}
        for _sink_attempt in range(1, 4):
            _sink_backoff = 5 * (2 ** (_sink_attempt - 1))
            print(f"[RETRY] ${t}: connection retry {_sink_attempt}/3, waiting {_sink_backoff}s")
            _retry_time_${e}.sleep(_sink_backoff)
            try:
                ${n}.write.mode("append").option("mergeSchema", "true").saveAsTable("${r}.__retry_sink_${e}")
                _cell_status_${e} = "RECOVERED"
                _recovered_${e} = True
                print(f"[RECOVERED] ${t}: retry {_sink_attempt} succeeded")
                break
            except Exception:
                if _sink_attempt == 3:
                    print(f"[FAILED] ${t}: all 3 connection retries exhausted")`}function Uc(e,n,t){if(!e.mapped||!e.code||e.code.startsWith("# TODO"))return"";const r=ve(e.name),i=t[e.name]||{},o=i.inputVars&&i.inputVars.length?i.inputVars[0].varName:"df_input",a=i.outputVar||"df_"+r,s=e.name.replace(/"/g,'\\"').replace(/'/g,"''");return e.role==="source"?qc(r,a,s,n):e.role==="transform"||e.role==="process"?zc(r,a,o,s,n):e.role==="sink"?Gc(r,a,o,s,n):e.role==="route"?Hc(r,a,o,s,n):""}function qc(e,n,t,r){return`
        # RECOVERY (source): try PERMISSIVE mode, then schema inference, then raise
        _err_msg_${e} = str(_e).lower()
        _source_recovered_${e} = False
        # Attempt 1: Re-read with PERMISSIVE mode and corrupt record column
        try:
            _recovery_base = globals().get("VOLUMES_BASE", "/Volumes/main/nifi_migration")
            ${n} = (spark.read
                .option("mode", "PERMISSIVE")
                .option("columnNameOfCorruptRecord", "_corrupt_record")
                .option("header", "true")
                .csv(_recovery_base + "/fallback"))
            _corrupt_count = ${n}.filter("_corrupt_record IS NOT NULL").count()
            if _corrupt_count > 0:
                print(f"[RECOVERED] ${t}: PERMISSIVE mode, {_corrupt_count} corrupt records flagged")
                # Log corrupt records to DLQ
                try:
                    ${n}.filter("_corrupt_record IS NOT NULL").write.mode("append").saveAsTable("${r}.__dead_letter_queue")
                except Exception:
                    pass
                ${n} = ${n}.filter("_corrupt_record IS NULL").drop("_corrupt_record")
            _cell_status_${e} = "RECOVERED"
            _source_recovered_${e} = True
            print(f"[RECOVERED] ${t}: re-read with PERMISSIVE mode")
        except Exception as _perm_err:
            print(f"[RECOVERY] ${t}: PERMISSIVE mode failed: {_perm_err}")
        # Attempt 2: Try with full schema inference
        if not _source_recovered_${e}:
            try:
                _recovery_base = globals().get("VOLUMES_BASE", "/Volumes/main/nifi_migration")
                ${n} = (spark.read
                    .option("inferSchema", "true")
                    .option("header", "true")
                    .option("multiLine", "true")
                    .json(_recovery_base + "/fallback"))
                _cell_status_${e} = "RECOVERED"
                _source_recovered_${e} = True
                print(f"[RECOVERED] ${t}: schema inference succeeded")
            except Exception as _inf_err:
                print(f"[RECOVERY] ${t}: schema inference failed: {_inf_err}")
        # Attempt 3: Check for specific error types
        if not _source_recovered_${e}:
            if "permission" in _err_msg_${e} or "access denied" in _err_msg_${e}:
                _cell_status_${e} = "SKIPPED"
                print(f"[SKIPPED] ${t}: permission denied -- check Unity Catalog grants and cluster IAM role")
            elif "not found" in _err_msg_${e} or "does not exist" in _err_msg_${e}:
                _cell_status_${e} = "SKIPPED"
                print(f"[SKIPPED] ${t}: source not found -- verify path/table/topic exists")
            else:
                print(f"[FAILED] ${t}: all recovery attempts exhausted")
        # Log recovery attempt
        try:
            spark.sql(f"""INSERT INTO ${r}.__execution_log VALUES (
                _RUN_ID, '${t}', 'source', 'recovery',
                current_timestamp(), '{_cell_status_${e}}',
                '{str(_e)[:200]}', 0, 0, 0.0, ''
            )""")
        except Exception:
            pass`}function zc(e,n,t,r,i){return`
        # RECOVERY (transform): try column subset, then raise (never passthrough)
        _err_msg_${e} = str(_e).lower()
        _transform_recovered_${e} = False
        # Check for column resolution errors
        if "cannot resolve" in _err_msg_${e} or "analysisexception" in _err_msg_${e}:
            try:
                import re as _re_recovery
                # Extract the missing column name from the error
                _col_match = _re_recovery.search(r"cannot resolve.*['\`]([^'\`]+)['\`]", str(_e))
                _missing_col = _col_match.group(1) if _col_match else None
                _avail_cols = ${t}.columns
                if _missing_col:
                    _keep_cols = [c for c in _avail_cols if c != _missing_col]
                    print(f"[RECOVERY] ${r}: column '{_missing_col}' not found, selecting {len(_keep_cols)} available columns")
                else:
                    _keep_cols = _avail_cols
                ${n} = ${t}.select(*_keep_cols)
                _cell_status_${e} = "RECOVERED"
                _transform_recovered_${e} = True
                print(f"[RECOVERED] ${r}: selected available columns")
            except Exception as _col_err:
                print(f"[RECOVERY FAILED] ${r}: column subset failed: {_col_err}")
        elif "schema" in _err_msg_${e} and "mismatch" in _err_msg_${e}:
            # Schema mismatch: try to cast to the expected types
            try:
                from pyspark.sql.functions import col
                ${n} = ${t}.select(*[col(c).cast("string").alias(c) for c in ${t}.columns])
                _cell_status_${e} = "RECOVERED"
                _transform_recovered_${e} = True
                print(f"[RECOVERED] ${r}: cast all columns to string to resolve schema mismatch")
            except Exception as _cast_err:
                print(f"[RECOVERY FAILED] ${r}: schema cast failed: {_cast_err}")
        elif "parseexception" in _err_msg_${e}:
            _cell_status_${e} = "SKIPPED"
            _transform_recovered_${e} = True
            print(f"[SKIPPED] ${r}: SQL parse error requires manual fix")
        # Log recovery attempt
        try:
            spark.sql(f"""INSERT INTO ${i}.__execution_log VALUES (
                _RUN_ID, '${r}', 'transform', 'recovery',
                current_timestamp(), '{_cell_status_${e}}',
                '{str(_e)[:200]}', 0, 0, 0.0, ''
            )""")
        except Exception:
            pass`}function Gc(e,n,t,r,i){return`
        # RECOVERY (sink): write ALL failed records to DLQ, then raise
        _err_msg_${e} = str(_e).lower()
        _sink_recovered_${e} = False
        # Write all failed records to dead letter queue (no limit)
        try:
            from pyspark.sql.functions import lit, current_timestamp as _cts
            _dlq_records = (${t}
                .withColumn("_dlq_source", lit("${r}"))
                .withColumn("_dlq_error", lit(str(_e)[:500]))
                .withColumn("_dlq_timestamp", _cts())
                .withColumn("_dlq_run_id", lit(_RUN_ID)))
            _dlq_records.write.mode("append").saveAsTable("${i}.__dead_letter_queue")
            _dlq_count = ${t}.count()
            _cell_status_${e} = "DLQ"
            print(f"[DLQ] ${r}: ALL {_dlq_count} records written to dead letter queue")
        except Exception as _dlq_err:
            print(f"[DLQ FAILED] ${r}: could not write to DLQ: {_dlq_err}")
        # Connection/timeout errors: retry with real exponential backoff
        if "connection" in _err_msg_${e} or "timeout" in _err_msg_${e}:
            import time as _retry_time
            for _sink_attempt in range(1, 4):
                _backoff = 5 * (2 ** (_sink_attempt - 1))
                print(f"[RETRY] ${r}: connection retry {_sink_attempt}/3, waiting {_backoff}s")
                _retry_time.sleep(_backoff)
                try:
                    ${t}.write.mode("append").option("mergeSchema", "true").saveAsTable("${i}.__retry_sink_${e}")
                    _cell_status_${e} = "RECOVERED"
                    _sink_recovered_${e} = True
                    print(f"[RECOVERED] ${r}: retry {_sink_attempt} succeeded")
                    break
                except Exception as _retry_err:
                    if _sink_attempt == 3:
                        print(f"[FAILED] ${r}: all 3 connection retries exhausted: {_retry_err}")
        elif "table or view not found" in _err_msg_${e}:
            _cell_status_${e} = "SKIPPED"
            _sink_recovered_${e} = True
            print(f"[SKIPPED] ${r}: target table not found -- create table DDL first, then re-run")
        elif "permission" in _err_msg_${e} or "access denied" in _err_msg_${e}:
            _cell_status_${e} = "SKIPPED"
            _sink_recovered_${e} = True
            print(f"[SKIPPED] ${r}: permission denied -- check WRITE grants on target table")
        # Log recovery attempt
        try:
            spark.sql(f"""INSERT INTO ${i}.__execution_log VALUES (
                _RUN_ID, '${r}', 'sink', 'recovery',
                current_timestamp(), '{_cell_status_${e}}',
                '{str(_e)[:200]}', 0, 0, 0.0, ''
            )""")
        except Exception:
            pass`}function Hc(e,n,t,r,i){return`
        # RECOVERY (route): routing errors are typically configuration issues
        _err_msg_${e} = str(_e).lower()
        if "cannot resolve" in _err_msg_${e}:
            _cell_status_${e} = "SKIPPED"
            print(f"[SKIPPED] ${r}: routing condition references missing column -- check filter expression")
        else:
            _cell_status_${e} = "SKIPPED"
            print(f"[SKIPPED] ${r}: routing failed -- review NiFi routing conditions")
        # Log recovery attempt
        try:
            spark.sql(f"""INSERT INTO ${i}.__execution_log VALUES (
                _RUN_ID, '${r}', 'route', 'recovery',
                current_timestamp(), '{_cell_status_${e}}',
                '{str(_e)[:200]}', 0, 0, 0.0, ''
            )""")
        except Exception:
            pass`}function Wc(e,n,t){const i=(n.connections||[]).filter(c=>c.sourceName===e.name);if(i.length<=1)return"";const o=ve(e.name),a={};if(i.forEach(c=>{const l=c.relationships&&c.relationships[0]||c.relationship||"success";a[l]||(a[l]=[]),a[l].push(c.destinationName)}),Object.keys(a).length<=1)return"";let s=`
# â”€â”€ Relationship Routing for `+e.name+` â”€â”€
`;return Object.entries(a).forEach(([c,l])=>{const d=l.map(u=>"df_"+ve(u));c==="success"||c==="matched"||c==="valid"?(s+='# Route "'+c+'" -> '+l.join(", ")+`
`,d.forEach(u=>{s+=u+" = df_"+o+`  # success path
`})):c==="failure"||c==="unmatched"||c==="invalid"?s+='# Route "'+c+'" -> '+l.join(", ")+` (error path)
`:(s+='# Route "'+c+'" -> '+l.join(", ")+`
`,d.forEach(u=>{s+=u+" = df_"+o+"_"+ve(c)+`  # conditional
`}))}),s}function Vc(e,{lineage:n,qualifiedSchema:t,nifi:r,fullProps:i,cellIndex:o}){const a=`[${e.role.toUpperCase()}] ${e.name} â†’ ${e.category}`,s=n[e.name]||{},c=(s.inputVars||[]).map(D=>`${D.varName} (${D.relationship})`).join(", ")||"none";let l=Oc(e);const d=i||{unused:{}},u=Object.keys(d.unused).length>0?`
# Unused NiFi properties:
`+Object.entries(d.unused).slice(0,8).map(([D,P])=>"#   "+D+": "+String(P).substring(0,80)).join(`
`):"",p=`# INTENT: ${{source:"DATA INGESTION - Reads data from an external system into the pipeline",sink:"DATA OUTPUT - Writes processed data to a target system or storage",transform:"DATA TRANSFORMATION - Modifies, enriches, or reshapes data in transit",route:"FLOW CONTROL - Routes data to different paths based on conditions",process:"DATA PROCESSING - Performs computation or business logic on data",utility:"UTILITY - Provides supporting functionality (logging, monitoring, etc.)"}[e.role]||"PROCESSING - "+e.type}`,m=`# NiFi Processor: ${e.type} (${e.name})`,f=`# Migration Confidence: ${Math.round((e.confidence||0)*100)}% | Status: ${e.mapped?"Mapped":"MANUAL IMPLEMENTATION REQUIRED"}`,h={...d.used||{},...d.unused||{}},_=Object.entries(h).slice(0,12),v=_.length>0?`
# Original NiFi Properties:
`+_.map(([D,P])=>{const $=String(P).substring(0,100);return`#   ${D} = ${$}`}).join(`
`):"",k=Wc(e,r);let E;if(e.mapped&&e.code&&!e.code.startsWith("# TODO")){const D=l!==e.code?{...e,code:l}:e;E=Nc(D,t,o,n);const P=Uc(D,t,n);P&&(E=E.replace(/(\s+raise _e)$/,($,x)=>P+x))}else E=`# ${a}
# ${e.desc}${e.notes?"  |  "+e.notes:""}
# Input: ${c}
${l}`;return E=`${p}
${m}
${f}${v}
# Input lineage: ${c}
# Output: ${s.outputVar||"df_"+ve(e.name)}${u}
${E}${k}`,{type:"code",label:a,source:E,role:e.role,processor:e.name,procType:e.type,confidence:e.confidence,mapped:e.mapped}}function Kc(e,n){return`# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXECUTION REPORT GENERATOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import json
from datetime import datetime

_exec_report = {
    "pipeline_name": "`+n+`",
    "generated_at": datetime.now().isoformat(),
    "total_processors": `+e.length+`,
    "mapped_processors": `+e.filter(t=>t.mapped).length+`,
    "coverage_pct": `+Math.round(e.filter(t=>t.mapped).length/Math.max(e.length,1)*100)+`,
    "processors": []
}

try:
    _exec_rows = spark.sql("""
        SELECT processor_name, processor_type, role, status, error_message,
               rows_processed, duration, confidence, upstream_procs
        FROM `+n+`.__execution_log ORDER BY timestamp
    """).collect()
    for _r in _exec_rows:
        _exec_report["processors"].append({
            "name": _r.processor_name, "type": _r.processor_type,
            "role": _r.role, "status": _r.status,
            "error": _r.error_message or "", "rows": _r.rows_processed or 0,
            "duration": _r.duration or "", "confidence": _r.confidence or 0
        })
except Exception as _e:
    print(f"[WARN] Could not read execution log: {_e}")

_successes = len([p for p in _exec_report["processors"] if p.get("status") == "SUCCESS"])
_failures = len([p for p in _exec_report["processors"] if p.get("status") == "FAILED"])
_recovered = len([p for p in _exec_report["processors"] if p.get("status") in ("RECOVERED","PASSTHROUGH","DLQ")])
_exec_report["summary"] = {"successes":_successes,"failures":_failures,"recovered":_recovered,
    "success_rate":round(_successes/max(len(_exec_report["processors"]),1)*100,1)}

try:
    _report_df = spark.createDataFrame([{"report_json":json.dumps(_exec_report),
        "generated_at":datetime.now().isoformat(),
        "success_rate":_exec_report["summary"]["success_rate"],
        "total_procs":_successes+_failures+_recovered}])
    _report_df.write.mode("append").saveAsTable("`+n+`.__execution_reports")
except: pass

print("=" * 60)
print("EXECUTION REPORT")
print("=" * 60)
print(f"Successes: {_successes} | Failures: {_failures} | Recovered: {_recovered}")
print(f"Success Rate: {_exec_report['summary']['success_rate']}%")
print("=" * 60)`}function Qc(e,n,t){const r=e.filter(s=>s.role==="source"),i=e.filter(s=>s.role==="sink");let o=r.slice(0,20).map(s=>{const c="df_"+ve(s.name);return'try: _source_vars["'+s.name+'"] = '+c+`.count()
except: _source_vars["`+s.name+'"] = -1'}).join(`
`),a=i.slice(0,20).map(s=>{const c="df_"+ve(s.name);return'try: _sink_vars["'+s.name+'"] = '+c+`.count()
except: _sink_vars["`+s.name+'"] = -1'}).join(`
`);return`# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END-TO-END VALIDATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import json
from datetime import datetime
_source_vars = {}
_sink_vars = {}
`+o+`
`+a+`

_validation_report = {
    "pipeline": "`+n+`",
    "timestamp": datetime.now().isoformat(),
    "source_row_counts": _source_vars,
    "sink_row_counts": _sink_vars,
    "total_source_rows": sum(v for v in _source_vars.values() if v > 0),
    "total_sink_rows": sum(v for v in _sink_vars.values() if v > 0)
}
_src_total = _validation_report["total_source_rows"]
_snk_total = _validation_report["total_sink_rows"]
if _src_total > 0 and _snk_total > 0:
    _retention = round(_snk_total / _src_total * 100, 1)
    _validation_report["data_retention_pct"] = _retention
    print(f"Data retention: {_retention}%")

try:
    _val_df = spark.createDataFrame([{"report_json":json.dumps(_validation_report),
        "validated_at":datetime.now().isoformat(),
        "source_rows":_src_total,"sink_rows":_snk_total}])
    _val_df.write.mode("append").saveAsTable("`+n+`.__validation_reports")
except: pass

print("=" * 60)
print("END-TO-END VALIDATION COMPLETE")
print("=" * 60)
print(json.dumps(_validation_report, indent=2))`}function Jc(e,n,t){if(!e||e.length<2)return null;const r=e.map(c=>n.find(l=>l.name===c)).filter(Boolean);if(r.length===0)return null;const i=r.some(c=>/InvokeHTTP|PostHTTP|GetHTTP/i.test(c.type)),o=ve(e[0]),a=Zc(r,t);if(i){const c=a||"    _loop_result = df  # passthrough (no mapped code)";return`# Cycle detected: ${e.join(" â†’ ")} â†’ (loop back)
# Pattern: API retry loop â€” converted to tenacity retry
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError

@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=60))
def _retry_loop_${o}(df):
    """Retry loop for: ${e.join(" â†’ ")}"""
    result = df
${c}
    result = _loop_result
    return result

try:
    df_${o} = _retry_loop_${o}(df_${o})
except RetryError:
    print(f"[LOOP] Retry cycle exhausted for ${e[0]}")`}const s=a||"    _loop_result = _loop_df_"+o+"  # passthrough (no mapped code)";return`# Cycle detected: ${e.join(" â†’ ")} â†’ (loop back)
# Pattern: Conditional loop â€” converted to while iteration
_max_iterations = 100
_iteration = 0
_loop_df_${o} = df_${o}
_loop_result = _loop_df_${o}
while _iteration < _max_iterations:
    _iteration += 1
${s}
    if _loop_result.count() == 0:
        break  # No more records to process
    _loop_df_${o} = _loop_result
df_${o} = _loop_df_${o}
print(f"[LOOP] Completed {_iteration} iterations for ${e[0]}")`}function Zc(e,n){const t=[];for(const r of e){if(!r.mapped||!r.code||r.code.startsWith("# TODO"))continue;const o=(n[r.name]||{}).outputVar||"df_"+ve(r.name);t.push(`    # --- ${r.name} (${r.type}) ---`);const a=r.code.split(`
`).map(s=>"    "+s).join(`
`);t.push(a),t.push(`    _loop_result = ${o}`)}return t.length>0?t.join(`
`):null}function Xc(e){const n=[];return e&&e.forEach((t,r)=>{const i=typeof t=="string"?t:t.code||t.textContent||"",o=[],a=i.match(/\{[a-z_]+\}/g)||[];a.length>3&&o.push(`${a.length} unresolved placeholders: ${a.slice(0,3).join(", ")}...`);const s=(i.match(/\(/g)||[]).length,c=(i.match(/\)/g)||[]).length;Math.abs(s-c)>2&&o.push(`Unbalanced parentheses: ${s} open, ${c} close`);const l=i.match(/\$\{[^}]+\}/g)||[];l.length>0&&o.push(`${l.length} unresolved NiFi EL expressions: ${l.slice(0,2).join(", ")}...`);const d=new Set;i.includes("requests.")&&d.add("requests"),i.includes("boto3.")&&d.add("boto3"),i.includes("paramiko.")&&d.add("paramiko"),i.includes("pika.")&&d.add("pika"),i.includes("stomp.")&&d.add("stomp"),i.includes("pymongo")&&d.add("pymongo"),i.includes("elasticsearch")&&d.add("elasticsearch");const u=new Set;(i.match(/^import\s+(\w+)/gm)||[]).forEach(p=>u.add(p.replace("import ",""))),(i.match(/^from\s+(\w+)/gm)||[]).forEach(p=>u.add(p.replace("from ","")));const g=[...d].filter(p=>!u.has(p));g.length>0&&o.push(`Missing imports: ${g.join(", ")}`),n.push({cellIndex:r,valid:o.length===0,issues:o})}),n}function Yc(e){const n=[];e.forEach((o,a)=>{if(o.type!=="code")return;const s=o.source||"",c=s.match(/# *(TODO|FIXME|PLACEHOLDER|HACK)[: ].*/gi);c&&c.forEach(m=>{n.push({cell:a,severity:"high",code:"GEN_TODO_STUB",message:`Stub code found: ${m.trim().substring(0,80)}`,fix:"Replace with real implementation or explicit NotImplementedError"})});const l=s.match(/\{[a-z_]+\}/gi);if(l){const f=[...new Set(l)].filter(h=>{const _=h.replace(/[{}]/g,"\\$&"),v=new RegExp(`[f]['"].*${_}.*['"]`,"g"),k=(s.match(new RegExp(_,"g"))||[]).length;return(s.match(v)||[]).length<k});f.length>0&&n.push({cell:a,severity:"high",code:"GEN_UNRESOLVED_PLACEHOLDER",message:`Unresolved placeholders: ${f.join(", ")}`,fix:"Configure Databricks settings or check template resolution"})}const d=s.match(/\$\{[^}]+\}/g);if(d){const m=d.filter(f=>{const h=s.split(`
`).find(_=>_.includes(f));return h&&!h.trimStart().startsWith("#")});m.length>0&&n.push({cell:a,severity:"high",code:"GEN_UNRESOLVED_EL",message:`Unresolved NiFi EL expressions: ${m.slice(0,3).join(", ")}`,fix:"NiFi Expression Language was not translated to PySpark"})}/except\s*:\s*\n\s*pass/g.test(s)&&n.push({cell:a,severity:"medium",code:"GEN_BARE_EXCEPT",message:"Bare except:pass swallows all errors silently",fix:"Add specific exception types and logging"}),(/\beval\s*\(/.test(s)||/\bexec\s*\(/.test(s))&&n.push({cell:a,severity:"critical",code:"GEN_UNSAFE_EVAL",message:"eval() or exec() detected -- security risk",fix:"Replace with safe alternative (literal parsing, ast.literal_eval, etc.)"}),[/password\s*=\s*["'][^"']+["']/i,/api_key\s*=\s*["'][^"']+["']/i,/secret\s*=\s*["'][^"']+["']/i,/token\s*=\s*["'][A-Za-z0-9+/=]{20,}["']/i].forEach(m=>{if(m.test(s)){const f=s.match(m);n.push({cell:a,severity:"critical",code:"GEN_HARDCODED_CRED",message:`Possible hardcoded credential: ${(f?.[0]||"").substring(0,40)}...`,fix:'Use dbutils.secrets.get(scope="...", key="...") instead'})}}),(/["']\/dbfs\//.test(s)||/["']\/mnt\//.test(s))&&n.push({cell:a,severity:"medium",code:"GEN_DEPRECATED_PATH",message:"Legacy DBFS/mount path detected",fix:"Use Unity Catalog Volumes: /Volumes/catalog/schema/volume/"}),/\.collect\(\)/.test(s)&&!/\.limit\(\d+\).*\.collect\(\)/.test(s)&&n.push({cell:a,severity:"medium",code:"GEN_UNBOUNDED_COLLECT",message:".collect() without .limit() may cause OOM on large DataFrames",fix:"Add .limit(N) before .collect() or use .toPandas() with spark.conf row limits"});let g=0;for(const m of s)m==="("?g++:m===")"&&g--;Math.abs(g)>1&&n.push({cell:a,severity:"high",code:"GEN_UNBALANCED_PARENS",message:`Unbalanced parentheses (depth: ${g})`,fix:"Check for missing closing parentheses"}),s.match(/df_(\w+)\s*=\s*df_(\w+)\s*\n/g)&&s.split(`
`).filter(m=>!m.startsWith("#")&&m.trim()).length<=3&&n.push({cell:a,severity:"medium",code:"GEN_PASSTHROUGH",message:"Cell appears to be a passthrough with no real processing",fix:"Implement actual transformation or mark processor as unsupported"})});const t={critical:15,high:8,medium:3,low:1},r=n.reduce((o,a)=>o+(t[a.severity]||3),0),i=Math.max(0,Math.min(100,100-r));return{issues:n,score:i}}function el(e){return e.forEach(n=>{if(n.type!=="code")return;let t=n.source||"";t=t.replace(/# *TODO[: ]+(.*)/gi,(i,o)=>`# MANUAL REVIEW REQUIRED: ${o.trim()}`),t=t.replace(/# *FIXME[: ]+(.*)/gi,(i,o)=>`# MANUAL REVIEW REQUIRED: ${o.trim()}`),t=t.replace(/# *PLACEHOLDER[: ]+(.*)/gi,(i,o)=>`# MANUAL REVIEW REQUIRED: ${o.trim()}`),t=t.replace(/except\s*:\s*\n(\s*)pass/g,`except Exception as _scrub_e:
$1print(f"[WARNING] Suppressed error: {_scrub_e}")`),t=t.replace(/def (\w+)\(([^)]*)\):\s*\n(\s*)pass\b/g,`def $1($2):
$3raise NotImplementedError("$1 requires implementation")`),t=t.split(`
`).map(i=>i.trimStart().startsWith("#")?i:i.replace(/["']\/dbfs\/([^"']+)["']/g,'"/Volumes/{catalog}/{schema}/data/$1"').replace(/["']\/mnt\/([^"']+)["']/g,'"/Volumes/{catalog}/{schema}/mounts/$1"')).join(`
`),n.source=t}),e}const tl=[{pattern:/aes_encrypt|aes_decrypt/,minDBR:"10.4",feature:"AES encryption functions"},{pattern:/MERGE\s+INTO/i,minDBR:"5.1",feature:"MERGE INTO (Delta Lake)"},{pattern:/CREATE\s+VOLUME/i,minDBR:"13.3",feature:"Unity Catalog Volumes"},{pattern:/cloudFiles/,minDBR:"8.1",feature:"Auto Loader (cloudFiles)"},{pattern:/pandas_udf/,minDBR:"5.3",feature:"Pandas UDFs"},{pattern:/mapInPandas|applyInPandas/,minDBR:"10.0",feature:"mapInPandas/applyInPandas"},{pattern:/OPTIMIZE\s+.*ZORDER/i,minDBR:"5.1",feature:"OPTIMIZE ZORDER"},{pattern:/VACUUM/i,minDBR:"5.1",feature:"Delta VACUUM"},{pattern:/CREATE\s+STREAMING\s+TABLE/i,minDBR:"13.1",feature:"DLT Streaming Tables"},{pattern:/\.observe\(/,minDBR:"11.0",feature:"DataFrame.observe()"},{pattern:/spark\.readStream\.format\("kafka"\)/,minDBR:"5.1",feature:"Kafka Structured Streaming"},{pattern:/IDENTITY\s+COLUMN/i,minDBR:"13.3",feature:"Identity columns"},{pattern:/CLUSTER\s+BY/i,minDBR:"13.3",feature:"Liquid clustering"}];function nl(e,n){const t=parseFloat(n)||14.3,r=[];let i=0;return e.forEach((o,a)=>{if(o.type!=="code")return;const s=o.source||"";tl.forEach(c=>{if(c.pattern.test(s)){const l=parseFloat(c.minDBR);l>t&&r.push({cell:a,feature:c.feature,minDBR:c.minDBR}),i=Math.max(i,l)}})}),{minRequired:String(i||"14.0"),issues:r}}function rl(e){return`# Runtime Version Check
import re as _rc_re
_dbr_version = spark.conf.get("spark.databricks.clusterUsageTags.sparkVersion", "0.0")
_dbr_match = _rc_re.search(r"(\\d+\\.\\d+)", _dbr_version)
_dbr_major = float(_dbr_match.group(1)) if _dbr_match else 0.0
if _dbr_major < ${parseFloat(e)||14}:
    raise RuntimeError(
        f"This notebook requires Databricks Runtime ${e}+, "
        f"but cluster is running {_dbr_version}. "
        f"Please upgrade or select a compatible cluster."
    )
print(f"[RUNTIME] Databricks Runtime {_dbr_version} (>= ${e} required)")`}function sl(e,n,t){const r=e.catalog||"main",i=e.schema||"default",o=e.secretScope||"migration_secrets",a=(n||[]).filter(l=>!["pyspark","delta","dbutils"].includes(l)).map(l=>`    "${l}"`).join(`,
`),s=(t||[]).map(l=>`    ("${o}", "${l}")`).join(`,
`);return{type:"code",label:"Pre-Flight Validation",source:`# ============================================================
# PRE-FLIGHT VALIDATION
# Checks environment readiness before pipeline execution.
# ============================================================
import importlib, time as _pf_time

_pf_start = _pf_time.time()
_pf_errors = []
_pf_warnings = []

# 1. Verify catalog and schema exist
try:
    spark.sql(f"USE CATALOG ${r}")
    spark.sql(f"USE SCHEMA ${r}.${i}")
    print(f"[PREFLIGHT] Catalog ${r}.${i} verified")
except Exception as _e:
    _pf_errors.append(f"Catalog/schema ${r}.${i} not accessible: {_e}")

# 2. Create execution tracking tables if missing
for _tbl_name, _tbl_ddl in [
    ("__execution_log", """CREATE TABLE IF NOT EXISTS ${r}.${i}.__execution_log (
        run_id STRING, processor STRING, proc_type STRING, role STRING,
        timestamp TIMESTAMP, status STRING, error STRING,
        rows LONG, duration_ms LONG, confidence DOUBLE, upstream STRING
    ) USING DELTA"""),
    ("__dead_letter_queue", """CREATE TABLE IF NOT EXISTS ${r}.${i}.__dead_letter_queue (
        run_id STRING, processor STRING, error STRING, timestamp TIMESTAMP,
        record_count LONG, sample_data STRING
    ) USING DELTA"""),
]:
    try:
        spark.sql(f"DESCRIBE TABLE ${r}.${i}.{_tbl_name}")
    except Exception:
        try:
            spark.sql(_tbl_ddl)
            print(f"[PREFLIGHT] Created {_tbl_name}")
        except Exception as _e2:
            _pf_warnings.append(f"Could not create {_tbl_name}: {_e2}")

# 3. Check required Python packages
_pf_missing_pkgs = []
for _pkg in [
${a}
]:
    try:
        importlib.import_module(_pkg.split(".")[0])
    except ImportError:
        _pf_missing_pkgs.append(_pkg)

if _pf_missing_pkgs:
    _pf_warnings.append(f"Missing packages (install via %pip): {', '.join(_pf_missing_pkgs)}")

# 4. Check secret scopes
_pf_secret_checks = [
${s}
]
for _scope, _key in _pf_secret_checks:
    try:
        dbutils.secrets.get(scope=_scope, key=_key)
    except Exception as _e:
        _pf_errors.append(f"Secret {_scope}/{_key} not accessible: {_e}")

# 5. Generate unique run ID for this execution
import uuid as _pf_uuid
_RUN_ID = str(_pf_uuid.uuid4())[:8]
print(f"[PREFLIGHT] Run ID: {_RUN_ID}")

# Report results
_pf_elapsed = round((_pf_time.time() - _pf_start) * 1000)
if _pf_errors:
    print(f"\\n{'='*60}")
    print(f"PREFLIGHT FAILED ({len(_pf_errors)} errors, {len(_pf_warnings)} warnings)")
    for _e in _pf_errors:
        print(f"  ERROR: {_e}")
    for _w in _pf_warnings:
        print(f"  WARN:  {_w}")
    print(f"{'='*60}")
    raise RuntimeError(f"Pre-flight validation failed: {len(_pf_errors)} errors. Fix issues above before running pipeline.")
else:
    if _pf_warnings:
        for _w in _pf_warnings:
            print(f"  WARN: {_w}")
    print(f"[PREFLIGHT] All checks passed ({_pf_elapsed}ms)")`,role:"config",processor:"__preflight",procType:"PreflightCheck",confidence:1,mapped:!0}}function Po(e,n,t,r){r=r||{};const i=r.catalog||"",o=r.schema||"nifi_migration",a=i?`${i}.${o}`:o,s=[],c=Pc(e,n),l=Dc(e),d=Tc(e,n),u={};d.forEach(M=>{u[M.name]=Ac(M,n)});const g=n.processGroups&&n.processGroups[0]?n.processGroups[0].name:"NiFi Flow",p=d.filter(M=>M.mapped).length,m=Math.round(p/Math.max(d.length,1)*100);s.push(Ic({flowName:g,totalProcessors:d.length,mappedCount:p,coveragePct:m,qualifiedSchema:a}));const f=[`# Notebook Overview: ${g}`,`# Generated: ${new Date().toISOString().split("T")[0]}`,`# Processors: ${d.length} total | ${p} mapped | ${d.length-p} manual`,`# Coverage: ${m}% | Target: ${a}`,"#","# Cell Structure:","#   1. Header & Overview          - Migration metadata and notebook map","#   2. Environment Parameters     - Widget-based dev/staging/prod config","#   3. Package Installation        - pip install for third-party libraries","#   4. Imports & Configuration     - PySpark imports, catalog/schema setup","#   5. Pre-flight Validation       - Verify cluster, packages, secrets","#   6. Execution Framework         - Tracking tables, dead letter queue","#   7. Schema DDL                  - Unity Catalog table definitions","#   8. DataFrame Lineage Map       - Variable dependency graph"];let h=9;Object.entries((()=>{const M={};return d.forEach(z=>{const U=z.group||"(root)";M[U]||(M[U]=[]),M[U].push(z)}),M})()).forEach(([M,z])=>{f.push(`#   ${h}. Process Group: ${M} (${z.length} processors)`),h++,z.forEach(U=>{const X=U.mapped?U.confidence>=.8?"AUTO":"PARTIAL":"MANUAL",F=(c[U.name]?.inputVars||[]).map(N=>N.procName).join(", ")||"none";f.push(`#       ${h}. [${X}] ${U.name} (${U.type}) -> deps: ${F}`),h++})}),f.push(`#   ${h}. Execution Report`),f.push(`#   ${h+1}. End-to-End Validation`),f.push(`#   ${h+2}. Pipeline Complete`),f.push("#"),f.push(`# Compute: ${r.computeType||"cluster"} | Runtime: DBR ${r.runtimeVersion||"14.3"}`),f.push(`# Cloud: ${r.cloudProvider||"azure"} | Node: ${r.nodeType||"Standard_DS3_v2"}`),s.push({type:"code",label:"Notebook Overview",source:f.join(`
`)+`
print("[OVERVIEW] Notebook structure loaded")`,role:"config"});const v=["# Environment Parameters - Widget-based configuration","# Toggle between dev/staging/prod environments",'dbutils.widgets.dropdown("environment", "dev", ["dev", "staging", "prod"], "Target Environment")','dbutils.widgets.text("catalog_override", "", "Catalog Override (blank=use default)")','dbutils.widgets.text("schema_override", "", "Schema Override (blank=use default)")','dbutils.widgets.dropdown("dry_run", "false", ["true", "false"], "Dry Run Mode")','dbutils.widgets.dropdown("log_level", "INFO", ["DEBUG", "INFO", "WARN", "ERROR"], "Log Level")',"",'_ENV = dbutils.widgets.get("environment")','_DRY_RUN = dbutils.widgets.get("dry_run") == "true"','_LOG_LEVEL = dbutils.widgets.get("log_level")',"","# Environment-specific configuration","_ENV_CONFIG = {",`    "dev":     {"catalog": "${i||"dev_catalog"}",     "schema": "${o}_dev",     "parallel": 2},`,`    "staging": {"catalog": "${i||"staging_catalog"}", "schema": "${o}_staging", "parallel": 4},`,`    "prod":    {"catalog": "${i||i}",       "schema": "${o}",         "parallel": 8},`,"}","","# Apply overrides","_active_cfg = _ENV_CONFIG[_ENV]",'_cat_override = dbutils.widgets.get("catalog_override")','_sch_override = dbutils.widgets.get("schema_override")','if _cat_override: _active_cfg["catalog"] = _cat_override','if _sch_override: _active_cfg["schema"] = _sch_override',"",'ACTIVE_CATALOG = _active_cfg["catalog"]','ACTIVE_SCHEMA = _active_cfg["schema"]',"","if _DRY_RUN:",'    print(f"[DRY RUN] Environment: {_ENV} | Catalog: {ACTIVE_CATALOG} | Schema: {ACTIVE_SCHEMA}")',"else:",'    print(f"[LIVE] Environment: {_ENV} | Catalog: {ACTIVE_CATALOG} | Schema: {ACTIVE_SCHEMA}")'].join(`
`);s.push({type:"code",label:"Environment Parameters",source:v,role:"config"}),l.pipCell&&s.push(l.pipCell),s.push(Fc({smartImportsCode:l.code,catalogName:i,schemaName:o,secretScope:r.secretScope}));const k=(l.packages||[]).filter(M=>M!=="pyspark"),E=[];d.forEach(M=>{if(!M.code)return;const z=M.code.matchAll(/dbutils\.secrets\.get\s*\(\s*scope\s*=\s*["']([^"']+)["']\s*,\s*key\s*=\s*["']([^"']+)["']/g);for(const U of z)E.push(U[2])}),s.push(sl({catalog:i,schema:o,secretScope:r.secretScope},k,[...new Set(E)])),s.push({type:"code",label:"Execution Framework Setup",source:`# Execution Tracking Framework
from datetime import datetime

spark.sql(f"""
CREATE TABLE IF NOT EXISTS ${a}.__execution_log (
  processor_name STRING, processor_type STRING, role STRING,
  timestamp TIMESTAMP DEFAULT current_timestamp(), status STRING,
  error_message STRING, rows_processed LONG, duration STRING,
  confidence INT, upstream_procs STRING
) USING DELTA TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')
""")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS ${a}.__dead_letter_queue (
  source_processor STRING, error STRING, record_data STRING,
  timestamp STRING, _ingested_at TIMESTAMP DEFAULT current_timestamp()
) USING DELTA TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')
""")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS ${a}.__execution_reports (
  report_json STRING, generated_at STRING, success_rate DOUBLE, total_procs INT
) USING DELTA
""")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS ${a}.__validation_reports (
  report_json STRING, validated_at STRING, source_rows LONG, sink_rows LONG
) USING DELTA
""")

print(f"[FRAMEWORK] Execution tracking ready: ${a}")`,role:"utility",processor:"Framework Setup",procType:"Internal",confidence:1,mapped:!0});const D=t&&t.tables||[],P=Lc({catalogName:i,schemaName:o,qualifiedSchema:a,tables:D});P&&s.push(P);const $=Object.entries(c).slice(0,30).map(([M,z])=>{const U=(z.inputVars||[]).map(X=>X.procName).join(", ")||"(source)";return`# ${z.outputVar} <- ${U}`}).join(`
`);s.push({type:"code",label:"DataFrame Lineage Map",source:`# DataFrame Lineage Map
${$}
${Object.keys(c).length>30?"# ... and "+(Object.keys(c).length-30)+" more":""}
print("[LINEAGE] DataFrame lineage map loaded â€” ${Object.keys(c).length} variables tracked")`,role:"config"});const x={};d.forEach(M=>{const z=M.group||"(root)";x[z]||(x[z]=[]),x[z].push(M)});let S=s.length;Object.entries(x).forEach(([M,z])=>{const U=z.filter(X=>X.mapped).length;s.push({type:"md",label:M,source:`## Process Group: ${M}
**${z.length} processors** | ${U} mapped | ${z.length-U} manual

*Cells ordered by connection topology*`,role:"config"}),z.forEach(X=>{S++,s.push(Vc(X,{lineage:c,qualifiedSchema:a,nifi:n,fullProps:u[X.name],cellIndex:S}))})});const C=d.filter(M=>M.role==="source"&&M.mapped),A=d.filter(M=>M.role==="sink"&&M.mapped);if(C.length>0){const M=["# Input Data Validation","# Verify schema, record counts, and null rates for source DataFrames","from pyspark.sql.functions import col, count, when, lit","","_input_validation_results = []"];C.slice(0,15).forEach(z=>{const U="df_"+ve(z.name);M.push("try:",`    _df = ${U}`,"    _row_count = _df.count()","    _col_count = len(_df.columns)","    _null_counts = {c: _df.where(col(c).isNull()).count() for c in _df.columns[:20]}","    _null_pct = {c: round(n / max(_row_count, 1) * 100, 1) for c, n in _null_counts.items()}","    _high_nulls = [c for c, p in _null_pct.items() if p > 50]",`    _result = {"processor": "${z.name}", "rows": _row_count, "columns": _col_count, "high_null_cols": _high_nulls, "schema": _df.dtypes[:10]}`,"    _input_validation_results.append(_result)",'    _status = "WARN" if _high_nulls else "OK"',`    print(f"[{_status}] ${z.name}: {_row_count} rows, {_col_count} cols" + (f", HIGH NULLS: {_high_nulls}" if _high_nulls else ""))`,"except NameError:",`    print(f"[SKIP] ${z.name}: DataFrame not yet defined")`,"except Exception as _e:",`    print(f"[FAIL] ${z.name}: {_e}")`,"")}),M.push('print(f"[INPUT VALIDATION] {len(_input_validation_results)} source(s) checked")'),s.push({type:"code",label:"Input Data Validation",source:M.join(`
`),role:"utility",processor:"InputValidation",procType:"Internal",confidence:1,mapped:!0})}if(A.length>0){const M=["# Output Data Validation","# Verify schema consistency, record counts, and checkpoint for sink DataFrames","from pyspark.sql.functions import col, count, when, lit","","_output_validation_results = []"];A.slice(0,15).forEach(z=>{const U="df_"+ve(z.name);M.push("try:",`    _df = ${U}`,"    _row_count = _df.count()","    _col_count = len(_df.columns)","    _null_counts = {c: _df.where(col(c).isNull()).count() for c in _df.columns[:20]}","    _null_pct = {c: round(n / max(_row_count, 1) * 100, 1) for c, n in _null_counts.items()}","    _high_nulls = [c for c, p in _null_pct.items() if p > 50]",`    _result = {"processor": "${z.name}", "rows": _row_count, "columns": _col_count, "high_null_cols": _high_nulls}`,"    _output_validation_results.append(_result)",'    _status = "WARN" if _high_nulls else "OK"',`    print(f"[{_status}] ${z.name}: {_row_count} rows, {_col_count} cols" + (f", HIGH NULLS: {_high_nulls}" if _high_nulls else ""))`,"except NameError:",`    print(f"[SKIP] ${z.name}: DataFrame not yet defined")`,"except Exception as _e:",`    print(f"[FAIL] ${z.name}: {_e}")`,"")}),M.push('print(f"[OUTPUT VALIDATION] {len(_output_validation_results)} sink(s) checked")'),s.push({type:"code",label:"Output Data Validation",source:M.join(`
`),role:"utility",processor:"OutputValidation",procType:"Internal",confidence:1,mapped:!0})}s.push({type:"code",label:"Pipeline Checkpoint",source:["# Pipeline Checkpoint","# Persist intermediate state for recovery and auditing","import json","from datetime import datetime","","_checkpoint_data = {",'    "pipeline": "'+a+'",','    "checkpoint_time": datetime.now().isoformat(),','    "environment": _ENV if "_ENV" in dir() else "unknown",','    "dry_run": _DRY_RUN if "_DRY_RUN" in dir() else False,','    "source_validations": len(_input_validation_results) if "_input_validation_results" in dir() else 0,','    "sink_validations": len(_output_validation_results) if "_output_validation_results" in dir() else 0,',"}","","try:","    _ckpt_df = spark.createDataFrame([{",'        "checkpoint_json": json.dumps(_checkpoint_data),','        "checkpoint_time": datetime.now().isoformat(),','        "pipeline": "'+a+'"',"    }])",'    _ckpt_df.write.mode("append").saveAsTable("'+a+'.__pipeline_checkpoints")',`    print(f"[CHECKPOINT] State saved at {_checkpoint_data['checkpoint_time']}")`,"except Exception as _e:",'    print(f"[CHECKPOINT] Save failed (non-fatal): {_e}")'].join(`
`),role:"utility",processor:"Checkpoint",procType:"Internal",confidence:1,mapped:!0}),s.push({type:"code",label:"Execution Report",source:Kc(d,a),role:"utility",processor:"ExecutionReport",procType:"Internal",confidence:1,mapped:!0}),s.push({type:"code",label:"End-to-End Validation",source:Qc(d,a),role:"utility",processor:"E2EValidation",procType:"Internal",confidence:1,mapped:!0}),s.push({type:"sql",label:"Migration Error Table",source:`CREATE TABLE IF NOT EXISTS ${a}.__migration_errors (
  processor_name STRING, processor_type STRING,
  error_time TIMESTAMP, error_message STRING
) USING DELTA;`,role:"config"});try{let M;try{M=window.analyzeFlowGraph||null}catch{M=null}if(M){const z=M(n.processors||[],n.connections||[]);z.circularRefs&&z.circularRefs.length>0&&z.circularRefs.forEach(U=>{const X=Jc(U.cycle,d,c);X&&s.push({type:"code",label:"Loop: "+U.cycle[0],source:X,role:"transform",processor:U.cycle[0],procType:"CycleLoop",confidence:.75,mapped:!0})})}}catch(M){console.warn("Cycle-to-loop generation:",M)}s.push({type:"code",label:"Pipeline Complete",source:`# Final status
import json
try:
    _exec_log = spark.sql("SELECT status, count(*) as cnt FROM ${a}.__execution_log GROUP BY status").collect()
    _counts = {r.status: r.cnt for r in _exec_log}
    _ok = _counts.get("SUCCESS", 0)
    _fail = _counts.get("FAILED", 0)
    _recov = sum(v for k,v in _counts.items() if k in ("RECOVERED","PASSTHROUGH","DLQ"))
    print("=" * 60)
    print(f"PIPELINE COMPLETE: {_ok} success, {_fail} failed, {_recov} recovered")
    print(f"Success rate: {round(_ok/max(_ok+_fail+_recov,1)*100,1)}%")
    print("=" * 60)
    if _fail > 0:
        display(spark.sql("SELECT * FROM ${a}.__execution_log WHERE status='FAILED'"))
    dbutils.notebook.exit(json.dumps({"status":"COMPLETE","success":_ok,"failed":_fail,"recovered":_recov}))
except Exception as _e:
    print(f"[WARN] Status check failed: {_e}")
    dbutils.notebook.exit("COMPLETE")`,role:"utility"}),s.forEach(M=>{M.source&&(i?M.source=Ha(M.source,r):M.source=M.source.replace(/<catalog>\./g,"").replace(/\/Volumes\/<catalog>\//g,"/Volumes/").replace(/\/<catalog>\//g,"/").replace(/<catalog>/g,"").replace(/<schema>/g,o).replace(/\{schema\}/g,o))}),el(s);const w=r.runtimeVersion||"14.3",R=nl(s,w);if(parseFloat(R.minRequired)>0){const M=rl(R.minRequired),z=s.findIndex(U=>U.role!=="config"&&U.type!=="md")||3;s.splice(z,0,{type:"code",label:"Runtime Version Check",source:M,role:"config",processor:"__runtime_check",procType:"RuntimeCheck",confidence:1,mapped:!0})}R.issues.length>0&&s.push({type:"code",label:"Runtime Compatibility Warnings",source:`# Runtime Compatibility Warnings
`+R.issues.map(M=>`# Cell ${M.cell}: "${M.feature}" requires DBR ${M.minDBR}+ (target: ${w})`).join(`
`)+`
print("[COMPAT] ${R.issues.length} feature(s) may require DBR ${R.minRequired}+")`,role:"utility",processor:"CompatCheck",procType:"Internal",confidence:1,mapped:!0});const y=Yc(s);if(y.issues.length>0){const M={};y.issues.forEach(U=>{M[U.severity]||(M[U.severity]=[]),M[U.severity].push(U)});const z=["# Databricks Code Validation Report",`# Score: ${y.score}/100`,`# Issues: ${y.issues.length} (${Object.keys(M).map(U=>`${U}: ${M[U].length}`).join(", ")})`];y.issues.forEach(U=>{z.push(`# [${U.severity.toUpperCase()}] Cell ${U.cell}: ${U.code} - ${U.message}`),z.push(`#   Fix: ${U.fix}`)}),s.push({type:"code",label:"Databricks Validation Report",source:z.join(`
`)+`
print("[VALIDATION] Score: ${y.score}/100, ${y.issues.length} issues found")`,role:"utility",processor:"DbxValidator",procType:"Internal",confidence:1,mapped:!0})}const J=Xc(s.map(M=>M.source||"")).filter(M=>!M.valid);return J.length>0&&s.push({type:"code",label:"Code Validation Report",source:`# Code Validation Report
# `+J.length+` cells with potential issues:
`+J.map(M=>"# Cell "+M.cellIndex+": "+M.issues.join("; ")).join(`
`),role:"utility",processor:"CodeValidator",procType:"Internal",confidence:1,mapped:!0}),typeof window<"u"&&(window._lastNotebookCells=s,window._lastLineage=c),{cells:s,flowName:g,lineage:c,metadata:{processorCount:d.length,mappedCount:p,generatedAt:new Date().toISOString(),config:{catalog:i,schema:o},improvements:["lineage","smartImports","topoSort","adaptiveCode","errorFramework","autoRecovery","fullProperties","relationshipRouting","executionReport","e2eValidation","nelParser","phiDetection","sharedClusters","cycleDetection","streamingGuard"]}}}function Ro(e,n,t){t=t||{};const r=t.workspacePath||"/Workspace/Migrations/NiFi",i=t.sparkVersion||"14.3.x-scala2.12",o=t.nodeType||"Standard_DS3_v2",a=t.numWorkers||2,s=n.connections||[],c={};(n.processors||[]).forEach(m=>{c[m.name]=m.group||"(root)"});const l=[...new Set(Object.values(c))],d={};l.forEach(m=>{d[m]=new Set}),s.forEach(m=>{const f=c[m.sourceName],h=c[m.destinationName];f&&h&&f!==h&&d[h].add(f)});const u="nifi_migration_cluster",g=[{job_cluster_key:u,new_cluster:{spark_version:i,node_type_id:o,num_workers:a,spark_conf:{"spark.databricks.delta.optimizeWrite.enabled":"true","spark.databricks.delta.autoCompact.enabled":"true","spark.sql.adaptive.enabled":"true","spark.sql.shuffle.partitions":"auto"},custom_tags:{source:"nifi_migration"}}}],p=l.map(m=>({task_key:ve(m),description:`Process group: ${m}`,notebook_task:{notebook_path:`${r}/${ve(m)}_notebook`,source:"WORKSPACE"},depends_on:[...d[m]].map(f=>({task_key:ve(f)})),job_cluster_key:u}));return{name:`NiFi_Migration_${ve(l[0]||"flow")}`,job_clusters:g,tasks:p,format:"MULTI_TASK",tags:{source:"nifi_migration",generated_by:"seg_demo"}}}const ol=["source","route","transform","process","sink","utility"],_t={source:"#3B82F6",route:"#EAB308",transform:"#A855F7",process:"#6366F1",sink:"#21C354",utility:"#808495"};function Do(e,n){const t=e.length,r=e.filter(R=>R.mapped).length,i={};ol.forEach(R=>{i[R]={total:0,mapped:0,unmapped:0,procs:[]}}),e.forEach(R=>{const y=i[R.role]||i.process;y.total++,R.mapped?y.mapped++:y.unmapped++,y.procs.push(R)});const o={};e.forEach(R=>{o[R.group]||(o[R.group]={total:0,mapped:0,unmapped:0,procs:[]}),o[R.group].total++,R.mapped?o[R.group].mapped++:o[R.group].unmapped++,o[R.group].procs.push(R)});const a=e.filter(R=>!R.mapped||R.confidence<Se.PARTIAL).map(R=>({name:R.name,type:R.type,group:R.group,role:R.role,reason:R.gapReason||`Low confidence mapping (${Math.round(R.confidence*100)}%)`,recommendation:R.type.match(/^(Listen|Handle)/)?"Consider Databricks Model Serving or external API gateway":R.type.match(/^Execute(Script|Stream)/)?"Manual translation required â€” review original script logic":R.type.match(/^(Put|Send)(Email|TCP|Syslog)/)?"Use webhook notification service or Databricks workflow alerts":"Review processor documentation and implement custom PySpark logic"})),s={Listen:[{alternative:"Databricks Model Serving Endpoint",feasibility:"High",effort:"Medium",snippet:`# Deploy as Model Serving endpoint
import mlflow
mlflow.pyfunc.log_model(...)
# Configure REST endpoint in Databricks Serving`},{alternative:"Azure Function / AWS Lambda webhook",feasibility:"Medium",effort:"Low",snippet:`# Serverless function forwards HTTP requests to Databricks
# Use Jobs API: POST /api/2.1/jobs/run-now`},{alternative:"Structured Streaming + Kafka bridge",feasibility:"Medium",effort:"High",snippet:`df = spark.readStream.format("kafka")\\
  .option("subscribe", "nifi-http-topic")\\
  .load()`}],Handle:[{alternative:"Databricks Model Serving Endpoint",feasibility:"High",effort:"Medium",snippet:`# Create serving endpoint for HTTP request handling
from databricks.sdk import WorkspaceClient
w = WorkspaceClient()
w.serving_endpoints.create(...)`},{alternative:"API Gateway + Jobs API",feasibility:"Medium",effort:"Low",snippet:`# Route HTTP through API Gateway to Databricks Jobs
# POST /api/2.1/jobs/run-now with request payload`}],ExecuteScript:[{alternative:"PySpark UDF (inline translation)",feasibility:"High",effort:"High",snippet:`from pyspark.sql.functions import udf
@udf(returnType=StringType())
def custom_logic(value):
    # Translate NiFi script logic here
    return transformed_value
df = df.withColumn("result", custom_logic(col("input")))`},{alternative:"Databricks notebook job with parameters",feasibility:"Medium",effort:"Medium",snippet:`dbutils.notebook.run("./custom_script_notebook", timeout_seconds=300,
  arguments={"input_table": "...", "output_table": "..."})`},{alternative:"Delta Live Tables expectations",feasibility:"Low",effort:"High",snippet:`@dlt.expect_or_drop("valid_data", "value IS NOT NULL")
@dlt.table
def processed():
    return spark.read.table("input_table")`}],ExecuteStream:[{alternative:"Subprocess via %sh magic",feasibility:"Medium",effort:"Medium",snippet:`# MAGIC %sh
# MAGIC /path/to/external_command --input /tmp/data --output /tmp/result`},{alternative:"PySpark subprocess module",feasibility:"Medium",effort:"Low",snippet:`import subprocess
result = subprocess.run(["command", "--arg"], capture_output=True)
print(result.stdout.decode())`}],PutEmail:[{alternative:"Databricks Workflow notification",feasibility:"High",effort:"Low",snippet:`# Configure email notification in workflow JSON:
# "email_notifications": {"on_success": ["team@company.com"]}`},{alternative:"SendGrid / SES API call",feasibility:"Medium",effort:"Low",snippet:`import requests
api_key = dbutils.secrets.get("scope", "sendgrid_key")
requests.post("https://api.sendgrid.com/v3/mail/send",
  headers={"Authorization": f"Bearer {api_key}"},
  json={"personalizations": [...], "from": {...}, "content": [...]})`}],PutTCP:[{alternative:"Webhook via requests library",feasibility:"High",effort:"Low",snippet:`import requests
response = requests.post("http://target:port/endpoint",
  json={"data": row.asDict()}, timeout=30)`},{alternative:"Kafka producer",feasibility:"Medium",effort:"Medium",snippet:`df.selectExpr("CAST(key AS STRING)", "to_json(struct(*)) AS value")\\
  .write.format("kafka")\\
  .option("kafka.bootstrap.servers", "host:9092")\\
  .option("topic", "output-topic").save()`}]},c=a.map(R=>{const y=Object.keys(s).find(O=>R.type.includes(O));return{...R,alternatives:y?s[y]:[{alternative:"Custom PySpark implementation",feasibility:"Medium",effort:"High",snippet:`# Manual implementation for ${R.type}
# Review NiFi processor docs and translate logic to PySpark
# df = spark.read.table("input")
# ... custom transformation ...
# df.write.saveAsTable("output")`},{alternative:"Databricks Partner Connect integration",feasibility:"Low",effort:"Medium",snippet:`# Check Databricks Partner Connect for third-party tools
# that can replace this NiFi processor functionality`}]}}),l=a.map(R=>{const y=al(R.type),O=il(R.type);return{name:R.name,type:R.type,role:R.role,nifiDescription:y,databricksApproach:O.approach,keyDifferences:O.differences,migrationSteps:O.steps,estimatedEffort:O.effort}}),d={source:5,sink:4,transform:3,route:3,process:4,utility:1},u=a.map(R=>{const y=d[R.role]||3,O=(n.connections||[]).some(N=>(n.processors||[]).find(Q=>Q.id===N.source?.id)?.name===R.name),J=O?2:0,M=R.type.match(/ExecuteScript|ExecuteStream|Custom/)?5:R.type.match(/Listen|Handle/)?4:R.type.match(/Put|Send/)?3:2,z=y+J+M,X=Math.round(z/12*100),F=X>=70?"red":X>=40?"amber":"green";return{name:R.name,type:R.type,role:R.role,criticalityScore:y,downstreamImpact:O?"High":"Low",complexityScore:M,totalRiskScore:X,rating:F,rationale:`Role weight: ${y}/5, Downstream: ${O?"Yes (+2)":"No"}, Complexity: ${M}/5`}}),g={source:4,sink:4,transform:2,route:3,process:6,utility:1},p={source:16,sink:12,transform:8,route:8,process:24,utility:4},m=e.filter(R=>R.mapped).reduce((R,y)=>R+(g[y.role]||3),0),f=a.reduce((R,y)=>R+(p[y.role]||12),0),h=Math.round((m+f)*.4),_=m+f+h,v=_<=80?1:_<=200?2:_<=500?3:4,k=Math.ceil(_/(v*32)),E={phases:[{name:"Phase 1: Environment Setup & Review",weeks:Math.max(1,Math.ceil(k*.15)),tasks:["Set up Databricks workspace","Review NiFi flow architecture","Configure Unity Catalog","Set up CI/CD pipeline"]},{name:"Phase 2: Mapped Processor Migration",weeks:Math.max(1,Math.ceil(k*.3)),tasks:[`Migrate ${r} auto-mapped processors`,"Validate PySpark equivalents","Unit test each processor cell"]},{name:"Phase 3: Gap Resolution",weeks:Math.max(1,Math.ceil(k*.3)),tasks:[`Implement ${a.length} unmapped processors`,"Build custom UDFs where needed","Integration testing"]},{name:"Phase 4: Testing & Validation",weeks:Math.max(1,Math.ceil(k*.15)),tasks:["End-to-end pipeline testing","Data comparison validation","Performance benchmarking"]},{name:"Phase 5: Cutover & Monitoring",weeks:Math.max(1,Math.ceil(k*.1)),tasks:["Parallel run with NiFi","Production cutover","Post-migration monitoring"]}],totalWeeks:k,totalHours:_,mappedHours:m,gapHours:f,testingHours:h,recommendedTeamSize:v,teamComposition:v===1?["1 Senior Data Engineer (PySpark + NiFi experience)"]:v===2?["1 Senior Data Engineer (PySpark lead)","1 Data Engineer (testing & validation)"]:v===3?["1 Senior Data Engineer (PySpark lead)","1 Data Engineer (processor migration)","1 QA Engineer (testing & validation)"]:["1 Tech Lead (architecture)","1 Senior Data Engineer (PySpark)","1 Data Engineer (processor migration)","1 QA Engineer (testing)"]},D={processorCriteria:e.map(R=>({name:R.name,type:R.type,mapped:R.mapped,acceptanceTests:R.mapped?[`${R.name}: Output schema matches expected structure`,`${R.name}: Row count within 1% tolerance of NiFi output`,`${R.name}: No unexpected null values in required columns`,R.role==="transform"?`${R.name}: Transformation logic produces equivalent results`:null,R.role==="source"?`${R.name}: Data ingestion completes within SLA window`:null,R.role==="sink"?`${R.name}: Target system receives all records`:null].filter(Boolean):[`${R.name}: Manual implementation reviewed and approved`,`${R.name}: Custom code passes unit tests`,`${R.name}: Output validated against NiFi reference data`]})),pipelineCriteria:[{metric:"Data Completeness",target:">=99.5%",description:"Total output rows / total input rows"},{metric:"Schema Fidelity",target:"100%",description:"All column names, types, and constraints match"},{metric:"Processing Latency",target:"Within 2x NiFi baseline",description:"End-to-end pipeline execution time"},{metric:"Data Quality Score",target:">=95%",description:"Pass rate across all quality rules"},{metric:"Error Rate",target:"<0.1%",description:"Records routed to dead letter queue"}],comparisonMetrics:[{metric:"Row Count Delta",description:"abs(nifi_rows - databricks_rows) / nifi_rows * 100",threshold:"<1%"},{metric:"Column Hash Match",description:"MD5 hash comparison of sorted column values",threshold:"100% match"},{metric:"Null Rate Delta",description:"Per-column null percentage difference",threshold:"<0.5%"},{metric:"Value Distribution",description:"KS-test p-value for numeric columns",threshold:"p > 0.05"}]},P=[];a.length>t*.2&&P.push("High gap rate â€” consider custom UDFs for unsupported processor types"),e.some(R=>R.type.match(/Listen|Handle/))&&P.push("HTTP endpoints detected â€” evaluate Databricks Model Serving for REST API replacement"),e.some(R=>R.type.match(/Consume.*Kafka|Subscribe/))&&P.push("Streaming sources present â€” use Structured Streaming with Auto Loader trigger intervals"),(n.controllerServices||[]).length&&P.push(`${n.controllerServices.length} controller service(s) detected â€” map credentials to Databricks secret scopes`),r>t*.8&&P.push("High coverage â€” prioritize testing the mapped processors before addressing gaps"),P.push("Run the generated notebook in a Databricks workspace to validate each cell");const $=t?Math.round(r/t*100):0,x={source:2,sink:2,transform:1,route:1.5,process:3,script:4,custom:5},S=e.reduce((R,y)=>R+(x[y.role]||2),0),C=e.filter(R=>R.mapped).reduce((R,y)=>R+(x[y.role]||2),0),A=S>0?Math.round(C/S*100):0,w=A>=85?"Low":A>=60?"Medium":"High";return{summary:{totalProcessors:t,mappedProcessors:r,unmappedProcessors:t-r,coveragePercent:$,totalProcessGroups:Object.keys(o).length,totalConnections:(n.connections||[]).length,controllerServices:(n.controllerServices||[]).length},byRole:i,byGroup:o,gaps:a,recommendations:P,effort:w,gapPlaybook:c,processorGuides:l,riskMatrix:u,timeline:E,successCriteria:D}}function al(e){return{ListenHTTP:"Starts an HTTP(S) server and listens for incoming requests on a configurable port. Routes FlowFiles based on request method and path.",HandleHTTPRequest:"Receives HTTP requests as part of a request/response cycle. Works with HandleHTTPResponse to create REST endpoints.",HandleHTTPResponse:"Sends HTTP responses back to clients. Completes request/response cycles started by HandleHTTPRequest.",ExecuteScript:"Executes a user-provided script (Groovy, Python, Ruby, etc.) to process FlowFiles. Supports full scripting flexibility.",ExecuteStreamCommand:"Executes an external command/process, feeding FlowFile content as stdin and capturing stdout as the output FlowFile.",PutEmail:"Sends an email using SMTP. Supports HTML/text content, attachments from FlowFile content, and dynamic recipients.",PutTCP:"Sends FlowFile content over a TCP connection to a configured endpoint.",PutSyslog:"Sends FlowFile content as syslog messages to a remote syslog server.",ConsumeKafka:"Consumes messages from Apache Kafka topics. Supports consumer groups, offset management, and SSL.",PublishKafka:"Publishes FlowFile content as messages to Apache Kafka topics.",GetFile:"Reads files from a local or network file system directory.",PutFile:"Writes FlowFile content to a file on the local or network file system.",GetSFTP:"Fetches files from a remote SFTP server.",PutSFTP:"Uploads files to a remote SFTP server.",GetHTTP:"Fetches content from a configurable HTTP/HTTPS URL.",InvokeHTTP:"Sends HTTP requests and routes responses. Supports all HTTP methods, headers, and authentication.",QueryDatabaseTable:"Executes a SQL query against a database and outputs the results as FlowFiles.",PutDatabaseRecord:"Inserts, updates, or upserts records into a database table."}[e]||`NiFi processor that handles ${e.replace(/([A-Z])/g," $1").trim().toLowerCase()} operations.`}function il(e){return e.match(/^(Listen|Handle)HTTP/)?{approach:"Databricks Model Serving or external API Gateway triggering Jobs API",differences:["NiFi provides built-in HTTP server; Databricks uses Model Serving or external gateways","Request/response cycle requires async pattern in Databricks","Authentication handled differently (NiFi SSL vs Databricks tokens)"],steps:["Create a Model Serving endpoint or set up API Gateway","Implement request processing as a Databricks job","Configure authentication and rate limiting","Set up monitoring and alerting"],effort:"High (8-16 hours)"}:e.match(/^Execute(Script|Stream)/)?{approach:"PySpark UDFs, notebook jobs, or %sh magic commands",differences:["NiFi scripts run per-FlowFile; PySpark UDFs run per-row in parallel","NiFi supports Groovy/Ruby/Python; Databricks primarily uses Python/Scala/SQL","State management differs significantly"],steps:["Analyze the original script logic and identify pure functions","Translate to PySpark UDF or DataFrame operations","Handle state management through Delta tables if needed","Test with representative data samples"],effort:"High (12-24 hours per script)"}:e.match(/^(Put|Send)(Email|TCP|Syslog)/)?{approach:"Databricks workflow notifications, external API calls, or webhook integrations",differences:["NiFi has native protocol support; Databricks uses REST APIs or libraries","Notification is typically at job level, not per-record","Batching and retry logic must be explicitly implemented"],steps:["Identify notification requirements (per-record vs batch)","Set up external service (SendGrid, SNS, etc.) if per-record notification needed","Configure Databricks workflow notifications for job-level alerts","Implement retry and dead-letter handling"],effort:"Low-Medium (4-8 hours)"}:{approach:"Custom PySpark implementation with equivalent business logic",differences:["NiFi processor handles data record-by-record; PySpark processes in batches","Error handling patterns differ (NiFi relationships vs try/except)","Configuration management through widgets instead of NiFi properties"],steps:["Review NiFi processor documentation and properties","Design equivalent PySpark logic (DataFrame operations preferred)","Implement error handling and dead-letter routing","Validate output against NiFi reference data"],effort:"Medium (6-12 hours)"}}function Ve(e,n){const t=document.getElementById(e);t&&(t.innerHTML=`<div class="alert alert-error">${B(n)}</div>`)}function cl(e,n,t){const r=(e||"").toLowerCase();if(/\.(xml|xml\.gz)$/i.test(r))return{format:"XML",cssClass:"fmt-xml"};if(/\.(json|json\.gz)$/i.test(r))return{format:"JSON",cssClass:"fmt-json"};if(/\.sql$/i.test(r))return{format:"SQL",cssClass:"fmt-sql"};if(/\.(gz|zip|nar|jar|tgz|tar\.gz|tar)$/i.test(r))return{format:"Archive",cssClass:"fmt-archive"};if(/\.(docx|xlsx)$/i.test(r))return{format:"Document",cssClass:"fmt-document"};if(t&&t.length>=2){if(t[0]===31&&t[1]===139)return{format:"Archive (GZip)",cssClass:"fmt-archive"};if(t[0]===80&&t[1]===75)return{format:"Archive (ZIP)",cssClass:"fmt-archive"}}if(typeof n=="string"&&n.trim()){const i=n.trimStart();if(i.startsWith("<")||i.startsWith("<?xml"))return{format:"XML",cssClass:"fmt-xml"};if(i.startsWith("{")||i.startsWith("["))return{format:"JSON",cssClass:"fmt-json"}}return{format:"Unknown",cssClass:"fmt-unknown"}}function cs(e){return e<1024?e+" B":e<1024*1024?(e/1024).toFixed(1)+" KB":(e/(1024*1024)).toFixed(1)+" MB"}function ll(e,n){let t='<div class="parse-error-detail">';const r=e.message||String(e),i=r.match(/line\s+(\d+)/i)||r.match(/position\s+(\d+)/i);if(i&&typeof n=="string"){const a=parseInt(i[1],10),s=n.split(`
`),c=Math.max(0,a-3),l=Math.min(s.length,a+2);t+='<div style="font-size:0.78rem;color:var(--text2);margin-bottom:6px">Near line '+B(String(a))+":</div>",t+='<pre style="margin:0;padding:8px;background:var(--bg);border-radius:4px;font-size:0.75rem;overflow-x:auto">';for(let d=c;d<l;d++){const u=d+1,g=u===a,p=g?">>> ":"    ";t+='<span style="'+(g?"color:#fca5a5;font-weight:700":"color:var(--text2)")+'">'+B(p+u+" | "+(s[d]||"").substring(0,120))+`</span>
`}t+="</pre>"}const o=[];return/xml/i.test(r)&&(/unexpected.*eof|premature.*end/i.test(r)&&o.push("File appears truncated. Re-export from NiFi and try again."),/encoding/i.test(r)&&o.push("Try saving the file as UTF-8 without BOM."),/entity/i.test(r)&&o.push("Check for unescaped special characters (&, <, >) in property values."),o.length||o.push("Ensure this is a valid NiFi template XML or flow.xml.gz export.")),/json/i.test(r)&&(/unexpected.*token/i.test(r)&&o.push("Check for trailing commas or missing quotes in the JSON."),/unexpected.*end/i.test(r)&&o.push("File appears truncated. Re-export and try again."),o.length||o.push("Ensure this is a valid NiFi Registry JSON export.")),/version/i.test(r)&&o.push("This file may be from an unsupported NiFi version. Supported: NiFi 1.x templates and NiFi Registry JSON."),o.length&&(t+='<div class="error-suggestion"><strong>Suggestions:</strong><ul style="margin:4px 0 0 16px">',o.forEach(a=>{t+="<li>"+B(a)+"</li>"}),t+="</ul></div>"),t+="</div>",t}function dl(e,n,t){const r={},i=["source","route","transform","process","sink","utility"];i.forEach(d=>{r[d]=0}),e.processors.forEach(d=>{const u=t(d.type);r[u]=(r[u]||0)+1});const o=Math.max(1,...Object.values(r)),a={};e.processors.forEach(d=>{a[d.type]=(a[d.type]||0)+1});const s=Object.entries(a).sort((d,u)=>u[1]-d[1]).slice(0,5);let c="";e.version?c=e.version:e.flowEncodingVersion&&(c="Encoding v"+e.flowEncodingVersion);let l='<div class="flow-summary-card">';return l+='<div class="fsc-header">',l+='<span class="fsc-title">'+B(n)+"</span>",c&&(l+='<span class="fsc-version">NiFi '+B(c)+"</span>"),l+="</div>",l+='<div class="fsc-roles">',i.forEach(d=>{const u=r[d]||0;if(u===0)return;const g=_t[d]||"#808495",p=Math.round(u/o*100);l+='<div class="fsc-role-bar">',l+='<span style="color:'+g+';min-width:62px;text-transform:uppercase;font-weight:600">'+d+"</span>",l+='<div style="flex:1;min-width:40px;max-width:80px;background:var(--surface2);border-radius:3px;height:6px">',l+='<div class="fsc-role-fill" style="width:'+p+"%;background:"+g+'"></div></div>',l+='<span style="color:var(--text2);min-width:20px">'+u+"</span>",l+="</div>"}),l+="</div>",s.length&&(l+='<div class="fsc-chips">',s.forEach(([d,u])=>{const g=t(d),p=_t[g]||"#808495";l+='<span class="fsc-chip" style="border-color:'+p+"44;color:"+p+'">'+B(d)+' <span style="opacity:0.6">x'+u+"</span></span>"}),l+="</div>"),e.clouderaTools&&e.clouderaTools.length&&(l+='<div style="margin-top:8px;font-size:0.8rem;color:var(--text2)">External Systems: <strong style="color:var(--amber)">'+e.clouderaTools.length+"</strong></div>"),l+="</div>",l}function pl(e){if(!e||!e.length)return"";const n={critical:[],warning:[],info:[]};e.forEach(i=>{const o=i.toLowerCase();/error|fail|corrupt|invalid|unsupported|missing required/i.test(o)?n.critical.push(i):/warn|deprecated|unknown|unrecognized|skipped|ignored/i.test(o)?n.warning.push(i):n.info.push(i)});const t={critical:{label:"Critical",icon:"&#x26D4;",css:"severity-critical"},warning:{label:"Warning",icon:"&#x26A0;",css:"severity-warning"},info:{label:"Info",icon:"&#x2139;",css:"severity-info"}};let r='<div style="margin-top:12px">';r+='<h3 style="margin-bottom:8px">Parse Warnings ('+e.length+")</h3>";for(const[i,o]of Object.entries(n)){if(!o.length)continue;const a=t[i];r+='<div class="parse-warning-group'+(i==="critical"?" open":"")+'">',r+='<div class="parse-warning-header '+a.css+'" data-expander-toggle>',r+="<span>"+a.icon+" "+a.label+"</span>",r+='<span class="pw-count">'+o.length+" item"+(o.length!==1?"s":"")+"</span>",r+='<span class="expander-arrow" style="margin-left:8px">&#9654;</span>',r+="</div>",r+='<div class="parse-warning-body">',o.forEach(s=>{r+='<div class="parse-warning-item">'+B(s)+"</div>"}),r+="</div></div>"}return r+="</div>",r}async function bn(){const e=co(),n=po(),t=document.getElementById("pasteInput"),r=e||(t?t.value:"");if(!r.trim()&&!n){Ve("parseResults","Please upload or paste a NiFi flow file (XML, JSON, SQL, or archive).");return}const i=tt(),o=document.getElementById("parseBtn");o&&(o.disabled=!0),ge("load","processing");const a=document.getElementById("parseProgress");a&&(a.style.display="flex");const s=document.getElementById("parsePBar"),c=document.getElementById("parsePPct"),l=document.getElementById("parsePStatus"),d=(x,S)=>{s&&(s.style.width=x+"%"),c&&(c.textContent=x+"%"),l&&(l.textContent=S)},u=lo(),g=cl(u,r,n),p=typeof r=="string"?new Blob([r]).size:n?n.byteLength:0;d(5,"Detected format: "+g.format+"..."),await new Promise(x=>setTimeout(x,20));{const x=document.getElementById("parseResults");if(x){let S='<div style="display:flex;align-items:center;gap:8px;flex-wrap:wrap;margin:8px 0">';if(u&&(S+='<span style="font-size:0.9rem">'+B(u)+"</span>"),S+='<span class="format-badge '+g.cssClass+'">'+B(g.format)+"</span>",p>0){const C=cs(p);p>50*1024*1024?S+='<span class="file-size-warn">&#x26A0; '+B(C)+" (exceeds 50MB limit)</span>":p>10*1024*1024?S+='<span class="file-size-warn">&#x26A0; '+B(C)+" (large file, may be slow)</span>":S+='<span class="file-size-info">'+B(C)+"</span>"}S+="</div>",x.innerHTML=S}}const m=/archive/i.test(g.format);m?d(10,"Extracting archive contents..."):d(10,"Cleaning & parsing input..."),await new Promise(x=>setTimeout(x,20));let f;try{f=await dt(r,u||"NiFi Flow",{bytes:n})}catch(x){nt(i),$e(new we("Parse failed: "+x.message,{code:"PARSE_FAILED",phase:"parse",severity:"high",cause:x}));let S='<div class="alert alert-error">Failed to parse: '+B(x.message)+"</div>";S+=ll(x,r),f&&f._nifi&&f._nifi.processors&&f._nifi.processors.length>0&&(S+='<div class="alert alert-warn" style="margin-top:8px">Partial parse: found '+f._nifi.processors.length+" processor(s) before failure.</div>");const C=document.getElementById("parseResults");C&&(C.innerHTML=(C.innerHTML||"")+S),o&&(o.disabled=!1),ge("load","ready"),a&&(a.style.display="none");return}if(d(20,"Validating flow..."),await new Promise(x=>setTimeout(x,20)),!f||!f._nifi||f._nifi.processors.length===0){Ve("parseResults","No processors found. Supported formats: NiFi XML/JSON, SQL scripts, .gz/.zip/.nar/.jar archives, .docx/.xlsx documents."),o&&(o.disabled=!1),ge("load","ready"),a&&(a.style.display="none");return}if(d(50,"Processing flow..."),f._deferredProcessorWork)try{const x=f._deferredProcessorWork,S=50,C=x.processors.length;for(let A=0;A<C;A+=S){x.batchFn(x.processors.slice(A,A+S));const w=50+Math.round(A/C*30);m?d(w,"Extracting... "+Math.min(A+S,C)+"/"+C+" files"):d(w,"Analyzing processor "+(A+1)+"/"+C+"..."),await new Promise(R=>setTimeout(R,0))}x.finalize()}catch(x){$e(new we("Deferred processor work failed",{code:"DEFERRED_WORK_FAILED",phase:"parse",cause:x}))}d(85,"Building resource manifest..."),await new Promise(x=>setTimeout(x,20)),Xe({parsed:f,manifest:Co(f._nifi)}),d(95,"Rendering results...");const h=f._nifi,_=Pe;let v='<div style="display:flex;align-items:center;gap:8px;flex-wrap:wrap;margin:8px 0">';u&&(v+='<span style="font-size:0.9rem">'+B(u)+"</span>"),v+='<span class="format-badge '+g.cssClass+'">'+B(g.format)+"</span>",p>0&&(v+='<span class="file-size-info">'+B(cs(p))+"</span>"),v+="</div>",v+='<div class="alert alert-success" style="margin-top:8px">Successfully parsed NiFi flow: <strong>'+B(f.source_name)+"</strong></div>",v+=dl(h,f.source_name,_),v+=at([{label:"Processors",value:h.processors.length},{label:"Connections",value:h.connections.length},{label:"Process Groups",value:h.processGroups.length},{label:"Controller Services",value:h.controllerServices.length},{label:"External Systems",value:h.clouderaTools.length}]);const k={};h.processors.forEach(x=>{k[x.type]=(k[x.type]||0)+1});const E=Object.entries(k).sort((x,S)=>S[1]-x[1]).map(([x,S])=>{const C=_(x),A=_t[C]||"#808495";return['<span style="color:'+A+';font-weight:600">'+B(x)+"</span>",S,'<span class="badge" style="background:'+A+"22;color:"+A+'">'+C+"</span>"]});v+=yr("Processor Types ("+Object.keys(k).length+" unique)",Kt(["Type","Count","Role"],E)),f.parse_warnings.length&&(v+=pl(f.parse_warnings));const D=document.getElementById("parseResults");D&&(D.innerHTML=v),d(100,"Done!"),o&&(o.disabled=!1),ge("load","done"),gt("analyze");const P=document.getElementById("analyzeNotReady"),$=document.getElementById("analyzeReady");P&&P.classList.add("hidden"),$&&$.classList.remove("hidden"),setTimeout(()=>{a&&(a.style.display="none")},500);try{if(await new Promise(x=>setTimeout(x,50)),it("analyze"),await To(),!ce().analysis){console.error("[auto-run] Analysis failed, stopping pipeline");return}if(await new Promise(x=>setTimeout(x,50)),it("assess"),await Ao(),!ce().assessment){console.error("[auto-run] Assessment failed, stopping pipeline");return}if(await new Promise(x=>setTimeout(x,50)),it("convert"),await Io(),!ce().notebook){console.error("[auto-run] Notebook generation failed, stopping pipeline");return}if(await new Promise(x=>setTimeout(x,50)),it("report"),await Fo(),!ce().migrationReport){console.error("[auto-run] Report generation failed, stopping pipeline");return}if(await new Promise(x=>setTimeout(x,50)),typeof window.generateFinalReport=="function"&&(it("reportFinal"),await window.generateFinalReport(),!ce().finalReport)){console.error("[auto-run] Final report failed, stopping pipeline");return}if(await new Promise(x=>setTimeout(x,50)),typeof window.runValidation=="function"&&(it("validate"),await window.runValidation(),!ce().validation)){console.error("[auto-run] Validation failed, stopping pipeline");return}await new Promise(x=>setTimeout(x,50)),typeof window.runValueAnalysis=="function"&&(it("value"),await window.runValueAnalysis())}catch(x){$e(new we("Auto-run pipeline failed",{code:"PIPELINE_AUTO_RUN_FAILED",phase:"pipeline",cause:x}))}}async function To(){const e=zn("analyze");if(!e.ok){Ve("analyzeResults","Prerequisites not met: "+e.missing.join(", ")+". Please complete earlier steps first.");return}const n=ce(),t=tt();ge("analyze","processing");try{const r=n.parsed._nifi,i=n.manifest,o=Or(r),a=Hn(r);let s="";const c=Object.keys(r.deepPropertyInventory?.nifiEL||{}).length,l=r.sqlTables?r.sqlTables.length:0,d=Object.keys(r.deepPropertyInventory?.credentialRefs||{}).length,u=Object.keys(a).length;s+=at([{label:"Processors",value:r.processors.length},{label:"Connections",value:r.connections.length},{label:"Process Groups",value:r.processGroups.length},{label:"Controller Services",value:r.controllerServices.length},{label:"External Systems",value:u},{label:"EL Expressions",value:c},{label:"SQL Tables",value:l},{label:"Credentials",value:d}]);const g=Lr(n.parsed),p=bi(g,n.parsed);s+='<hr class="divider"><h3>Flow Visualization</h3>',s+='<div id="analysisTierContainer" style="position:relative"></div><div id="analysisTierDetail"></div><div id="analysisTierLegend"></div>';const m=Object.keys(a);if(m.length&&(s+='<hr class="divider"><h3>External Systems &amp; Dependencies ('+m.length+")</h3>",s+='<p style="color:var(--text2);font-size:0.82rem;margin-bottom:8px">Detected from processor types, JDBC URLs, and properties.</p>',m.forEach(E=>{const D=a[E];let $='<div class="sys-detail-row"><span class="sys-label">Processors:</span><span class="sys-value">'+D.processors.map(x=>{const S=ht[x.type]?ht[x.type].conf:0;return'<span class="conf-dot '+(S>=Se.MAPPED?"high":S>=Se.PARTIAL?"med":"low")+'"></span>'+B(x.name)+' <span style="color:var(--text2)">('+x.direction+")</span>"}).join("<br>")+"</span></div>";$+='<div class="sys-detail-row"><span class="sys-label">Databricks:</span><span class="sys-value">'+B(D.dbxApproach)+"</span></div>",D.jdbcUrls.length&&($+='<div class="sys-detail-row"><span class="sys-label">JDBC:</span><span class="sys-value"><code style="font-size:0.75rem">'+D.jdbcUrls.map(x=>B(x)).join("<br>")+"</code></span></div>"),D.credentials.length&&($+='<div class="sys-detail-row"><span class="sys-label">Credentials:</span><span class="sys-value" style="color:var(--amber)">'+D.credentials.map(x=>B(x)).join(", ")+"</span></div>"),D.packages.length&&($+='<div class="sys-detail-row"><span class="sys-label">Packages:</span><span class="sys-value"><code>'+D.packages.join(", ")+"</code></span></div>"),s+=yr(B(D.name)+' <span style="color:var(--text2);font-size:0.8rem">('+D.processors.length+" processor"+(D.processors.length!==1?"s":"")+")</span>",$,!1)})),s+='<hr class="divider"><h3>Processor Inventory ('+r.processors.length+")</h3>",s+='<p style="color:var(--text2);font-size:0.82rem;margin-bottom:8px">Click to expand for properties, scheduling, and dependencies.</p>',r.processors.forEach(E=>{const D=Pe(E.type),P=_t[D]||"#808495",$=ht[E.type],x=$?$.conf:0,S=x>=Se.MAPPED?"high":x>=Se.PARTIAL?"med":"low",C=o.upstream[E.name]||[],A=o.downstream[E.name]||[],w=o.fullUpstream[E.name]||[],R=o.fullDownstream[E.name]||[];let y='<div style="display:grid;grid-template-columns:1fr 1fr;gap:8px;margin-bottom:8px">';y+='<div><strong style="font-size:0.78rem;color:var(--text2)">Type:</strong> '+B(E.type)+"</div>",y+='<div><strong style="font-size:0.78rem;color:var(--text2)">Role:</strong> <span style="color:'+P+'">'+D+"</span></div>",y+='<div><strong style="font-size:0.78rem;color:var(--text2)">Group:</strong> '+B(E.group||"(root)")+"</div>",y+='<div><strong style="font-size:0.78rem;color:var(--text2)">State:</strong> '+(E.state||"UNKNOWN")+"</div>",y+='<div><strong style="font-size:0.78rem;color:var(--text2)">Scheduling:</strong> '+(E.schedulingStrategy||"-")+" / "+(E.schedulingPeriod||"-")+"</div>",y+='<div><strong style="font-size:0.78rem;color:var(--text2)">Confidence:</strong> <span class="conf-dot '+S+'"></span>'+Math.round(x*100)+"%</div></div>",(C.length||A.length)&&(y+='<div style="display:flex;gap:16px;margin-bottom:8px;font-size:0.8rem">',C.length&&(y+='<div><strong style="color:#3B82F6">Upstream ('+w.length+"):</strong> "+C.map(M=>B(M)).join(", ")+"</div>"),A.length&&(y+='<div><strong style="color:#21C354">Downstream ('+R.length+"):</strong> "+A.map(M=>B(M)).join(", ")+"</div>"),y+="</div>");const O=Object.entries(E.properties||{});O.length&&(y+='<table style="width:100%;font-size:0.78rem;border-collapse:collapse">',O.forEach(([M,z])=>{const U=mo(M)?"********":z,F=z&&z.includes("${")?String(U).replace(/\$\{([^}]+)\}/g,(N,H)=>`<span class="el-highlight">\${${B(H)}}</span>`):B(String(U||""));y+='<tr><td style="color:var(--text2);padding:2px 6px;border-bottom:1px solid var(--border);white-space:nowrap">'+B(M)+'</td><td style="padding:2px 6px;border-bottom:1px solid var(--border);word-break:break-all">'+F+"</td></tr>"}),y+="</table>"),$&&(y+='<div style="margin-top:8px;padding:8px;background:var(--bg);border-radius:4px;font-size:0.75rem"><strong style="color:var(--green)">Databricks: </strong>'+B($.desc)+'<br><span style="color:var(--text2)">'+B($.notes)+"</span></div>");const J='<span class="conf-dot '+S+'"></span><span style="color:'+P+'">['+D.toUpperCase()+"]</span> "+B(E.name)+' <span style="color:var(--text2);font-size:0.8rem">'+B(E.type)+"</span>";s+=yr(J,y,!1)}),s+='<hr class="divider"><h3>Connection Map ('+r.connections.length+")</h3>",r.connections.length){const E=r.connections.map(D=>[B(D.sourceName||D.sourceId),B(D.destinationName||D.destinationId),(D.relationships||[]).map(P=>'<span class="badge" style="background:var(--surface2);font-size:0.7rem">'+B(P)+"</span>").join(" "),D.backPressure?B(D.backPressure):"-"]);s+=Kt(["Source","Destination","Relationships","Back Pressure"],E)}r.controllerServices.length&&(s+='<hr class="divider"><h3>Controller Services ('+r.controllerServices.length+")</h3>",s+=Kt(["Name","Type","State","Properties"],r.controllerServices.map(E=>[B(E.name),B(E.type),E.state||"-",Object.keys(E.properties||{}).length+" props"])));const f=Object.entries(r.deepPropertyInventory.nifiEL||{});f.length&&(s+='<hr class="divider"><h3>NiFi Expression Language ('+f.length+")</h3>",s+=Kt(["Expression","Used By"],f.slice(0,50).map(([E,D])=>['<code class="el-highlight">'+B(E.substring(0,80))+"</code>",(Array.isArray(D)?D:[D]).map(P=>B(String(P))).join(", ")])),f.length>50&&(s+='<p style="color:var(--text2);font-size:0.82rem">... and '+(f.length-50)+" more</p>"));const h={TIMER_DRIVEN:0,CRON_DRIVEN:0,EVENT_DRIVEN:0,OTHER:0};r.processors.forEach(E=>{const D=(E.schedulingStrategy||"").toUpperCase();D in h?h[D]++:h.OTHER++}),s+='<hr class="divider"><h3>Scheduling Summary</h3>',s+=at([{label:"Timer Driven",value:h.TIMER_DRIVEN},{label:"Cron Driven",value:h.CRON_DRIVEN},{label:"Event Driven",value:h.EVENT_DRIVEN}]);const _=document.getElementById("analyzeResults");_&&(_.innerHTML=s),Xe({analysis:{blueprint:g,tierData:p,depGraph:o,systems:a}}),setTimeout(()=>{try{Li(p,"analysisTierContainer","analysisTierDetail","analysisTierLegend")}catch(E){console.error("[analyze] Tier diagram render error:",E)}},50),ge("analyze","done"),gt("assess");const v=document.getElementById("assessNotReady"),k=document.getElementById("assessReady");v&&v.classList.add("hidden"),k&&k.classList.remove("hidden")}catch(r){nt(t),$e(new we("Analysis failed: "+r.message,{code:"ANALYZE_FAILED",phase:"analyze",cause:r})),Ve("analyzeResults","Analysis failed: "+r.message),ge("analyze","ready")}}async function Ao(){const e=zn("assess");if(!e.ok){Ve("assessResults","Prerequisites not met: "+e.missing.join(", ")+". Please complete earlier steps first.");return}const n=ce(),t=tt();ge("assess","processing");try{const r=n.parsed._nifi,i=Mr(r),o=Or(r),a=Hn(r),s=i.length,c=i.filter(F=>F.mapped&&F.confidence>=Se.MAPPED),l=i.filter(F=>F.mapped&&F.confidence>0&&F.confidence<Se.MAPPED),d=i.filter(F=>!F.mapped||F.confidence===0),u=i.filter(F=>F.mapped),g=s?c.length/s:0,p=u.length?u.reduce((F,N)=>F+N.confidence,0)/u.length:0,m=s?u.length/s:0,f=Object.keys(a).length,h=Math.max(0,1-f/20),_=Math.round(g*50+p*20+m*20+h*10),v=c.length*.5+l.length*2+d.length*5,k=Math.ceil(v/5),E=_>=75?"green":_>=40?"amber":"red",D=_>=75?"&#x1F7E2;":_>=40?"&#x1F7E1;":"&#x1F534;",P=_>=75?"HIGH READINESS":_>=40?"MODERATE READINESS":"LOW READINESS";let $='<hr class="divider">';$+='<div class="score-big" style="color:var(--'+E+')">'+D+" "+P+" &mdash; "+_+"%</div>",$+=at([{label:"Auto-Convert",value:c.length,color:"var(--green)"},{label:"Manual",value:l.length,color:"var(--amber)"},{label:"Unsupported",value:d.length,color:"var(--red)"},{label:"Effort Est.",value:v.toFixed(0)+" days (~"+k+" wks)"}]),$+=at([{label:"Auto-Convert % (50w)",value:Math.round(g*100)+"%"},{label:"Avg Confidence (20w)",value:Math.round(p*100)+"%"},{label:"Coverage (20w)",value:Math.round(m*100)+"%"},{label:"Simplicity (10w)",value:Math.round(h*100)+"%"}]);const x=s?c.length/s*100:0,S=s?l.length/s*100:0,C=s?d.length/s*100:0;$+='<div class="effort-bar">',x>0&&($+='<div class="effort-seg" style="width:'+x+'%;background:var(--green)">'+c.length+" auto</div>"),S>0&&($+='<div class="effort-seg" style="width:'+S+'%;background:var(--amber)">'+l.length+" manual</div>"),C>0&&($+='<div class="effort-seg" style="width:'+C+'%;background:var(--red)">'+d.length+" unsupported</div>"),$+="</div>";const A={};i.forEach(F=>{const N=(o.fullUpstream[F.name]||[]).length,H=(o.fullDownstream[F.name]||[]).length,Q=N*H;let V="Low",ee="risk-low";Q>=12||N>=4&&H>=4?(V="Critical",ee="risk-critical"):Q>=6||N>=3||H>=3?(V="High",ee="risk-high"):(Q>=2||N>=2||H>=2)&&(V="Med",ee="risk-med"),A[F.name]={risk:V,riskCss:ee,fanIn:N,fanOut:H,connectivity:Q}});const w={};i.forEach(F=>{let N,H;F.role==="source"?(N=1,H="phase-1"):F.role==="sink"?(N=3,H="phase-3"):(N=2,H="phase-2"),w[F.name]={phase:N,phaseCss:H}}),$+='<hr class="divider"><h3>Per-Processor Confidence</h3>';const R=i.map(F=>{const N=F.confidence>=Se.MAPPED?"high":F.confidence>=Se.PARTIAL?"med":"low",H=o.fullUpstream[F.name]||[],Q=o.fullDownstream[F.name]||[],V=ht[F.type],ee=A[F.name]||{risk:"Low",riskCss:"risk-low",fanIn:0,fanOut:0},se=w[F.name]||{phase:2,phaseCss:"phase-2"},te=V?Math.round(V.conf*100):0,ae=Object.keys((r.processors.find(q=>q.name===F.name)||{}).properties||{}).length,ie=Math.min(100,Math.round(ae>0?80:50)),fe=F.role==="source"||F.role==="sink"?90:F.role==="transform"?85:75;let _e='<span class="conf-dot '+N+'"></span>'+Math.round(F.confidence*100)+"%";_e+='<span class="info-tooltip-trigger">?<span class="info-tooltip-content">',_e+="<strong>Confidence Breakdown</strong><br>",_e+="Template: "+te+"%<br>",_e+="Props Coverage: "+ie+"%<br>",_e+="Role Match: "+fe+"%",_e+="</span></span>";let ye;if(F.mapped){ye=B((F.desc||"").substring(0,50));const q=V?"Mapped via "+B(V.cat)+" template. "+B(V.notes||""):"Role-based fallback template";ye+='<span class="info-tooltip-trigger">?<span class="info-tooltip-content">',ye+="<strong>Why this mapping</strong><br>"+q,ye+="</span></span>"}else ye='<em style="color:var(--red)">'+B((F.gapReason||"Unmapped").substring(0,50))+"</em>";const b='<span class="risk-badge '+ee.riskCss+'">'+ee.risk+"</span>",K='<span class="phase-badge '+se.phaseCss+'">Phase '+se.phase+"</span>";return[B(F.name),'<span style="color:'+(_t[F.role]||"#808495")+'">'+F.role+"</span>",B(F.group),_e,ye,b,K,F.confidence>=Se.MAPPED?"0.5d":F.confidence>=Se.PARTIAL?"2d":"5d",H.length+" up / "+Q.length+" down"]});$+=Kt(["Processor","Role","Group","Confidence","Approach","Risk","Phase","Effort","Deps"],R);const y={1:0,2:0,3:0};Object.values(w).forEach(F=>{y[F.phase]++}),$+='<hr class="divider"><h3>Suggested Migration Phases</h3>',$+='<div style="display:flex;gap:16px;flex-wrap:wrap;margin:8px 0">',$+='<div style="flex:1;min-width:140px;padding:12px;background:var(--surface);border-radius:8px;border-left:3px solid #60a5fa">',$+='<div style="font-weight:700;color:#60a5fa">Phase 1: Sources</div>',$+='<div style="font-size:0.85rem;color:var(--text2)">'+y[1]+" processor"+(y[1]!==1?"s":"")+"</div>",$+='<div style="font-size:0.78rem;color:var(--text2);margin-top:4px">Ingestion endpoints, file readers, streaming consumers</div>',$+="</div>",$+='<div style="flex:1;min-width:140px;padding:12px;background:var(--surface);border-radius:8px;border-left:3px solid #c084fc">',$+='<div style="font-weight:700;color:#c084fc">Phase 2: Transforms</div>',$+='<div style="font-size:0.85rem;color:var(--text2)">'+y[2]+" processor"+(y[2]!==1?"s":"")+"</div>",$+='<div style="font-size:0.78rem;color:var(--text2);margin-top:4px">Routing, transformation, processing, enrichment</div>',$+="</div>",$+='<div style="flex:1;min-width:140px;padding:12px;background:var(--surface);border-radius:8px;border-left:3px solid #86efac">',$+='<div style="font-weight:700;color:#86efac">Phase 3: Sinks</div>',$+='<div style="font-size:0.85rem;color:var(--text2)">'+y[3]+" processor"+(y[3]!==1?"s":"")+"</div>",$+='<div style="font-size:0.78rem;color:var(--text2);margin-top:4px">Output writers, database sinks, notification endpoints</div>',$+="</div>",$+="</div>";const O={},J={};i.forEach(F=>{Nr(F.type).forEach(H=>{H.pip.forEach(Q=>{O[Q]||(O[Q]=new Set),O[Q].add(F.name),J[Q]||(J[Q]=H)})})});const M=new Set;if(Object.keys(O).forEach(F=>M.add(F)),M.size){$+='<hr class="divider"><h3>Required Packages ('+M.size+")</h3>";const F={};[...M].forEach(H=>{const Q=H.replace(/[^a-zA-Z]/g,"").toLowerCase();F[Q]||(F[Q]=[]),F[Q].push(H)});const N=Object.values(F).filter(H=>H.length>1);N.length&&($+='<div class="pkg-warn">&#x26A0; Potential package conflicts detected: ',$+=N.map(H=>H.map(Q=>"<code>"+B(Q)+"</code>").join(" vs ")).join("; "),$+=". Review versions carefully.</div>"),$+='<div class="pkg-grid">',[...M].sort().forEach(H=>{const Q=O[H],V=J[H];$+='<div class="pkg-card">',$+='<div class="pkg-name">'+B(H)+"</div>",V&&V.desc&&($+='<div style="font-size:0.72rem;color:var(--text2)">'+B(V.desc)+"</div>"),V&&V.dbx&&($+='<div class="pkg-dbx">'+B(V.dbx)+"</div>"),$+='<div class="pkg-procs">Used by: '+[...Q].map(ee=>B(ee)).join(", ")+"</div>",$+="</div>"}),$+="</div>",$+=`<pre style="background:var(--bg);padding:12px;border-radius:6px;font-size:0.8rem;margin-top:8px"># requirements.txt
`,[...M].sort().forEach(H=>{$+=H+`
`}),$+="</pre>"}const z=document.getElementById("assessResults");z&&(z.innerHTML=$),Xe({assessment:{mappings:i,readinessScore:_,autoCount:c.length,manualCount:l.length,unsupportedCount:d.length,effortDays:v,systems:a,depGraph:o}}),ge("assess","done"),gt("convert");const U=document.getElementById("convertNotReady"),X=document.getElementById("convertReady");U&&U.classList.add("hidden"),X&&X.classList.remove("hidden")}catch(r){nt(t),$e(new we("Assessment failed: "+r.message,{code:"ASSESS_FAILED",phase:"assess",cause:r})),Ve("assessResults","Assessment failed: "+r.message),ge("assess","ready")}}async function Io(){const e=zn("convert");if(!e.ok){Ve("notebookResults","Prerequisites not met: "+e.missing.join(", ")+". Please complete earlier steps first.");return}const n=ce(),t=tt();ge("convert","processing");try{const r=n.parsed._nifi,i=ro(),o=n.assessment?.mappings||Mr(r),s=Po(o,r,n.analysis?n.analysis.blueprint:Lr(n.parsed),i).cells,c=new Set;o.forEach(f=>{Nr(f.type).forEach(h=>h.pip.forEach(_=>c.add(_)))}),c.size&&s.unshift({type:"code",role:"config",label:"Package Requirements",source:`# Install required packages
`+[...c].sort().map(f=>"# MAGIC %pip install "+f).join(`
`)+`
dbutils.library.restartPython()`});const l=Ro(o,r,i);Xe({notebook:{mappings:o,cells:s,workflow:l,config:i}});let d='<hr class="divider">';d+="<h3>Processor Mapping</h3>";const u=o.map(f=>[B(f.name),`<span style="color:${_t[f.role]||"#808495"}">${B(f.role)}</span>`,B(f.group||"â€”"),f.mapped?B(f.desc):'<em style="color:var(--text2)">No equivalent</em>',f.mapped?`<span class="conf-badge ${f.confidence>=.8?"conf-high":f.confidence>=.5?"conf-med":"conf-low"}">${Math.round(f.confidence*100)}%</span>`:'<span class="conf-badge conf-none">â€”</span>']);d+=`<div class="table-scroll"><table class="mapping-table"><thead><tr><th>NiFi Processor</th><th>Role</th><th>Group</th><th>Databricks Equivalent</th><th>Confidence</th></tr></thead><tbody>${u.map(f=>`<tr>${f.map(h=>`<td>${h}</td>`).join("")}</tr>`).join("")}</tbody></table></div>`,d+='<hr class="divider"><h3>Generated Notebook</h3>',d+='<div class="notebook-preview">',s.forEach((f,h)=>{const _=f.label||(f.type==="md"?"markdown":"code"),v=f.role?"lb-"+f.role:"lb-config",k=f.type==="md"?' <span style="opacity:0.5">[md]</span>':f.type==="sql"?' <span style="opacity:0.5">[sql]</span>':"",E=f.source.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;");d+=`<div class="notebook-cell"><div class="cell-label ${v}">[${h+1}] ${_}${k}</div><pre>${E}</pre></div>`}),d+="</div>",d+='<hr class="divider"><div style="display:flex;gap:8px;flex-wrap:wrap">',d+='<button class="btn" id="dlNotebookBtnInline">Download .py Notebook</button>',d+='<button class="btn" id="dlWorkflowBtnInline">Download Workflow JSON</button>',d+="</div>";const g=document.getElementById("notebookResults");if(g){g.innerHTML=d;const f=document.getElementById("dlNotebookBtnInline");f&&f.addEventListener("click",()=>{typeof window.downloadNotebook=="function"&&window.downloadNotebook()});const h=document.getElementById("dlWorkflowBtnInline");h&&h.addEventListener("click",()=>{typeof window.downloadWorkflow=="function"&&window.downloadWorkflow()})}ge("convert","done");const p=document.getElementById("reportNotReady"),m=document.getElementById("reportReady");p&&p.classList.add("hidden"),m&&m.classList.remove("hidden"),gt("report")}catch(r){nt(t),$e(new we("Notebook generation failed: "+r.message,{code:"GENERATE_FAILED",phase:"convert",cause:r})),Ve("notebookResults","Notebook generation failed: "+r.message),ge("convert","ready")}}async function Fo(){const e=zn("report");if(!e.ok){Ve("reportResults","Prerequisites not met: "+e.missing.join(", ")+". Please complete earlier steps first.");return}const n=ce(),t=tt();ge("report","processing");try{const r=n.parsed._nifi,i=Do(n.notebook.mappings,r);Xe({migrationReport:i});const o=i.summary;let a='<hr class="divider">';const s=o.coveragePercent,c=s>=85?"green":s>=60?"amber":"red",l=s>=85?"HIGH COVERAGE":s>=60?"PARTIAL COVERAGE":"LOW COVERAGE";if(a+=`<div class="score-big">${l} â€” ${s}%</div>`,a+=at([["Total Processors",o.totalProcessors],["Mapped",o.mappedProcessors],["Unmapped",o.unmappedProcessors],["Process Groups",o.totalProcessGroups],["Connections",o.totalConnections],["Effort",i.effort]]),a+='<hr class="divider"><h3>Coverage by Role</h3>',["source","route","transform","process","sink","utility"].forEach(m=>{const f=i.byRole[m];if(!f)return;const h=f.total?Math.round(f.mapped/f.total*100):0,_=h>=85?"green":h>=60?"amber":"red",v=_t[m]||"#808495";a+='<div style="margin:8px 0">',a+='<div style="display:flex;justify-content:space-between;align-items:center;margin-bottom:2px">',a+=`<span style="font-weight:600;color:${v};text-transform:uppercase;font-size:0.85rem">${m}</span>`,a+=`<span style="font-size:0.85rem;color:var(--text2)">${f.mapped}/${f.total} (${h}%)</span>`,a+="</div>",a+=`<div class="progress-bar"><div class="progress-fill ${_}" style="width:${h}%"></div></div>`,a+="</div>"}),i.gapPlaybook&&i.gapPlaybook.length?(a+='<hr class="divider"><h3>Gap Analysis &amp; Resolution Playbook</h3>',i.gapPlaybook.forEach(m=>{a+='<div class="gap-card" style="margin-bottom:12px">',a+=`<div class="gap-title">${B(m.name)} <span class="gap-meta">${B(m.type)} &middot; ${B(m.group||"ungrouped")}</span></div>`,a+=`<div class="gap-rec" style="margin-bottom:8px">${B(m.recommendation||"Manual implementation required")}</div>`,m.alternatives&&m.alternatives.length&&(a+='<div style="margin-top:8px;font-size:0.85rem"><strong>Resolution Alternatives:</strong></div>',m.alternatives.forEach((f,h)=>{const _=f.feasibility==="High"?"#4caf50":f.feasibility==="Medium"?"#ff9800":"#f44336";a+=`<div style="margin:6px 0 6px 12px;padding:8px;background:var(--bg2);border-radius:6px;border-left:3px solid ${_}">`,a+='<div style="display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap">',a+=`<strong>${h+1}. ${B(f.alternative)}</strong>`,a+=`<span style="font-size:0.8rem;color:${_}">Feasibility: ${B(f.feasibility)} | Effort: ${B(f.effort)}</span>`,a+="</div>",f.snippet&&(a+=`<pre style="margin:6px 0 0;padding:6px;background:var(--bg3);border-radius:4px;font-size:0.78rem;overflow-x:auto">${B(f.snippet)}</pre>`),a+="</div>"})),a+="</div>"})):i.gaps&&i.gaps.length&&(a+='<hr class="divider"><h3>Gap Analysis</h3>',i.gaps.forEach(m=>{a+='<div class="gap-card">',a+=`<div class="gap-title">${B(m.name)} <span class="gap-meta">${B(m.type)} &middot; ${B(m.group||"ungrouped")}</span></div>`,a+=`<div class="gap-rec">${B(m.recommendation||"Manual implementation required")}</div>`,a+="</div>"})),i.processorGuides&&i.processorGuides.length&&(a+='<hr class="divider"><h3>Per-Processor Migration Guides</h3>',i.processorGuides.forEach(m=>{a+='<div style="margin:8px 0;padding:10px;background:var(--bg2);border-radius:8px;border:1px solid var(--border)">',a+=`<div style="font-weight:700;font-size:0.95rem;margin-bottom:6px">${B(m.name)} <span style="color:var(--text2);font-weight:400">(${B(m.type)})</span></div>`,a+='<div style="display:grid;grid-template-columns:1fr 1fr;gap:8px;font-size:0.85rem">',a+=`<div><strong>NiFi Behavior:</strong><br>${B(m.nifiDescription)}</div>`,a+=`<div><strong>Databricks Approach:</strong><br>${B(m.databricksApproach)}</div>`,a+="</div>",m.keyDifferences&&m.keyDifferences.length&&(a+='<div style="margin-top:6px;font-size:0.82rem"><strong>Key Differences:</strong><ul style="margin:4px 0;padding-left:18px">',m.keyDifferences.forEach(f=>{a+=`<li>${B(f)}</li>`}),a+="</ul></div>"),m.migrationSteps&&m.migrationSteps.length&&(a+='<div style="margin-top:4px;font-size:0.82rem"><strong>Migration Steps:</strong><ol style="margin:4px 0;padding-left:18px">',m.migrationSteps.forEach(f=>{a+=`<li>${B(f)}</li>`}),a+="</ol></div>"),a+=`<div style="margin-top:4px;font-size:0.82rem;color:var(--text2)">Estimated effort: ${B(m.estimatedEffort)}</div>`,a+="</div>"})),i.riskMatrix&&i.riskMatrix.length){a+='<hr class="divider"><h3>Risk &amp; Impact Matrix</h3>',a+='<div class="table-scroll"><table class="mapping-table"><thead><tr>',a+="<th>Processor</th><th>Type</th><th>Role</th><th>Criticality</th><th>Downstream</th><th>Complexity</th><th>Risk Score</th></tr></thead><tbody>",i.riskMatrix.forEach(_=>{const v=_.rating==="red"?"#f44336":_.rating==="amber"?"#ff9800":"#4caf50",k=_.rating==="red"?"rgba(244,67,54,0.08)":_.rating==="amber"?"rgba(255,152,0,0.08)":"rgba(76,175,80,0.08)";a+=`<tr style="background:${k}">`,a+=`<td>${B(_.name)}</td>`,a+=`<td>${B(_.type)}</td>`,a+=`<td>${B(_.role)}</td>`,a+=`<td style="text-align:center">${_.criticalityScore}/5</td>`,a+=`<td style="text-align:center">${B(_.downstreamImpact)}</td>`,a+=`<td style="text-align:center">${_.complexityScore}/5</td>`,a+=`<td style="text-align:center;font-weight:700;color:${v}">${_.totalRiskScore}%</td>`,a+="</tr>"}),a+="</tbody></table></div>";const m=i.riskMatrix.filter(_=>_.rating==="red").length,f=i.riskMatrix.filter(_=>_.rating==="amber").length,h=i.riskMatrix.filter(_=>_.rating==="green").length;a+='<div style="margin-top:8px;font-size:0.85rem;display:flex;gap:16px">',a+=`<span style="color:#f44336;font-weight:600">Critical: ${m}</span>`,a+=`<span style="color:#ff9800;font-weight:600">Moderate: ${f}</span>`,a+=`<span style="color:#4caf50;font-weight:600">Low: ${h}</span>`,a+="</div>"}if(i.timeline){const m=i.timeline;a+='<hr class="divider"><h3>Timeline &amp; Resource Estimation</h3>',a+=at([["Total Hours",m.totalHours+"h"],["Mapped Work",m.mappedHours+"h"],["Gap Work",m.gapHours+"h"],["Testing",m.testingHours+"h"],["Calendar Weeks",m.totalWeeks+"w"],["Team Size",m.recommendedTeamSize]]),a+='<div style="margin-top:12px">';const f=["#2196F3","#4CAF50","#FF9800","#9C27B0","#F44336"];m.phases.forEach((h,_)=>{const v=Math.round(h.weeks/Math.max(m.totalWeeks,1)*100);a+='<div style="margin:8px 0">',a+='<div style="display:flex;justify-content:space-between;font-size:0.85rem;margin-bottom:2px">',a+=`<strong>${B(h.name)}</strong>`,a+=`<span style="color:var(--text2)">${h.weeks} week(s)</span>`,a+="</div>",a+=`<div class="progress-bar"><div class="progress-fill" style="width:${v}%;background:${f[_%f.length]}"></div></div>`,a+='<ul style="margin:4px 0;padding-left:18px;font-size:0.82rem;color:var(--text2)">',h.tasks.forEach(k=>{a+=`<li>${B(k)}</li>`}),a+="</ul></div>"}),a+="</div>",m.teamComposition&&(a+='<div style="margin-top:8px;font-size:0.85rem"><strong>Recommended Team:</strong><ul style="margin:4px 0;padding-left:18px">',m.teamComposition.forEach(h=>{a+=`<li>${B(h)}</li>`}),a+="</ul></div>")}if(i.successCriteria){const m=i.successCriteria;a+='<hr class="divider"><h3>Migration Success Criteria</h3>',a+='<div style="margin:8px 0"><strong style="font-size:0.9rem">Pipeline-Level Acceptance Criteria</strong></div>',a+='<div class="table-scroll"><table class="mapping-table"><thead><tr><th>Metric</th><th>Target</th><th>Description</th></tr></thead><tbody>',m.pipelineCriteria.forEach(h=>{a+=`<tr><td style="font-weight:600">${B(h.metric)}</td><td>${B(h.target)}</td><td style="font-size:0.85rem">${B(h.description)}</td></tr>`}),a+="</tbody></table></div>",a+='<div style="margin:12px 0 8px"><strong style="font-size:0.9rem">Data Comparison Metrics</strong></div>',a+='<div class="table-scroll"><table class="mapping-table"><thead><tr><th>Metric</th><th>Method</th><th>Threshold</th></tr></thead><tbody>',m.comparisonMetrics.forEach(h=>{a+=`<tr><td style="font-weight:600">${B(h.metric)}</td><td style="font-size:0.85rem">${B(h.description)}</td><td>${B(h.threshold)}</td></tr>`}),a+="</tbody></table></div>";const f=m.processorCriteria.slice(0,8);f.length&&(a+=`<div style="margin:12px 0 8px"><strong style="font-size:0.9rem">Processor-Level Acceptance Tests</strong> <span style="font-size:0.82rem;color:var(--text2)">(showing ${f.length} of ${m.processorCriteria.length})</span></div>`,f.forEach(h=>{a+='<div style="margin:4px 0;padding:6px;background:var(--bg2);border-radius:4px;font-size:0.85rem">',a+=`<strong>${B(h.name)}</strong> <span style="color:var(--text2)">(${B(h.type)})</span>`,a+='<ul style="margin:2px 0;padding-left:16px">',h.acceptanceTests.forEach(_=>{a+=`<li>${B(_)}</li>`}),a+="</ul></div>"}))}i.recommendations&&i.recommendations.length&&(a+='<hr class="divider"><h3>Recommendations</h3>',a+='<ul style="margin:0;padding-left:20px">',i.recommendations.forEach(m=>{a+=`<li style="margin:4px 0">${B(m)}</li>`}),a+="</ul>"),a+='<hr class="divider"><div style="display:flex;gap:8px;flex-wrap:wrap;align-items:center">',a+='<button class="btn" id="dlReportBtnInline">Download Report (Markdown)</button>',a+="</div>";const u=document.getElementById("reportResults");if(u){u.innerHTML=a;const m=document.getElementById("dlReportBtnInline");m&&m.addEventListener("click",()=>{typeof window.downloadReport=="function"&&window.downloadReport()})}ge("report","done");const g=document.getElementById("reportFinalNotReady"),p=document.getElementById("reportFinalReady");g&&g.classList.add("hidden"),p&&p.classList.remove("hidden"),gt("reportFinal")}catch(r){nt(t),$e(new we("Report generation failed: "+r.message,{code:"REPORT_FAILED",phase:"report",cause:r})),Ve("reportResults","Report generation failed: "+r.message),ge("report","ready")}}function ul(e,n="jsonExplorer"){let t="";return t+=`<div class="json-explorer" id="${B(n)}">`,t+='<div class="json-explorer-toolbar">',t+=`<input type="text" class="json-search-input" placeholder="Search keys or values..." data-json-search="${B(n)}" />`,t+=`<span class="json-search-count" id="${B(n)}-search-count"></span>`,t+=`<button class="btn btn-secondary json-expand-all" data-json-expand-all="${B(n)}" style="padding:4px 10px;font-size:0.75rem">Expand All</button>`,t+=`<button class="btn btn-secondary json-collapse-all" data-json-collapse-all="${B(n)}" style="padding:4px 10px;font-size:0.75rem">Collapse All</button>`,t+="</div>",t+='<div class="json-tree-container">',t+=vr(e,"",0),t+="</div>",t+=`<div class="json-path-toast" id="${B(n)}-path-toast"></div>`,t+="</div>",t}function vr(e,n,t,r){if(e===null)return`<span class="json-null" data-json-path="${B(n)}">null</span>`;if(e===void 0)return`<span class="json-null" data-json-path="${B(n)}">undefined</span>`;const i=typeof e;if(i==="string"){const o=e.length>200?e.substring(0,200)+"...":e;return`<span class="json-string" data-json-path="${B(n)}">"${B(o)}"</span>`}if(i==="number")return`<span class="json-number" data-json-path="${B(n)}">${e}</span>`;if(i==="boolean")return`<span class="json-boolean" data-json-path="${B(n)}">${e}</span>`;if(Array.isArray(e)){if(e.length===0)return`<span class="json-bracket" data-json-path="${B(n)}">[]</span>`;const o=t<2;let a=`<span class="json-toggle ${o?"open":""}" data-json-toggle data-json-path="${B(n)}">`;return a+='<span class="json-arrow">&#9654;</span>',a+='<span class="json-bracket">[</span>',a+=`<span class="json-count">${e.length} items</span>`,a+="</span>",a+=`<div class="json-children ${o?"":"hidden"}">`,e.forEach((s,c)=>{const l=n+"["+c+"]";a+=`<div class="json-entry" style="padding-left:${(t+1)*16}px">`,a+=`<span class="json-index">${c}: </span>`,a+=vr(s,l,t+1),c<e.length-1&&(a+='<span class="json-comma">,</span>'),a+="</div>"}),a+="</div>",a+=`<span class="json-bracket json-close-bracket ${o?"":"hidden"}">]</span>`,a}if(i==="object"){const o=Object.keys(e);if(o.length===0)return`<span class="json-bracket" data-json-path="${B(n)}">{}</span>`;const a=t<2;let s=`<span class="json-toggle ${a?"open":""}" data-json-toggle data-json-path="${B(n)}">`;return s+='<span class="json-arrow">&#9654;</span>',s+='<span class="json-bracket">{</span>',s+=`<span class="json-count">${o.length} keys</span>`,s+="</span>",s+=`<div class="json-children ${a?"":"hidden"}">`,o.forEach((c,l)=>{const d=n?n+"."+c:c;s+=`<div class="json-entry" style="padding-left:${(t+1)*16}px">`,s+=`<span class="json-key" data-json-path="${B(d)}">"${B(c)}"</span>`,s+='<span class="json-colon">: </span>',s+=vr(e[c],d,t+1),l<o.length-1&&(s+='<span class="json-comma">,</span>'),s+="</div>"}),s+="</div>",s+=`<span class="json-bracket json-close-bracket ${a?"":"hidden"}">}</span>`,s}return`<span>${B(String(e))}</span>`}function fl(){typeof document>"u"||(document.addEventListener("click",e=>{const n=e.target.closest("[data-json-toggle]");if(n){e.preventDefault(),n.classList.toggle("open");const o=n.nextElementSibling;if(o&&o.classList.contains("json-children")){o.classList.toggle("hidden");const a=o.nextElementSibling;a&&a.classList.contains("json-close-bracket")&&a.classList.toggle("hidden")}return}const t=e.target.closest("[data-json-path]");if(t&&t.dataset.jsonPath){const o=t.dataset.jsonPath,a=t.closest(".json-explorer");if(a){const s=a.querySelector(".json-path-toast");s&&navigator.clipboard.writeText(o).then(()=>{s.textContent="Copied: "+o,s.classList.add("visible"),setTimeout(()=>s.classList.remove("visible"),2e3)}).catch(()=>{s.textContent=o,s.classList.add("visible"),setTimeout(()=>s.classList.remove("visible"),2e3)})}return}const r=e.target.closest("[data-json-expand-all]");if(r){const o=r.dataset.jsonExpandAll,a=document.getElementById(o);a&&(a.querySelectorAll(".json-toggle").forEach(s=>s.classList.add("open")),a.querySelectorAll(".json-children").forEach(s=>s.classList.remove("hidden")),a.querySelectorAll(".json-close-bracket").forEach(s=>s.classList.remove("hidden")));return}const i=e.target.closest("[data-json-collapse-all]");if(i){const o=i.dataset.jsonCollapseAll,a=document.getElementById(o);a&&(a.querySelectorAll(".json-toggle").forEach(s=>s.classList.remove("open")),a.querySelectorAll(".json-children").forEach(s=>s.classList.add("hidden")),a.querySelectorAll(".json-close-bracket").forEach(s=>s.classList.add("hidden")));return}}),document.addEventListener("input",e=>{if(!e.target.matches("[data-json-search]"))return;const n=e.target.value.toLowerCase().trim(),t=e.target.dataset.jsonSearch,r=document.getElementById(t);if(!r)return;const i=document.getElementById(t+"-search-count"),o=r.querySelectorAll(".json-entry");if(!n){o.forEach(s=>{s.style.display="",s.querySelectorAll(".json-highlight").forEach(c=>{c.classList.remove("json-highlight")})}),i&&(i.textContent="");return}let a=0;o.forEach(s=>{if(s.textContent.toLowerCase().includes(n)){s.style.display="",a++;let l=s.parentElement;for(;l&&!l.classList.contains("json-explorer");){if(l.classList.contains("json-children")&&l.classList.contains("hidden")){l.classList.remove("hidden");const d=l.previousElementSibling;d&&d.classList.add("open");const u=l.nextElementSibling;u&&u.classList.contains("json-close-bracket")&&u.classList.remove("hidden")}l=l.parentElement}}else s.style.display="none"}),i&&(i.textContent=a+" match"+(a!==1?"es":""))}))}const ls=["load","analyze","assess","convert","report","reportFinal","validate","value"];function ml(){if(typeof document>"u")return;const e=document.getElementById("tabs");if(!e)return;const n=document.createElement("div");n.className="step-progress-bar",n.id="stepProgressBar",n.innerHTML=`
    <span class="step-progress-label">Progress</span>
    <div class="step-progress-track">
      <div class="step-progress-fill" id="stepProgressFill" style="width:0%"></div>
    </div>
    <span class="step-progress-pct" id="stepProgressPct">0%</span>
  `,e.parentNode.insertBefore(n,e.nextSibling)}function lt(){if(typeof document>"u")return;let e=0;ls.forEach(i=>{const o=document.querySelector(`.tab[data-tab="${i}"]`);o&&o.classList.contains("done")&&e++});const n=Math.round(e/ls.length*100),t=document.getElementById("stepProgressFill"),r=document.getElementById("stepProgressPct");t&&(t.style.width=n+"%"),r&&(r.textContent=n+"%")}function Lo(e,n){const t={deadEnds:[],orphans:[],circularRefs:[],disconnected:[],backpressure:[]};if(!e||!n)return t;const r={},i={};e.forEach(p=>{r[p.id]=p,i[p.name]=p});const o={},a={};e.forEach(p=>{o[p.id]=[],a[p.id]=[]}),n.forEach(p=>{const m=p.sourceId||p.source&&p.source.id,f=p.destinationId||p.destination&&p.destination.id;m&&f&&o[m]&&a[f]&&(o[m].push(f),a[f].push(m))});const s=/^(Put|Log|Publish|Send|Notify|PutEmail|PutSlack)/;e.forEach(p=>{o[p.id]&&o[p.id].length===0&&!s.test(p.type)&&t.deadEnds.push({name:p.name,type:p.type,id:p.id})});const c=/^(Get|List|Listen|Consume|Generate|TailFile|HandleHttp)/;e.forEach(p=>{a[p.id]&&a[p.id].length===0&&!c.test(p.type)&&t.orphans.push({name:p.name,type:p.type,id:p.id})});const l=new Set,d=new Set,u=new Set;function g(p,m,f=0){if(!(f>2e3)){if(d.has(p)){const h=m.concat(p).map(v=>r[v]?r[v].name:v),_=[...h].sort().join("|");return u.has(_)||(u.add(_),t.circularRefs.push({cycle:h})),!0}if(l.has(p))return!1;l.add(p),d.add(p);for(const h of o[p]||[])g(h,m.concat(p),f+1);return d.delete(p),!1}}return e.forEach(p=>{l.has(p.id)||g(p.id,[])}),e.forEach(p=>{o[p.id]&&o[p.id].length===0&&a[p.id]&&a[p.id].length===0&&t.disconnected.push({name:p.name,type:p.type,id:p.id})}),t}const hl=[{name:"SQL Injection",regex:/(\bDROP\s+TABLE\b|\bUNION\s+SELECT\b|\bxp_cmdshell\b|\bEXEC\s+sp_|\bDELETE\s+FROM\b.*;\s*--|;\s*DROP\b|'\s*OR\s+'[^']*'\s*=\s*')/i,severity:"CRITICAL"},{name:"Command Injection",regex:/(rm\s+-rf\s+\/|curl\s+evil|wget\s+.*\|\s*bash|\/bin\/sh\s+-[ic]|exec\s*\(|system\s*\(|subprocess)/i,severity:"CRITICAL"},{name:"SSRF",regex:/(169\.254\.169\.254|metadata\.google\.internal|localhost:\d{4}\/admin)/i,severity:"HIGH"},{name:"Hardcoded Secret",regex:/(password\s*=\s*['"][^'"]{3,}|secret\s*=\s*['"][^'"]{3,}|api_key\s*=\s*['"][^'"]{3,})/i,severity:"HIGH"},{name:"Reverse Shell",regex:/(\/dev\/tcp\/|nc\s+-[el]|ncat\s|socket\.SOCK_STREAM.*connect|bash\s+-i\s*>&)/i,severity:"CRITICAL"},{name:"Path Traversal",regex:/(\.\.\/\.\.\/|\/etc\/passwd|\/etc\/shadow|\/root\/)/i,severity:"MEDIUM"},{name:"Dangerous File Access",regex:/(\.pem|\.key|\.crt|credentials\.json|\.env\b)/i,severity:"MEDIUM"},{name:"Perl Injection",regex:/(use\s+IO::Socket|eval\s*\(|open\s*\(\s*F\s*,\s*'\|)/i,severity:"CRITICAL"},{name:"Java Exploitation",regex:/(Runtime\.getRuntime\(\)|ProcessBuilder|jndi:ldap|Class\.forName)/i,severity:"CRITICAL"},{name:"Data Exfiltration",regex:/(LOAD_FILE|INTO\s+OUTFILE|COPY\s+.*\s+TO\s+'\/|UTL_HTTP\.REQUEST)/i,severity:"HIGH"}];function gl(e){const n=[];return e.forEach(t=>{const r=Object.values(t.properties||{}).join(" ");hl.forEach(i=>{const o=i.regex.exec(r);o&&n.push({processor:t.name,type:t.type,finding:i.name,severity:i.severity,snippet:o[0].substring(0,100)})})}),n}const _l=["minute","hour","day-of-month","month","day-of-week"],yl=["second","minute","hour","day-of-month","month","day-of-week"],vn=[null,"January","February","March","April","May","June","July","August","September","October","November","December"],kn=["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"];function bl(e,n){if(e==="*")return`every ${n}`;if(e==="?")return`any ${n}`;if(e.includes("/")){const[t,r]=e.split("/");return t==="*"?`every ${r} ${n}s`:`every ${r} ${n}s starting at ${t}`}if(e.includes("-")&&!e.includes(",")){const[t,r]=e.split("-");return n==="day-of-week"?`${kn[t]||t} through ${kn[r]||r}`:n==="month"?`${vn[t]||t} through ${vn[r]||r}`:`${n}s ${t} through ${r}`}if(e.includes(",")){const t=e.split(",");return n==="day-of-week"?t.map(r=>kn[r]||r).join(", "):n==="month"?t.map(r=>vn[r]||r).join(", "):`${n}s ${t.join(", ")}`}return n==="day-of-week"?kn[e]||`day ${e}`:n==="month"?vn[e]||`month ${e}`:`at ${n} ${e}`}function vl(e){if(typeof e!="string")return{valid:!1,description:"Invalid CRON expression",fields:{}};const n=e.trim().split(/\s+/);if(n.length<5||n.length>7)return{valid:!1,description:`Invalid CRON expression (${n.length} fields, expected 5-6)`,fields:{}};const r=n.length>=6?yl:_l,i={},o=[];for(let s=0;s<r.length&&s<n.length;s++)i[r[s]]=n[s],n[s]!=="*"&&n[s]!=="?"&&o.push(bl(n[s],r[s]));return{valid:!0,description:o.length>0?o.join(", "):"every minute (all wildcards)",fields:i}}function kl(e){const n=[],t=[],r=[];return(e||[]).forEach(i=>{const o=i.schedulingStrategy||"TIMER_DRIVEN",a=i.schedulingPeriod||"0 sec";if(o==="CRON_DRIVEN"){const s=vl(a);t.push({processor:i.name,type:i.type,group:i.group||"(root)",expression:a,...s})}else o==="EVENT_DRIVEN"?r.push({processor:i.name,type:i.type,group:i.group||"(root)"}):n.push({processor:i.name,type:i.type,group:i.group||"(root)",period:a,isZero:a==="0 sec"||a==="0 ms"||a==="0"})}),{timers:n,cronSchedules:t,eventDriven:r,summary:{totalProcessors:(e||[]).length,timerDriven:n.length,cronDriven:t.length,eventDrivenCount:r.length,zeroPeriodCount:n.filter(i=>i.isZero).length}}}function Sl(e){const n={filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set};return(e||[]).forEach(t=>{const r=t.properties||{},i=Object.values(r),o=Object.keys(r);for(let a=0;a<i.length;a++){const s=i[a];if(!s||typeof s!="string")continue;const c=s.length;if(c>3&&s.includes("/")){const l=s.match(Ue.filePath);l&&l.forEach(d=>{d.length>3&&!/^\/\//.test(d)&&(n.filePaths[d]||(n.filePaths[d]=[]),n.filePaths[d].push({processor:t.name,group:t.group,property:o[a]}))})}if(c>8&&/https?:/.test(s)){const l=s.match(Ue.urlPattern);l&&l.forEach(d=>{n.urls[d]||(n.urls[d]=[]),n.urls[d].push({processor:t.name,group:t.group,property:o[a]})})}if(c>10&&s.startsWith("jdbc:")){const l=s.match(Ue.jdbcUrl);l&&l.forEach(d=>{n.jdbcUrls[d]||(n.jdbcUrls[d]=[]),n.jdbcUrls[d].push({processor:t.name,group:t.group,property:o[a]})})}if(c>3&&s.includes("${")){const l=s.match(Ue.nifiEL);l&&l.forEach(d=>{const u=d.slice(2,-1);n.nifiEL[u]||(n.nifiEL[u]=[]),n.nifiEL[u].push({processor:t.name,group:t.group,property:o[a]})})}if(c>8&&Ue.cronExpr.test(s)&&(n.cronExprs[s.trim()]||(n.cronExprs[s.trim()]=[]),n.cronExprs[s.trim()].push({processor:t.name,group:t.group,property:o[a]})),Ue.credentialKey.test(o[a])){const l=s.length>3?s.substring(0,2)+"***"+s.substring(s.length-1):"***";n.credentialRefs[o[a]]||(n.credentialRefs[o[a]]=[]),n.credentialRefs[o[a]].push({processor:t.name,group:t.group,value:l})}if(c>5){Ue.hostPort.lastIndex=0;let l;for(;(l=Ue.hostPort.exec(s))!==null;){const d=l[2]?l[1]+":"+l[2]:l[1];!/\.(css|js|html|png|jpg|gif|svg|ico|woff|ttf|eot)$/i.test(d)&&!/^(www\.|http|example\.com|localhost)/i.test(l[1])&&(n.hostPorts[d]||(n.hostPorts[d]=[]),n.hostPorts[d].push({processor:t.name,group:t.group,property:o[a]}))}}if(Ue.dataFormat.test(s)){const l=s.match(Ue.dataFormat);l&&l.forEach(d=>n.dataFormats.add(d.toLowerCase()))}}t.schedulingStrategy==="CRON_DRIVEN"&&t.schedulingPeriod&&(n.cronExprs[t.schedulingPeriod]||(n.cronExprs[t.schedulingPeriod]=[]),n.cronExprs[t.schedulingPeriod].push({processor:t.name,group:t.group,property:"schedulingPeriod"}))}),n}function wl(e){e.deepPropertyInventory||(e.deepPropertyInventory=Sl(e.processors));const n=Or(e),t=Hn(e),r=ho(n.downstream),i=Co(e),o=Lo(e.processors,e.connections),a=gl(e.processors||[]),s=[];(e.processors||[]).forEach(d=>{const u=$o(d.properties);u.length>0&&s.push({processor:d.name,type:d.type,group:d.group,fields:u})});const c=kl(e.processors);let l=null;return e.tables&&e.tables.length>0&&(l=Lr(e)),{dependencyGraph:n,externalSystems:t,cycles:r,manifest:i,flowGraph:o,securityFindings:a,phiResults:s,scheduling:c,blueprint:l,deepPropertyInventory:e.deepPropertyInventory}}function El(e){const{mappings:n,nifi:t,blueprint:r,config:i}=e,o=Po(n,t,r,i),a=Ro(n,t,i);return{notebook:o,workflow:a}}async function xl({processors:e,mappings:n,mappingByName:t,allCellTextLower:r,batchSize:i=100,onProgress:o}){const a=[];e.forEach(g=>{try{const p=Pe(g.type);let m="";const f=g.properties||{};if(p==="source"){const _=Object.values(f).filter(v=>v&&typeof v=="string").join(" ").match(/\b(s3|hdfs|kafka|jdbc|file|ftp|sftp|http)/i);m="INGEST data"+(_?" from "+_[0].toUpperCase():"")}else if(p==="sink")m="WRITE/OUTPUT data"+(f.Directory?" to "+f.Directory:"")+(f["Topic Name"]?" to Kafka:"+f["Topic Name"]:"");else if(p==="transform")m="TRANSFORM data"+(g.type.includes("JSON")?" (JSON)":g.type.includes("SQL")?" (SQL)":g.type.includes("Attribute")?" (attributes)":"");else if(p==="route"){const h=Object.keys(f).filter(_=>_!=="Routing Strategy").length;m="ROUTE/BRANCH"+(h>0?" ("+h+" conditions)":"")}else p==="process"?m="PROCESS data ("+g.type.replace(/^org\.apache\.nifi\.processors?\.\w+\./,"")+")":m="UTILITY ("+g.type.replace(/^org\.apache\.nifi\.processors?\.\w+\./,"")+")";a.push({name:g.name,type:g.type,role:p,intent:m,props:f})}catch(p){console.warn("[intent-analyzer] Error processing "+(g.name||"unknown")+":",p),a.push({name:g.name||"unknown",type:g.type||"unknown",role:"unknown",intent:"ERROR: could not classify",props:g.properties||{}})}});let s=0,c=0,l=0;const d=[];for(let g=0;g<a.length;g+=i)a.slice(g,g+i).forEach(m=>{const f=t[m.name];if(!f){l++,d.push({proc:m.name,type:m.type,intent:m.intent,issue:"No mapping found â€” processor entirely missing from notebook"});return}if(!f.mapped){l++,d.push({proc:m.name,type:m.type,intent:m.intent,issue:"Unmapped â€” no Databricks equivalent generated"});return}const h=f.name.replace(/[^a-zA-Z0-9]/g,"_").toLowerCase();if(!(r.includes(h)||r.includes(m.name.toLowerCase()))){c++,d.push({proc:m.name,type:m.type,intent:m.intent,issue:"Mapped but no dedicated notebook cell references this processor"});return}f.confidence>=Se.MAPPED?s++:(c++,d.push({proc:m.name,type:m.type,intent:m.intent,issue:"Low confidence ("+Math.round(f.confidence*100)+"%) â€” intent may not be fully preserved"}))}),o&&o(8+Math.round(g/a.length*20),"Intent analysis: "+Math.min(g+i,a.length)+"/"+a.length+" processors..."),await new Promise(m=>setTimeout(m,0));const u=a.length?Math.round(s/a.length*100):0;return{nifiIntents:a,intentMatched:s,intentPartial:c,intentMissing:l,intentGaps:d,intentScore:u}}const Cl=70,$l=30;async function Pl({mappings:e,procByName:n,cellTextsLower:t,findCellsWithVar:r,batchSize:i=100,onProgress:o}){const a=[];let s=0,c=0;for(let d=0;d<e.length;d+=i)e.slice(d,d+i).forEach(g=>{const p=g.name.replace(/[^a-zA-Z0-9]/g,"_").toLowerCase(),m=r(p);if(m.length===0){const $=g.name.toLowerCase(),x=r($);x.length>0&&m.push(...x)}const f=n[g.name],h=f?f.properties||{}:{},_=Object.entries(h).filter(([$,x])=>x&&!/password|secret|token/i.test($));let v=0,k=[];if(m.length>0){const $=m.map(x=>t[x]).join(`
`);_.forEach(([x,S])=>{const C=x.toLowerCase().replace(/[\s-]/g,""),A=String(S).toLowerCase().substring(0,50);$.includes(C)||$.includes(A)||$.includes(x.toLowerCase())?v++:k.push(x)})}const D=(_.length>0?Math.round(v/_.length*100):100)||0;k||(k=[]);const P=g.mapped?m.length===0?"no-cell":D>=Cl?"good":D>=$l?"partial":"weak":"missing";P==="good"?s++:c++,a.push({name:g.name,type:g.type,role:g.role,mapped:g.mapped,cellCount:m.length,cellIndices:m,propTotal:_.length,propsInCode:v,propCoverage:D||0,propsMissing:k||[],status:P,confidence:g.confidence,desc:g.desc})}),o&&o(30+Math.round(d/e.length*25),"Line validation: "+Math.min(d+i,e.length)+"/"+e.length+" mappings..."),await new Promise(g=>setTimeout(g,0));const l=e.length?Math.round(s/e.length*100):0;return{lineResults:a,lineMatched:s,lineGaps:c,lineScore:l}}async function Rl({nifi:e,systems:n,allCellTextLower:t,onProgress:r}){let i=0;const o=[],a=e.connections.length,s=e.processors.length;let c=0;e.connections.forEach(x=>{const S=(x.sourceName||"").replace(/[^a-zA-Z0-9]/g,"_").toLowerCase(),C=(x.destinationName||"").replace(/[^a-zA-Z0-9]/g,"_").toLowerCase();S&&C&&t.includes(S)&&t.includes(C)&&c++});const l=a?Math.round(c/a*100):100;o.push({label:"Flow Topology Preserved",score:l,detail:c+"/"+a+" connections reflected in variable chaining"}),r&&r(65,"Checking processor type identifiability..."),await new Promise(x=>setTimeout(x,0));let d=0;e.processors.forEach(x=>{const S=x.type.split(".").pop().toLowerCase();(t.includes(S)||t.includes("nifi: "+S)||t.includes(x.type.toLowerCase()))&&d++});const u=s?Math.round(d/s*100):100;o.push({label:"Processor Types Identifiable",score:u,detail:d+"/"+s+" NiFi types referenced in notebook comments/code"});let g=0,p=0;e.processors.forEach(x=>{x.schedulingPeriod&&x.schedulingPeriod!=="0 sec"&&((t.includes(x.schedulingPeriod.toLowerCase())||t.includes("trigger"))&&g++,p++)});const m=p?Math.round(g/p*100):100;o.push({label:"Scheduling Parameters Preserved",score:m,detail:g+"/"+p+" scheduling configs captured"}),r&&r(72,"Checking controller services and external systems..."),await new Promise(x=>setTimeout(x,0));const f=e.controllerServices.length;let h=0;e.controllerServices.forEach(x=>{const S=x.name.replace(/[^a-zA-Z0-9]/g,"_").toLowerCase();(t.includes(S)||t.includes(x.name.toLowerCase()))&&h++});const _=f?Math.round(h/f*100):100;o.push({label:"Controller Services Referenced",score:_,detail:h+"/"+f+" controller services found in notebook"});const v=Object.keys(n);let k=0;v.forEach(x=>{t.includes(x.toLowerCase())&&k++});const E=v.length?Math.round(k/v.length*100):100;o.push({label:"External Systems Connected",score:E,detail:k+"/"+v.length+" external systems referenced in code"});const D=[];e.processors.forEach(x=>{x.autoTerminatedRelationships&&x.autoTerminatedRelationships.length>0&&D.push({name:x.name,rels:x.autoTerminatedRelationships})});const P=t.includes("try:")||t.includes("except")||t.includes('.option("failonerror"');let $=0;return P&&($+=50),D.length>0&&($+=Math.min(D.length*10,30)),(t.includes("dead_letter")||t.includes("dlq"))&&($+=20),$=Math.min($,100),o.push({label:"Error Handling Coverage",score:$,detail:P?"Try/except or error options found in notebook":"No explicit error handling â€” "+D.length+" processors have auto-terminated failure relationships"}),i=Math.round(o.reduce((x,S)=>x+S.score,0)/o.length),{reScore:i,reChecks:o}}async function Dl({mappings:e,procByName:n,batchSize:t=100,onProgress:r}){const i=[];let o=0,a=0,s=0;for(let l=0;l<e.length;l+=t)e.slice(l,l+t).forEach(u=>{const g=n[u.name];if(!g)return;const p=Pe(u.type),m=g.properties||{},f=[];p==="source"&&f.push("Data ingestion"),p==="sink"&&f.push("Data output/write"),p==="transform"&&f.push("Data transformation"),p==="route"&&f.push("Conditional routing"),p==="process"&&f.push("Data processing"),p==="utility"&&f.push("Utility"),(u.type.includes("SQL")||u.type.includes("Sql"))&&f.push("SQL execution"),(u.type.includes("JSON")||u.type.includes("Json"))&&f.push("JSON processing"),u.type.includes("Avro")&&f.push("Avro serialization"),(u.type.includes("CSV")||u.type.includes("Csv"))&&f.push("CSV processing"),u.type.includes("Kafka")&&f.push("Kafka messaging"),(u.type.includes("HDFS")||u.type.includes("Hdfs"))&&f.push("HDFS file operations"),(u.type.includes("S3")||u.type.includes("AWS"))&&f.push("S3/AWS operations"),(u.type.includes("Encrypt")||u.type.includes("Hash"))&&f.push("Encryption/hashing"),(u.type.includes("HTTP")||u.type.includes("Http"))&&f.push("HTTP communication"),(u.type.includes("Merge")||u.type.includes("Split"))&&f.push("Data merge/split"),u.type.includes("Attribute")&&f.push("Attribute manipulation"),(u.type.includes("Wait")||u.type.includes("Notify"))&&f.push("Flow coordination"),u.type.includes("Kudu")&&f.push("Kudu table operations");const h=Object.entries(m).filter(([E,D])=>D&&String(D).includes("${"));h.length&&f.push("NiFi Expression Language ("+h.length+" expressions)");let _=[],v="missing";u.mapped&&u.confidence>=Se.MAPPED?(v="mapped",o++,u.desc&&_.push(u.desc)):u.mapped?(v="partial",a++,u.desc&&_.push(u.desc+" (low confidence)")):s++;const k=Nr(u.type);i.push({name:u.name,type:u.type,role:p,nifiFunctions:f,dbxFunctions:_,status:v,confidence:u.confidence,packages:k,elCount:h.length})}),r&&r(78+Math.round(l/e.length*15),"Function mapping: "+Math.min(l+t,e.length)+"/"+e.length+"..."),await new Promise(u=>setTimeout(u,0));const c=e.length?Math.round((o+a*.5)/e.length*100):0;return{funcResults:i,funcMapped:o,funcPartial:a,funcMissing:s,funcScore:c}}function Tl({intentGaps:e,lineGapItems:n,nifiDatabricksMap:t}){const r=[...e];n.forEach(a=>{if(!r.some(s=>s.proc===a.name)){const s=(a.type||"").split(".").pop()||"Unknown",c=(a.propsMissing||[]).slice(0,3).join(", "),l=typeof a.propCoverage=="number"?Math.round(a.propCoverage):0;let d;a.status==="missing"?d=`No Databricks mapping found for ${s}`:a.status==="no-cell"?d="Mapped but no notebook cell generated for this processor":d=`Low property coverage (${l}%)${c?" â€” missing: "+c:""}`,r.push({proc:a.name||"Unknown",type:a.type||"",intent:"",role:a.role||"",confidence:a.confidence,propCoverage:a.propCoverage,issue:d})}});const i={};r.forEach(a=>{const s=a.type.split(".").pop();i[s]||(i[s]=[]),i[s].push(a)});const o={};return Object.entries(i).sort((a,s)=>s[1].length-a[1].length).forEach(([a,s])=>{const c=s.some(d=>d.issue.includes("Missing")||d.issue.includes("Unmapped")||d.issue.includes("No mapping"))?"HIGH":"MEDIUM",l=t?t[a]:null;o[a]={gaps:s,severity:c,mapEntry:l}}),{allGaps:r,gapsByType:o}}async function ds({nifi:e,mappings:n,cells:t,systems:r,nifiDatabricksMap:i,onProgress:o}){const a=o||(()=>{});a(2,"Building lookup indexes..."),await new Promise(P=>setTimeout(P,0));const s={};e.processors.forEach(P=>{s[P.name]=P});const c={};n.forEach(P=>{c[P.name]=P});const l=t.map(P=>(P.source||"").toLowerCase()),d=l.join(`
`);function u(P){const $=[];for(let x=0;x<l.length;x++)l[x].includes(P)&&$.push(x);return $}a(5,"Building connection graph..."),await new Promise(P=>setTimeout(P,0));const g={};e.connections.forEach(P=>{g[P.sourceName]||(g[P.sourceName]=[]),g[P.sourceName].push(P)});const p=[];let m;a(8,"Running intent analysis ("+e.processors.length+" processors)...");try{m=await xl({processors:e.processors,mappings:n,mappingByName:c,allCellTextLower:d,onProgress:a})}catch(P){$e(new we("Intent analysis failed: "+P.message,{code:"VALIDATE_INTENT_MISMATCH",phase:"validate",severity:"high",cause:P})),p.push({phase:"intent",message:P.message}),m={nifiIntents:[],intentMatched:0,intentPartial:0,intentMissing:0,intentGaps:[],intentScore:0}}let f;a(30,"Running line validation ("+n.length+" mappings)...");try{f=await Pl({mappings:n,procByName:s,cellTextsLower:l,findCellsWithVar:u,onProgress:a})}catch(P){$e(new we("Line validation failed: "+P.message,{code:"VALIDATE_SCHEMA_VIOLATION",phase:"validate",severity:"high",cause:P})),p.push({phase:"line",message:P.message}),f={lineResults:[],lineMatched:0,lineGaps:0,lineScore:0}}let h;a(58,"Running reverse engineering readiness checks...");try{h=await Rl({nifi:e,systems:r,allCellTextLower:d,onProgress:a})}catch(P){$e(new we("Reverse engineering check failed: "+P.message,{code:"VALIDATE_SCHEMA_VIOLATION",phase:"validate",severity:"high",cause:P})),p.push({phase:"reverse-engineering",message:P.message}),h={reChecks:[],reScore:0}}let _;a(78,"Running function mapping analysis...");try{_=await Dl({mappings:n,procByName:s,onProgress:a})}catch(P){$e(new we("Function mapping failed: "+P.message,{code:"VALIDATE_INTENT_MISMATCH",phase:"validate",severity:"high",cause:P})),p.push({phase:"function-mapping",message:P.message}),_={funcResults:[],funcMapped:0,funcPartial:0,funcMissing:0,funcScore:0}}a(95,"Computing overall score..."),await new Promise(P=>setTimeout(P,0));const v=Math.round((m.intentScore+f.lineScore+h.reScore+_.funcScore)/4),k=f.lineResults.filter(P=>P.status!=="good"),E=Tl({intentGaps:m.intentGaps,lineGapItems:k,nifiDatabricksMap:i});a(97,"Checking notebook imports...");const D=Al(t);return a(100,"Validation complete!"),{overallScore:v,intentScore:m.intentScore,lineScore:f.lineScore,reScore:h.reScore,funcScore:_.funcScore,intentGaps:m.intentGaps,nifiIntents:m.nifiIntents,intentMatched:m.intentMatched,intentPartial:m.intentPartial,intentMissing:m.intentMissing,lineResults:f.lineResults,lineMatched:f.lineMatched,lineGaps:f.lineGaps,reChecks:h.reChecks,funcResults:_.funcResults,funcMapped:_.funcMapped,funcPartial:_.funcPartial,funcMissing:_.funcMissing,allGaps:E.allGaps,gapsByType:E.gapsByType,connMap:g,missingImports:D,validationErrors:p,timestamp:new Date().toISOString()}}function Al(e){const n=[{pattern:/\bspark\.read/i,symbol:"SparkSession",suggestion:"from pyspark.sql import SparkSession"},{pattern:/\bcol\s*\(/i,symbol:"col",suggestion:"from pyspark.sql.functions import col"},{pattern:/\blit\s*\(/i,symbol:"lit",suggestion:"from pyspark.sql.functions import lit"},{pattern:/\bwhen\s*\(/i,symbol:"when",suggestion:"from pyspark.sql.functions import when"},{pattern:/\bexpr\s*\(/i,symbol:"expr",suggestion:"from pyspark.sql.functions import expr"},{pattern:/\bstruct\s*\(/i,symbol:"struct",suggestion:"from pyspark.sql.functions import struct"},{pattern:/\barray\s*\(/i,symbol:"array",suggestion:"from pyspark.sql.functions import array"},{pattern:/\bfrom_json\s*\(/i,symbol:"from_json",suggestion:"from pyspark.sql.functions import from_json"},{pattern:/\bto_json\s*\(/i,symbol:"to_json",suggestion:"from pyspark.sql.functions import to_json"},{pattern:/\bStructType\s*\(/i,symbol:"StructType",suggestion:"from pyspark.sql.types import StructType"},{pattern:/\bStructField\s*\(/i,symbol:"StructField",suggestion:"from pyspark.sql.types import StructField"},{pattern:/\bStringType\s*\(/i,symbol:"StringType",suggestion:"from pyspark.sql.types import StringType"},{pattern:/\bIntegerType\s*\(/i,symbol:"IntegerType",suggestion:"from pyspark.sql.types import IntegerType"},{pattern:/\bpandas_udf/i,symbol:"pandas_udf",suggestion:"from pyspark.sql.functions import pandas_udf"},{pattern:/\bWindow\./i,symbol:"Window",suggestion:"from pyspark.sql.window import Window"},{pattern:/\bjson\.loads/i,symbol:"json",suggestion:"import json"},{pattern:/\bre\./i,symbol:"re",suggestion:"import re"},{pattern:/\bdatetime\./i,symbol:"datetime",suggestion:"from datetime import datetime"},{pattern:/\bbase64\./i,symbol:"base64",suggestion:"import base64"},{pattern:/\bhashlib\./i,symbol:"hashlib",suggestion:"import hashlib"},{pattern:/\bdbutils\.secrets\.get\b/i,symbol:"dbutils",lib:"databricks-sdk",note:"Databricks secrets API",suggestion:"# dbutils is pre-installed on Databricks clusters"},{pattern:/\bpd\.\w+|pandas\.\w+/i,symbol:"pandas",lib:"pandas",note:"Pandas DataFrame operations",suggestion:"import pandas as pd"},{pattern:/\buuid\b/i,symbol:"uuid",lib:"uuid",note:"UUID generation",suggestion:"import uuid"}],t=e.map(o=>o.source||"").join(`
`),r=t.split(`
`).filter(o=>/^\s*(import |from .+ import )/.test(o)).join(`
`),i=[];for(const{pattern:o,symbol:a,suggestion:s}of n)if(o.test(t)&&!r.includes(a)){const c=e.findIndex(l=>o.test(l.source||""));i.push({symbol:a,usedIn:c,suggestion:s})}return i}function Il(e,n){const t=e.length,r=e.filter(d=>d.mapped&&d.confidence>=Se.EXACT).length,i=e.filter(d=>d.mapped).length,o=n.connections||[],a=new Set(e.filter(d=>d.mapped).map(d=>d.name)),s=o.length,c=o.filter(d=>a.has(d.sourceName)&&a.has(d.destinationName)).length,l=e.map((d,u)=>{let g;return d.mapped?d.confidence>=Se.EXACT?g="exact":g="functional":g="gap",{idx:u+1,name:d.name,type:d.type,group:d.group||"â€”",role:d.role,equiv:d.mapped?d.desc:"â€”",category:d.mapped?d.category:"â€”",matchType:g,confidence:d.confidence,code:d.code}});return{exact:{count:r,total:t,pct:t?Math.round(r/t*100):0},functional:{count:i,total:t,pct:t?Math.round(i/t*100):0},actions:{count:c,total:s,pct:s?Math.round(c/s*100):0},rows:l}}const Fl=[/password/i,/secret/i,/token/i,/\bkey\b/i,/credential/i,/api[_.]?key/i,/private[_.]?key/i,/passphrase/i,/auth/i,/\bpin\b/i,/certificate/i,/signing/i],Ll=[{pattern:/(jdbc:\w+:\/\/)([^:]+):([^@]+)@/g,replace:"$1$2:***@"},{pattern:/AKIA[0-9A-Z]{16}/g,replace:"***AWS_KEY***"},{pattern:/(password|pwd|pass)\s*=\s*[^;}&\s]+/gi,replace:"$1=***"},{pattern:/Bearer\s+[A-Za-z0-9\-._~+\/]+=*/g,replace:"Bearer ***"},{pattern:/-----BEGIN\s+[\w\s]+-----[\s\S]*?-----END\s+[\w\s]+-----/g,replace:"***CERTIFICATE***"},{pattern:/[A-Za-z0-9+/]{40,}={0,2}/g,replace:"***BASE64***"}],Ol=/^(https?:\/\/|s3[an]?:\/\/|abfss?:\/\/|gs:\/\/)([^\/\s?#]+)(.*)/i;let Mn=0;function Nl(){Mn=0}function ps(e,n){for(const t of Fl)if(t.test(e))return Mn++,"***";if(typeof n=="string"){let t=n,r=!1;for(const{pattern:o,replace:a}of Ll){const s=t;o.lastIndex=0,t=t.replace(o,a),t!==s&&(r=!0)}const i=t.match(Ol);return i&&!r&&(t=i[1]+i[2]+"/***",r=!0),r&&Mn++,t}return n}function Oo(e){Nl();const n=e.parsed?e.parsed._nifi:null,t={meta:{generated:new Date().toISOString(),tool:"NiFi Flow Analyzer",version:"2.0.1",flow_name:e.parsed?e.parsed.source_name:"unknown"},flow_summary:{source_name:e.parsed?e.parsed.source_name:"Unknown",processor_count:n?n.processors.length:0,connection_count:n?n.connections.length:0,process_group_count:n?n.processGroups.length:0,controller_service_count:n?n.controllerServices.length:0,external_system_count:n?n.clouderaTools.length:0},processors:n?n.processors.map(r=>({name:r.name,type:r.type,group:r.group,state:r.state,role:Pe(r.type),scheduling:{strategy:r.schedulingStrategy,period:r.schedulingPeriod},properties:Object.fromEntries(Object.entries(r.properties).map(([i,o])=>[i,ps(i,o)]))})):[],connections:n?n.connections.map(r=>({source:r.sourceName,destination:r.destinationName,relationships:r.relationships,backPressure:r.backPressure})):[],controller_services:n?n.controllerServices.map(r=>({name:r.name,type:r.type,properties:Object.fromEntries(Object.entries(r.properties).map(([i,o])=>[i,ps(i,o)]))})):[],assessment:e.assessment?{readiness_score:e.assessment.readinessScore,auto_convertible:e.assessment.autoCount,manual_conversion:e.assessment.manualCount,unsupported:e.assessment.unsupportedCount,mappings:e.assessment.mappings?e.assessment.mappings.map(r=>({name:r.name,nifi_type:r.nifiType||r.type,role:r.role,mapped:r.mapped,databricks:r.desc,confidence:r.confidence})):[]}:null,notebook:e.notebook?{cell_count:e.notebook.cells.length,config:e.notebook.config}:null,migration_report:e.migrationReport||null,manifest:e.manifest?{directories:Object.keys(e.manifest.directories).length,sql_tables:Object.keys(e.manifest.sqlTables).length,http_endpoints:e.manifest.httpEndpoints.length,kafka_topics:e.manifest.kafkaTopics.length,scripts:e.manifest.scripts.length,db_connections:e.manifest.dbConnections.length,external_systems:(e.manifest.clouderaTools||[]).length}:null,deep_property_inventory:n?{file_paths:Object.keys(n.deepPropertyInventory.filePaths||{}).length,urls:Object.keys(n.deepPropertyInventory.urls||{}).length,jdbc_urls:Object.keys(n.deepPropertyInventory.jdbcUrls||{}).length,nifi_el_expressions:Object.keys(n.deepPropertyInventory.nifiEL||{}).length,cron_expressions:Object.keys(n.deepPropertyInventory.cronExprs||{}).length,credential_references:Object.keys(n.deepPropertyInventory.credentialRefs||{}).length}:null,sensitive_properties_masked:0};return t.sensitive_properties_masked=Mn,t}function Ml(e){if(!e)return{processorCount:0,connectionCount:0,connectionDensity:0,maxDepth:0,roleDistribution:{},externalSystems:0};const n=e.processors||[],t=e.connections||[],r=n.length>0?(t.length/n.length).toFixed(2):0,i=(e.processGroups||[]).reduce((s,c)=>{const l=(c.path||c.name||"").split("/").length;return Math.max(s,l)},1),o={};n.forEach(s=>{const c=Pe(s.type)||"unknown";o[c]=(o[c]||0)+1});const a=(e.clouderaTools||[]).length;return{processorCount:n.length,connectionCount:t.length,connectionDensity:r,maxDepth:i,roleDistribution:o,externalSystems:a}}function Bl(e,n,t){const r={source:"#3B82F6",transform:"#A855F7",route:"#EAB308",process:"#6366F1",sink:"#21C354",unknown:"#8b949e"},i=Object.entries(e).sort((a,s)=>s[1]-a[1]);let o='<div style="margin:12px 0">';return i.forEach(([a,s])=>{const c=n>0?Math.round(s/n*100):0,l=r[a]||"#8b949e";o+='<div style="display:flex;align-items:center;gap:8px;margin:4px 0">',o+=`<span style="min-width:80px;font-size:0.82rem;text-transform:capitalize;color:var(--text2)">${t(a)}</span>`,o+='<div style="flex:1;height:20px;background:var(--surface2);border-radius:4px;overflow:hidden">',o+=`<div style="height:100%;width:${c}%;background:${l};border-radius:4px;transition:width 0.4s"></div></div>`,o+=`<span style="min-width:60px;font-size:0.82rem;font-weight:600;text-align:right">${s} (${c}%)</span>`,o+="</div>"}),o+="</div>",o}function jl(e,n,t){const r=e,i=r.assessment?.readiness_score||0,o=r.flow_summary.processor_count,a=r.flow_summary.connection_count,s=r.flow_summary.external_system_count;let c="Low",l="var(--green)";o>30||s>5||parseFloat(n.connectionDensity)>3?(c="High",l="var(--red)"):(o>10||s>2||parseFloat(n.connectionDensity)>2)&&(c="Medium",l="var(--amber)");const d=r.assessment?.auto_convertible||0,u=r.assessment?.manual_conversion||0,g=r.assessment?.unsupported||0,p=Math.max(1,Math.ceil(d*.25+u*2+g*4)),m=(p/5).toFixed(1);let f,h;i>=80&&g===0?(f="Proceed",h="exec-proceed"):i>=50||g<=2?(f="Caution",h="exec-caution"):(f="Needs Planning",h="exec-planning");const _=[];g>0&&_.push(`${g} unsupported processor(s) require custom implementation`),s>3&&_.push(`${s} external system integrations increase migration complexity`),n.maxDepth>3&&_.push(`Deeply nested process groups (depth ${n.maxDepth}) may need refactoring`),u>d&&_.push("More manual than auto-convertible processors"),_.length===0&&_.push("No significant risks identified");let v='<div class="exec-summary">';return v+="<h4>Executive Summary</h4>",v+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:12px;margin-bottom:16px">',[{label:"Flow Complexity",value:c,color:l},{label:"Readiness Score",value:i+"%",color:i>=80?"var(--green)":i>=50?"var(--amber)":"var(--red)"},{label:"Effort Estimate",value:m+" wks",color:"var(--primary)"},{label:"Recommendation",value:f,color:h==="exec-proceed"?"var(--green)":h==="exec-caution"?"var(--amber)":"var(--red)"}].forEach(E=>{v+='<div style="text-align:center;padding:12px;background:var(--surface);border:1px solid var(--border);border-radius:8px">',v+=`<div style="font-size:1.4rem;font-weight:800;color:${E.color}">${t(String(E.value))}</div>`,v+=`<div style="font-size:0.78rem;color:var(--text2)">${t(E.label)}</div></div>`}),v+="</div>",v+=`<div style="text-align:center;margin:12px 0"><span class="exec-recommendation ${h}">${t(f)}</span></div>`,v+='<h4 style="margin:16px 0 8px;font-size:0.9rem">Key Risks</h4>',v+='<ul style="margin:0;padding-left:20px;font-size:0.85rem">',_.forEach(E=>{v+=`<li style="margin:4px 0;color:var(--text2)">${t(E)}</li>`}),v+="</ul>",v+='<div style="margin-top:12px;padding:10px;background:var(--surface2);border-radius:6px;font-size:0.85rem;color:var(--text)">',v+=`This NiFi flow has <strong>${o}</strong> processors, <strong>${a}</strong> connections, `,v+=`and <strong>${s}</strong> external system dependencies. `,v+=`With a readiness score of <strong>${i}%</strong>, estimated migration effort is <strong>${m} weeks</strong> `,v+=`(${p} person-days).`,v+="</div>",v+="</div>",v}function Ul(e,n,t){const r=Oo(e),i=r,o=e.parsed?e.parsed._nifi:null,a=Ml(o);let s="";return s+=jl(r,a,t),r.sensitive_properties_masked>0&&(s+=`<div class="alert alert-warn" style="margin:12px 0"><strong>${r.sensitive_properties_masked}</strong> sensitive properties masked in this report.</div>`),s+='<hr class="divider">',s+="<h3>Report Summary</h3>",s+=n([{label:"Processors",value:i.flow_summary.processor_count},{label:"Connections",value:i.flow_summary.connection_count},{label:"External Systems",value:i.flow_summary.external_system_count},{label:"Readiness",value:i.assessment?i.assessment.readiness_score+"%":"N/A"}]),i.assessment&&(s+="<h3>Assessment Overview</h3>",s+=n([{label:"Auto-Convert",value:i.assessment.auto_convertible,color:"var(--green)"},{label:"Manual",value:i.assessment.manual_conversion,color:"var(--amber)"},{label:"Unsupported",value:i.assessment.unsupported,color:"var(--red)"}])),s+='<hr class="divider"><h3>Statistical Dashboard</h3>',s+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(140px,1fr));gap:12px;margin-bottom:16px">',[{label:"Processor Count",value:a.processorCount,color:"var(--primary)"},{label:"Conn. Density",value:a.connectionDensity,color:"var(--text)"},{label:"Max Depth",value:a.maxDepth,color:"var(--amber)"},{label:"Ext. Systems",value:a.externalSystems,color:"var(--red)"}].forEach(l=>{s+='<div style="text-align:center;padding:12px;background:var(--surface);border:1px solid var(--border);border-radius:8px">',s+=`<div style="font-size:1.6rem;font-weight:800;color:${l.color}">${l.value}</div>`,s+=`<div style="font-size:0.78rem;color:var(--text2)">${t(l.label)}</div></div>`}),s+="</div>",s+='<h4 style="margin:12px 0 4px">Processor Role Distribution</h4>',s+=Bl(a.roleDistribution,a.processorCount,t),s+='<hr class="divider"><h3>Report Data Explorer</h3>',s+='<p style="font-size:0.8rem;color:var(--text2);margin:0 0 8px">Click any key or value to copy its JSON path. Use search to filter by key/value.</p>',s+=ul(i,"finalReportExplorer"),s+='<hr class="divider"><div style="display:flex;gap:8px;flex-wrap:wrap;align-items:center">',s+='<button class="btn btn-primary" id="finalReportDownloadBtn">Download Full Report (JSON)</button>',s+='<div class="format-selector">',s+='<select id="finalReportFormatSelect">',s+='<option value="">More formats...</option>',s+='<option value="html">HTML Report</option>',s+='<option value="markdown">Markdown Report</option>',s+='<option value="csv-processors">CSV (Processor List)</option>',s+='<option value="csv-gaps">CSV (Gap List)</option>',s+="</select>",s+="</div>",s+='<button class="btn btn-secondary" id="finalReportExportBtn">Export</button>',s+="</div>",{html:s,report:r}}function Bn(e){if(typeof e=="string")return e.replace(/[\x00-\x1f\x7f-\x9f]/g,"");if(Array.isArray(e))return e.map(Bn);if(e&&typeof e=="object"){const n={};for(const[t,r]of Object.entries(e))n[t]=Bn(r);return n}return e}const ql={MergeContent:{reason:"Spark handles partitioned reads natively â€” no need to merge small files manually",savings:"medium",risk:"low"},MergeRecord:{reason:"Spark handles partitioned reads natively â€” no need to merge records manually",savings:"medium",risk:"low"},CompressContent:{reason:"Delta Lake handles compression (zstd/snappy) automatically",savings:"low",risk:"none"},UnpackContent:{reason:"Delta Lake handles decompression automatically on read",savings:"low",risk:"none"},SplitText:{reason:"Spark reads entire datasets at once â€” no need to split text into individual records",savings:"medium",risk:"low"},SplitJson:{reason:"Spark reads JSON files as DataFrames natively â€” no splitting needed",savings:"medium",risk:"low"},SplitXml:{reason:"spark-xml reads entire XML documents â€” no splitting needed",savings:"medium",risk:"low"},SplitContent:{reason:"Spark operates on entire partitions â€” no content splitting needed",savings:"medium",risk:"low"},SplitAvro:{reason:"Spark reads Avro files as DataFrames natively",savings:"medium",risk:"low"},SplitRecord:{reason:"Spark operates on entire DataFrames â€” individual record splitting unnecessary",savings:"medium",risk:"low"},UpdateAttribute:{reason:"Use .withColumn() to add/modify columns â€” no separate attribute update step",savings:"low",risk:"none"},RouteOnAttribute:{reason:"Use .filter() or .when() for simple attribute-based routing",savings:"low",risk:"low"},LogAttribute:{reason:"Use Spark logging or display() â€” no dedicated log processor needed",savings:"low",risk:"none"},LogMessage:{reason:"Use print() or logging module â€” no dedicated log processor needed",savings:"low",risk:"none"},Wait:{reason:"Use Databricks Workflows task dependencies instead of in-flow waits",savings:"medium",risk:"low"},Notify:{reason:"Use Databricks Workflows task dependencies instead of notifications",savings:"medium",risk:"low"},DetectDuplicate:{reason:"Use dropDuplicates() â€” built into Spark DataFrame API",savings:"medium",risk:"low"},ControlRate:{reason:"Spark handles backpressure natively via Structured Streaming",savings:"low",risk:"none"},DistributeLoad:{reason:"Spark handles data distribution across executors automatically",savings:"medium",risk:"none"},ValidateRecord:{reason:"Use DLT expectations for declarative data quality rules",savings:"low",risk:"low"},RetryFlowFile:{reason:"Use Spark retry mechanisms or Workflows retry policies",savings:"low",risk:"low"},MonitorActivity:{reason:"Use Databricks Workflows monitoring and Spark UI",savings:"low",risk:"none"},DebugFlow:{reason:"Use Spark UI, display(), or notebook debugging â€” no dedicated debug processor",savings:"low",risk:"none"},CountText:{reason:"Use df.count() â€” built into Spark DataFrame API",savings:"low",risk:"none"},AttributesToJSON:{reason:"Use to_json() â€” built into PySpark",savings:"low",risk:"none"},GenerateFlowFile:{reason:"Use spark.range() or createDataFrame() for test data generation",savings:"low",risk:"none"},EnforceOrder:{reason:"Use orderBy() â€” built into Spark DataFrame API",savings:"low",risk:"none"},Funnel:{reason:"Use DataFrame union() â€” NiFi funnels have no Databricks equivalent needed",savings:"low",risk:"none"},InputPort:{reason:"NiFi ports not needed â€” data flows handled by notebooks/jobs",savings:"low",risk:"none"},OutputPort:{reason:"NiFi ports not needed â€” data flows handled by notebooks/jobs",savings:"low",risk:"none"}},qe={file_polling:{nifi:"GetFile/ListFile polling with scheduling",dbx:"Auto Loader (cloudFiles) with file notification mode",benefit:"10-100x faster file discovery, exactly-once guarantees, schema evolution"},batch_loop:{nifi:"Repeated batch processing via CRON-scheduled processors",dbx:"Structured Streaming with trigger(availableNow=True)",benefit:"Incremental processing, checkpoint-based exactly-once, auto-scaling"},schema_mgmt:{nifi:"Schema Registry + Avro/JSON schema enforcement per processor",dbx:"Unity Catalog schema governance with automatic schema evolution",benefit:"Centralized governance, lineage tracking, access controls"},data_quality:{nifi:"ValidateRecord + RouteOnAttribute for quality checks",dbx:"DLT Expectations (expect, expect_or_drop, expect_or_fail)",benefit:"Declarative quality rules, automatic quarantine, quality dashboards"},dedup:{nifi:"DetectDuplicate processor with distributed cache",dbx:"dropDuplicates() or MERGE INTO with Delta Lake",benefit:"No external cache needed, ACID guarantees, time travel"},merge_small:{nifi:"MergeContent to combine small files",dbx:"Delta Lake Auto Optimize + Auto Compaction",benefit:"Automatic small file compaction, no manual merge logic"},scheduling:{nifi:"CRON-driven processor scheduling with backpressure",dbx:"Databricks Workflows with task dependencies and triggers",benefit:"DAG-based orchestration, conditional logic, cost-optimized clusters"},caching:{nifi:"DistributedMapCache for lookup enrichment",dbx:"Broadcast variables or Delta Lake lookups",benefit:"No external cache infrastructure, automatic distribution"},security:{nifi:"Per-processor credentials and NiFi Registry policies",dbx:"Unity Catalog + Secret Scopes + identity federation",benefit:"Centralized IAM, fine-grained ACLs, audit logging"},monitoring:{nifi:"NiFi bulletins, provenance, and flow status",dbx:"Spark UI, Ganglia, custom Databricks dashboards",benefit:"Deep execution insights, cost tracking, ML-integrated monitoring"}},zl={MergeContent:"Delta Lake Auto Optimize + Auto Compaction handle small file merging automatically.",MergeRecord:"Spark DataFrames handle records as partitions -- no need for manual record merging.",CompressContent:"Delta Lake uses zstd/snappy compression by default on all writes.",UnpackContent:"Spark reads compressed files (gzip, snappy, zstd) natively.",SplitText:"spark.read.text() reads entire files; use .split() or regex for row splitting.",SplitJson:"spark.read.json() reads entire JSON files as DataFrames.",SplitXml:"spark-xml library reads XML documents directly into DataFrames.",SplitContent:"Spark processes data as partitions -- no manual content splitting needed.",SplitAvro:'spark.read.format("avro") reads Avro files natively as DataFrames.',SplitRecord:"Spark operates on DataFrames at partition level -- individual record splitting is unnecessary.",UpdateAttribute:".withColumn() and .withColumnRenamed() handle all attribute/column transformations.",RouteOnAttribute:".filter() or .when()/.otherwise() handle attribute-based routing.",LogAttribute:"Use display() in notebooks, or logging module; Spark UI shows all execution details.",LogMessage:"Use print() or Python logging module in notebooks.",Wait:"Databricks Workflows task dependencies replace in-flow waits with proper DAG orchestration.",Notify:"Databricks Workflows task dependencies handle cross-task notifications.",DetectDuplicate:"dropDuplicates() is built into DataFrame API; Delta MERGE handles upserts.",ControlRate:"Structured Streaming handles backpressure natively; no manual rate control needed.",DistributeLoad:"Spark distributes data across executors via partitioning -- automatic load balancing.",ValidateRecord:"DLT Expectations (expect, expect_or_drop, expect_or_fail) provide declarative quality rules.",RetryFlowFile:"Databricks Workflows retry policies handle job-level and task-level retries.",MonitorActivity:"Spark UI, Databricks Workflows monitoring, and Ganglia provide comprehensive monitoring.",DebugFlow:"Notebook debugging, display(), and Spark UI replace dedicated debug processors.",CountText:"df.count() provides native row counting in Spark.",AttributesToJSON:"to_json() function in PySpark handles struct-to-JSON conversion.",GenerateFlowFile:"spark.range() or spark.createDataFrame() generate test data.",EnforceOrder:"orderBy() provides native sorting in Spark.",Funnel:"DataFrame union() merges multiple DataFrames -- NiFi funnels have no equivalent needed.",InputPort:"Databricks Jobs pass parameters between tasks -- NiFi ports are not needed.",OutputPort:"Databricks Jobs pass parameters between tasks -- NiFi ports are not needed."};function No({nifi:e,notebook:n,escapeHTML:t}){const r=e.processors||[],i=e.connections||[];let o="";o+='<h3 style="margin:0 0 12px;color:var(--primary)">1. What This Workflow Does</h3>';const a=r.filter(T=>Pe(T.type)==="source"),s=r.filter(T=>Pe(T.type)==="sink"),c=r.filter(T=>Pe(T.type)==="transform"),l=r.filter(T=>Pe(T.type)==="route"),d=r.filter(T=>Pe(T.type)==="process"),u=[...new Set(a.map(T=>T.type))],g=[...new Set(s.map(T=>T.type))],p=Hn(e),m=Object.values(p).map(T=>T.name);let f='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px;margin-bottom:16px">';f+='<p style="font-size:0.95rem;line-height:1.6;margin:0">',f+=`This NiFi flow consists of <strong>${r.length} processors</strong> organized into <strong>${e.processGroups.length} process group(s)</strong>. `,f+=`It ingests data from <strong>${a.length} source(s)</strong> (${u.join(", ")||"none identified"}), `,f+=`applies <strong>${c.length} transformation(s)</strong> and <strong>${l.length} routing decision(s)</strong>, `,f+=`then delivers to <strong>${s.length} destination(s)</strong> (${g.join(", ")||"none identified"}).`,m.length&&(f+=` External systems involved: <strong>${m.join(", ")}</strong>.`),f+="</p></div>",f+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:12px;margin-bottom:16px">',[{role:"Sources",count:a.length,color:"#3B82F6",icon:"&#9654;"},{role:"Transforms",count:c.length,color:"#A855F7",icon:"&#9881;"},{role:"Routing",count:l.length,color:"#EAB308",icon:"&#8644;"},{role:"Processing",count:d.length,color:"#6366F1",icon:"&#9881;"},{role:"Sinks",count:s.length,color:"#21C354",icon:"&#9632;"}].forEach(T=>{f+=`<div style="background:${T.color}11;border:1px solid ${T.color}44;border-radius:8px;padding:12px;text-align:center">`,f+=`<div style="font-size:1.5rem">${T.icon}</div>`,f+=`<div style="font-size:1.8rem;font-weight:700;color:${T.color}">${T.count}</div>`,f+=`<div style="font-size:0.8rem;color:var(--text2)">${T.role}</div></div>`}),f+="</div>",Object.keys(p).length&&(f+='<h4 style="margin:12px 0 6px">Integration Points</h4>',f+='<div style="display:flex;gap:8px;flex-wrap:wrap">',Object.values(p).forEach(T=>{const G=T.processors.map(W=>W.direction).includes("WRITE")&&T.processors.map(W=>W.direction).includes("READ")?"&#8644;":T.processors.map(W=>W.direction).includes("WRITE")?"&#8594;":"&#8592;";f+=`<span style="display:inline-flex;align-items:center;gap:4px;padding:4px 10px;background:var(--surface);border:1px solid var(--border);border-radius:6px;font-size:0.82rem">${G} <strong>${t(T.name)}</strong> <span style="color:var(--text2)">(${T.processors.length})</span></span>`}),f+="</div>");const _=new Set;r.forEach(T=>{/JSON/i.test(T.type)&&_.add("JSON"),/XML/i.test(T.type)&&_.add("XML"),/Avro/i.test(T.type)&&_.add("Avro"),/CSV|Delimited/i.test(T.type)&&_.add("CSV"),/Parquet/i.test(T.type)&&_.add("Parquet"),/ORC/i.test(T.type)&&_.add("ORC");const G=Object.values(T.properties||{}).join(" ");/json/i.test(G)&&_.add("JSON"),/xml/i.test(G)&&_.add("XML"),/csv|delimited/i.test(G)&&_.add("CSV"),/avro/i.test(G)&&_.add("Avro"),/parquet/i.test(G)&&_.add("Parquet")}),_.size&&(f+=`<p style="margin:12px 0 0;font-size:0.85rem;color:var(--text2)">Data formats detected: <strong>${[..._].join(", ")}</strong></p>`),o+=f,o+='<h3 style="margin:24px 0 12px;color:var(--primary)">2. How to Build It Better in Databricks</h3>';const v=[];r.some(T=>/^(GetFile|ListFile|TailFile|FetchFile)$/i.test(T.type))&&v.push({...qe.file_polling,category:"Ingestion",priority:1}),r.some(T=>T.schedulingStrategy==="CRON_DRIVEN"||/TIMER_DRIVEN/i.test(T.schedulingStrategy))&&v.push({...qe.batch_loop,category:"Processing",priority:2}),r.some(T=>/Schema|Avro|Record/i.test(T.type))&&v.push({...qe.schema_mgmt,category:"Governance",priority:2}),r.some(T=>/Validate|RouteOn/i.test(T.type))&&v.push({...qe.data_quality,category:"Quality",priority:1}),r.some(T=>/DetectDuplicate/i.test(T.type))&&v.push({...qe.dedup,category:"Deduplication",priority:2}),r.some(T=>/MergeContent|MergeRecord/i.test(T.type))&&v.push({...qe.merge_small,category:"Optimization",priority:2}),r.length>5&&v.push({...qe.scheduling,category:"Orchestration",priority:3}),r.some(T=>/Cache|Lookup/i.test(T.type))&&v.push({...qe.caching,category:"Enrichment",priority:3}),e.controllerServices.some(T=>/SSL|Kerberos|LDAP|Credential/i.test(T.type))&&v.push({...qe.security,category:"Security",priority:1}),v.push({...qe.monitoring,category:"Monitoring",priority:3}),v.sort((T,G)=>T.priority-G.priority),o+='<div style="display:grid;gap:12px">',v.forEach(T=>{const G=T.priority===1?"var(--green)":T.priority===2?"var(--primary)":"var(--text2)",W=T.priority===1?"HIGH":T.priority===2?"MEDIUM":"LOW";o+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:14px">',o+='<div style="display:flex;justify-content:space-between;align-items:center;margin-bottom:8px">',o+=`<strong style="font-size:0.95rem">${t(T.category)}</strong>`,o+=`<span style="font-size:0.72rem;padding:2px 8px;border-radius:4px;background:${G}22;color:${G};font-weight:700">${W} PRIORITY</span>`,o+="</div>",o+='<div style="display:grid;grid-template-columns:1fr 1fr;gap:12px;font-size:0.85rem">',o+=`<div><div style="color:var(--text2);font-size:0.75rem;margin-bottom:4px">CURRENT (NiFi)</div>${t(T.nifi)}</div>`,o+=`<div><div style="color:var(--green);font-size:0.75rem;margin-bottom:4px">RECOMMENDED (Databricks)</div><strong>${t(T.dbx)}</strong></div>`,o+="</div>",o+=`<div style="margin-top:8px;font-size:0.82rem;color:var(--text2)">&#9889; ${t(T.benefit)}</div>`,o+="</div>"}),o+="</div>",o+=`<h3 style="margin:24px 0 12px;color:var(--primary)">3. Steps That Aren't Needed in Databricks</h3>`;const k=[];if(r.forEach(T=>{const G=ql[T.type];G&&k.push({name:T.name,type:T.type,group:T.group,...G,replacement:zl[T.type]||"Handled natively by Databricks/Spark."})}),k.length===0)o+='<div class="val-ok">All processors in this flow serve essential functions in the Databricks migration.</div>';else{const T={high:3,medium:2,low:1},G=k.sort((W,j)=>(T[j.savings]||0)-(T[W.savings]||0));o+=`<div class="alert alert-success" style="margin-bottom:12px"><strong>${k.length}</strong> of ${r.length} processors (${Math.round(k.length/r.length*100)}%) can be eliminated in Databricks</div>`,o+='<div class="table-scroll"><table style="font-size:0.82rem"><thead><tr><th>Processor</th><th>Type</th><th>Why Not Needed</th><th>Databricks Replacement</th><th>Savings</th><th>Risk</th></tr></thead><tbody>',G.forEach(W=>{const j=W.savings==="high"?"var(--green)":W.savings==="medium"?"var(--primary)":"var(--text2)",Z=W.risk==="high"?"var(--red)":W.risk==="medium"?"var(--amber)":W.risk==="low"?"var(--text2)":"var(--green)";o+=`<tr><td><strong>${t(W.name)}</strong></td>`,o+=`<td><code style="font-size:0.75rem">${t(W.type)}</code></td>`,o+=`<td style="font-size:0.8rem">${t(W.reason)}</td>`,o+=`<td style="font-size:0.8rem;color:var(--green)">${t(W.replacement)}</td>`,o+=`<td><span style="color:${j};font-weight:600">${W.savings.toUpperCase()}</span></td>`,o+=`<td><span style="color:${Z};font-weight:600">${(W.risk||"none").toUpperCase()}</span></td></tr>`}),o+="</tbody></table></div>"}o+='<h3 style="margin:24px 0 12px;color:var(--primary)">4. Quantified Complexity Reduction</h3>';const E=r.length,D=k.length,P=E-D,$=E>0?Math.round(D/E*100):0,x=n?(n.cells||[]).length:P,S=Math.max(3,x-D);o+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(180px,1fr));gap:12px;margin-bottom:16px">',[{label:"NiFi Processors",value:E,color:"var(--text2)",sub:"current flow"},{label:"Can Be Dropped",value:D,color:"var(--amber)",sub:`${$}% reduction`},{label:"Essential Steps",value:P,color:"var(--green)",sub:"remain in Databricks"},{label:"Notebook Cells",value:S,color:"var(--primary)",sub:"projected output"}].forEach(T=>{o+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:14px;text-align:center">',o+=`<div style="font-size:2rem;font-weight:800;color:${T.color}">${T.value}</div>`,o+=`<div style="font-size:0.85rem;font-weight:600">${T.label}</div>`,o+=`<div style="font-size:0.75rem;color:var(--text2)">${T.sub}</div></div>`}),o+="</div>";const A=e.processGroups.reduce((T,G)=>{const W=(G.path||G.name||"").split("/").length;return Math.max(T,W)},1),w=Math.min(l.length>0?1+l.length*.15:1,3),R=(e.controllerServices||[]).length*4,y=Math.round((r.length*2+i.length+R+Object.keys(p).length*5+A*3)*w),O=P*2+Object.keys(p).length*3,J=y>0?Math.round((1-O/y)*100):0;o+='<h4 style="margin:12px 0 6px">Complexity Scoring Formula</h4>',o+='<div class="table-scroll"><table style="font-size:0.82rem"><thead><tr><th>Factor</th><th>Count</th><th>Weight</th><th>Score</th><th>Rationale</th></tr></thead><tbody>',[{factor:"Processors",count:r.length,weight:"x2",score:r.length*2,rationale:"Each processor = discrete transformation step"},{factor:"Connections",count:i.length,weight:"x1",score:i.length,rationale:"Data flow links between processors"},{factor:"Controller Services",count:(e.controllerServices||[]).length,weight:"x4",score:R,rationale:"Shared services add cross-cutting complexity"},{factor:"External Systems",count:Object.keys(p).length,weight:"x5",score:Object.keys(p).length*5,rationale:"Each integration adds deployment complexity"},{factor:"Nesting Depth",count:A,weight:"x3",score:A*3,rationale:"Deeper nesting increases maintenance burden"},{factor:"Routing Multiplier",count:l.length+" routes",weight:"x"+w.toFixed(2),score:"(applied to total)",rationale:"Conditional routing adds branching complexity"}].forEach(T=>{o+=`<tr><td><strong>${t(String(T.factor))}</strong></td><td>${t(String(T.count))}</td><td>${t(String(T.weight))}</td><td>${t(String(T.score))}</td><td style="font-size:0.78rem;color:var(--text2)">${t(T.rationale)}</td></tr>`}),o+=`<tr style="font-weight:700;border-top:2px solid var(--border)"><td>NiFi Total</td><td></td><td></td><td style="color:var(--red)">${y} pts</td><td></td></tr>`,o+=`<tr style="font-weight:700"><td>Databricks Projected</td><td></td><td></td><td style="color:var(--green)">${O} pts</td><td>Essential x2 + ExtSys x3</td></tr>`,o+="</tbody></table></div>",o+='<div class="stacked-bar-container" style="margin:16px 0">',o+='<div style="display:flex;justify-content:space-between;font-size:0.82rem;margin-bottom:4px"><span>NiFi Complexity</span><strong style="color:var(--red)">'+y+" pts</strong></div>",o+='<div class="stacked-bar">';const z=y>0?Math.round(r.length*2*w/y*100):0,U=y>0?Math.round(i.length*w/y*100):0,X=y>0?Math.round(R*w/y*100):0,F=y>0?Math.round(Object.keys(p).length*5*w/y*100):0,N=Math.max(0,100-z-U-X-F);o+=`<div class="stacked-bar-seg" style="width:${z}%;background:#3B82F6">${z>8?z+"%":""}</div>`,o+=`<div class="stacked-bar-seg" style="width:${U}%;background:#A855F7">${U>8?U+"%":""}</div>`,o+=`<div class="stacked-bar-seg" style="width:${X}%;background:#EAB308">${X>8?X+"%":""}</div>`,o+=`<div class="stacked-bar-seg" style="width:${F}%;background:#EF4444">${F>8?F+"%":""}</div>`,o+=`<div class="stacked-bar-seg" style="width:${N}%;background:#8b949e">${N>8?N+"%":""}</div>`,o+="</div>",o+='<div class="stacked-bar-legend">',o+='<span class="stacked-bar-legend-item"><span class="stacked-bar-legend-dot" style="background:#3B82F6"></span>Processors</span>',o+='<span class="stacked-bar-legend-item"><span class="stacked-bar-legend-dot" style="background:#A855F7"></span>Connections</span>',o+='<span class="stacked-bar-legend-item"><span class="stacked-bar-legend-dot" style="background:#EAB308"></span>Controller Svc</span>',o+='<span class="stacked-bar-legend-item"><span class="stacked-bar-legend-dot" style="background:#EF4444"></span>Ext Systems</span>',o+='<span class="stacked-bar-legend-item"><span class="stacked-bar-legend-dot" style="background:#8b949e"></span>Nesting/Routing</span>',o+="</div>",o+='<div style="display:flex;justify-content:space-between;font-size:0.82rem;margin:12px 0 4px"><span>Databricks Projected</span><strong style="color:var(--green)">'+O+" pts</strong></div>";const H=y>0?Math.round(O/y*100):50;if(o+=`<div style="height:24px;background:var(--green)22;border-radius:6px;overflow:hidden"><div style="height:100%;width:${H}%;background:var(--green);border-radius:6px"></div></div>`,o+=`<div style="text-align:center;margin-top:8px;font-size:1.1rem;font-weight:700;color:var(--green)">${J}% complexity reduction</div>`,o+="</div>",k.length){const T={};k.forEach(G=>{const W=G.type.match(/Merge|Split/)?"Merge/Split":G.type.match(/Log|Debug|Count|Monitor/)?"Logging/Monitoring":G.type.match(/Route|Distribute|Control|Detect/)?"Routing/Control":G.type.match(/Update|Attribute|Enforce/)?"Attribute Management":G.type.match(/Wait|Notify|Retry/)?"Flow Control":G.type.match(/Validate/)?"Validation":G.type.match(/Compress|Unpack/)?"Compression":G.type.match(/Generate|Funnel|Port/)?"NiFi Internal":"Other";T[W]||(T[W]=[]),T[W].push(G)}),o+='<h4 style="margin:12px 0 6px">Dropped Processors by Category</h4>',o+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:8px">',Object.entries(T).sort((G,W)=>W[1].length-G[1].length).forEach(([G,W])=>{o+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:6px;padding:10px">',o+=`<strong style="font-size:0.85rem">${t(G)}</strong> <span style="color:var(--text2);font-size:0.8rem">(${W.length})</span>`,o+=`<div style="font-size:0.78rem;color:var(--text2);margin-top:4px">${W.map(j=>t(j.type)).join(", ")}</div></div>`}),o+="</div>"}o+='<h3 style="margin:24px 0 12px;color:var(--primary)">5. Migration ROI Summary</h3>';const V=[{name:"ACID Transactions",desc:"Delta Lake provides full ACID guarantees on all data operations",icon:"&#128274;",when:()=>!0,useCases:["Reliable upserts (MERGE INTO)","Consistent reads during writes","Transaction log for auditability"]},{name:"Time Travel",desc:"Query and restore previous versions of data with VERSION AS OF",icon:"&#9200;",when:()=>!0,useCases:["Roll back bad data loads","Audit historical states","Reproduce pipeline results"]},{name:"Unity Catalog Governance",desc:"Centralized access control, lineage, and audit logging",icon:"&#128737;",when:()=>(e.controllerServices||[]).length>0||Object.keys(p).length>0,useCases:["Fine-grained access control","Data lineage tracking","Cross-workspace governance"]},{name:"ML Integration",desc:"Seamless integration with MLflow, Feature Store, and model serving",icon:"&#129302;",when:()=>r.some(T=>/Execute(Script|Stream)|Predict|ML|Model/i.test(T.type)),useCases:["Model training on pipeline data","Feature engineering","Real-time model serving"]},{name:"Auto Scaling",desc:"Automatic cluster scaling based on workload demand",icon:"&#128200;",when:()=>r.length>10,useCases:["Handle variable data volumes","Cost optimization","Peak load management"]},{name:"Photon Engine",desc:"Vectorized query engine for 2-8x performance improvement",icon:"&#9889;",when:()=>c.length>3||r.some(T=>/SQL|Query|Convert/i.test(T.type)),useCases:["Faster SQL transformations","Reduced processing time","Lower compute costs"]},{name:"Delta Live Tables",desc:"Declarative data pipelines with built-in quality management",icon:"&#128736;",when:()=>!0,useCases:["Declarative pipeline definitions","Automatic error handling","Built-in quality dashboards"]},{name:"Liquid Clustering",desc:"Automatic data layout optimization replacing manual Z-ordering",icon:"&#128204;",when:()=>r.some(T=>/Merge|Partition|Sort/i.test(T.type)),useCases:["Automatic data layout","Faster query performance","No manual optimization"]},{name:"Structured Streaming",desc:"Unified batch and streaming with exactly-once processing",icon:"&#127919;",when:()=>r.some(T=>/Consume|Subscribe|Listen|Kafka|JMS/i.test(T.type)),useCases:["Real-time data processing","Exactly-once semantics","Unified batch/stream code"]},{name:"Secret Scopes",desc:"Secure credential management replacing NiFi controller services",icon:"&#128272;",when:()=>(e.controllerServices||[]).some(T=>/SSL|Password|Credential|DBCP/i.test(T.type)),useCases:["Centralized secrets","Key rotation","Audit access"]}].filter(T=>T.when());o+='<div class="roi-grid">';const ee=Math.max(2,Math.ceil(E*.5+Object.keys(p).length*1)),se=Math.max(1,Math.ceil(P*.2+Object.keys(p).length*.5)),te=ee-se,ae=te*52,ie=85,fe=ae*ie;o+=`<div class="roi-card"><div class="roi-value" style="color:var(--green)">$${fe.toLocaleString()}</div><div class="roi-label">Est. Annual Savings</div><div class="roi-detail">${ae} hours/year at $${ie}/hr</div></div>`,o+=`<div class="roi-card"><div class="roi-value" style="color:var(--primary)">${te}h/wk</div><div class="roi-label">Weekly Time Savings</div><div class="roi-detail">${ee}h (NiFi) vs ${se}h (Databricks)</div></div>`,o+=`<div class="roi-card"><div class="roi-value" style="color:var(--amber)">${J}%</div><div class="roi-label">Complexity Reduction</div><div class="roi-detail">${y} pts to ${O} pts</div></div>`,o+=`<div class="roi-card"><div class="roi-value" style="color:var(--green)">${$}%</div><div class="roi-label">Component Reduction</div><div class="roi-detail">${D} of ${E} processors eliminated</div></div>`,o+="</div>",o+='<h4 style="margin:16px 0 8px">Productivity Gains</h4>',o+='<div style="display:grid;grid-template-columns:1fr 1fr;gap:16px;margin-bottom:16px">',o+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px">',o+='<h4 style="margin:0 0 12px;font-size:0.95rem">Complexity Comparison</h4>';const _e=100,ye=y>0?Math.round(O/y*100):50;o+=`<div style="margin-bottom:12px"><div style="display:flex;justify-content:space-between;font-size:0.82rem;margin-bottom:4px"><span>NiFi (current)</span><strong>${y} pts</strong></div>`,o+=`<div style="height:20px;background:var(--red)33;border-radius:4px;overflow:hidden"><div style="height:100%;width:${_e}%;background:var(--red);border-radius:4px"></div></div></div>`,o+=`<div><div style="display:flex;justify-content:space-between;font-size:0.82rem;margin-bottom:4px"><span>Databricks (projected)</span><strong>${O} pts</strong></div>`,o+=`<div style="height:20px;background:var(--green)33;border-radius:4px;overflow:hidden"><div style="height:100%;width:${ye}%;background:var(--green);border-radius:4px"></div></div></div>`,o+=`<div style="text-align:center;margin-top:12px;font-size:1.1rem;font-weight:700;color:var(--green)">${J}% complexity reduction</div>`,o+="</div>",o+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px">',o+='<h4 style="margin:0 0 12px;font-size:0.95rem">Migration Summary</h4>',[{label:"Processors eliminated",value:`${D} of ${E} (${$}%)`},{label:"External systems",value:`${Object.keys(p).length} integration(s)`},{label:"Controller services",value:`${(e.controllerServices||[]).length} to migrate`},{label:"Process groups",value:`${e.processGroups.length} â†’ Databricks jobs`},{label:"Notebook cells",value:`${S} (vs ${E} NiFi processors)`},{label:"New capabilities",value:`${V.length} Databricks features gained`}].forEach(T=>{o+=`<div style="display:flex;justify-content:space-between;padding:4px 0;border-bottom:1px solid var(--border);font-size:0.85rem"><span style="color:var(--text2)">${T.label}</span><strong>${T.value}</strong></div>`}),o+="</div></div>",o+='<h4 style="margin:16px 0 8px">Risk Reduction</h4>',o+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:8px;margin-bottom:16px">',[{label:"Data Loss Risk",nifi:"Medium (manual checkpoints)",dbx:"Low (ACID + exactly-once)",color:"var(--green)"},{label:"Credential Exposure",nifi:"Medium (per-processor secrets)",dbx:"Low (Secret Scopes + IAM)",color:"var(--green)"},{label:"Downtime Impact",nifi:"High (single cluster)",dbx:"Low (auto-scaling + HA)",color:"var(--green)"},{label:"Compliance",nifi:"Manual audit trails",dbx:"Unity Catalog lineage + audit",color:"var(--primary)"}].forEach(T=>{o+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:6px;padding:10px">',o+=`<div style="font-weight:600;font-size:0.85rem;margin-bottom:6px">${t(T.label)}</div>`,o+=`<div style="font-size:0.78rem"><span style="color:var(--red)">Before:</span> ${t(T.nifi)}</div>`,o+=`<div style="font-size:0.78rem"><span style="color:${T.color}">After:</span> ${t(T.dbx)}</div></div>`}),o+="</div>",o+='<h3 style="margin:24px 0 12px;color:var(--primary)">6. Implementation Roadmap</h3>';const q=[];q.push({phase:"Phase 1: Foundation",title:"Environment Setup & Secret Migration",effort:`${Math.max(1,Math.ceil((e.controllerServices||[]).length*.5))} days`,tasks:["Set up Unity Catalog schema","Migrate secrets to Databricks Secret Scopes","Configure cluster policies"],dependencies:"None"}),q.push({phase:"Phase 2: Core Pipeline",title:"Source + Transform + Sink Migration",effort:`${Math.max(2,Math.ceil(P*.5))} days`,tasks:[`Migrate ${a.length} source processor(s)`,`Migrate ${c.length} transform processor(s)`,`Migrate ${s.length} sink processor(s)`],dependencies:"Phase 1"}),Object.keys(p).length>0&&q.push({phase:"Phase 3: Integrations",title:"External System Connectivity",effort:`${Math.max(1,Object.keys(p).length*2)} days`,tasks:Object.values(p).map(T=>`Connect to ${T.name} (${T.processors.length} processor(s))`),dependencies:"Phase 2"}),q.push({phase:`Phase ${Object.keys(p).length>0?"4":"3"}: Testing`,title:"Validation & Data Quality",effort:`${Math.max(2,Math.ceil(E*.3))} days`,tasks:["Run data comparison between NiFi and Databricks outputs","Validate all processor mappings","Set up DLT Expectations for quality rules"],dependencies:`Phase ${Object.keys(p).length>0?"3":"2"}`}),q.push({phase:`Phase ${Object.keys(p).length>0?"5":"4"}: Cutover`,title:"Production Deployment",effort:"2-3 days",tasks:["Configure Databricks Workflows scheduling","Set up monitoring and alerting","Parallel run and cutover"],dependencies:"Previous phase"}),o+='<div class="roadmap-timeline">',q.forEach(T=>{o+='<div class="roadmap-item">',o+=`<div class="roadmap-phase">${t(T.phase)}</div>`,o+=`<div class="roadmap-title">${t(T.title)}</div>`,o+=`<div class="roadmap-effort">Effort: <strong>${t(T.effort)}</strong> | Depends on: ${t(T.dependencies)}</div>`,o+='<ul style="margin:6px 0 0;padding-left:18px;font-size:0.8rem;color:var(--text2)">',T.tasks.forEach(G=>{o+=`<li style="margin:2px 0">${t(G)}</li>`}),o+="</ul></div>"}),o+="</div>",o+='<h3 style="margin:24px 0 12px;color:var(--primary)">7. Capability Discovery Guide</h3>',o+='<p style="font-size:0.85rem;color:var(--text2);margin-bottom:12px">New capabilities gained with Databricks, with use cases relevant to <strong>this flow</strong>.</p>',o+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:12px">',V.forEach(T=>{o+='<div style="background:var(--green)08;border:1px solid var(--green)33;border-radius:8px;padding:14px">',o+='<div style="display:flex;align-items:center;gap:8px;margin-bottom:8px">',o+=`<span style="font-size:1.2rem">${T.icon}</span>`,o+=`<strong style="font-size:0.92rem">${t(T.name)}</strong>`,o+="</div>",o+=`<div style="font-size:0.82rem;color:var(--text2);margin-bottom:8px">${t(T.desc)}</div>`,T.useCases&&T.useCases.length&&(o+='<div style="font-size:0.78rem;font-weight:600;color:var(--green);margin-bottom:4px">Relevant Use Cases:</div>',o+='<ul style="margin:0;padding-left:16px;font-size:0.78rem;color:var(--text2)">',T.useCases.forEach(G=>{o+=`<li style="margin:2px 0">${t(G)}</li>`}),o+="</ul>"),o+="</div>"}),o+="</div>",o+='<div style="margin-top:24px;text-align:center">',o+='<button class="btn btn-primary" id="valueAnalysisDownloadBtn">Download Value Analysis (JSON)</button>',o+="</div>";const L=q.reduce((T,G)=>{const W=G.effort.match(/(\d+)/);return T+(W?parseInt(W[1]):0)},0),I={summary:{totalProcessors:E,droppable:D,essential:P,reductionPct:$,nifiComplexity:y,dbxComplexity:O,complexityReduction:J},droppableProcessors:k.map(T=>({...T,replacement:T.replacement})),recommendations:v.map(T=>({category:T.category,nifi:T.nifi,dbx:T.dbx,benefit:T.benefit,priority:T.priority})),externalSystems:Object.keys(p),newCapabilities:V.map(T=>({name:T.name,useCases:T.useCases||[]})),roi:{annualSavings:fe,hoursSavedPerWeek:te,nifiHoursPerWeek:ee,dbxHoursPerWeek:se,complexityReduction:J},roadmap:q.map(T=>({phase:T.phase,title:T.title,effort:T.effort,dependencies:T.dependencies})),totalEstimatedEffort:L+" days",timestamp:new Date().toISOString()};return{html:o,valueAnalysis:I}}function It(e){const n=new Date().toISOString().replace(/[:.]/g,"-").substring(0,16);return`${e?.parsed?.source_name?e.parsed.source_name.replace(/\.[^.]+$/,"").replace(/[^a-zA-Z0-9_-]/g,"_").substring(0,40):"nifi_migration"}_${n}`}function Ft(e,n){if(!e||e.size===0)return console.error("[download] Empty blob, skipping download for:",n),!1;const t=document.createElement("a"),r=URL.createObjectURL(e);return t.href=r,t.download=n,t.click(),setTimeout(()=>URL.revokeObjectURL(r),100),!0}function Sn(e){if(!e.notebook)return console.warn("[download] No notebook in state â€” complete Convert step first."),!1;const n=e.notebook.cells;if(!n||n.length===0)return console.warn("[download] Notebook has no cells â€” cannot download empty notebook."),!1;const t=n.filter(o=>!o.source||o.source.trim().length===0);t.length>0&&console.warn(`[download] ${t.length} empty cells detected in notebook.`);const r=n.map(o=>o.type==="md"||o.type==="markdown"||o.role==="markdown"?`# MAGIC %md
`+o.source.split(`
`).map(a=>"# MAGIC "+a).join(`
`):o.type==="sql"?`# MAGIC %sql
`+o.source.split(`
`).map(a=>"# MAGIC "+a).join(`
`):o.source),i=`# Databricks notebook source
# Generated: `+new Date().toISOString()+`
# Cells: `+n.length+`

`+r.join(`

# COMMAND ----------

`);return Ft(new Blob([i],{type:"text/plain"}),It(e)+"_notebook.py")}function wn(e){if(!e.notebook?.workflow)return console.warn("[download] No workflow in state â€” complete Convert step first."),!1;const n={...e.notebook.workflow,_meta:{generated:new Date().toISOString(),tool:"NiFi Flow Analyzer v2.0.1",source:e.parsed?.source_name||"unknown"}};return Ft(new Blob([JSON.stringify(n,null,2)],{type:"application/json"}),It(e)+"_workflow.json")}function En(e){if(!e.migrationReport)return console.warn("[download] No migration report in state â€” complete Report step first."),!1;const n=e.migrationReport,t=n.summary;let r=`# NiFi â†’ Databricks Migration Report

`;return r+=`> Generated: ${new Date().toISOString()} | Tool: NiFi Flow Analyzer v2.0.1

`,r+=`## Summary
| Metric | Value |
|--------|-------|
`,r+=`| Total Processors | ${t.totalProcessors} |
| Mapped | ${t.mappedProcessors} |
| Unmapped | ${t.unmappedProcessors} |
`,r+=`| Coverage | ${t.coveragePercent}% |
| Process Groups | ${t.totalProcessGroups} |
| Effort | ${n.effort} |

`,r+=`## By Role
| Role | Mapped | Total | % |
|------|--------|-------|---|
`,Object.entries(n.byRole).forEach(([i,o])=>{r+=`| ${i} | ${o.mapped} | ${o.total} | ${o.total?Math.round(o.mapped/o.total*100):0}% |
`}),r+=`
## By Group
| Group | Mapped | Total | % |
|-------|--------|-------|---|
`,Object.entries(n.byGroup).forEach(([i,o])=>{r+=`| ${i} | ${o.mapped} | ${o.total} | ${o.total?Math.round(o.mapped/o.total*100):0}% |
`}),n.gaps&&n.gaps.length&&(r+=`
## Gaps (${n.gaps.length})
| Processor | Type | Group | Recommendation |
|-----------|------|-------|----------------|
`,n.gaps.forEach(i=>{r+=`| ${i.processor||i.name} | ${i.type} | ${i.group||"â€”"} | ${i.recommendation||"Manual"} |
`})),n.recommendations&&n.recommendations.length&&(r+=`
## Recommendations
`,n.recommendations.forEach(i=>{r+=`- ${i}
`})),Ft(new Blob([r],{type:"text/markdown"}),It(e)+"_migration_report.md")}function er(e){if(!e.finalReport)return console.warn("[download] No final report in state â€” complete Final Report step first."),!1;const n={...Bn(e.finalReport),_meta:{generated:new Date().toISOString(),tool:"NiFi Flow Analyzer v2.0.1",source:e.parsed?.source_name||"unknown"}},t=JSON.stringify(n,null,2);return Ft(new Blob([t],{type:"application/json"}),It(e)+"_analysis_report.json")}function tr(e){if(!e.validation)return console.warn("[download] No validation in state â€” complete Validate step first."),!1;const n={...Bn(e.validation),_meta:{generated:new Date().toISOString(),tool:"NiFi Flow Analyzer v2.0.1"}},t=JSON.stringify(n,null,2);return Ft(new Blob([t],{type:"application/json"}),It(e)+"_validation_report.json")}function nr(e){if(!e.valueAnalysis)return console.warn("[download] No value analysis in state â€” complete Value Analysis step first."),!1;const t={...typeof e.valueAnalysis=="string"?{html:e.valueAnalysis}:e.valueAnalysis,_meta:{generated:new Date().toISOString(),tool:"NiFi Flow Analyzer v2.0.1"}},r=JSON.stringify(t,null,2);return Ft(new Blob([r],{type:"application/json"}),It(e)+"_value_analysis.json")}function us(e){if(!e||e.length===0){alert("No notebook cells generated yet. Run the conversion pipeline first.");return}let n=`# Databricks notebook source
`;n+=`# Generated by NiFi Flow Analyzer â€” Automated Migration
`,n+="# Date: "+new Date().toISOString().split("T")[0]+`

`,e.forEach((o,a)=>{a>0&&(n+=`
# COMMAND ----------

`);const s=o.source||o.code||"";o.type==="markdown"||o.type==="md"||o.role==="markdown"?(n+=`# MAGIC %md
`,s.split(`
`).forEach(c=>{n+="# MAGIC "+c+`
`})):o.type==="sql"?(n+=`# MAGIC %sql
`,s.split(`
`).forEach(c=>{n+="# MAGIC "+c+`
`})):n+=s+`
`});const t=new Blob([n],{type:"text/plain"}),r=URL.createObjectURL(t),i=document.createElement("a");i.href=r,i.download="nifi_migration_notebook.py",i.click(),setTimeout(()=>URL.revokeObjectURL(r),100)}function fs(e){if(!e||e.length===0){alert("No notebook cells generated yet. Run the conversion pipeline first.");return}const n={nbformat:4,nbformat_minor:5,metadata:{kernelspec:{display_name:"Python 3",language:"python",name:"python3"},language_info:{name:"python",version:"3.10.0"}},cells:e.map(o=>{const a=o.type==="markdown"||o.type==="md"||o.role==="markdown",s={cell_type:a?"markdown":"code",metadata:{},source:(o.source||o.code||"").split(`
`).map((c,l,d)=>l<d.length-1?c+`
`:c)};return a||(s.outputs=[],s.execution_count=null),s})},t=new Blob([JSON.stringify(n,null,2)],{type:"application/json"}),r=URL.createObjectURL(t),i=document.createElement("a");i.href=r,i.download="nifi_migration_notebook.ipynb",i.click(),setTimeout(()=>URL.revokeObjectURL(r),100)}function Gl(e,n){if(!e){alert("No NiFi flow loaded yet.");return}const t=e.processGroups||[],r=e.connections||[],i=n(t,r);let o=`# Databricks Workflow â€” Generated from NiFi Flow
`;o+="# Date: "+new Date().toISOString().split("T")[0]+`

`,o+=`name: nifi_migration_workflow
`,o+=`tasks:
`,i.tasks.forEach(l=>{!l.task_key||!l.notebook_task?.notebook_path||(o+='  - task_key: "'+l.task_key.replace(/"/g,'\\"')+`"
`,o+=`    notebook_task:
`,o+='      notebook_path: "'+l.notebook_task.notebook_path.replace(/"/g,'\\"')+`"
`,o+=`      source: WORKSPACE
`,l.depends_on&&l.depends_on.length>0&&(o+=`    depends_on:
`,l.depends_on.forEach(d=>{d.task_key&&(o+='      - task_key: "'+d.task_key.replace(/"/g,'\\"')+`"
`)})),o+=`
`)});const a=new Blob([o],{type:"text/yaml"}),s=URL.createObjectURL(a),c=document.createElement("a");c.href=s,c.download="nifi_migration_workflow.yml",c.click(),setTimeout(()=>URL.revokeObjectURL(s),100)}function Hl(e,{escapeHTML:n,metricsHTML:t}={}){const r={};return e.notebook&&e.parsed&&e.parsed._nifi&&(r.migrationReport=Do(e.notebook.mappings||e.assessment?.mappings||[],e.parsed._nifi)),e.assessment&&e.parsed&&e.parsed._nifi&&(r.comparison=Il(e.assessment.mappings,e.parsed._nifi)),e.parsed&&(r.finalReport=Oo(e)),e.parsed&&e.parsed._nifi&&e.notebook&&n&&(r.valueAnalysis=No({nifi:e.parsed._nifi,notebook:e.notebook,escapeHTML:n})),r}function Wn(e,n){const t=document.createElement("a"),r=URL.createObjectURL(e);t.href=r,t.download=n,t.click(),setTimeout(()=>URL.revokeObjectURL(r),100)}function Vn(e){const n=new Date().toISOString().replace(/[:.]/g,"-").substring(0,16);return`${(e?.meta?.flow_name||"nifi_report").replace(/\.[^.]+$/,"").replace(/[^a-zA-Z0-9_-]/g,"_").substring(0,40)}_${n}`}function Wl(e){const n=e,t=n.assessment?.readiness_score||"N/A";let r=`<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><title>NiFi Migration Report</title>
<style>
body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',sans-serif;max-width:900px;margin:40px auto;padding:0 20px;color:#333;line-height:1.6}
h1{color:#1a73e8;border-bottom:2px solid #1a73e8;padding-bottom:8px}
h2{color:#2c3e50;margin-top:24px}
table{width:100%;border-collapse:collapse;margin:12px 0}
th,td{padding:8px 12px;border:1px solid #ddd;text-align:left;font-size:0.9rem}
th{background:#f5f5f5;font-weight:600}
.metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(180px,1fr));gap:12px;margin:16px 0}
.metric-card{background:#f8f9fa;border:1px solid #e0e0e0;border-radius:8px;padding:16px;text-align:center}
.metric-value{font-size:1.8rem;font-weight:800;color:#1a73e8}
.metric-label{font-size:0.85rem;color:#666}
.badge{display:inline-block;padding:2px 10px;border-radius:12px;font-size:0.8rem;font-weight:600}
.badge-green{background:#d4edda;color:#155724}
.badge-amber{background:#fff3cd;color:#856404}
.badge-red{background:#f8d7da;color:#721c24}
.footer{margin-top:40px;padding-top:12px;border-top:1px solid #ddd;font-size:0.8rem;color:#999}
</style></head><body>
<h1>NiFi Flow Migration Report</h1>
<p><strong>Flow:</strong> ${n.meta?.flow_name||"Unknown"} | <strong>Generated:</strong> ${n.meta?.generated||new Date().toISOString()}</p>

<h2>Summary</h2>
<div class="metric-grid">
<div class="metric-card"><div class="metric-value">${n.flow_summary?.processor_count||0}</div><div class="metric-label">Processors</div></div>
<div class="metric-card"><div class="metric-value">${n.flow_summary?.connection_count||0}</div><div class="metric-label">Connections</div></div>
<div class="metric-card"><div class="metric-value">${n.flow_summary?.external_system_count||0}</div><div class="metric-label">External Systems</div></div>
<div class="metric-card"><div class="metric-value">${t}${typeof t=="number"?"%":""}</div><div class="metric-label">Readiness Score</div></div>
</div>`;n.assessment&&(r+=`<h2>Assessment</h2>
<div class="metric-grid">
<div class="metric-card"><div class="metric-value" style="color:#28a745">${n.assessment.auto_convertible||0}</div><div class="metric-label">Auto-Convertible</div></div>
<div class="metric-card"><div class="metric-value" style="color:#ffc107">${n.assessment.manual_conversion||0}</div><div class="metric-label">Manual Conversion</div></div>
<div class="metric-card"><div class="metric-value" style="color:#dc3545">${n.assessment.unsupported||0}</div><div class="metric-label">Unsupported</div></div>
</div>`),n.processors&&n.processors.length&&(r+=`<h2>Processors (${n.processors.length})</h2>
<table><thead><tr><th>Name</th><th>Type</th><th>Role</th><th>Group</th><th>State</th></tr></thead><tbody>`,n.processors.forEach(i=>{r+=`<tr><td>${i.name}</td><td><code>${i.type}</code></td><td>${i.role||"-"}</td><td>${i.group||"-"}</td><td>${i.state||"-"}</td></tr>`}),r+="</tbody></table>"),n.assessment?.mappings?.length&&(r+=`<h2>Mapping Assessment</h2>
<table><thead><tr><th>Processor</th><th>NiFi Type</th><th>Mapped?</th><th>Databricks Equivalent</th><th>Confidence</th></tr></thead><tbody>`,n.assessment.mappings.forEach(i=>{const o=i.confidence>=.8?"badge-green":i.confidence>=.5?"badge-amber":"badge-red";r+=`<tr><td>${i.name}</td><td><code>${i.nifi_type}</code></td><td>${i.mapped?"Yes":"No"}</td><td>${i.databricks||"-"}</td><td><span class="badge ${o}">${Math.round((i.confidence||0)*100)}%</span></td></tr>`}),r+="</tbody></table>"),r+=`<div class="footer">Generated by NiFi Flow Analyzer v2.0.1 | ${new Date().toISOString()}</div>
</body></html>`,Wn(new Blob([r],{type:"text/html"}),Vn(e)+"_report.html")}function Vl(e){const n=e;let t=`# NiFi Flow Migration Report

`;t+=`> **Flow:** ${n.meta?.flow_name||"Unknown"} | **Generated:** ${n.meta?.generated||new Date().toISOString()}

`,t+=`## Summary

`,t+=`| Metric | Value |
|--------|-------|
`,t+=`| Processors | ${n.flow_summary?.processor_count||0} |
`,t+=`| Connections | ${n.flow_summary?.connection_count||0} |
`,t+=`| Process Groups | ${n.flow_summary?.process_group_count||0} |
`,t+=`| Controller Services | ${n.flow_summary?.controller_service_count||0} |
`,t+=`| External Systems | ${n.flow_summary?.external_system_count||0} |

`,n.assessment&&(t+=`## Assessment

`,t+=`| Metric | Value |
|--------|-------|
`,t+=`| Readiness Score | ${n.assessment.readiness_score}% |
`,t+=`| Auto-Convertible | ${n.assessment.auto_convertible} |
`,t+=`| Manual Conversion | ${n.assessment.manual_conversion} |
`,t+=`| Unsupported | ${n.assessment.unsupported} |

`),n.processors&&n.processors.length&&(t+=`## Processors (${n.processors.length})

`,t+=`| Name | Type | Role | Group |
|------|------|------|-------|
`,n.processors.forEach(r=>{t+=`| ${r.name} | ${r.type} | ${r.role||"-"} | ${r.group||"-"} |
`}),t+=`
`),n.assessment?.mappings?.length&&(t+=`## Mapping Assessment

`,t+=`| Processor | NiFi Type | Mapped | Databricks | Confidence |
|-----------|-----------|--------|------------|------------|
`,n.assessment.mappings.forEach(r=>{t+=`| ${r.name} | ${r.nifi_type} | ${r.mapped?"Yes":"No"} | ${r.databricks||"-"} | ${Math.round((r.confidence||0)*100)}% |
`}),t+=`
`),t+=`---
*Generated by NiFi Flow Analyzer v2.0.1 | ${new Date().toISOString()}*
`,Wn(new Blob([t],{type:"text/markdown"}),Vn(e)+"_report.md")}function Kl(e){const n=[["Name","Type","Role","Group","State","Scheduling Strategy","Scheduling Period"]];(e.processors||[]).forEach(r=>{n.push([Be(r.name),Be(r.type),Be(r.role||""),Be(r.group||""),Be(r.state||""),Be(r.scheduling?.strategy||""),Be(r.scheduling?.period||"")])});const t=n.map(r=>r.join(",")).join(`
`);Wn(new Blob([t],{type:"text/csv"}),Vn(e)+"_processors.csv")}function Ql(e){const n=[["Processor","NiFi Type","Mapped","Databricks Equivalent","Confidence","Role"]];(e.assessment?.mappings||[]).filter(r=>!r.mapped).forEach(r=>{n.push([Be(r.name),Be(r.nifi_type),"No",Be(r.databricks||""),Math.round((r.confidence||0)*100)+"%",Be(r.role||"")])});const t=n.map(r=>r.join(",")).join(`
`);Wn(new Blob([t],{type:"text/csv"}),Vn(e)+"_gaps.csv")}function Be(e){return e=String(e||""),e.includes(",")||e.includes('"')||e.includes(`
`)?'"'+e.replace(/"/g,'""')+'"':e}const Jl={"Intent Match":"Measures how well the generated Databricks notebook preserves the original intent of each NiFi processor. Checks that each processor's purpose (ingestion, transformation, routing, etc.) is represented in the output code.","Line Coverage":"Measures the percentage of NiFi processor properties and configurations that have corresponding code in the generated notebook cells. Higher coverage means fewer manual adjustments needed.","Reverse Eng.":"Evaluates whether the generated code could reconstruct the original NiFi flow behavior, including data flow paths, error handling, and external system connectivity.","Function Map":"Checks that each NiFi processor function has a corresponding PySpark/Databricks function call. Identifies gaps where NiFi capabilities lack a direct Databricks equivalent."};document.addEventListener("DOMContentLoaded",()=>{Oa();const e=za();Object.entries({cfgCatalog:e.catalog,cfgSchema:e.schema,cfgScope:e.secretScope,cfgCloud:e.cloudProvider,cfgNodeType:e.nodeType,cfgWorkers:e.numWorkers,cfgWorkspacePath:e.workspacePath,cfgComputeType:e.computeType||xe.computeType,cfgRuntimeVersion:e.runtimeVersion||xe.runtimeVersion}).forEach(([C,A])=>{const w=document.getElementById(C);w&&(w.value=A??"")});function n(C){const A=document.getElementById("cfgNodeType");if(!A)return;const w=Jr[C]||Jr.azure,R=A.value;A.innerHTML="",w.forEach(y=>{const O=document.createElement("option");O.value=y.value,O.textContent=y.label,A.appendChild(O)}),w.some(y=>y.value===R)?A.value=R:A.value=w[0].value}n(e.cloudProvider||xe.cloudProvider);const t=document.getElementById("cfgNodeType");t&&e.nodeType&&(t.value=e.nodeType);const r=document.getElementById("cfgCloud");r&&r.addEventListener("change",()=>{n(r.value)}),window.addEventListener("unhandledrejection",C=>{$e(new we(C.reason?.message||"Unhandled promise rejection",{code:"UNHANDLED_REJECTION",severity:"high",cause:C.reason}))}),window.addEventListener("error",C=>{$e(new we(C.message||"Uncaught error",{code:"UNCAUGHT_ERROR",severity:"high",context:{filename:C.filename,lineno:C.lineno}}))}),Ya(),uo(),fl(),ml(),Za(document.body),Qa(C=>{ao(C)});const i=document.getElementById("fileInput");i&&i.addEventListener("change",async()=>{Gt(),Ht(),pt();const C=document.getElementById("errorList");C&&(C.innerHTML="");const{handleFile:A}=await kt(async()=>{const{handleFile:w}=await Promise.resolve().then(()=>ni);return{handleFile:w}},void 0,import.meta.url);await A(),bn()});const o=document.getElementById("parseBtn");o&&o.addEventListener("click",()=>{Gt(),Ht(),pt();const C=document.getElementById("errorList");C&&(C.innerHTML=""),bn()}),document.querySelectorAll("[data-sample-flow]").forEach(C=>{C.addEventListener("click",()=>{const A=C.dataset.sampleFlow;Gt(),Ht(),pt();const w=document.getElementById("errorList");w&&(w.innerHTML=""),si(A,bn)})}),document.querySelectorAll("[data-sample-file]").forEach(C=>{C.addEventListener("click",()=>{const A=C.dataset.sampleFile,w=C.dataset.sampleName||A.split("/").pop();Gt(),Ht(),pt();const R=document.getElementById("errorList");R&&(R.innerHTML=""),oi(A,w,bn)})});const c=document.getElementById("analyzeBtn");c&&c.addEventListener("click",async()=>{c.disabled=!0;try{await To()}finally{c.disabled=!1,lt()}});const l=document.getElementById("assessBtn");l&&l.addEventListener("click",async()=>{l.disabled=!0;try{await Ao()}finally{l.disabled=!1,lt()}});const d=document.getElementById("convertBtn");d&&d.addEventListener("click",async()=>{d.disabled=!0;try{await Io()}finally{d.disabled=!1,lt()}});const u=document.getElementById("reportBtn");u&&u.addEventListener("click",async()=>{u.disabled=!0;try{await Fo()}finally{u.disabled=!1,lt()}});const g=document.getElementById("finalReportBtn");g&&g.addEventListener("click",async()=>{g.disabled=!0;try{await window.generateFinalReport()}finally{g.disabled=!1}});const p=document.getElementById("validateBtn");p&&p.addEventListener("click",async()=>{p.disabled=!0;try{await window.runValidation()}finally{p.disabled=!1}});const m=document.getElementById("valueBtn");m&&m.addEventListener("click",async()=>{m.disabled=!0;try{await window.runValueAnalysis()}finally{m.disabled=!1}});const f=document.getElementById("downloadNotebookBtn");f&&f.addEventListener("click",()=>{try{Sn(ce())}catch(C){console.error("[download]",C),alert("Download failed: "+C.message)}});const h=document.getElementById("downloadWorkflowBtn");h&&h.addEventListener("click",()=>{try{wn(ce())}catch(C){console.error("[download]",C),alert("Download failed: "+C.message)}});const _=document.getElementById("downloadReportBtn");_&&_.addEventListener("click",()=>{try{En(ce())}catch(C){console.error("[download]",C),alert("Download failed: "+C.message)}});const v=document.getElementById("downloadFinalReportBtn");v&&v.addEventListener("click",()=>{try{er(ce())}catch(C){console.error("[download]",C),alert("Download failed: "+C.message)}});const k=document.getElementById("downloadValidationBtn");k&&k.addEventListener("click",()=>{try{tr(ce())}catch(C){console.error("[download]",C),alert("Download failed: "+C.message)}});const E=document.getElementById("downloadValueBtn");E&&E.addEventListener("click",()=>{try{nr(ce())}catch(C){console.error("[download]",C),alert("Download failed: "+C.message)}});const D=document.getElementById("exportDatabricksBtn");D&&D.addEventListener("click",()=>{const A=ce().notebook?.cells||[];us(A)});const P=document.getElementById("exportJupyterBtn");P&&P.addEventListener("click",()=>{const A=ce().notebook?.cells||[];fs(A)});const $=document.getElementById("exportWorkflowYamlBtn");$&&$.addEventListener("click",()=>{const C=ce();C.parsed?._nifi&&Gl(C.parsed._nifi,(A,w)=>({tasks:(A||[]).map((R,y)=>({task_key:R.name?.replace(/\s+/g,"_").toLowerCase()||`task_${y}`,notebook_task:{notebook_path:`/Workspace/Migrations/NiFi/${R.name||"task_"+y}`},depends_on:y>0?[{task_key:(A[y-1].name||`task_${y-1}`).replace(/\s+/g,"_").toLowerCase()}]:[]}))}))});const x=document.getElementById("cfgSaveBtn");x&&x.addEventListener("click",()=>{const C=ro();Ga(C),Xn.emit("config:saved",C);const A=x.textContent;x.textContent="Saved!",x.disabled=!0,setTimeout(()=>{x.textContent=A,x.disabled=!1},1500)});const S=document.getElementById("cfgResetBtn");S&&S.addEventListener("click",()=>{const C={...xe},A={cfgCatalog:C.catalog,cfgSchema:C.schema,cfgScope:C.secretScope,cfgCloud:C.cloudProvider,cfgComputeType:C.computeType,cfgRuntimeVersion:C.runtimeVersion,cfgWorkers:C.numWorkers,cfgWorkspacePath:C.workspacePath};Object.entries(A).forEach(([R,y])=>{const O=document.getElementById(R);O&&(O.value=y??"")}),n(C.cloudProvider);const w=document.getElementById("cfgNodeType");w&&(w.value=C.nodeType),Xn.emit("config:reset",C)}),document.body.addEventListener("click",C=>{const A=C.target.closest("[data-action]");if(A){const O=A.dataset.action;C.preventDefault(),O==="download-notebook"?Sn(ce()):O==="download-workflow"?wn(ce()):O==="download-report-markdown"&&En(ce());return}const w=C.target.closest("[data-copy-code]");if(w){C.preventDefault();const O=w.dataset.copyCode;navigator.clipboard.writeText(O).then(()=>{const J=w.textContent;w.textContent="Copied!",setTimeout(()=>{w.textContent=J},1500)}).catch(()=>{});return}const R=C.target.closest("button");if(!R)return;const y=R.textContent.trim().toLowerCase();y.includes("download")&&y.includes("notebook")?(C.preventDefault(),Sn(ce())):y.includes("download")&&y.includes("workflow")?(C.preventDefault(),wn(ce())):y.includes("download")&&y.includes("report")&&y.includes("markdown")&&(C.preventDefault(),En(ce()))}),window.downloadNotebook=()=>Sn(ce()),window.downloadWorkflow=()=>wn(ce()),window.downloadReport=()=>En(ce()),window.downloadFinalReport=()=>er(ce()),window.downloadValidationReport=()=>tr(ce()),window.downloadValueAnalysis=()=>nr(ce()),window.exportAsDatabricksNotebook=()=>{const C=ce();us(C.notebook?.cells||[])},window.exportAsJupyterNotebook=()=>{const C=ce();fs(C.notebook?.cells||[])},window.parseFlow=dt,window.runAnalysisEngine=wl,window.mapNiFiToDatabricks=Mr,window.generateNotebookAndWorkflow=El,window.runValidationEngine=ds,window.generateReportSuite=Hl,window.analyzeFlowGraph=Lo,window.generateFinalReport=async()=>{const C=ce();if(!C.parsed)return;const A=tt();ge("reportFinal","processing");try{const{html:w,report:R}=Ul(C,at,B);Xe({finalReport:R});const y=document.getElementById("reportFinalResults");y&&(y.innerHTML=w);const O=document.getElementById("finalReportDownloadBtn");O&&O.addEventListener("click",()=>er(ce()));const J=document.getElementById("finalReportExportBtn");J&&J.addEventListener("click",()=>{const U=document.getElementById("finalReportFormatSelect");if(!U||!U.value)return;const F=ce().finalReport;if(F){switch(U.value){case"html":Wl(F);break;case"markdown":Vl(F);break;case"csv-processors":Kl(F);break;case"csv-gaps":Ql(F);break}U.value=""}}),ge("reportFinal","done"),lt(),gt("validate");const M=document.getElementById("validateNotReady"),z=document.getElementById("validateReady");M&&M.classList.add("hidden"),z&&z.classList.remove("hidden")}catch(w){nt(A),$e(new we("Final report failed: "+w.message,{code:"FINAL_REPORT_FAILED",phase:"reportFinal",cause:w}));const R=document.getElementById("reportFinalResults");R&&(R.innerHTML=`<div class="alert alert-error">${B("Final report failed: "+w.message)}</div>`),ge("reportFinal","ready")}},window.runValidation=async()=>{const C=ce();if(!C.parsed||!C.parsed._nifi||!C.notebook)return;const A=tt();ge("validate","processing");const w=document.getElementById("validateResults");try{const R=await ds({nifi:C.parsed._nifi,mappings:C.notebook.mappings||C.assessment?.mappings||[],cells:C.notebook.cells||[],systems:C.assessment?.systems||{},nifiDatabricksMap:ht,onProgress:(J,M)=>{w&&(w.innerHTML=`<div style="color:var(--text2);padding:16px">${B(M)} (${J}%)</div>`)}});if(Xe({validation:R}),w){const J=R.overallScore||0;let z=`<hr class="divider"><div class="score-big" style="color:var(--${J>=90?"green":J>=70?"amber":"red"})">Validation Score: ${Math.round(J)}%</div>`;if(z+='<div class="metrics">',[{label:"Intent Match",value:Math.round(R.intentScore||0)+"%"},{label:"Line Coverage",value:Math.round(R.lineScore||0)+"%"},{label:"Reverse Eng.",value:Math.round(R.reScore||0)+"%"},{label:"Function Map",value:Math.round(R.funcScore||0)+"%"}].forEach(F=>{const N=Jl[F.label]||"";z+=`<div class="metric"><div class="label">${B(F.label)}`,N&&(z+=`<span class="info-tooltip"><span class="tooltip-icon">?</span><span class="tooltip-text">${B(N)}</span></span>`),z+=`</div><div class="value">${B(F.value)}</div></div>`}),z+="</div>",R.allGaps&&R.allGaps.length){const F=R.allGaps.map(ee=>{let se="LOW";const te=(ee.issue||ee.gap||ee.reason||ee.message||"").toLowerCase();return te.includes("no mapping")||te.includes("no databricks")||te.includes("missing")||te.includes("unmapped")?se="CRITICAL":te.includes("no notebook cell")||te.includes("no cell")?se="HIGH":(te.includes("low property")||te.includes("coverage"))&&(se="MEDIUM"),{...ee,severity:se}}),N={CRITICAL:0,HIGH:1,MEDIUM:2,LOW:3};F.sort((ee,se)=>N[ee.severity]-N[se.severity]);const H={CRITICAL:0,HIGH:0,MEDIUM:0,LOW:0};F.forEach(ee=>{H[ee.severity]++}),z+='<hr class="divider"><h3>Gaps ('+F.length+")</h3>",z+='<div style="display:flex;gap:8px;margin-bottom:12px;flex-wrap:wrap">',H.CRITICAL>0&&(z+=`<span class="severity-badge severity-critical">Critical: ${H.CRITICAL}</span>`),H.HIGH>0&&(z+=`<span class="severity-badge severity-high">High: ${H.HIGH}</span>`),H.MEDIUM>0&&(z+=`<span class="severity-badge severity-medium">Medium: ${H.MEDIUM}</span>`),H.LOW>0&&(z+=`<span class="severity-badge severity-low">Low: ${H.LOW}</span>`),z+="</div>";const Q=50,V=Math.ceil(F.length/Q);for(let ee=0;ee<V;ee++){const se=ee*Q,te=Math.min(se+Q,F.length),ae=V>1?`Gaps ${se+1}-${te}`:"All Gaps";z+=`<div class="expander ${ee===0?"open":""}"><div class="expander-header" data-expander-toggle><span>${ae}</span><span class="expander-arrow">â–¶</span></div><div class="expander-body">`,z+='<ul style="margin:0;padding-left:20px;font-size:0.85rem;list-style:none">',F.slice(se,te).forEach(ie=>{const _e=`<span class="severity-badge ${"severity-"+ie.severity.toLowerCase()}" style="margin-right:6px">${ie.severity}</span>`,ye=ie.role?'<span class="ns ns-'+B(ie.role)+'" style="font-size:0.7rem;margin-right:4px">'+B(ie.role)+"</span>":"",b=ie.confidence!=null?' <span style="opacity:0.6;font-size:0.75rem">('+Math.round((ie.confidence||0)*100)+"%)</span>":"";if(z+='<li style="margin:6px 0;padding:6px;background:var(--surface);border-radius:4px">'+_e+ye+"<strong>"+B(ie.proc||ie.processor||ie.name||"Unknown")+"</strong>"+b+": "+B(ie.issue||ie.gap||ie.reason||ie.message||"Unknown gap"),ie.severity==="CRITICAL"||ie.severity==="HIGH"){const K=(ie.type||"").split(".").pop()||"UnknownProcessor",q=ht[K];if(q&&q.tpl)z+='<div class="remediation-block" style="margin-top:6px">',z+=`<div class="remediation-header"><span>Suggested skeleton code</span><button class="remediation-copy-btn" data-copy-code="${B(q.tpl)}">Copy</button></div>`,z+=`<div class="remediation-code">${B(q.tpl)}</div>`,z+="</div>";else{const L=`# TODO: Implement ${K} equivalent
# NiFi processor: ${B(ie.proc||ie.name||"Unknown")}
# Original intent: ${B(ie.issue||ie.gap||"")}
df = spark.read.format("delta").load("...")
# Add transformation logic here
df.write.format("delta").mode("append").saveAsTable("catalog.schema.table")`;z+='<div class="remediation-block" style="margin-top:6px">',z+=`<div class="remediation-header"><span>Generic skeleton</span><button class="remediation-copy-btn" data-copy-code="${B(L)}">Copy</button></div>`,z+=`<div class="remediation-code">${B(L)}</div>`,z+="</div>"}}z+="</li>"}),z+="</ul></div></div>"}}R.missingImports&&R.missingImports.length&&(z+='<hr class="divider"><h3>Missing Imports ('+R.missingImports.length+")</h3>",R.missingImports.forEach(F=>{z+='<div class="remediation-block" style="margin:6px 0">',z+=`<div class="remediation-header"><span><code>${B(F.symbol||"")}</code> needed by cell ${B(String(F.usedIn||""))}</span><button class="remediation-copy-btn" data-copy-code="${B(F.suggestion||"")}">Copy</button></div>`,z+=`<div class="remediation-code">${B(F.suggestion||"")}</div>`,z+="</div>"})),R.validationErrors&&R.validationErrors.length&&(z+='<hr class="divider"><h3>Validation Warnings ('+R.validationErrors.length+")</h3>",z+='<div class="alert alert-warn">',R.validationErrors.forEach(F=>{z+=`<div style="margin:4px 0"><strong>${B(F.phase)}</strong>: ${B(F.message)}</div>`}),z+="</div>"),z+='<hr class="divider"><button class="btn" id="validationDownloadBtn">Download Validation Report</button>',w.innerHTML=z;const X=document.getElementById("validationDownloadBtn");X&&X.addEventListener("click",()=>tr(ce()))}ge("validate","done"),lt(),gt("value");const y=document.getElementById("valueNotReady"),O=document.getElementById("valueReady");y&&y.classList.add("hidden"),O&&O.classList.remove("hidden")}catch(R){nt(A),$e(new we("Validation failed: "+R.message,{code:"VALIDATE_FAILED",phase:"validate",cause:R})),w&&(w.innerHTML=`<div class="alert alert-error">${B("Validation failed: "+R.message)}</div>`),ge("validate","ready")}},window.runValueAnalysis=async()=>{const C=ce();if(!C.parsed||!C.parsed._nifi||!C.notebook)return;const A=tt();ge("value","processing");try{const w=No({nifi:C.parsed._nifi,notebook:C.notebook,escapeHTML:B});Xe({valueAnalysis:typeof w=="object"?w:{html:w}});const R=document.getElementById("valueResults");if(R){R.innerHTML=typeof w=="string"?w:w?.html||"";const y=document.getElementById("valueAnalysisDownloadBtn");y&&y.addEventListener("click",()=>nr(ce()))}ge("value","done"),lt()}catch(w){nt(A),$e(new we("Value analysis failed: "+w.message,{code:"VALUE_ANALYSIS_FAILED",phase:"value",cause:w}));const R=document.getElementById("valueResults");R&&(R.innerHTML=`<div class="alert alert-error">${B("Value analysis failed: "+w.message)}</div>`),ge("value","ready")}},console.info("[main] NiFi Flow Analyzer initialized"),Xn.emit("app:ready",{config:e})});/*! pako 2.1.0 https://github.com/nodeca/pako @license (MIT AND Zlib) */const Zl=4,ms=0,hs=1,Xl=2;function Lt(e){let n=e.length;for(;--n>=0;)e[n]=0}const Yl=0,Mo=1,ed=2,td=3,nd=258,Br=29,ln=256,Yt=ln+1+Br,Pt=30,jr=19,Bo=2*Yt+1,ut=15,rr=16,rd=7,Ur=256,jo=16,Uo=17,qo=18,kr=new Uint8Array([0,0,0,0,0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,0]),Ln=new Uint8Array([0,0,0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13]),sd=new Uint8Array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,3,7]),zo=new Uint8Array([16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15]),od=512,Je=new Array((Yt+2)*2);Lt(Je);const Jt=new Array(Pt*2);Lt(Jt);const en=new Array(od);Lt(en);const tn=new Array(nd-td+1);Lt(tn);const qr=new Array(Br);Lt(qr);const jn=new Array(Pt);Lt(jn);function sr(e,n,t,r,i){this.static_tree=e,this.extra_bits=n,this.extra_base=t,this.elems=r,this.max_length=i,this.has_stree=e&&e.length}let Go,Ho,Wo;function or(e,n){this.dyn_tree=e,this.max_code=0,this.stat_desc=n}const Vo=e=>e<256?en[e]:en[256+(e>>>7)],nn=(e,n)=>{e.pending_buf[e.pending++]=n&255,e.pending_buf[e.pending++]=n>>>8&255},Te=(e,n,t)=>{e.bi_valid>rr-t?(e.bi_buf|=n<<e.bi_valid&65535,nn(e,e.bi_buf),e.bi_buf=n>>rr-e.bi_valid,e.bi_valid+=t-rr):(e.bi_buf|=n<<e.bi_valid&65535,e.bi_valid+=t)},Ge=(e,n,t)=>{Te(e,t[n*2],t[n*2+1])},Ko=(e,n)=>{let t=0;do t|=e&1,e>>>=1,t<<=1;while(--n>0);return t>>>1},ad=e=>{e.bi_valid===16?(nn(e,e.bi_buf),e.bi_buf=0,e.bi_valid=0):e.bi_valid>=8&&(e.pending_buf[e.pending++]=e.bi_buf&255,e.bi_buf>>=8,e.bi_valid-=8)},id=(e,n)=>{const t=n.dyn_tree,r=n.max_code,i=n.stat_desc.static_tree,o=n.stat_desc.has_stree,a=n.stat_desc.extra_bits,s=n.stat_desc.extra_base,c=n.stat_desc.max_length;let l,d,u,g,p,m,f=0;for(g=0;g<=ut;g++)e.bl_count[g]=0;for(t[e.heap[e.heap_max]*2+1]=0,l=e.heap_max+1;l<Bo;l++)d=e.heap[l],g=t[t[d*2+1]*2+1]+1,g>c&&(g=c,f++),t[d*2+1]=g,!(d>r)&&(e.bl_count[g]++,p=0,d>=s&&(p=a[d-s]),m=t[d*2],e.opt_len+=m*(g+p),o&&(e.static_len+=m*(i[d*2+1]+p)));if(f!==0){do{for(g=c-1;e.bl_count[g]===0;)g--;e.bl_count[g]--,e.bl_count[g+1]+=2,e.bl_count[c]--,f-=2}while(f>0);for(g=c;g!==0;g--)for(d=e.bl_count[g];d!==0;)u=e.heap[--l],!(u>r)&&(t[u*2+1]!==g&&(e.opt_len+=(g-t[u*2+1])*t[u*2],t[u*2+1]=g),d--)}},Qo=(e,n,t)=>{const r=new Array(ut+1);let i=0,o,a;for(o=1;o<=ut;o++)i=i+t[o-1]<<1,r[o]=i;for(a=0;a<=n;a++){let s=e[a*2+1];s!==0&&(e[a*2]=Ko(r[s]++,s))}},cd=()=>{let e,n,t,r,i;const o=new Array(ut+1);for(t=0,r=0;r<Br-1;r++)for(qr[r]=t,e=0;e<1<<kr[r];e++)tn[t++]=r;for(tn[t-1]=r,i=0,r=0;r<16;r++)for(jn[r]=i,e=0;e<1<<Ln[r];e++)en[i++]=r;for(i>>=7;r<Pt;r++)for(jn[r]=i<<7,e=0;e<1<<Ln[r]-7;e++)en[256+i++]=r;for(n=0;n<=ut;n++)o[n]=0;for(e=0;e<=143;)Je[e*2+1]=8,e++,o[8]++;for(;e<=255;)Je[e*2+1]=9,e++,o[9]++;for(;e<=279;)Je[e*2+1]=7,e++,o[7]++;for(;e<=287;)Je[e*2+1]=8,e++,o[8]++;for(Qo(Je,Yt+1,o),e=0;e<Pt;e++)Jt[e*2+1]=5,Jt[e*2]=Ko(e,5);Go=new sr(Je,kr,ln+1,Yt,ut),Ho=new sr(Jt,Ln,0,Pt,ut),Wo=new sr(new Array(0),sd,0,jr,rd)},Jo=e=>{let n;for(n=0;n<Yt;n++)e.dyn_ltree[n*2]=0;for(n=0;n<Pt;n++)e.dyn_dtree[n*2]=0;for(n=0;n<jr;n++)e.bl_tree[n*2]=0;e.dyn_ltree[Ur*2]=1,e.opt_len=e.static_len=0,e.sym_next=e.matches=0},Zo=e=>{e.bi_valid>8?nn(e,e.bi_buf):e.bi_valid>0&&(e.pending_buf[e.pending++]=e.bi_buf),e.bi_buf=0,e.bi_valid=0},gs=(e,n,t,r)=>{const i=n*2,o=t*2;return e[i]<e[o]||e[i]===e[o]&&r[n]<=r[t]},ar=(e,n,t)=>{const r=e.heap[t];let i=t<<1;for(;i<=e.heap_len&&(i<e.heap_len&&gs(n,e.heap[i+1],e.heap[i],e.depth)&&i++,!gs(n,r,e.heap[i],e.depth));)e.heap[t]=e.heap[i],t=i,i<<=1;e.heap[t]=r},_s=(e,n,t)=>{let r,i,o=0,a,s;if(e.sym_next!==0)do r=e.pending_buf[e.sym_buf+o++]&255,r+=(e.pending_buf[e.sym_buf+o++]&255)<<8,i=e.pending_buf[e.sym_buf+o++],r===0?Ge(e,i,n):(a=tn[i],Ge(e,a+ln+1,n),s=kr[a],s!==0&&(i-=qr[a],Te(e,i,s)),r--,a=Vo(r),Ge(e,a,t),s=Ln[a],s!==0&&(r-=jn[a],Te(e,r,s)));while(o<e.sym_next);Ge(e,Ur,n)},Sr=(e,n)=>{const t=n.dyn_tree,r=n.stat_desc.static_tree,i=n.stat_desc.has_stree,o=n.stat_desc.elems;let a,s,c=-1,l;for(e.heap_len=0,e.heap_max=Bo,a=0;a<o;a++)t[a*2]!==0?(e.heap[++e.heap_len]=c=a,e.depth[a]=0):t[a*2+1]=0;for(;e.heap_len<2;)l=e.heap[++e.heap_len]=c<2?++c:0,t[l*2]=1,e.depth[l]=0,e.opt_len--,i&&(e.static_len-=r[l*2+1]);for(n.max_code=c,a=e.heap_len>>1;a>=1;a--)ar(e,t,a);l=o;do a=e.heap[1],e.heap[1]=e.heap[e.heap_len--],ar(e,t,1),s=e.heap[1],e.heap[--e.heap_max]=a,e.heap[--e.heap_max]=s,t[l*2]=t[a*2]+t[s*2],e.depth[l]=(e.depth[a]>=e.depth[s]?e.depth[a]:e.depth[s])+1,t[a*2+1]=t[s*2+1]=l,e.heap[1]=l++,ar(e,t,1);while(e.heap_len>=2);e.heap[--e.heap_max]=e.heap[1],id(e,n),Qo(t,c,e.bl_count)},ys=(e,n,t)=>{let r,i=-1,o,a=n[1],s=0,c=7,l=4;for(a===0&&(c=138,l=3),n[(t+1)*2+1]=65535,r=0;r<=t;r++)o=a,a=n[(r+1)*2+1],!(++s<c&&o===a)&&(s<l?e.bl_tree[o*2]+=s:o!==0?(o!==i&&e.bl_tree[o*2]++,e.bl_tree[jo*2]++):s<=10?e.bl_tree[Uo*2]++:e.bl_tree[qo*2]++,s=0,i=o,a===0?(c=138,l=3):o===a?(c=6,l=3):(c=7,l=4))},bs=(e,n,t)=>{let r,i=-1,o,a=n[1],s=0,c=7,l=4;for(a===0&&(c=138,l=3),r=0;r<=t;r++)if(o=a,a=n[(r+1)*2+1],!(++s<c&&o===a)){if(s<l)do Ge(e,o,e.bl_tree);while(--s!==0);else o!==0?(o!==i&&(Ge(e,o,e.bl_tree),s--),Ge(e,jo,e.bl_tree),Te(e,s-3,2)):s<=10?(Ge(e,Uo,e.bl_tree),Te(e,s-3,3)):(Ge(e,qo,e.bl_tree),Te(e,s-11,7));s=0,i=o,a===0?(c=138,l=3):o===a?(c=6,l=3):(c=7,l=4)}},ld=e=>{let n;for(ys(e,e.dyn_ltree,e.l_desc.max_code),ys(e,e.dyn_dtree,e.d_desc.max_code),Sr(e,e.bl_desc),n=jr-1;n>=3&&e.bl_tree[zo[n]*2+1]===0;n--);return e.opt_len+=3*(n+1)+5+5+4,n},dd=(e,n,t,r)=>{let i;for(Te(e,n-257,5),Te(e,t-1,5),Te(e,r-4,4),i=0;i<r;i++)Te(e,e.bl_tree[zo[i]*2+1],3);bs(e,e.dyn_ltree,n-1),bs(e,e.dyn_dtree,t-1)},pd=e=>{let n=4093624447,t;for(t=0;t<=31;t++,n>>>=1)if(n&1&&e.dyn_ltree[t*2]!==0)return ms;if(e.dyn_ltree[18]!==0||e.dyn_ltree[20]!==0||e.dyn_ltree[26]!==0)return hs;for(t=32;t<ln;t++)if(e.dyn_ltree[t*2]!==0)return hs;return ms};let vs=!1;const ud=e=>{vs||(cd(),vs=!0),e.l_desc=new or(e.dyn_ltree,Go),e.d_desc=new or(e.dyn_dtree,Ho),e.bl_desc=new or(e.bl_tree,Wo),e.bi_buf=0,e.bi_valid=0,Jo(e)},Xo=(e,n,t,r)=>{Te(e,(Yl<<1)+(r?1:0),3),Zo(e),nn(e,t),nn(e,~t),t&&e.pending_buf.set(e.window.subarray(n,n+t),e.pending),e.pending+=t},fd=e=>{Te(e,Mo<<1,3),Ge(e,Ur,Je),ad(e)},md=(e,n,t,r)=>{let i,o,a=0;e.level>0?(e.strm.data_type===Xl&&(e.strm.data_type=pd(e)),Sr(e,e.l_desc),Sr(e,e.d_desc),a=ld(e),i=e.opt_len+3+7>>>3,o=e.static_len+3+7>>>3,o<=i&&(i=o)):i=o=t+5,t+4<=i&&n!==-1?Xo(e,n,t,r):e.strategy===Zl||o===i?(Te(e,(Mo<<1)+(r?1:0),3),_s(e,Je,Jt)):(Te(e,(ed<<1)+(r?1:0),3),dd(e,e.l_desc.max_code+1,e.d_desc.max_code+1,a+1),_s(e,e.dyn_ltree,e.dyn_dtree)),Jo(e),r&&Zo(e)},hd=(e,n,t)=>(e.pending_buf[e.sym_buf+e.sym_next++]=n,e.pending_buf[e.sym_buf+e.sym_next++]=n>>8,e.pending_buf[e.sym_buf+e.sym_next++]=t,n===0?e.dyn_ltree[t*2]++:(e.matches++,n--,e.dyn_ltree[(tn[t]+ln+1)*2]++,e.dyn_dtree[Vo(n)*2]++),e.sym_next===e.sym_end);var gd=ud,_d=Xo,yd=md,bd=hd,vd=fd,kd={_tr_init:gd,_tr_stored_block:_d,_tr_flush_block:yd,_tr_tally:bd,_tr_align:vd};const Sd=(e,n,t,r)=>{let i=e&65535|0,o=e>>>16&65535|0,a=0;for(;t!==0;){a=t>2e3?2e3:t,t-=a;do i=i+n[r++]|0,o=o+i|0;while(--a);i%=65521,o%=65521}return i|o<<16|0};var rn=Sd;const wd=()=>{let e,n=[];for(var t=0;t<256;t++){e=t;for(var r=0;r<8;r++)e=e&1?3988292384^e>>>1:e>>>1;n[t]=e}return n},Ed=new Uint32Array(wd()),xd=(e,n,t,r)=>{const i=Ed,o=r+t;e^=-1;for(let a=r;a<o;a++)e=e>>>8^i[(e^n[a])&255];return e^-1};var Ee=xd,yt={2:"need dictionary",1:"stream end",0:"","-1":"file error","-2":"stream error","-3":"data error","-4":"insufficient memory","-5":"buffer error","-6":"incompatible version"},dn={Z_NO_FLUSH:0,Z_PARTIAL_FLUSH:1,Z_SYNC_FLUSH:2,Z_FULL_FLUSH:3,Z_FINISH:4,Z_BLOCK:5,Z_TREES:6,Z_OK:0,Z_STREAM_END:1,Z_NEED_DICT:2,Z_ERRNO:-1,Z_STREAM_ERROR:-2,Z_DATA_ERROR:-3,Z_MEM_ERROR:-4,Z_BUF_ERROR:-5,Z_NO_COMPRESSION:0,Z_BEST_SPEED:1,Z_BEST_COMPRESSION:9,Z_DEFAULT_COMPRESSION:-1,Z_FILTERED:1,Z_HUFFMAN_ONLY:2,Z_RLE:3,Z_FIXED:4,Z_DEFAULT_STRATEGY:0,Z_BINARY:0,Z_TEXT:1,Z_UNKNOWN:2,Z_DEFLATED:8};const{_tr_init:Cd,_tr_stored_block:wr,_tr_flush_block:$d,_tr_tally:rt,_tr_align:Pd}=kd,{Z_NO_FLUSH:st,Z_PARTIAL_FLUSH:Rd,Z_FULL_FLUSH:Dd,Z_FINISH:Oe,Z_BLOCK:ks,Z_OK:Ce,Z_STREAM_END:Ss,Z_STREAM_ERROR:He,Z_DATA_ERROR:Td,Z_BUF_ERROR:ir,Z_DEFAULT_COMPRESSION:Ad,Z_FILTERED:Id,Z_HUFFMAN_ONLY:xn,Z_RLE:Fd,Z_FIXED:Ld,Z_DEFAULT_STRATEGY:Od,Z_UNKNOWN:Nd,Z_DEFLATED:Kn}=dn,Md=9,Bd=15,jd=8,Ud=29,qd=256,Er=qd+1+Ud,zd=30,Gd=19,Hd=2*Er+1,Wd=15,le=3,et=258,We=et+le+1,Vd=32,Tt=42,zr=57,xr=69,Cr=73,$r=91,Pr=103,ft=113,Wt=666,Re=1,Ot=2,bt=3,Nt=4,Kd=3,mt=(e,n)=>(e.msg=yt[n],n),ws=e=>e*2-(e>4?9:0),Ye=e=>{let n=e.length;for(;--n>=0;)e[n]=0},Qd=e=>{let n,t,r,i=e.w_size;n=e.hash_size,r=n;do t=e.head[--r],e.head[r]=t>=i?t-i:0;while(--n);n=i,r=n;do t=e.prev[--r],e.prev[r]=t>=i?t-i:0;while(--n)};let Jd=(e,n,t)=>(n<<e.hash_shift^t)&e.hash_mask,ot=Jd;const Ae=e=>{const n=e.state;let t=n.pending;t>e.avail_out&&(t=e.avail_out),t!==0&&(e.output.set(n.pending_buf.subarray(n.pending_out,n.pending_out+t),e.next_out),e.next_out+=t,n.pending_out+=t,e.total_out+=t,e.avail_out-=t,n.pending-=t,n.pending===0&&(n.pending_out=0))},Ie=(e,n)=>{$d(e,e.block_start>=0?e.block_start:-1,e.strstart-e.block_start,n),e.block_start=e.strstart,Ae(e.strm)},ue=(e,n)=>{e.pending_buf[e.pending++]=n},zt=(e,n)=>{e.pending_buf[e.pending++]=n>>>8&255,e.pending_buf[e.pending++]=n&255},Rr=(e,n,t,r)=>{let i=e.avail_in;return i>r&&(i=r),i===0?0:(e.avail_in-=i,n.set(e.input.subarray(e.next_in,e.next_in+i),t),e.state.wrap===1?e.adler=rn(e.adler,n,i,t):e.state.wrap===2&&(e.adler=Ee(e.adler,n,i,t)),e.next_in+=i,e.total_in+=i,i)},Yo=(e,n)=>{let t=e.max_chain_length,r=e.strstart,i,o,a=e.prev_length,s=e.nice_match;const c=e.strstart>e.w_size-We?e.strstart-(e.w_size-We):0,l=e.window,d=e.w_mask,u=e.prev,g=e.strstart+et;let p=l[r+a-1],m=l[r+a];e.prev_length>=e.good_match&&(t>>=2),s>e.lookahead&&(s=e.lookahead);do if(i=n,!(l[i+a]!==m||l[i+a-1]!==p||l[i]!==l[r]||l[++i]!==l[r+1])){r+=2,i++;do;while(l[++r]===l[++i]&&l[++r]===l[++i]&&l[++r]===l[++i]&&l[++r]===l[++i]&&l[++r]===l[++i]&&l[++r]===l[++i]&&l[++r]===l[++i]&&l[++r]===l[++i]&&r<g);if(o=et-(g-r),r=g-et,o>a){if(e.match_start=n,a=o,o>=s)break;p=l[r+a-1],m=l[r+a]}}while((n=u[n&d])>c&&--t!==0);return a<=e.lookahead?a:e.lookahead},At=e=>{const n=e.w_size;let t,r,i;do{if(r=e.window_size-e.lookahead-e.strstart,e.strstart>=n+(n-We)&&(e.window.set(e.window.subarray(n,n+n-r),0),e.match_start-=n,e.strstart-=n,e.block_start-=n,e.insert>e.strstart&&(e.insert=e.strstart),Qd(e),r+=n),e.strm.avail_in===0)break;if(t=Rr(e.strm,e.window,e.strstart+e.lookahead,r),e.lookahead+=t,e.lookahead+e.insert>=le)for(i=e.strstart-e.insert,e.ins_h=e.window[i],e.ins_h=ot(e,e.ins_h,e.window[i+1]);e.insert&&(e.ins_h=ot(e,e.ins_h,e.window[i+le-1]),e.prev[i&e.w_mask]=e.head[e.ins_h],e.head[e.ins_h]=i,i++,e.insert--,!(e.lookahead+e.insert<le)););}while(e.lookahead<We&&e.strm.avail_in!==0)},ea=(e,n)=>{let t=e.pending_buf_size-5>e.w_size?e.w_size:e.pending_buf_size-5,r,i,o,a=0,s=e.strm.avail_in;do{if(r=65535,o=e.bi_valid+42>>3,e.strm.avail_out<o||(o=e.strm.avail_out-o,i=e.strstart-e.block_start,r>i+e.strm.avail_in&&(r=i+e.strm.avail_in),r>o&&(r=o),r<t&&(r===0&&n!==Oe||n===st||r!==i+e.strm.avail_in)))break;a=n===Oe&&r===i+e.strm.avail_in?1:0,wr(e,0,0,a),e.pending_buf[e.pending-4]=r,e.pending_buf[e.pending-3]=r>>8,e.pending_buf[e.pending-2]=~r,e.pending_buf[e.pending-1]=~r>>8,Ae(e.strm),i&&(i>r&&(i=r),e.strm.output.set(e.window.subarray(e.block_start,e.block_start+i),e.strm.next_out),e.strm.next_out+=i,e.strm.avail_out-=i,e.strm.total_out+=i,e.block_start+=i,r-=i),r&&(Rr(e.strm,e.strm.output,e.strm.next_out,r),e.strm.next_out+=r,e.strm.avail_out-=r,e.strm.total_out+=r)}while(a===0);return s-=e.strm.avail_in,s&&(s>=e.w_size?(e.matches=2,e.window.set(e.strm.input.subarray(e.strm.next_in-e.w_size,e.strm.next_in),0),e.strstart=e.w_size,e.insert=e.strstart):(e.window_size-e.strstart<=s&&(e.strstart-=e.w_size,e.window.set(e.window.subarray(e.w_size,e.w_size+e.strstart),0),e.matches<2&&e.matches++,e.insert>e.strstart&&(e.insert=e.strstart)),e.window.set(e.strm.input.subarray(e.strm.next_in-s,e.strm.next_in),e.strstart),e.strstart+=s,e.insert+=s>e.w_size-e.insert?e.w_size-e.insert:s),e.block_start=e.strstart),e.high_water<e.strstart&&(e.high_water=e.strstart),a?Nt:n!==st&&n!==Oe&&e.strm.avail_in===0&&e.strstart===e.block_start?Ot:(o=e.window_size-e.strstart,e.strm.avail_in>o&&e.block_start>=e.w_size&&(e.block_start-=e.w_size,e.strstart-=e.w_size,e.window.set(e.window.subarray(e.w_size,e.w_size+e.strstart),0),e.matches<2&&e.matches++,o+=e.w_size,e.insert>e.strstart&&(e.insert=e.strstart)),o>e.strm.avail_in&&(o=e.strm.avail_in),o&&(Rr(e.strm,e.window,e.strstart,o),e.strstart+=o,e.insert+=o>e.w_size-e.insert?e.w_size-e.insert:o),e.high_water<e.strstart&&(e.high_water=e.strstart),o=e.bi_valid+42>>3,o=e.pending_buf_size-o>65535?65535:e.pending_buf_size-o,t=o>e.w_size?e.w_size:o,i=e.strstart-e.block_start,(i>=t||(i||n===Oe)&&n!==st&&e.strm.avail_in===0&&i<=o)&&(r=i>o?o:i,a=n===Oe&&e.strm.avail_in===0&&r===i?1:0,wr(e,e.block_start,r,a),e.block_start+=r,Ae(e.strm)),a?bt:Re)},cr=(e,n)=>{let t,r;for(;;){if(e.lookahead<We){if(At(e),e.lookahead<We&&n===st)return Re;if(e.lookahead===0)break}if(t=0,e.lookahead>=le&&(e.ins_h=ot(e,e.ins_h,e.window[e.strstart+le-1]),t=e.prev[e.strstart&e.w_mask]=e.head[e.ins_h],e.head[e.ins_h]=e.strstart),t!==0&&e.strstart-t<=e.w_size-We&&(e.match_length=Yo(e,t)),e.match_length>=le)if(r=rt(e,e.strstart-e.match_start,e.match_length-le),e.lookahead-=e.match_length,e.match_length<=e.max_lazy_match&&e.lookahead>=le){e.match_length--;do e.strstart++,e.ins_h=ot(e,e.ins_h,e.window[e.strstart+le-1]),t=e.prev[e.strstart&e.w_mask]=e.head[e.ins_h],e.head[e.ins_h]=e.strstart;while(--e.match_length!==0);e.strstart++}else e.strstart+=e.match_length,e.match_length=0,e.ins_h=e.window[e.strstart],e.ins_h=ot(e,e.ins_h,e.window[e.strstart+1]);else r=rt(e,0,e.window[e.strstart]),e.lookahead--,e.strstart++;if(r&&(Ie(e,!1),e.strm.avail_out===0))return Re}return e.insert=e.strstart<le-1?e.strstart:le-1,n===Oe?(Ie(e,!0),e.strm.avail_out===0?bt:Nt):e.sym_next&&(Ie(e,!1),e.strm.avail_out===0)?Re:Ot},Et=(e,n)=>{let t,r,i;for(;;){if(e.lookahead<We){if(At(e),e.lookahead<We&&n===st)return Re;if(e.lookahead===0)break}if(t=0,e.lookahead>=le&&(e.ins_h=ot(e,e.ins_h,e.window[e.strstart+le-1]),t=e.prev[e.strstart&e.w_mask]=e.head[e.ins_h],e.head[e.ins_h]=e.strstart),e.prev_length=e.match_length,e.prev_match=e.match_start,e.match_length=le-1,t!==0&&e.prev_length<e.max_lazy_match&&e.strstart-t<=e.w_size-We&&(e.match_length=Yo(e,t),e.match_length<=5&&(e.strategy===Id||e.match_length===le&&e.strstart-e.match_start>4096)&&(e.match_length=le-1)),e.prev_length>=le&&e.match_length<=e.prev_length){i=e.strstart+e.lookahead-le,r=rt(e,e.strstart-1-e.prev_match,e.prev_length-le),e.lookahead-=e.prev_length-1,e.prev_length-=2;do++e.strstart<=i&&(e.ins_h=ot(e,e.ins_h,e.window[e.strstart+le-1]),t=e.prev[e.strstart&e.w_mask]=e.head[e.ins_h],e.head[e.ins_h]=e.strstart);while(--e.prev_length!==0);if(e.match_available=0,e.match_length=le-1,e.strstart++,r&&(Ie(e,!1),e.strm.avail_out===0))return Re}else if(e.match_available){if(r=rt(e,0,e.window[e.strstart-1]),r&&Ie(e,!1),e.strstart++,e.lookahead--,e.strm.avail_out===0)return Re}else e.match_available=1,e.strstart++,e.lookahead--}return e.match_available&&(r=rt(e,0,e.window[e.strstart-1]),e.match_available=0),e.insert=e.strstart<le-1?e.strstart:le-1,n===Oe?(Ie(e,!0),e.strm.avail_out===0?bt:Nt):e.sym_next&&(Ie(e,!1),e.strm.avail_out===0)?Re:Ot},Zd=(e,n)=>{let t,r,i,o;const a=e.window;for(;;){if(e.lookahead<=et){if(At(e),e.lookahead<=et&&n===st)return Re;if(e.lookahead===0)break}if(e.match_length=0,e.lookahead>=le&&e.strstart>0&&(i=e.strstart-1,r=a[i],r===a[++i]&&r===a[++i]&&r===a[++i])){o=e.strstart+et;do;while(r===a[++i]&&r===a[++i]&&r===a[++i]&&r===a[++i]&&r===a[++i]&&r===a[++i]&&r===a[++i]&&r===a[++i]&&i<o);e.match_length=et-(o-i),e.match_length>e.lookahead&&(e.match_length=e.lookahead)}if(e.match_length>=le?(t=rt(e,1,e.match_length-le),e.lookahead-=e.match_length,e.strstart+=e.match_length,e.match_length=0):(t=rt(e,0,e.window[e.strstart]),e.lookahead--,e.strstart++),t&&(Ie(e,!1),e.strm.avail_out===0))return Re}return e.insert=0,n===Oe?(Ie(e,!0),e.strm.avail_out===0?bt:Nt):e.sym_next&&(Ie(e,!1),e.strm.avail_out===0)?Re:Ot},Xd=(e,n)=>{let t;for(;;){if(e.lookahead===0&&(At(e),e.lookahead===0)){if(n===st)return Re;break}if(e.match_length=0,t=rt(e,0,e.window[e.strstart]),e.lookahead--,e.strstart++,t&&(Ie(e,!1),e.strm.avail_out===0))return Re}return e.insert=0,n===Oe?(Ie(e,!0),e.strm.avail_out===0?bt:Nt):e.sym_next&&(Ie(e,!1),e.strm.avail_out===0)?Re:Ot};function ze(e,n,t,r,i){this.good_length=e,this.max_lazy=n,this.nice_length=t,this.max_chain=r,this.func=i}const Vt=[new ze(0,0,0,0,ea),new ze(4,4,8,4,cr),new ze(4,5,16,8,cr),new ze(4,6,32,32,cr),new ze(4,4,16,16,Et),new ze(8,16,32,32,Et),new ze(8,16,128,128,Et),new ze(8,32,128,256,Et),new ze(32,128,258,1024,Et),new ze(32,258,258,4096,Et)],Yd=e=>{e.window_size=2*e.w_size,Ye(e.head),e.max_lazy_match=Vt[e.level].max_lazy,e.good_match=Vt[e.level].good_length,e.nice_match=Vt[e.level].nice_length,e.max_chain_length=Vt[e.level].max_chain,e.strstart=0,e.block_start=0,e.lookahead=0,e.insert=0,e.match_length=e.prev_length=le-1,e.match_available=0,e.ins_h=0};function ep(){this.strm=null,this.status=0,this.pending_buf=null,this.pending_buf_size=0,this.pending_out=0,this.pending=0,this.wrap=0,this.gzhead=null,this.gzindex=0,this.method=Kn,this.last_flush=-1,this.w_size=0,this.w_bits=0,this.w_mask=0,this.window=null,this.window_size=0,this.prev=null,this.head=null,this.ins_h=0,this.hash_size=0,this.hash_bits=0,this.hash_mask=0,this.hash_shift=0,this.block_start=0,this.match_length=0,this.prev_match=0,this.match_available=0,this.strstart=0,this.match_start=0,this.lookahead=0,this.prev_length=0,this.max_chain_length=0,this.max_lazy_match=0,this.level=0,this.strategy=0,this.good_match=0,this.nice_match=0,this.dyn_ltree=new Uint16Array(Hd*2),this.dyn_dtree=new Uint16Array((2*zd+1)*2),this.bl_tree=new Uint16Array((2*Gd+1)*2),Ye(this.dyn_ltree),Ye(this.dyn_dtree),Ye(this.bl_tree),this.l_desc=null,this.d_desc=null,this.bl_desc=null,this.bl_count=new Uint16Array(Wd+1),this.heap=new Uint16Array(2*Er+1),Ye(this.heap),this.heap_len=0,this.heap_max=0,this.depth=new Uint16Array(2*Er+1),Ye(this.depth),this.sym_buf=0,this.lit_bufsize=0,this.sym_next=0,this.sym_end=0,this.opt_len=0,this.static_len=0,this.matches=0,this.insert=0,this.bi_buf=0,this.bi_valid=0}const pn=e=>{if(!e)return 1;const n=e.state;return!n||n.strm!==e||n.status!==Tt&&n.status!==zr&&n.status!==xr&&n.status!==Cr&&n.status!==$r&&n.status!==Pr&&n.status!==ft&&n.status!==Wt?1:0},ta=e=>{if(pn(e))return mt(e,He);e.total_in=e.total_out=0,e.data_type=Nd;const n=e.state;return n.pending=0,n.pending_out=0,n.wrap<0&&(n.wrap=-n.wrap),n.status=n.wrap===2?zr:n.wrap?Tt:ft,e.adler=n.wrap===2?0:1,n.last_flush=-2,Cd(n),Ce},na=e=>{const n=ta(e);return n===Ce&&Yd(e.state),n},tp=(e,n)=>pn(e)||e.state.wrap!==2?He:(e.state.gzhead=n,Ce),ra=(e,n,t,r,i,o)=>{if(!e)return He;let a=1;if(n===Ad&&(n=6),r<0?(a=0,r=-r):r>15&&(a=2,r-=16),i<1||i>Md||t!==Kn||r<8||r>15||n<0||n>9||o<0||o>Ld||r===8&&a!==1)return mt(e,He);r===8&&(r=9);const s=new ep;return e.state=s,s.strm=e,s.status=Tt,s.wrap=a,s.gzhead=null,s.w_bits=r,s.w_size=1<<s.w_bits,s.w_mask=s.w_size-1,s.hash_bits=i+7,s.hash_size=1<<s.hash_bits,s.hash_mask=s.hash_size-1,s.hash_shift=~~((s.hash_bits+le-1)/le),s.window=new Uint8Array(s.w_size*2),s.head=new Uint16Array(s.hash_size),s.prev=new Uint16Array(s.w_size),s.lit_bufsize=1<<i+6,s.pending_buf_size=s.lit_bufsize*4,s.pending_buf=new Uint8Array(s.pending_buf_size),s.sym_buf=s.lit_bufsize,s.sym_end=(s.lit_bufsize-1)*3,s.level=n,s.strategy=o,s.method=t,na(e)},np=(e,n)=>ra(e,n,Kn,Bd,jd,Od),rp=(e,n)=>{if(pn(e)||n>ks||n<0)return e?mt(e,He):He;const t=e.state;if(!e.output||e.avail_in!==0&&!e.input||t.status===Wt&&n!==Oe)return mt(e,e.avail_out===0?ir:He);const r=t.last_flush;if(t.last_flush=n,t.pending!==0){if(Ae(e),e.avail_out===0)return t.last_flush=-1,Ce}else if(e.avail_in===0&&ws(n)<=ws(r)&&n!==Oe)return mt(e,ir);if(t.status===Wt&&e.avail_in!==0)return mt(e,ir);if(t.status===Tt&&t.wrap===0&&(t.status=ft),t.status===Tt){let i=Kn+(t.w_bits-8<<4)<<8,o=-1;if(t.strategy>=xn||t.level<2?o=0:t.level<6?o=1:t.level===6?o=2:o=3,i|=o<<6,t.strstart!==0&&(i|=Vd),i+=31-i%31,zt(t,i),t.strstart!==0&&(zt(t,e.adler>>>16),zt(t,e.adler&65535)),e.adler=1,t.status=ft,Ae(e),t.pending!==0)return t.last_flush=-1,Ce}if(t.status===zr){if(e.adler=0,ue(t,31),ue(t,139),ue(t,8),t.gzhead)ue(t,(t.gzhead.text?1:0)+(t.gzhead.hcrc?2:0)+(t.gzhead.extra?4:0)+(t.gzhead.name?8:0)+(t.gzhead.comment?16:0)),ue(t,t.gzhead.time&255),ue(t,t.gzhead.time>>8&255),ue(t,t.gzhead.time>>16&255),ue(t,t.gzhead.time>>24&255),ue(t,t.level===9?2:t.strategy>=xn||t.level<2?4:0),ue(t,t.gzhead.os&255),t.gzhead.extra&&t.gzhead.extra.length&&(ue(t,t.gzhead.extra.length&255),ue(t,t.gzhead.extra.length>>8&255)),t.gzhead.hcrc&&(e.adler=Ee(e.adler,t.pending_buf,t.pending,0)),t.gzindex=0,t.status=xr;else if(ue(t,0),ue(t,0),ue(t,0),ue(t,0),ue(t,0),ue(t,t.level===9?2:t.strategy>=xn||t.level<2?4:0),ue(t,Kd),t.status=ft,Ae(e),t.pending!==0)return t.last_flush=-1,Ce}if(t.status===xr){if(t.gzhead.extra){let i=t.pending,o=(t.gzhead.extra.length&65535)-t.gzindex;for(;t.pending+o>t.pending_buf_size;){let s=t.pending_buf_size-t.pending;if(t.pending_buf.set(t.gzhead.extra.subarray(t.gzindex,t.gzindex+s),t.pending),t.pending=t.pending_buf_size,t.gzhead.hcrc&&t.pending>i&&(e.adler=Ee(e.adler,t.pending_buf,t.pending-i,i)),t.gzindex+=s,Ae(e),t.pending!==0)return t.last_flush=-1,Ce;i=0,o-=s}let a=new Uint8Array(t.gzhead.extra);t.pending_buf.set(a.subarray(t.gzindex,t.gzindex+o),t.pending),t.pending+=o,t.gzhead.hcrc&&t.pending>i&&(e.adler=Ee(e.adler,t.pending_buf,t.pending-i,i)),t.gzindex=0}t.status=Cr}if(t.status===Cr){if(t.gzhead.name){let i=t.pending,o;do{if(t.pending===t.pending_buf_size){if(t.gzhead.hcrc&&t.pending>i&&(e.adler=Ee(e.adler,t.pending_buf,t.pending-i,i)),Ae(e),t.pending!==0)return t.last_flush=-1,Ce;i=0}t.gzindex<t.gzhead.name.length?o=t.gzhead.name.charCodeAt(t.gzindex++)&255:o=0,ue(t,o)}while(o!==0);t.gzhead.hcrc&&t.pending>i&&(e.adler=Ee(e.adler,t.pending_buf,t.pending-i,i)),t.gzindex=0}t.status=$r}if(t.status===$r){if(t.gzhead.comment){let i=t.pending,o;do{if(t.pending===t.pending_buf_size){if(t.gzhead.hcrc&&t.pending>i&&(e.adler=Ee(e.adler,t.pending_buf,t.pending-i,i)),Ae(e),t.pending!==0)return t.last_flush=-1,Ce;i=0}t.gzindex<t.gzhead.comment.length?o=t.gzhead.comment.charCodeAt(t.gzindex++)&255:o=0,ue(t,o)}while(o!==0);t.gzhead.hcrc&&t.pending>i&&(e.adler=Ee(e.adler,t.pending_buf,t.pending-i,i))}t.status=Pr}if(t.status===Pr){if(t.gzhead.hcrc){if(t.pending+2>t.pending_buf_size&&(Ae(e),t.pending!==0))return t.last_flush=-1,Ce;ue(t,e.adler&255),ue(t,e.adler>>8&255),e.adler=0}if(t.status=ft,Ae(e),t.pending!==0)return t.last_flush=-1,Ce}if(e.avail_in!==0||t.lookahead!==0||n!==st&&t.status!==Wt){let i=t.level===0?ea(t,n):t.strategy===xn?Xd(t,n):t.strategy===Fd?Zd(t,n):Vt[t.level].func(t,n);if((i===bt||i===Nt)&&(t.status=Wt),i===Re||i===bt)return e.avail_out===0&&(t.last_flush=-1),Ce;if(i===Ot&&(n===Rd?Pd(t):n!==ks&&(wr(t,0,0,!1),n===Dd&&(Ye(t.head),t.lookahead===0&&(t.strstart=0,t.block_start=0,t.insert=0))),Ae(e),e.avail_out===0))return t.last_flush=-1,Ce}return n!==Oe?Ce:t.wrap<=0?Ss:(t.wrap===2?(ue(t,e.adler&255),ue(t,e.adler>>8&255),ue(t,e.adler>>16&255),ue(t,e.adler>>24&255),ue(t,e.total_in&255),ue(t,e.total_in>>8&255),ue(t,e.total_in>>16&255),ue(t,e.total_in>>24&255)):(zt(t,e.adler>>>16),zt(t,e.adler&65535)),Ae(e),t.wrap>0&&(t.wrap=-t.wrap),t.pending!==0?Ce:Ss)},sp=e=>{if(pn(e))return He;const n=e.state.status;return e.state=null,n===ft?mt(e,Td):Ce},op=(e,n)=>{let t=n.length;if(pn(e))return He;const r=e.state,i=r.wrap;if(i===2||i===1&&r.status!==Tt||r.lookahead)return He;if(i===1&&(e.adler=rn(e.adler,n,t,0)),r.wrap=0,t>=r.w_size){i===0&&(Ye(r.head),r.strstart=0,r.block_start=0,r.insert=0);let c=new Uint8Array(r.w_size);c.set(n.subarray(t-r.w_size,t),0),n=c,t=r.w_size}const o=e.avail_in,a=e.next_in,s=e.input;for(e.avail_in=t,e.next_in=0,e.input=n,At(r);r.lookahead>=le;){let c=r.strstart,l=r.lookahead-(le-1);do r.ins_h=ot(r,r.ins_h,r.window[c+le-1]),r.prev[c&r.w_mask]=r.head[r.ins_h],r.head[r.ins_h]=c,c++;while(--l);r.strstart=c,r.lookahead=le-1,At(r)}return r.strstart+=r.lookahead,r.block_start=r.strstart,r.insert=r.lookahead,r.lookahead=0,r.match_length=r.prev_length=le-1,r.match_available=0,e.next_in=a,e.input=s,e.avail_in=o,r.wrap=i,Ce};var ap=np,ip=ra,cp=na,lp=ta,dp=tp,pp=rp,up=sp,fp=op,mp="pako deflate (from Nodeca project)",Zt={deflateInit:ap,deflateInit2:ip,deflateReset:cp,deflateResetKeep:lp,deflateSetHeader:dp,deflate:pp,deflateEnd:up,deflateSetDictionary:fp,deflateInfo:mp};const hp=(e,n)=>Object.prototype.hasOwnProperty.call(e,n);var gp=function(e){const n=Array.prototype.slice.call(arguments,1);for(;n.length;){const t=n.shift();if(t){if(typeof t!="object")throw new TypeError(t+"must be non-object");for(const r in t)hp(t,r)&&(e[r]=t[r])}}return e},_p=e=>{let n=0;for(let r=0,i=e.length;r<i;r++)n+=e[r].length;const t=new Uint8Array(n);for(let r=0,i=0,o=e.length;r<o;r++){let a=e[r];t.set(a,i),i+=a.length}return t},Qn={assign:gp,flattenChunks:_p};let sa=!0;try{String.fromCharCode.apply(null,new Uint8Array(1))}catch{sa=!1}const sn=new Uint8Array(256);for(let e=0;e<256;e++)sn[e]=e>=252?6:e>=248?5:e>=240?4:e>=224?3:e>=192?2:1;sn[254]=sn[254]=1;var yp=e=>{if(typeof TextEncoder=="function"&&TextEncoder.prototype.encode)return new TextEncoder().encode(e);let n,t,r,i,o,a=e.length,s=0;for(i=0;i<a;i++)t=e.charCodeAt(i),(t&64512)===55296&&i+1<a&&(r=e.charCodeAt(i+1),(r&64512)===56320&&(t=65536+(t-55296<<10)+(r-56320),i++)),s+=t<128?1:t<2048?2:t<65536?3:4;for(n=new Uint8Array(s),o=0,i=0;o<s;i++)t=e.charCodeAt(i),(t&64512)===55296&&i+1<a&&(r=e.charCodeAt(i+1),(r&64512)===56320&&(t=65536+(t-55296<<10)+(r-56320),i++)),t<128?n[o++]=t:t<2048?(n[o++]=192|t>>>6,n[o++]=128|t&63):t<65536?(n[o++]=224|t>>>12,n[o++]=128|t>>>6&63,n[o++]=128|t&63):(n[o++]=240|t>>>18,n[o++]=128|t>>>12&63,n[o++]=128|t>>>6&63,n[o++]=128|t&63);return n};const bp=(e,n)=>{if(n<65534&&e.subarray&&sa)return String.fromCharCode.apply(null,e.length===n?e:e.subarray(0,n));let t="";for(let r=0;r<n;r++)t+=String.fromCharCode(e[r]);return t};var vp=(e,n)=>{const t=n||e.length;if(typeof TextDecoder=="function"&&TextDecoder.prototype.decode)return new TextDecoder().decode(e.subarray(0,n));let r,i;const o=new Array(t*2);for(i=0,r=0;r<t;){let a=e[r++];if(a<128){o[i++]=a;continue}let s=sn[a];if(s>4){o[i++]=65533,r+=s-1;continue}for(a&=s===2?31:s===3?15:7;s>1&&r<t;)a=a<<6|e[r++]&63,s--;if(s>1){o[i++]=65533;continue}a<65536?o[i++]=a:(a-=65536,o[i++]=55296|a>>10&1023,o[i++]=56320|a&1023)}return bp(o,i)},kp=(e,n)=>{n=n||e.length,n>e.length&&(n=e.length);let t=n-1;for(;t>=0&&(e[t]&192)===128;)t--;return t<0||t===0?n:t+sn[e[t]]>n?t:n},on={string2buf:yp,buf2string:vp,utf8border:kp};function Sp(){this.input=null,this.next_in=0,this.avail_in=0,this.total_in=0,this.output=null,this.next_out=0,this.avail_out=0,this.total_out=0,this.msg="",this.state=null,this.data_type=2,this.adler=0}var oa=Sp;const aa=Object.prototype.toString,{Z_NO_FLUSH:wp,Z_SYNC_FLUSH:Ep,Z_FULL_FLUSH:xp,Z_FINISH:Cp,Z_OK:Un,Z_STREAM_END:$p,Z_DEFAULT_COMPRESSION:Pp,Z_DEFAULT_STRATEGY:Rp,Z_DEFLATED:Dp}=dn;function un(e){this.options=Qn.assign({level:Pp,method:Dp,chunkSize:16384,windowBits:15,memLevel:8,strategy:Rp},e||{});let n=this.options;n.raw&&n.windowBits>0?n.windowBits=-n.windowBits:n.gzip&&n.windowBits>0&&n.windowBits<16&&(n.windowBits+=16),this.err=0,this.msg="",this.ended=!1,this.chunks=[],this.strm=new oa,this.strm.avail_out=0;let t=Zt.deflateInit2(this.strm,n.level,n.method,n.windowBits,n.memLevel,n.strategy);if(t!==Un)throw new Error(yt[t]);if(n.header&&Zt.deflateSetHeader(this.strm,n.header),n.dictionary){let r;if(typeof n.dictionary=="string"?r=on.string2buf(n.dictionary):aa.call(n.dictionary)==="[object ArrayBuffer]"?r=new Uint8Array(n.dictionary):r=n.dictionary,t=Zt.deflateSetDictionary(this.strm,r),t!==Un)throw new Error(yt[t]);this._dict_set=!0}}un.prototype.push=function(e,n){const t=this.strm,r=this.options.chunkSize;let i,o;if(this.ended)return!1;for(n===~~n?o=n:o=n===!0?Cp:wp,typeof e=="string"?t.input=on.string2buf(e):aa.call(e)==="[object ArrayBuffer]"?t.input=new Uint8Array(e):t.input=e,t.next_in=0,t.avail_in=t.input.length;;){if(t.avail_out===0&&(t.output=new Uint8Array(r),t.next_out=0,t.avail_out=r),(o===Ep||o===xp)&&t.avail_out<=6){this.onData(t.output.subarray(0,t.next_out)),t.avail_out=0;continue}if(i=Zt.deflate(t,o),i===$p)return t.next_out>0&&this.onData(t.output.subarray(0,t.next_out)),i=Zt.deflateEnd(this.strm),this.onEnd(i),this.ended=!0,i===Un;if(t.avail_out===0){this.onData(t.output);continue}if(o>0&&t.next_out>0){this.onData(t.output.subarray(0,t.next_out)),t.avail_out=0;continue}if(t.avail_in===0)break}return!0};un.prototype.onData=function(e){this.chunks.push(e)};un.prototype.onEnd=function(e){e===Un&&(this.result=Qn.flattenChunks(this.chunks)),this.chunks=[],this.err=e,this.msg=this.strm.msg};function Gr(e,n){const t=new un(n);if(t.push(e,!0),t.err)throw t.msg||yt[t.err];return t.result}function Tp(e,n){return n=n||{},n.raw=!0,Gr(e,n)}function Ap(e,n){return n=n||{},n.gzip=!0,Gr(e,n)}var Ip=un,Fp=Gr,Lp=Tp,Op=Ap,Np={Deflate:Ip,deflate:Fp,deflateRaw:Lp,gzip:Op};const Cn=16209,Mp=16191;var Bp=function(n,t){let r,i,o,a,s,c,l,d,u,g,p,m,f,h,_,v,k,E,D,P,$,x,S,C;const A=n.state;r=n.next_in,S=n.input,i=r+(n.avail_in-5),o=n.next_out,C=n.output,a=o-(t-n.avail_out),s=o+(n.avail_out-257),c=A.dmax,l=A.wsize,d=A.whave,u=A.wnext,g=A.window,p=A.hold,m=A.bits,f=A.lencode,h=A.distcode,_=(1<<A.lenbits)-1,v=(1<<A.distbits)-1;e:do{m<15&&(p+=S[r++]<<m,m+=8,p+=S[r++]<<m,m+=8),k=f[p&_];t:for(;;){if(E=k>>>24,p>>>=E,m-=E,E=k>>>16&255,E===0)C[o++]=k&65535;else if(E&16){D=k&65535,E&=15,E&&(m<E&&(p+=S[r++]<<m,m+=8),D+=p&(1<<E)-1,p>>>=E,m-=E),m<15&&(p+=S[r++]<<m,m+=8,p+=S[r++]<<m,m+=8),k=h[p&v];n:for(;;){if(E=k>>>24,p>>>=E,m-=E,E=k>>>16&255,E&16){if(P=k&65535,E&=15,m<E&&(p+=S[r++]<<m,m+=8,m<E&&(p+=S[r++]<<m,m+=8)),P+=p&(1<<E)-1,P>c){n.msg="invalid distance too far back",A.mode=Cn;break e}if(p>>>=E,m-=E,E=o-a,P>E){if(E=P-E,E>d&&A.sane){n.msg="invalid distance too far back",A.mode=Cn;break e}if($=0,x=g,u===0){if($+=l-E,E<D){D-=E;do C[o++]=g[$++];while(--E);$=o-P,x=C}}else if(u<E){if($+=l+u-E,E-=u,E<D){D-=E;do C[o++]=g[$++];while(--E);if($=0,u<D){E=u,D-=E;do C[o++]=g[$++];while(--E);$=o-P,x=C}}}else if($+=u-E,E<D){D-=E;do C[o++]=g[$++];while(--E);$=o-P,x=C}for(;D>2;)C[o++]=x[$++],C[o++]=x[$++],C[o++]=x[$++],D-=3;D&&(C[o++]=x[$++],D>1&&(C[o++]=x[$++]))}else{$=o-P;do C[o++]=C[$++],C[o++]=C[$++],C[o++]=C[$++],D-=3;while(D>2);D&&(C[o++]=C[$++],D>1&&(C[o++]=C[$++]))}}else if((E&64)===0){k=h[(k&65535)+(p&(1<<E)-1)];continue n}else{n.msg="invalid distance code",A.mode=Cn;break e}break}}else if((E&64)===0){k=f[(k&65535)+(p&(1<<E)-1)];continue t}else if(E&32){A.mode=Mp;break e}else{n.msg="invalid literal/length code",A.mode=Cn;break e}break}}while(r<i&&o<s);D=m>>3,r-=D,m-=D<<3,p&=(1<<m)-1,n.next_in=r,n.next_out=o,n.avail_in=r<i?5+(i-r):5-(r-i),n.avail_out=o<s?257+(s-o):257-(o-s),A.hold=p,A.bits=m};const xt=15,Es=852,xs=592,Cs=0,lr=1,$s=2,jp=new Uint16Array([3,4,5,6,7,8,9,10,11,13,15,17,19,23,27,31,35,43,51,59,67,83,99,115,131,163,195,227,258,0,0]),Up=new Uint8Array([16,16,16,16,16,16,16,16,17,17,17,17,18,18,18,18,19,19,19,19,20,20,20,20,21,21,21,21,16,72,78]),qp=new Uint16Array([1,2,3,4,5,7,9,13,17,25,33,49,65,97,129,193,257,385,513,769,1025,1537,2049,3073,4097,6145,8193,12289,16385,24577,0,0]),zp=new Uint8Array([16,16,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25,25,26,26,27,27,28,28,29,29,64,64]),Gp=(e,n,t,r,i,o,a,s)=>{const c=s.bits;let l=0,d=0,u=0,g=0,p=0,m=0,f=0,h=0,_=0,v=0,k,E,D,P,$,x=null,S;const C=new Uint16Array(xt+1),A=new Uint16Array(xt+1);let w=null,R,y,O;for(l=0;l<=xt;l++)C[l]=0;for(d=0;d<r;d++)C[n[t+d]]++;for(p=c,g=xt;g>=1&&C[g]===0;g--);if(p>g&&(p=g),g===0)return i[o++]=1<<24|64<<16|0,i[o++]=1<<24|64<<16|0,s.bits=1,0;for(u=1;u<g&&C[u]===0;u++);for(p<u&&(p=u),h=1,l=1;l<=xt;l++)if(h<<=1,h-=C[l],h<0)return-1;if(h>0&&(e===Cs||g!==1))return-1;for(A[1]=0,l=1;l<xt;l++)A[l+1]=A[l]+C[l];for(d=0;d<r;d++)n[t+d]!==0&&(a[A[n[t+d]]++]=d);if(e===Cs?(x=w=a,S=20):e===lr?(x=jp,w=Up,S=257):(x=qp,w=zp,S=0),v=0,d=0,l=u,$=o,m=p,f=0,D=-1,_=1<<p,P=_-1,e===lr&&_>Es||e===$s&&_>xs)return 1;for(;;){R=l-f,a[d]+1<S?(y=0,O=a[d]):a[d]>=S?(y=w[a[d]-S],O=x[a[d]-S]):(y=96,O=0),k=1<<l-f,E=1<<m,u=E;do E-=k,i[$+(v>>f)+E]=R<<24|y<<16|O|0;while(E!==0);for(k=1<<l-1;v&k;)k>>=1;if(k!==0?(v&=k-1,v+=k):v=0,d++,--C[l]===0){if(l===g)break;l=n[t+a[d]]}if(l>p&&(v&P)!==D){for(f===0&&(f=p),$+=u,m=l-f,h=1<<m;m+f<g&&(h-=C[m+f],!(h<=0));)m++,h<<=1;if(_+=1<<m,e===lr&&_>Es||e===$s&&_>xs)return 1;D=v&P,i[D]=p<<24|m<<16|$-o|0}}return v!==0&&(i[$+v]=l-f<<24|64<<16|0),s.bits=p,0};var Xt=Gp;const Hp=0,ia=1,ca=2,{Z_FINISH:Ps,Z_BLOCK:Wp,Z_TREES:$n,Z_OK:vt,Z_STREAM_END:Vp,Z_NEED_DICT:Kp,Z_STREAM_ERROR:Me,Z_DATA_ERROR:la,Z_MEM_ERROR:da,Z_BUF_ERROR:Qp,Z_DEFLATED:Rs}=dn,Jn=16180,Ds=16181,Ts=16182,As=16183,Is=16184,Fs=16185,Ls=16186,Os=16187,Ns=16188,Ms=16189,qn=16190,Qe=16191,dr=16192,Bs=16193,pr=16194,js=16195,Us=16196,qs=16197,zs=16198,Pn=16199,Rn=16200,Gs=16201,Hs=16202,Ws=16203,Vs=16204,Ks=16205,ur=16206,Qs=16207,Js=16208,be=16209,pa=16210,ua=16211,Jp=852,Zp=592,Xp=15,Yp=Xp,Zs=e=>(e>>>24&255)+(e>>>8&65280)+((e&65280)<<8)+((e&255)<<24);function eu(){this.strm=null,this.mode=0,this.last=!1,this.wrap=0,this.havedict=!1,this.flags=0,this.dmax=0,this.check=0,this.total=0,this.head=null,this.wbits=0,this.wsize=0,this.whave=0,this.wnext=0,this.window=null,this.hold=0,this.bits=0,this.length=0,this.offset=0,this.extra=0,this.lencode=null,this.distcode=null,this.lenbits=0,this.distbits=0,this.ncode=0,this.nlen=0,this.ndist=0,this.have=0,this.next=null,this.lens=new Uint16Array(320),this.work=new Uint16Array(288),this.lendyn=null,this.distdyn=null,this.sane=0,this.back=0,this.was=0}const St=e=>{if(!e)return 1;const n=e.state;return!n||n.strm!==e||n.mode<Jn||n.mode>ua?1:0},fa=e=>{if(St(e))return Me;const n=e.state;return e.total_in=e.total_out=n.total=0,e.msg="",n.wrap&&(e.adler=n.wrap&1),n.mode=Jn,n.last=0,n.havedict=0,n.flags=-1,n.dmax=32768,n.head=null,n.hold=0,n.bits=0,n.lencode=n.lendyn=new Int32Array(Jp),n.distcode=n.distdyn=new Int32Array(Zp),n.sane=1,n.back=-1,vt},ma=e=>{if(St(e))return Me;const n=e.state;return n.wsize=0,n.whave=0,n.wnext=0,fa(e)},ha=(e,n)=>{let t;if(St(e))return Me;const r=e.state;return n<0?(t=0,n=-n):(t=(n>>4)+5,n<48&&(n&=15)),n&&(n<8||n>15)?Me:(r.window!==null&&r.wbits!==n&&(r.window=null),r.wrap=t,r.wbits=n,ma(e))},ga=(e,n)=>{if(!e)return Me;const t=new eu;e.state=t,t.strm=e,t.window=null,t.mode=Jn;const r=ha(e,n);return r!==vt&&(e.state=null),r},tu=e=>ga(e,Yp);let Xs=!0,fr,mr;const nu=e=>{if(Xs){fr=new Int32Array(512),mr=new Int32Array(32);let n=0;for(;n<144;)e.lens[n++]=8;for(;n<256;)e.lens[n++]=9;for(;n<280;)e.lens[n++]=7;for(;n<288;)e.lens[n++]=8;for(Xt(ia,e.lens,0,288,fr,0,e.work,{bits:9}),n=0;n<32;)e.lens[n++]=5;Xt(ca,e.lens,0,32,mr,0,e.work,{bits:5}),Xs=!1}e.lencode=fr,e.lenbits=9,e.distcode=mr,e.distbits=5},_a=(e,n,t,r)=>{let i;const o=e.state;return o.window===null&&(o.wsize=1<<o.wbits,o.wnext=0,o.whave=0,o.window=new Uint8Array(o.wsize)),r>=o.wsize?(o.window.set(n.subarray(t-o.wsize,t),0),o.wnext=0,o.whave=o.wsize):(i=o.wsize-o.wnext,i>r&&(i=r),o.window.set(n.subarray(t-r,t-r+i),o.wnext),r-=i,r?(o.window.set(n.subarray(t-r,t),0),o.wnext=r,o.whave=o.wsize):(o.wnext+=i,o.wnext===o.wsize&&(o.wnext=0),o.whave<o.wsize&&(o.whave+=i))),0},ru=(e,n)=>{let t,r,i,o,a,s,c,l,d,u,g,p,m,f,h=0,_,v,k,E,D,P,$,x;const S=new Uint8Array(4);let C,A;const w=new Uint8Array([16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15]);if(St(e)||!e.output||!e.input&&e.avail_in!==0)return Me;t=e.state,t.mode===Qe&&(t.mode=dr),a=e.next_out,i=e.output,c=e.avail_out,o=e.next_in,r=e.input,s=e.avail_in,l=t.hold,d=t.bits,u=s,g=c,x=vt;e:for(;;)switch(t.mode){case Jn:if(t.wrap===0){t.mode=dr;break}for(;d<16;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if(t.wrap&2&&l===35615){t.wbits===0&&(t.wbits=15),t.check=0,S[0]=l&255,S[1]=l>>>8&255,t.check=Ee(t.check,S,2,0),l=0,d=0,t.mode=Ds;break}if(t.head&&(t.head.done=!1),!(t.wrap&1)||(((l&255)<<8)+(l>>8))%31){e.msg="incorrect header check",t.mode=be;break}if((l&15)!==Rs){e.msg="unknown compression method",t.mode=be;break}if(l>>>=4,d-=4,$=(l&15)+8,t.wbits===0&&(t.wbits=$),$>15||$>t.wbits){e.msg="invalid window size",t.mode=be;break}t.dmax=1<<t.wbits,t.flags=0,e.adler=t.check=1,t.mode=l&512?Ms:Qe,l=0,d=0;break;case Ds:for(;d<16;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if(t.flags=l,(t.flags&255)!==Rs){e.msg="unknown compression method",t.mode=be;break}if(t.flags&57344){e.msg="unknown header flags set",t.mode=be;break}t.head&&(t.head.text=l>>8&1),t.flags&512&&t.wrap&4&&(S[0]=l&255,S[1]=l>>>8&255,t.check=Ee(t.check,S,2,0)),l=0,d=0,t.mode=Ts;case Ts:for(;d<32;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}t.head&&(t.head.time=l),t.flags&512&&t.wrap&4&&(S[0]=l&255,S[1]=l>>>8&255,S[2]=l>>>16&255,S[3]=l>>>24&255,t.check=Ee(t.check,S,4,0)),l=0,d=0,t.mode=As;case As:for(;d<16;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}t.head&&(t.head.xflags=l&255,t.head.os=l>>8),t.flags&512&&t.wrap&4&&(S[0]=l&255,S[1]=l>>>8&255,t.check=Ee(t.check,S,2,0)),l=0,d=0,t.mode=Is;case Is:if(t.flags&1024){for(;d<16;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}t.length=l,t.head&&(t.head.extra_len=l),t.flags&512&&t.wrap&4&&(S[0]=l&255,S[1]=l>>>8&255,t.check=Ee(t.check,S,2,0)),l=0,d=0}else t.head&&(t.head.extra=null);t.mode=Fs;case Fs:if(t.flags&1024&&(p=t.length,p>s&&(p=s),p&&(t.head&&($=t.head.extra_len-t.length,t.head.extra||(t.head.extra=new Uint8Array(t.head.extra_len)),t.head.extra.set(r.subarray(o,o+p),$)),t.flags&512&&t.wrap&4&&(t.check=Ee(t.check,r,p,o)),s-=p,o+=p,t.length-=p),t.length))break e;t.length=0,t.mode=Ls;case Ls:if(t.flags&2048){if(s===0)break e;p=0;do $=r[o+p++],t.head&&$&&t.length<65536&&(t.head.name+=String.fromCharCode($));while($&&p<s);if(t.flags&512&&t.wrap&4&&(t.check=Ee(t.check,r,p,o)),s-=p,o+=p,$)break e}else t.head&&(t.head.name=null);t.length=0,t.mode=Os;case Os:if(t.flags&4096){if(s===0)break e;p=0;do $=r[o+p++],t.head&&$&&t.length<65536&&(t.head.comment+=String.fromCharCode($));while($&&p<s);if(t.flags&512&&t.wrap&4&&(t.check=Ee(t.check,r,p,o)),s-=p,o+=p,$)break e}else t.head&&(t.head.comment=null);t.mode=Ns;case Ns:if(t.flags&512){for(;d<16;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if(t.wrap&4&&l!==(t.check&65535)){e.msg="header crc mismatch",t.mode=be;break}l=0,d=0}t.head&&(t.head.hcrc=t.flags>>9&1,t.head.done=!0),e.adler=t.check=0,t.mode=Qe;break;case Ms:for(;d<32;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}e.adler=t.check=Zs(l),l=0,d=0,t.mode=qn;case qn:if(t.havedict===0)return e.next_out=a,e.avail_out=c,e.next_in=o,e.avail_in=s,t.hold=l,t.bits=d,Kp;e.adler=t.check=1,t.mode=Qe;case Qe:if(n===Wp||n===$n)break e;case dr:if(t.last){l>>>=d&7,d-=d&7,t.mode=ur;break}for(;d<3;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}switch(t.last=l&1,l>>>=1,d-=1,l&3){case 0:t.mode=Bs;break;case 1:if(nu(t),t.mode=Pn,n===$n){l>>>=2,d-=2;break e}break;case 2:t.mode=Us;break;case 3:e.msg="invalid block type",t.mode=be}l>>>=2,d-=2;break;case Bs:for(l>>>=d&7,d-=d&7;d<32;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if((l&65535)!==(l>>>16^65535)){e.msg="invalid stored block lengths",t.mode=be;break}if(t.length=l&65535,l=0,d=0,t.mode=pr,n===$n)break e;case pr:t.mode=js;case js:if(p=t.length,p){if(p>s&&(p=s),p>c&&(p=c),p===0)break e;i.set(r.subarray(o,o+p),a),s-=p,o+=p,c-=p,a+=p,t.length-=p;break}t.mode=Qe;break;case Us:for(;d<14;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if(t.nlen=(l&31)+257,l>>>=5,d-=5,t.ndist=(l&31)+1,l>>>=5,d-=5,t.ncode=(l&15)+4,l>>>=4,d-=4,t.nlen>286||t.ndist>30){e.msg="too many length or distance symbols",t.mode=be;break}t.have=0,t.mode=qs;case qs:for(;t.have<t.ncode;){for(;d<3;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}t.lens[w[t.have++]]=l&7,l>>>=3,d-=3}for(;t.have<19;)t.lens[w[t.have++]]=0;if(t.lencode=t.lendyn,t.lenbits=7,C={bits:t.lenbits},x=Xt(Hp,t.lens,0,19,t.lencode,0,t.work,C),t.lenbits=C.bits,x){e.msg="invalid code lengths set",t.mode=be;break}t.have=0,t.mode=zs;case zs:for(;t.have<t.nlen+t.ndist;){for(;h=t.lencode[l&(1<<t.lenbits)-1],_=h>>>24,v=h>>>16&255,k=h&65535,!(_<=d);){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if(k<16)l>>>=_,d-=_,t.lens[t.have++]=k;else{if(k===16){for(A=_+2;d<A;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if(l>>>=_,d-=_,t.have===0){e.msg="invalid bit length repeat",t.mode=be;break}$=t.lens[t.have-1],p=3+(l&3),l>>>=2,d-=2}else if(k===17){for(A=_+3;d<A;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}l>>>=_,d-=_,$=0,p=3+(l&7),l>>>=3,d-=3}else{for(A=_+7;d<A;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}l>>>=_,d-=_,$=0,p=11+(l&127),l>>>=7,d-=7}if(t.have+p>t.nlen+t.ndist){e.msg="invalid bit length repeat",t.mode=be;break}for(;p--;)t.lens[t.have++]=$}}if(t.mode===be)break;if(t.lens[256]===0){e.msg="invalid code -- missing end-of-block",t.mode=be;break}if(t.lenbits=9,C={bits:t.lenbits},x=Xt(ia,t.lens,0,t.nlen,t.lencode,0,t.work,C),t.lenbits=C.bits,x){e.msg="invalid literal/lengths set",t.mode=be;break}if(t.distbits=6,t.distcode=t.distdyn,C={bits:t.distbits},x=Xt(ca,t.lens,t.nlen,t.ndist,t.distcode,0,t.work,C),t.distbits=C.bits,x){e.msg="invalid distances set",t.mode=be;break}if(t.mode=Pn,n===$n)break e;case Pn:t.mode=Rn;case Rn:if(s>=6&&c>=258){e.next_out=a,e.avail_out=c,e.next_in=o,e.avail_in=s,t.hold=l,t.bits=d,Bp(e,g),a=e.next_out,i=e.output,c=e.avail_out,o=e.next_in,r=e.input,s=e.avail_in,l=t.hold,d=t.bits,t.mode===Qe&&(t.back=-1);break}for(t.back=0;h=t.lencode[l&(1<<t.lenbits)-1],_=h>>>24,v=h>>>16&255,k=h&65535,!(_<=d);){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if(v&&(v&240)===0){for(E=_,D=v,P=k;h=t.lencode[P+((l&(1<<E+D)-1)>>E)],_=h>>>24,v=h>>>16&255,k=h&65535,!(E+_<=d);){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}l>>>=E,d-=E,t.back+=E}if(l>>>=_,d-=_,t.back+=_,t.length=k,v===0){t.mode=Ks;break}if(v&32){t.back=-1,t.mode=Qe;break}if(v&64){e.msg="invalid literal/length code",t.mode=be;break}t.extra=v&15,t.mode=Gs;case Gs:if(t.extra){for(A=t.extra;d<A;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}t.length+=l&(1<<t.extra)-1,l>>>=t.extra,d-=t.extra,t.back+=t.extra}t.was=t.length,t.mode=Hs;case Hs:for(;h=t.distcode[l&(1<<t.distbits)-1],_=h>>>24,v=h>>>16&255,k=h&65535,!(_<=d);){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if((v&240)===0){for(E=_,D=v,P=k;h=t.distcode[P+((l&(1<<E+D)-1)>>E)],_=h>>>24,v=h>>>16&255,k=h&65535,!(E+_<=d);){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}l>>>=E,d-=E,t.back+=E}if(l>>>=_,d-=_,t.back+=_,v&64){e.msg="invalid distance code",t.mode=be;break}t.offset=k,t.extra=v&15,t.mode=Ws;case Ws:if(t.extra){for(A=t.extra;d<A;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}t.offset+=l&(1<<t.extra)-1,l>>>=t.extra,d-=t.extra,t.back+=t.extra}if(t.offset>t.dmax){e.msg="invalid distance too far back",t.mode=be;break}t.mode=Vs;case Vs:if(c===0)break e;if(p=g-c,t.offset>p){if(p=t.offset-p,p>t.whave&&t.sane){e.msg="invalid distance too far back",t.mode=be;break}p>t.wnext?(p-=t.wnext,m=t.wsize-p):m=t.wnext-p,p>t.length&&(p=t.length),f=t.window}else f=i,m=a-t.offset,p=t.length;p>c&&(p=c),c-=p,t.length-=p;do i[a++]=f[m++];while(--p);t.length===0&&(t.mode=Rn);break;case Ks:if(c===0)break e;i[a++]=t.length,c--,t.mode=Rn;break;case ur:if(t.wrap){for(;d<32;){if(s===0)break e;s--,l|=r[o++]<<d,d+=8}if(g-=c,e.total_out+=g,t.total+=g,t.wrap&4&&g&&(e.adler=t.check=t.flags?Ee(t.check,i,g,a-g):rn(t.check,i,g,a-g)),g=c,t.wrap&4&&(t.flags?l:Zs(l))!==t.check){e.msg="incorrect data check",t.mode=be;break}l=0,d=0}t.mode=Qs;case Qs:if(t.wrap&&t.flags){for(;d<32;){if(s===0)break e;s--,l+=r[o++]<<d,d+=8}if(t.wrap&4&&l!==(t.total&4294967295)){e.msg="incorrect length check",t.mode=be;break}l=0,d=0}t.mode=Js;case Js:x=Vp;break e;case be:x=la;break e;case pa:return da;case ua:default:return Me}return e.next_out=a,e.avail_out=c,e.next_in=o,e.avail_in=s,t.hold=l,t.bits=d,(t.wsize||g!==e.avail_out&&t.mode<be&&(t.mode<ur||n!==Ps))&&_a(e,e.output,e.next_out,g-e.avail_out),u-=e.avail_in,g-=e.avail_out,e.total_in+=u,e.total_out+=g,t.total+=g,t.wrap&4&&g&&(e.adler=t.check=t.flags?Ee(t.check,i,g,e.next_out-g):rn(t.check,i,g,e.next_out-g)),e.data_type=t.bits+(t.last?64:0)+(t.mode===Qe?128:0)+(t.mode===Pn||t.mode===pr?256:0),(u===0&&g===0||n===Ps)&&x===vt&&(x=Qp),x},su=e=>{if(St(e))return Me;let n=e.state;return n.window&&(n.window=null),e.state=null,vt},ou=(e,n)=>{if(St(e))return Me;const t=e.state;return(t.wrap&2)===0?Me:(t.head=n,n.done=!1,vt)},au=(e,n)=>{const t=n.length;let r,i,o;return St(e)||(r=e.state,r.wrap!==0&&r.mode!==qn)?Me:r.mode===qn&&(i=1,i=rn(i,n,t,0),i!==r.check)?la:(o=_a(e,n,t,t),o?(r.mode=pa,da):(r.havedict=1,vt))};var iu=ma,cu=ha,lu=fa,du=tu,pu=ga,uu=ru,fu=su,mu=ou,hu=au,gu="pako inflate (from Nodeca project)",Ze={inflateReset:iu,inflateReset2:cu,inflateResetKeep:lu,inflateInit:du,inflateInit2:pu,inflate:uu,inflateEnd:fu,inflateGetHeader:mu,inflateSetDictionary:hu,inflateInfo:gu};function _u(){this.text=0,this.time=0,this.xflags=0,this.os=0,this.extra=null,this.extra_len=0,this.name="",this.comment="",this.hcrc=0,this.done=!1}var yu=_u;const ya=Object.prototype.toString,{Z_NO_FLUSH:bu,Z_FINISH:vu,Z_OK:an,Z_STREAM_END:hr,Z_NEED_DICT:gr,Z_STREAM_ERROR:ku,Z_DATA_ERROR:Ys,Z_MEM_ERROR:Su}=dn;function fn(e){this.options=Qn.assign({chunkSize:1024*64,windowBits:15,to:""},e||{});const n=this.options;n.raw&&n.windowBits>=0&&n.windowBits<16&&(n.windowBits=-n.windowBits,n.windowBits===0&&(n.windowBits=-15)),n.windowBits>=0&&n.windowBits<16&&!(e&&e.windowBits)&&(n.windowBits+=32),n.windowBits>15&&n.windowBits<48&&(n.windowBits&15)===0&&(n.windowBits|=15),this.err=0,this.msg="",this.ended=!1,this.chunks=[],this.strm=new oa,this.strm.avail_out=0;let t=Ze.inflateInit2(this.strm,n.windowBits);if(t!==an)throw new Error(yt[t]);if(this.header=new yu,Ze.inflateGetHeader(this.strm,this.header),n.dictionary&&(typeof n.dictionary=="string"?n.dictionary=on.string2buf(n.dictionary):ya.call(n.dictionary)==="[object ArrayBuffer]"&&(n.dictionary=new Uint8Array(n.dictionary)),n.raw&&(t=Ze.inflateSetDictionary(this.strm,n.dictionary),t!==an)))throw new Error(yt[t])}fn.prototype.push=function(e,n){const t=this.strm,r=this.options.chunkSize,i=this.options.dictionary;let o,a,s;if(this.ended)return!1;for(n===~~n?a=n:a=n===!0?vu:bu,ya.call(e)==="[object ArrayBuffer]"?t.input=new Uint8Array(e):t.input=e,t.next_in=0,t.avail_in=t.input.length;;){for(t.avail_out===0&&(t.output=new Uint8Array(r),t.next_out=0,t.avail_out=r),o=Ze.inflate(t,a),o===gr&&i&&(o=Ze.inflateSetDictionary(t,i),o===an?o=Ze.inflate(t,a):o===Ys&&(o=gr));t.avail_in>0&&o===hr&&t.state.wrap>0&&e[t.next_in]!==0;)Ze.inflateReset(t),o=Ze.inflate(t,a);switch(o){case ku:case Ys:case gr:case Su:return this.onEnd(o),this.ended=!0,!1}if(s=t.avail_out,t.next_out&&(t.avail_out===0||o===hr))if(this.options.to==="string"){let c=on.utf8border(t.output,t.next_out),l=t.next_out-c,d=on.buf2string(t.output,c);t.next_out=l,t.avail_out=r-l,l&&t.output.set(t.output.subarray(c,c+l),0),this.onData(d)}else this.onData(t.output.length===t.next_out?t.output:t.output.subarray(0,t.next_out));if(!(o===an&&s===0)){if(o===hr)return o=Ze.inflateEnd(this.strm),this.onEnd(o),this.ended=!0,!0;if(t.avail_in===0)break}}return!0};fn.prototype.onData=function(e){this.chunks.push(e)};fn.prototype.onEnd=function(e){e===an&&(this.options.to==="string"?this.result=this.chunks.join(""):this.result=Qn.flattenChunks(this.chunks)),this.chunks=[],this.err=e,this.msg=this.strm.msg};function Hr(e,n){const t=new fn(n);if(t.push(e),t.err)throw t.msg||yt[t.err];return t.result}function wu(e,n){return n=n||{},n.raw=!0,Hr(e,n)}var Eu=fn,xu=Hr,Cu=wu,$u=Hr,Pu={Inflate:Eu,inflate:xu,inflateRaw:Cu,ungzip:$u};const{Deflate:Ru,deflate:Du,deflateRaw:Tu,gzip:Au}=Np,{Inflate:Iu,inflate:Fu,inflateRaw:Lu,ungzip:Ou}=Pu;var ba=Ru,va=Du,ka=Tu,Sa=Au,wa=Iu,Ea=Fu,xa=Lu,Ca=Ou,$a=dn,Nu={Deflate:ba,deflate:va,deflateRaw:ka,gzip:Sa,Inflate:wa,inflate:Ea,inflateRaw:xa,ungzip:Ca,constants:$a};const Pa=Object.freeze(Object.defineProperty({__proto__:null,Deflate:ba,Inflate:wa,constants:$a,default:Nu,deflate:va,deflateRaw:ka,gzip:Sa,inflate:Ea,inflateRaw:xa,ungzip:Ca},Symbol.toStringTag,{value:"Module"}));var Dn=typeof globalThis<"u"?globalThis:typeof window<"u"?window:typeof global<"u"?global:typeof self<"u"?self:{};function Mu(e){return e&&e.__esModule&&Object.prototype.hasOwnProperty.call(e,"default")?e.default:e}function Tn(e){throw new Error('Could not dynamically require "'+e+'". Please configure the dynamicRequireTargets or/and ignoreDynamicRequires option of @rollup/plugin-commonjs appropriately for this require call to work.')}var _r={exports:{}};/*!

JSZip v3.10.1 - A JavaScript class for generating and reading zip files
<http://stuartk.com/jszip>

(c) 2009-2016 Stuart Knightley <stuart [at] stuartk.com>
Dual licenced under the MIT license or GPLv3. See https://raw.github.com/Stuk/jszip/main/LICENSE.markdown.

JSZip uses the library pako released under the MIT license :
https://github.com/nodeca/pako/blob/main/LICENSE
*/var eo;function Bu(){return eo||(eo=1,(function(e,n){(function(t){e.exports=t()})(function(){return(function t(r,i,o){function a(l,d){if(!i[l]){if(!r[l]){var u=typeof Tn=="function"&&Tn;if(!d&&u)return u(l,!0);if(s)return s(l,!0);var g=new Error("Cannot find module '"+l+"'");throw g.code="MODULE_NOT_FOUND",g}var p=i[l]={exports:{}};r[l][0].call(p.exports,function(m){var f=r[l][1][m];return a(f||m)},p,p.exports,t,r,i,o)}return i[l].exports}for(var s=typeof Tn=="function"&&Tn,c=0;c<o.length;c++)a(o[c]);return a})({1:[function(t,r,i){var o=t("./utils"),a=t("./support"),s="ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=";i.encode=function(c){for(var l,d,u,g,p,m,f,h=[],_=0,v=c.length,k=v,E=o.getTypeOf(c)!=="string";_<c.length;)k=v-_,u=E?(l=c[_++],d=_<v?c[_++]:0,_<v?c[_++]:0):(l=c.charCodeAt(_++),d=_<v?c.charCodeAt(_++):0,_<v?c.charCodeAt(_++):0),g=l>>2,p=(3&l)<<4|d>>4,m=1<k?(15&d)<<2|u>>6:64,f=2<k?63&u:64,h.push(s.charAt(g)+s.charAt(p)+s.charAt(m)+s.charAt(f));return h.join("")},i.decode=function(c){var l,d,u,g,p,m,f=0,h=0,_="data:";if(c.substr(0,_.length)===_)throw new Error("Invalid base64 input, it looks like a data url.");var v,k=3*(c=c.replace(/[^A-Za-z0-9+/=]/g,"")).length/4;if(c.charAt(c.length-1)===s.charAt(64)&&k--,c.charAt(c.length-2)===s.charAt(64)&&k--,k%1!=0)throw new Error("Invalid base64 input, bad content length.");for(v=a.uint8array?new Uint8Array(0|k):new Array(0|k);f<c.length;)l=s.indexOf(c.charAt(f++))<<2|(g=s.indexOf(c.charAt(f++)))>>4,d=(15&g)<<4|(p=s.indexOf(c.charAt(f++)))>>2,u=(3&p)<<6|(m=s.indexOf(c.charAt(f++))),v[h++]=l,p!==64&&(v[h++]=d),m!==64&&(v[h++]=u);return v}},{"./support":30,"./utils":32}],2:[function(t,r,i){var o=t("./external"),a=t("./stream/DataWorker"),s=t("./stream/Crc32Probe"),c=t("./stream/DataLengthProbe");function l(d,u,g,p,m){this.compressedSize=d,this.uncompressedSize=u,this.crc32=g,this.compression=p,this.compressedContent=m}l.prototype={getContentWorker:function(){var d=new a(o.Promise.resolve(this.compressedContent)).pipe(this.compression.uncompressWorker()).pipe(new c("data_length")),u=this;return d.on("end",function(){if(this.streamInfo.data_length!==u.uncompressedSize)throw new Error("Bug : uncompressed data size mismatch")}),d},getCompressedWorker:function(){return new a(o.Promise.resolve(this.compressedContent)).withStreamInfo("compressedSize",this.compressedSize).withStreamInfo("uncompressedSize",this.uncompressedSize).withStreamInfo("crc32",this.crc32).withStreamInfo("compression",this.compression)}},l.createWorkerFrom=function(d,u,g){return d.pipe(new s).pipe(new c("uncompressedSize")).pipe(u.compressWorker(g)).pipe(new c("compressedSize")).withStreamInfo("compression",u)},r.exports=l},{"./external":6,"./stream/Crc32Probe":25,"./stream/DataLengthProbe":26,"./stream/DataWorker":27}],3:[function(t,r,i){var o=t("./stream/GenericWorker");i.STORE={magic:"\0\0",compressWorker:function(){return new o("STORE compression")},uncompressWorker:function(){return new o("STORE decompression")}},i.DEFLATE=t("./flate")},{"./flate":7,"./stream/GenericWorker":28}],4:[function(t,r,i){var o=t("./utils"),a=(function(){for(var s,c=[],l=0;l<256;l++){s=l;for(var d=0;d<8;d++)s=1&s?3988292384^s>>>1:s>>>1;c[l]=s}return c})();r.exports=function(s,c){return s!==void 0&&s.length?o.getTypeOf(s)!=="string"?(function(l,d,u,g){var p=a,m=g+u;l^=-1;for(var f=g;f<m;f++)l=l>>>8^p[255&(l^d[f])];return-1^l})(0|c,s,s.length,0):(function(l,d,u,g){var p=a,m=g+u;l^=-1;for(var f=g;f<m;f++)l=l>>>8^p[255&(l^d.charCodeAt(f))];return-1^l})(0|c,s,s.length,0):0}},{"./utils":32}],5:[function(t,r,i){i.base64=!1,i.binary=!1,i.dir=!1,i.createFolders=!0,i.date=null,i.compression=null,i.compressionOptions=null,i.comment=null,i.unixPermissions=null,i.dosPermissions=null},{}],6:[function(t,r,i){var o=null;o=typeof Promise<"u"?Promise:t("lie"),r.exports={Promise:o}},{lie:37}],7:[function(t,r,i){var o=typeof Uint8Array<"u"&&typeof Uint16Array<"u"&&typeof Uint32Array<"u",a=t("pako"),s=t("./utils"),c=t("./stream/GenericWorker"),l=o?"uint8array":"array";function d(u,g){c.call(this,"FlateWorker/"+u),this._pako=null,this._pakoAction=u,this._pakoOptions=g,this.meta={}}i.magic="\b\0",s.inherits(d,c),d.prototype.processChunk=function(u){this.meta=u.meta,this._pako===null&&this._createPako(),this._pako.push(s.transformTo(l,u.data),!1)},d.prototype.flush=function(){c.prototype.flush.call(this),this._pako===null&&this._createPako(),this._pako.push([],!0)},d.prototype.cleanUp=function(){c.prototype.cleanUp.call(this),this._pako=null},d.prototype._createPako=function(){this._pako=new a[this._pakoAction]({raw:!0,level:this._pakoOptions.level||-1});var u=this;this._pako.onData=function(g){u.push({data:g,meta:u.meta})}},i.compressWorker=function(u){return new d("Deflate",u)},i.uncompressWorker=function(){return new d("Inflate",{})}},{"./stream/GenericWorker":28,"./utils":32,pako:38}],8:[function(t,r,i){function o(p,m){var f,h="";for(f=0;f<m;f++)h+=String.fromCharCode(255&p),p>>>=8;return h}function a(p,m,f,h,_,v){var k,E,D=p.file,P=p.compression,$=v!==l.utf8encode,x=s.transformTo("string",v(D.name)),S=s.transformTo("string",l.utf8encode(D.name)),C=D.comment,A=s.transformTo("string",v(C)),w=s.transformTo("string",l.utf8encode(C)),R=S.length!==D.name.length,y=w.length!==C.length,O="",J="",M="",z=D.dir,U=D.date,X={crc32:0,compressedSize:0,uncompressedSize:0};m&&!f||(X.crc32=p.crc32,X.compressedSize=p.compressedSize,X.uncompressedSize=p.uncompressedSize);var F=0;m&&(F|=8),$||!R&&!y||(F|=2048);var N=0,H=0;z&&(N|=16),_==="UNIX"?(H=798,N|=(function(V,ee){var se=V;return V||(se=ee?16893:33204),(65535&se)<<16})(D.unixPermissions,z)):(H=20,N|=(function(V){return 63&(V||0)})(D.dosPermissions)),k=U.getUTCHours(),k<<=6,k|=U.getUTCMinutes(),k<<=5,k|=U.getUTCSeconds()/2,E=U.getUTCFullYear()-1980,E<<=4,E|=U.getUTCMonth()+1,E<<=5,E|=U.getUTCDate(),R&&(J=o(1,1)+o(d(x),4)+S,O+="up"+o(J.length,2)+J),y&&(M=o(1,1)+o(d(A),4)+w,O+="uc"+o(M.length,2)+M);var Q="";return Q+=`
\0`,Q+=o(F,2),Q+=P.magic,Q+=o(k,2),Q+=o(E,2),Q+=o(X.crc32,4),Q+=o(X.compressedSize,4),Q+=o(X.uncompressedSize,4),Q+=o(x.length,2),Q+=o(O.length,2),{fileRecord:u.LOCAL_FILE_HEADER+Q+x+O,dirRecord:u.CENTRAL_FILE_HEADER+o(H,2)+Q+o(A.length,2)+"\0\0\0\0"+o(N,4)+o(h,4)+x+O+A}}var s=t("../utils"),c=t("../stream/GenericWorker"),l=t("../utf8"),d=t("../crc32"),u=t("../signature");function g(p,m,f,h){c.call(this,"ZipFileWorker"),this.bytesWritten=0,this.zipComment=m,this.zipPlatform=f,this.encodeFileName=h,this.streamFiles=p,this.accumulate=!1,this.contentBuffer=[],this.dirRecords=[],this.currentSourceOffset=0,this.entriesCount=0,this.currentFile=null,this._sources=[]}s.inherits(g,c),g.prototype.push=function(p){var m=p.meta.percent||0,f=this.entriesCount,h=this._sources.length;this.accumulate?this.contentBuffer.push(p):(this.bytesWritten+=p.data.length,c.prototype.push.call(this,{data:p.data,meta:{currentFile:this.currentFile,percent:f?(m+100*(f-h-1))/f:100}}))},g.prototype.openedSource=function(p){this.currentSourceOffset=this.bytesWritten,this.currentFile=p.file.name;var m=this.streamFiles&&!p.file.dir;if(m){var f=a(p,m,!1,this.currentSourceOffset,this.zipPlatform,this.encodeFileName);this.push({data:f.fileRecord,meta:{percent:0}})}else this.accumulate=!0},g.prototype.closedSource=function(p){this.accumulate=!1;var m=this.streamFiles&&!p.file.dir,f=a(p,m,!0,this.currentSourceOffset,this.zipPlatform,this.encodeFileName);if(this.dirRecords.push(f.dirRecord),m)this.push({data:(function(h){return u.DATA_DESCRIPTOR+o(h.crc32,4)+o(h.compressedSize,4)+o(h.uncompressedSize,4)})(p),meta:{percent:100}});else for(this.push({data:f.fileRecord,meta:{percent:0}});this.contentBuffer.length;)this.push(this.contentBuffer.shift());this.currentFile=null},g.prototype.flush=function(){for(var p=this.bytesWritten,m=0;m<this.dirRecords.length;m++)this.push({data:this.dirRecords[m],meta:{percent:100}});var f=this.bytesWritten-p,h=(function(_,v,k,E,D){var P=s.transformTo("string",D(E));return u.CENTRAL_DIRECTORY_END+"\0\0\0\0"+o(_,2)+o(_,2)+o(v,4)+o(k,4)+o(P.length,2)+P})(this.dirRecords.length,f,p,this.zipComment,this.encodeFileName);this.push({data:h,meta:{percent:100}})},g.prototype.prepareNextSource=function(){this.previous=this._sources.shift(),this.openedSource(this.previous.streamInfo),this.isPaused?this.previous.pause():this.previous.resume()},g.prototype.registerPrevious=function(p){this._sources.push(p);var m=this;return p.on("data",function(f){m.processChunk(f)}),p.on("end",function(){m.closedSource(m.previous.streamInfo),m._sources.length?m.prepareNextSource():m.end()}),p.on("error",function(f){m.error(f)}),this},g.prototype.resume=function(){return!!c.prototype.resume.call(this)&&(!this.previous&&this._sources.length?(this.prepareNextSource(),!0):this.previous||this._sources.length||this.generatedError?void 0:(this.end(),!0))},g.prototype.error=function(p){var m=this._sources;if(!c.prototype.error.call(this,p))return!1;for(var f=0;f<m.length;f++)try{m[f].error(p)}catch{}return!0},g.prototype.lock=function(){c.prototype.lock.call(this);for(var p=this._sources,m=0;m<p.length;m++)p[m].lock()},r.exports=g},{"../crc32":4,"../signature":23,"../stream/GenericWorker":28,"../utf8":31,"../utils":32}],9:[function(t,r,i){var o=t("../compressions"),a=t("./ZipFileWorker");i.generateWorker=function(s,c,l){var d=new a(c.streamFiles,l,c.platform,c.encodeFileName),u=0;try{s.forEach(function(g,p){u++;var m=(function(v,k){var E=v||k,D=o[E];if(!D)throw new Error(E+" is not a valid compression method !");return D})(p.options.compression,c.compression),f=p.options.compressionOptions||c.compressionOptions||{},h=p.dir,_=p.date;p._compressWorker(m,f).withStreamInfo("file",{name:g,dir:h,date:_,comment:p.comment||"",unixPermissions:p.unixPermissions,dosPermissions:p.dosPermissions}).pipe(d)}),d.entriesCount=u}catch(g){d.error(g)}return d}},{"../compressions":3,"./ZipFileWorker":8}],10:[function(t,r,i){function o(){if(!(this instanceof o))return new o;if(arguments.length)throw new Error("The constructor with parameters has been removed in JSZip 3.0, please check the upgrade guide.");this.files=Object.create(null),this.comment=null,this.root="",this.clone=function(){var a=new o;for(var s in this)typeof this[s]!="function"&&(a[s]=this[s]);return a}}(o.prototype=t("./object")).loadAsync=t("./load"),o.support=t("./support"),o.defaults=t("./defaults"),o.version="3.10.1",o.loadAsync=function(a,s){return new o().loadAsync(a,s)},o.external=t("./external"),r.exports=o},{"./defaults":5,"./external":6,"./load":11,"./object":15,"./support":30}],11:[function(t,r,i){var o=t("./utils"),a=t("./external"),s=t("./utf8"),c=t("./zipEntries"),l=t("./stream/Crc32Probe"),d=t("./nodejsUtils");function u(g){return new a.Promise(function(p,m){var f=g.decompressed.getContentWorker().pipe(new l);f.on("error",function(h){m(h)}).on("end",function(){f.streamInfo.crc32!==g.decompressed.crc32?m(new Error("Corrupted zip : CRC32 mismatch")):p()}).resume()})}r.exports=function(g,p){var m=this;return p=o.extend(p||{},{base64:!1,checkCRC32:!1,optimizedBinaryString:!1,createFolders:!1,decodeFileName:s.utf8decode}),d.isNode&&d.isStream(g)?a.Promise.reject(new Error("JSZip can't accept a stream when loading a zip file.")):o.prepareContent("the loaded zip file",g,!0,p.optimizedBinaryString,p.base64).then(function(f){var h=new c(p);return h.load(f),h}).then(function(f){var h=[a.Promise.resolve(f)],_=f.files;if(p.checkCRC32)for(var v=0;v<_.length;v++)h.push(u(_[v]));return a.Promise.all(h)}).then(function(f){for(var h=f.shift(),_=h.files,v=0;v<_.length;v++){var k=_[v],E=k.fileNameStr,D=o.resolve(k.fileNameStr);m.file(D,k.decompressed,{binary:!0,optimizedBinaryString:!0,date:k.date,dir:k.dir,comment:k.fileCommentStr.length?k.fileCommentStr:null,unixPermissions:k.unixPermissions,dosPermissions:k.dosPermissions,createFolders:p.createFolders}),k.dir||(m.file(D).unsafeOriginalName=E)}return h.zipComment.length&&(m.comment=h.zipComment),m})}},{"./external":6,"./nodejsUtils":14,"./stream/Crc32Probe":25,"./utf8":31,"./utils":32,"./zipEntries":33}],12:[function(t,r,i){var o=t("../utils"),a=t("../stream/GenericWorker");function s(c,l){a.call(this,"Nodejs stream input adapter for "+c),this._upstreamEnded=!1,this._bindStream(l)}o.inherits(s,a),s.prototype._bindStream=function(c){var l=this;(this._stream=c).pause(),c.on("data",function(d){l.push({data:d,meta:{percent:0}})}).on("error",function(d){l.isPaused?this.generatedError=d:l.error(d)}).on("end",function(){l.isPaused?l._upstreamEnded=!0:l.end()})},s.prototype.pause=function(){return!!a.prototype.pause.call(this)&&(this._stream.pause(),!0)},s.prototype.resume=function(){return!!a.prototype.resume.call(this)&&(this._upstreamEnded?this.end():this._stream.resume(),!0)},r.exports=s},{"../stream/GenericWorker":28,"../utils":32}],13:[function(t,r,i){var o=t("readable-stream").Readable;function a(s,c,l){o.call(this,c),this._helper=s;var d=this;s.on("data",function(u,g){d.push(u)||d._helper.pause(),l&&l(g)}).on("error",function(u){d.emit("error",u)}).on("end",function(){d.push(null)})}t("../utils").inherits(a,o),a.prototype._read=function(){this._helper.resume()},r.exports=a},{"../utils":32,"readable-stream":16}],14:[function(t,r,i){r.exports={isNode:typeof Buffer<"u",newBufferFrom:function(o,a){if(Buffer.from&&Buffer.from!==Uint8Array.from)return Buffer.from(o,a);if(typeof o=="number")throw new Error('The "data" argument must not be a number');return new Buffer(o,a)},allocBuffer:function(o){if(Buffer.alloc)return Buffer.alloc(o);var a=new Buffer(o);return a.fill(0),a},isBuffer:function(o){return Buffer.isBuffer(o)},isStream:function(o){return o&&typeof o.on=="function"&&typeof o.pause=="function"&&typeof o.resume=="function"}}},{}],15:[function(t,r,i){function o(D,P,$){var x,S=s.getTypeOf(P),C=s.extend($||{},d);C.date=C.date||new Date,C.compression!==null&&(C.compression=C.compression.toUpperCase()),typeof C.unixPermissions=="string"&&(C.unixPermissions=parseInt(C.unixPermissions,8)),C.unixPermissions&&16384&C.unixPermissions&&(C.dir=!0),C.dosPermissions&&16&C.dosPermissions&&(C.dir=!0),C.dir&&(D=_(D)),C.createFolders&&(x=h(D))&&v.call(this,x,!0);var A=S==="string"&&C.binary===!1&&C.base64===!1;$&&$.binary!==void 0||(C.binary=!A),(P instanceof u&&P.uncompressedSize===0||C.dir||!P||P.length===0)&&(C.base64=!1,C.binary=!0,P="",C.compression="STORE",S="string");var w=null;w=P instanceof u||P instanceof c?P:m.isNode&&m.isStream(P)?new f(D,P):s.prepareContent(D,P,C.binary,C.optimizedBinaryString,C.base64);var R=new g(D,w,C);this.files[D]=R}var a=t("./utf8"),s=t("./utils"),c=t("./stream/GenericWorker"),l=t("./stream/StreamHelper"),d=t("./defaults"),u=t("./compressedObject"),g=t("./zipObject"),p=t("./generate"),m=t("./nodejsUtils"),f=t("./nodejs/NodejsStreamInputAdapter"),h=function(D){D.slice(-1)==="/"&&(D=D.substring(0,D.length-1));var P=D.lastIndexOf("/");return 0<P?D.substring(0,P):""},_=function(D){return D.slice(-1)!=="/"&&(D+="/"),D},v=function(D,P){return P=P!==void 0?P:d.createFolders,D=_(D),this.files[D]||o.call(this,D,null,{dir:!0,createFolders:P}),this.files[D]};function k(D){return Object.prototype.toString.call(D)==="[object RegExp]"}var E={load:function(){throw new Error("This method has been removed in JSZip 3.0, please check the upgrade guide.")},forEach:function(D){var P,$,x;for(P in this.files)x=this.files[P],($=P.slice(this.root.length,P.length))&&P.slice(0,this.root.length)===this.root&&D($,x)},filter:function(D){var P=[];return this.forEach(function($,x){D($,x)&&P.push(x)}),P},file:function(D,P,$){if(arguments.length!==1)return D=this.root+D,o.call(this,D,P,$),this;if(k(D)){var x=D;return this.filter(function(C,A){return!A.dir&&x.test(C)})}var S=this.files[this.root+D];return S&&!S.dir?S:null},folder:function(D){if(!D)return this;if(k(D))return this.filter(function(S,C){return C.dir&&D.test(S)});var P=this.root+D,$=v.call(this,P),x=this.clone();return x.root=$.name,x},remove:function(D){D=this.root+D;var P=this.files[D];if(P||(D.slice(-1)!=="/"&&(D+="/"),P=this.files[D]),P&&!P.dir)delete this.files[D];else for(var $=this.filter(function(S,C){return C.name.slice(0,D.length)===D}),x=0;x<$.length;x++)delete this.files[$[x].name];return this},generate:function(){throw new Error("This method has been removed in JSZip 3.0, please check the upgrade guide.")},generateInternalStream:function(D){var P,$={};try{if(($=s.extend(D||{},{streamFiles:!1,compression:"STORE",compressionOptions:null,type:"",platform:"DOS",comment:null,mimeType:"application/zip",encodeFileName:a.utf8encode})).type=$.type.toLowerCase(),$.compression=$.compression.toUpperCase(),$.type==="binarystring"&&($.type="string"),!$.type)throw new Error("No output type specified.");s.checkSupport($.type),$.platform!=="darwin"&&$.platform!=="freebsd"&&$.platform!=="linux"&&$.platform!=="sunos"||($.platform="UNIX"),$.platform==="win32"&&($.platform="DOS");var x=$.comment||this.comment||"";P=p.generateWorker(this,$,x)}catch(S){(P=new c("error")).error(S)}return new l(P,$.type||"string",$.mimeType)},generateAsync:function(D,P){return this.generateInternalStream(D).accumulate(P)},generateNodeStream:function(D,P){return(D=D||{}).type||(D.type="nodebuffer"),this.generateInternalStream(D).toNodejsStream(P)}};r.exports=E},{"./compressedObject":2,"./defaults":5,"./generate":9,"./nodejs/NodejsStreamInputAdapter":12,"./nodejsUtils":14,"./stream/GenericWorker":28,"./stream/StreamHelper":29,"./utf8":31,"./utils":32,"./zipObject":35}],16:[function(t,r,i){r.exports=t("stream")},{stream:void 0}],17:[function(t,r,i){var o=t("./DataReader");function a(s){o.call(this,s);for(var c=0;c<this.data.length;c++)s[c]=255&s[c]}t("../utils").inherits(a,o),a.prototype.byteAt=function(s){return this.data[this.zero+s]},a.prototype.lastIndexOfSignature=function(s){for(var c=s.charCodeAt(0),l=s.charCodeAt(1),d=s.charCodeAt(2),u=s.charCodeAt(3),g=this.length-4;0<=g;--g)if(this.data[g]===c&&this.data[g+1]===l&&this.data[g+2]===d&&this.data[g+3]===u)return g-this.zero;return-1},a.prototype.readAndCheckSignature=function(s){var c=s.charCodeAt(0),l=s.charCodeAt(1),d=s.charCodeAt(2),u=s.charCodeAt(3),g=this.readData(4);return c===g[0]&&l===g[1]&&d===g[2]&&u===g[3]},a.prototype.readData=function(s){if(this.checkOffset(s),s===0)return[];var c=this.data.slice(this.zero+this.index,this.zero+this.index+s);return this.index+=s,c},r.exports=a},{"../utils":32,"./DataReader":18}],18:[function(t,r,i){var o=t("../utils");function a(s){this.data=s,this.length=s.length,this.index=0,this.zero=0}a.prototype={checkOffset:function(s){this.checkIndex(this.index+s)},checkIndex:function(s){if(this.length<this.zero+s||s<0)throw new Error("End of data reached (data length = "+this.length+", asked index = "+s+"). Corrupted zip ?")},setIndex:function(s){this.checkIndex(s),this.index=s},skip:function(s){this.setIndex(this.index+s)},byteAt:function(){},readInt:function(s){var c,l=0;for(this.checkOffset(s),c=this.index+s-1;c>=this.index;c--)l=(l<<8)+this.byteAt(c);return this.index+=s,l},readString:function(s){return o.transformTo("string",this.readData(s))},readData:function(){},lastIndexOfSignature:function(){},readAndCheckSignature:function(){},readDate:function(){var s=this.readInt(4);return new Date(Date.UTC(1980+(s>>25&127),(s>>21&15)-1,s>>16&31,s>>11&31,s>>5&63,(31&s)<<1))}},r.exports=a},{"../utils":32}],19:[function(t,r,i){var o=t("./Uint8ArrayReader");function a(s){o.call(this,s)}t("../utils").inherits(a,o),a.prototype.readData=function(s){this.checkOffset(s);var c=this.data.slice(this.zero+this.index,this.zero+this.index+s);return this.index+=s,c},r.exports=a},{"../utils":32,"./Uint8ArrayReader":21}],20:[function(t,r,i){var o=t("./DataReader");function a(s){o.call(this,s)}t("../utils").inherits(a,o),a.prototype.byteAt=function(s){return this.data.charCodeAt(this.zero+s)},a.prototype.lastIndexOfSignature=function(s){return this.data.lastIndexOf(s)-this.zero},a.prototype.readAndCheckSignature=function(s){return s===this.readData(4)},a.prototype.readData=function(s){this.checkOffset(s);var c=this.data.slice(this.zero+this.index,this.zero+this.index+s);return this.index+=s,c},r.exports=a},{"../utils":32,"./DataReader":18}],21:[function(t,r,i){var o=t("./ArrayReader");function a(s){o.call(this,s)}t("../utils").inherits(a,o),a.prototype.readData=function(s){if(this.checkOffset(s),s===0)return new Uint8Array(0);var c=this.data.subarray(this.zero+this.index,this.zero+this.index+s);return this.index+=s,c},r.exports=a},{"../utils":32,"./ArrayReader":17}],22:[function(t,r,i){var o=t("../utils"),a=t("../support"),s=t("./ArrayReader"),c=t("./StringReader"),l=t("./NodeBufferReader"),d=t("./Uint8ArrayReader");r.exports=function(u){var g=o.getTypeOf(u);return o.checkSupport(g),g!=="string"||a.uint8array?g==="nodebuffer"?new l(u):a.uint8array?new d(o.transformTo("uint8array",u)):new s(o.transformTo("array",u)):new c(u)}},{"../support":30,"../utils":32,"./ArrayReader":17,"./NodeBufferReader":19,"./StringReader":20,"./Uint8ArrayReader":21}],23:[function(t,r,i){i.LOCAL_FILE_HEADER="PK",i.CENTRAL_FILE_HEADER="PK",i.CENTRAL_DIRECTORY_END="PK",i.ZIP64_CENTRAL_DIRECTORY_LOCATOR="PK\x07",i.ZIP64_CENTRAL_DIRECTORY_END="PK",i.DATA_DESCRIPTOR="PK\x07\b"},{}],24:[function(t,r,i){var o=t("./GenericWorker"),a=t("../utils");function s(c){o.call(this,"ConvertWorker to "+c),this.destType=c}a.inherits(s,o),s.prototype.processChunk=function(c){this.push({data:a.transformTo(this.destType,c.data),meta:c.meta})},r.exports=s},{"../utils":32,"./GenericWorker":28}],25:[function(t,r,i){var o=t("./GenericWorker"),a=t("../crc32");function s(){o.call(this,"Crc32Probe"),this.withStreamInfo("crc32",0)}t("../utils").inherits(s,o),s.prototype.processChunk=function(c){this.streamInfo.crc32=a(c.data,this.streamInfo.crc32||0),this.push(c)},r.exports=s},{"../crc32":4,"../utils":32,"./GenericWorker":28}],26:[function(t,r,i){var o=t("../utils"),a=t("./GenericWorker");function s(c){a.call(this,"DataLengthProbe for "+c),this.propName=c,this.withStreamInfo(c,0)}o.inherits(s,a),s.prototype.processChunk=function(c){if(c){var l=this.streamInfo[this.propName]||0;this.streamInfo[this.propName]=l+c.data.length}a.prototype.processChunk.call(this,c)},r.exports=s},{"../utils":32,"./GenericWorker":28}],27:[function(t,r,i){var o=t("../utils"),a=t("./GenericWorker");function s(c){a.call(this,"DataWorker");var l=this;this.dataIsReady=!1,this.index=0,this.max=0,this.data=null,this.type="",this._tickScheduled=!1,c.then(function(d){l.dataIsReady=!0,l.data=d,l.max=d&&d.length||0,l.type=o.getTypeOf(d),l.isPaused||l._tickAndRepeat()},function(d){l.error(d)})}o.inherits(s,a),s.prototype.cleanUp=function(){a.prototype.cleanUp.call(this),this.data=null},s.prototype.resume=function(){return!!a.prototype.resume.call(this)&&(!this._tickScheduled&&this.dataIsReady&&(this._tickScheduled=!0,o.delay(this._tickAndRepeat,[],this)),!0)},s.prototype._tickAndRepeat=function(){this._tickScheduled=!1,this.isPaused||this.isFinished||(this._tick(),this.isFinished||(o.delay(this._tickAndRepeat,[],this),this._tickScheduled=!0))},s.prototype._tick=function(){if(this.isPaused||this.isFinished)return!1;var c=null,l=Math.min(this.max,this.index+16384);if(this.index>=this.max)return this.end();switch(this.type){case"string":c=this.data.substring(this.index,l);break;case"uint8array":c=this.data.subarray(this.index,l);break;case"array":case"nodebuffer":c=this.data.slice(this.index,l)}return this.index=l,this.push({data:c,meta:{percent:this.max?this.index/this.max*100:0}})},r.exports=s},{"../utils":32,"./GenericWorker":28}],28:[function(t,r,i){function o(a){this.name=a||"default",this.streamInfo={},this.generatedError=null,this.extraStreamInfo={},this.isPaused=!0,this.isFinished=!1,this.isLocked=!1,this._listeners={data:[],end:[],error:[]},this.previous=null}o.prototype={push:function(a){this.emit("data",a)},end:function(){if(this.isFinished)return!1;this.flush();try{this.emit("end"),this.cleanUp(),this.isFinished=!0}catch(a){this.emit("error",a)}return!0},error:function(a){return!this.isFinished&&(this.isPaused?this.generatedError=a:(this.isFinished=!0,this.emit("error",a),this.previous&&this.previous.error(a),this.cleanUp()),!0)},on:function(a,s){return this._listeners[a].push(s),this},cleanUp:function(){this.streamInfo=this.generatedError=this.extraStreamInfo=null,this._listeners=[]},emit:function(a,s){if(this._listeners[a])for(var c=0;c<this._listeners[a].length;c++)this._listeners[a][c].call(this,s)},pipe:function(a){return a.registerPrevious(this)},registerPrevious:function(a){if(this.isLocked)throw new Error("The stream '"+this+"' has already been used.");this.streamInfo=a.streamInfo,this.mergeStreamInfo(),this.previous=a;var s=this;return a.on("data",function(c){s.processChunk(c)}),a.on("end",function(){s.end()}),a.on("error",function(c){s.error(c)}),this},pause:function(){return!this.isPaused&&!this.isFinished&&(this.isPaused=!0,this.previous&&this.previous.pause(),!0)},resume:function(){if(!this.isPaused||this.isFinished)return!1;var a=this.isPaused=!1;return this.generatedError&&(this.error(this.generatedError),a=!0),this.previous&&this.previous.resume(),!a},flush:function(){},processChunk:function(a){this.push(a)},withStreamInfo:function(a,s){return this.extraStreamInfo[a]=s,this.mergeStreamInfo(),this},mergeStreamInfo:function(){for(var a in this.extraStreamInfo)Object.prototype.hasOwnProperty.call(this.extraStreamInfo,a)&&(this.streamInfo[a]=this.extraStreamInfo[a])},lock:function(){if(this.isLocked)throw new Error("The stream '"+this+"' has already been used.");this.isLocked=!0,this.previous&&this.previous.lock()},toString:function(){var a="Worker "+this.name;return this.previous?this.previous+" -> "+a:a}},r.exports=o},{}],29:[function(t,r,i){var o=t("../utils"),a=t("./ConvertWorker"),s=t("./GenericWorker"),c=t("../base64"),l=t("../support"),d=t("../external"),u=null;if(l.nodestream)try{u=t("../nodejs/NodejsStreamOutputAdapter")}catch{}function g(m,f){return new d.Promise(function(h,_){var v=[],k=m._internalType,E=m._outputType,D=m._mimeType;m.on("data",function(P,$){v.push(P),f&&f($)}).on("error",function(P){v=[],_(P)}).on("end",function(){try{var P=(function($,x,S){switch($){case"blob":return o.newBlob(o.transformTo("arraybuffer",x),S);case"base64":return c.encode(x);default:return o.transformTo($,x)}})(E,(function($,x){var S,C=0,A=null,w=0;for(S=0;S<x.length;S++)w+=x[S].length;switch($){case"string":return x.join("");case"array":return Array.prototype.concat.apply([],x);case"uint8array":for(A=new Uint8Array(w),S=0;S<x.length;S++)A.set(x[S],C),C+=x[S].length;return A;case"nodebuffer":return Buffer.concat(x);default:throw new Error("concat : unsupported type '"+$+"'")}})(k,v),D);h(P)}catch($){_($)}v=[]}).resume()})}function p(m,f,h){var _=f;switch(f){case"blob":case"arraybuffer":_="uint8array";break;case"base64":_="string"}try{this._internalType=_,this._outputType=f,this._mimeType=h,o.checkSupport(_),this._worker=m.pipe(new a(_)),m.lock()}catch(v){this._worker=new s("error"),this._worker.error(v)}}p.prototype={accumulate:function(m){return g(this,m)},on:function(m,f){var h=this;return m==="data"?this._worker.on(m,function(_){f.call(h,_.data,_.meta)}):this._worker.on(m,function(){o.delay(f,arguments,h)}),this},resume:function(){return o.delay(this._worker.resume,[],this._worker),this},pause:function(){return this._worker.pause(),this},toNodejsStream:function(m){if(o.checkSupport("nodestream"),this._outputType!=="nodebuffer")throw new Error(this._outputType+" is not supported by this method");return new u(this,{objectMode:this._outputType!=="nodebuffer"},m)}},r.exports=p},{"../base64":1,"../external":6,"../nodejs/NodejsStreamOutputAdapter":13,"../support":30,"../utils":32,"./ConvertWorker":24,"./GenericWorker":28}],30:[function(t,r,i){if(i.base64=!0,i.array=!0,i.string=!0,i.arraybuffer=typeof ArrayBuffer<"u"&&typeof Uint8Array<"u",i.nodebuffer=typeof Buffer<"u",i.uint8array=typeof Uint8Array<"u",typeof ArrayBuffer>"u")i.blob=!1;else{var o=new ArrayBuffer(0);try{i.blob=new Blob([o],{type:"application/zip"}).size===0}catch{try{var a=new(self.BlobBuilder||self.WebKitBlobBuilder||self.MozBlobBuilder||self.MSBlobBuilder);a.append(o),i.blob=a.getBlob("application/zip").size===0}catch{i.blob=!1}}}try{i.nodestream=!!t("readable-stream").Readable}catch{i.nodestream=!1}},{"readable-stream":16}],31:[function(t,r,i){for(var o=t("./utils"),a=t("./support"),s=t("./nodejsUtils"),c=t("./stream/GenericWorker"),l=new Array(256),d=0;d<256;d++)l[d]=252<=d?6:248<=d?5:240<=d?4:224<=d?3:192<=d?2:1;l[254]=l[254]=1;function u(){c.call(this,"utf-8 decode"),this.leftOver=null}function g(){c.call(this,"utf-8 encode")}i.utf8encode=function(p){return a.nodebuffer?s.newBufferFrom(p,"utf-8"):(function(m){var f,h,_,v,k,E=m.length,D=0;for(v=0;v<E;v++)(64512&(h=m.charCodeAt(v)))==55296&&v+1<E&&(64512&(_=m.charCodeAt(v+1)))==56320&&(h=65536+(h-55296<<10)+(_-56320),v++),D+=h<128?1:h<2048?2:h<65536?3:4;for(f=a.uint8array?new Uint8Array(D):new Array(D),v=k=0;k<D;v++)(64512&(h=m.charCodeAt(v)))==55296&&v+1<E&&(64512&(_=m.charCodeAt(v+1)))==56320&&(h=65536+(h-55296<<10)+(_-56320),v++),h<128?f[k++]=h:(h<2048?f[k++]=192|h>>>6:(h<65536?f[k++]=224|h>>>12:(f[k++]=240|h>>>18,f[k++]=128|h>>>12&63),f[k++]=128|h>>>6&63),f[k++]=128|63&h);return f})(p)},i.utf8decode=function(p){return a.nodebuffer?o.transformTo("nodebuffer",p).toString("utf-8"):(function(m){var f,h,_,v,k=m.length,E=new Array(2*k);for(f=h=0;f<k;)if((_=m[f++])<128)E[h++]=_;else if(4<(v=l[_]))E[h++]=65533,f+=v-1;else{for(_&=v===2?31:v===3?15:7;1<v&&f<k;)_=_<<6|63&m[f++],v--;1<v?E[h++]=65533:_<65536?E[h++]=_:(_-=65536,E[h++]=55296|_>>10&1023,E[h++]=56320|1023&_)}return E.length!==h&&(E.subarray?E=E.subarray(0,h):E.length=h),o.applyFromCharCode(E)})(p=o.transformTo(a.uint8array?"uint8array":"array",p))},o.inherits(u,c),u.prototype.processChunk=function(p){var m=o.transformTo(a.uint8array?"uint8array":"array",p.data);if(this.leftOver&&this.leftOver.length){if(a.uint8array){var f=m;(m=new Uint8Array(f.length+this.leftOver.length)).set(this.leftOver,0),m.set(f,this.leftOver.length)}else m=this.leftOver.concat(m);this.leftOver=null}var h=(function(v,k){var E;for((k=k||v.length)>v.length&&(k=v.length),E=k-1;0<=E&&(192&v[E])==128;)E--;return E<0||E===0?k:E+l[v[E]]>k?E:k})(m),_=m;h!==m.length&&(a.uint8array?(_=m.subarray(0,h),this.leftOver=m.subarray(h,m.length)):(_=m.slice(0,h),this.leftOver=m.slice(h,m.length))),this.push({data:i.utf8decode(_),meta:p.meta})},u.prototype.flush=function(){this.leftOver&&this.leftOver.length&&(this.push({data:i.utf8decode(this.leftOver),meta:{}}),this.leftOver=null)},i.Utf8DecodeWorker=u,o.inherits(g,c),g.prototype.processChunk=function(p){this.push({data:i.utf8encode(p.data),meta:p.meta})},i.Utf8EncodeWorker=g},{"./nodejsUtils":14,"./stream/GenericWorker":28,"./support":30,"./utils":32}],32:[function(t,r,i){var o=t("./support"),a=t("./base64"),s=t("./nodejsUtils"),c=t("./external");function l(f){return f}function d(f,h){for(var _=0;_<f.length;++_)h[_]=255&f.charCodeAt(_);return h}t("setimmediate"),i.newBlob=function(f,h){i.checkSupport("blob");try{return new Blob([f],{type:h})}catch{try{var _=new(self.BlobBuilder||self.WebKitBlobBuilder||self.MozBlobBuilder||self.MSBlobBuilder);return _.append(f),_.getBlob(h)}catch{throw new Error("Bug : can't construct the Blob.")}}};var u={stringifyByChunk:function(f,h,_){var v=[],k=0,E=f.length;if(E<=_)return String.fromCharCode.apply(null,f);for(;k<E;)h==="array"||h==="nodebuffer"?v.push(String.fromCharCode.apply(null,f.slice(k,Math.min(k+_,E)))):v.push(String.fromCharCode.apply(null,f.subarray(k,Math.min(k+_,E)))),k+=_;return v.join("")},stringifyByChar:function(f){for(var h="",_=0;_<f.length;_++)h+=String.fromCharCode(f[_]);return h},applyCanBeUsed:{uint8array:(function(){try{return o.uint8array&&String.fromCharCode.apply(null,new Uint8Array(1)).length===1}catch{return!1}})(),nodebuffer:(function(){try{return o.nodebuffer&&String.fromCharCode.apply(null,s.allocBuffer(1)).length===1}catch{return!1}})()}};function g(f){var h=65536,_=i.getTypeOf(f),v=!0;if(_==="uint8array"?v=u.applyCanBeUsed.uint8array:_==="nodebuffer"&&(v=u.applyCanBeUsed.nodebuffer),v)for(;1<h;)try{return u.stringifyByChunk(f,_,h)}catch{h=Math.floor(h/2)}return u.stringifyByChar(f)}function p(f,h){for(var _=0;_<f.length;_++)h[_]=f[_];return h}i.applyFromCharCode=g;var m={};m.string={string:l,array:function(f){return d(f,new Array(f.length))},arraybuffer:function(f){return m.string.uint8array(f).buffer},uint8array:function(f){return d(f,new Uint8Array(f.length))},nodebuffer:function(f){return d(f,s.allocBuffer(f.length))}},m.array={string:g,array:l,arraybuffer:function(f){return new Uint8Array(f).buffer},uint8array:function(f){return new Uint8Array(f)},nodebuffer:function(f){return s.newBufferFrom(f)}},m.arraybuffer={string:function(f){return g(new Uint8Array(f))},array:function(f){return p(new Uint8Array(f),new Array(f.byteLength))},arraybuffer:l,uint8array:function(f){return new Uint8Array(f)},nodebuffer:function(f){return s.newBufferFrom(new Uint8Array(f))}},m.uint8array={string:g,array:function(f){return p(f,new Array(f.length))},arraybuffer:function(f){return f.buffer},uint8array:l,nodebuffer:function(f){return s.newBufferFrom(f)}},m.nodebuffer={string:g,array:function(f){return p(f,new Array(f.length))},arraybuffer:function(f){return m.nodebuffer.uint8array(f).buffer},uint8array:function(f){return p(f,new Uint8Array(f.length))},nodebuffer:l},i.transformTo=function(f,h){if(h=h||"",!f)return h;i.checkSupport(f);var _=i.getTypeOf(h);return m[_][f](h)},i.resolve=function(f){for(var h=f.split("/"),_=[],v=0;v<h.length;v++){var k=h[v];k==="."||k===""&&v!==0&&v!==h.length-1||(k===".."?_.pop():_.push(k))}return _.join("/")},i.getTypeOf=function(f){return typeof f=="string"?"string":Object.prototype.toString.call(f)==="[object Array]"?"array":o.nodebuffer&&s.isBuffer(f)?"nodebuffer":o.uint8array&&f instanceof Uint8Array?"uint8array":o.arraybuffer&&f instanceof ArrayBuffer?"arraybuffer":void 0},i.checkSupport=function(f){if(!o[f.toLowerCase()])throw new Error(f+" is not supported by this platform")},i.MAX_VALUE_16BITS=65535,i.MAX_VALUE_32BITS=-1,i.pretty=function(f){var h,_,v="";for(_=0;_<(f||"").length;_++)v+="\\x"+((h=f.charCodeAt(_))<16?"0":"")+h.toString(16).toUpperCase();return v},i.delay=function(f,h,_){setImmediate(function(){f.apply(_||null,h||[])})},i.inherits=function(f,h){function _(){}_.prototype=h.prototype,f.prototype=new _},i.extend=function(){var f,h,_={};for(f=0;f<arguments.length;f++)for(h in arguments[f])Object.prototype.hasOwnProperty.call(arguments[f],h)&&_[h]===void 0&&(_[h]=arguments[f][h]);return _},i.prepareContent=function(f,h,_,v,k){return c.Promise.resolve(h).then(function(E){return o.blob&&(E instanceof Blob||["[object File]","[object Blob]"].indexOf(Object.prototype.toString.call(E))!==-1)&&typeof FileReader<"u"?new c.Promise(function(D,P){var $=new FileReader;$.onload=function(x){D(x.target.result)},$.onerror=function(x){P(x.target.error)},$.readAsArrayBuffer(E)}):E}).then(function(E){var D=i.getTypeOf(E);return D?(D==="arraybuffer"?E=i.transformTo("uint8array",E):D==="string"&&(k?E=a.decode(E):_&&v!==!0&&(E=(function(P){return d(P,o.uint8array?new Uint8Array(P.length):new Array(P.length))})(E))),E):c.Promise.reject(new Error("Can't read the data of '"+f+"'. Is it in a supported JavaScript type (String, Blob, ArrayBuffer, etc) ?"))})}},{"./base64":1,"./external":6,"./nodejsUtils":14,"./support":30,setimmediate:54}],33:[function(t,r,i){var o=t("./reader/readerFor"),a=t("./utils"),s=t("./signature"),c=t("./zipEntry"),l=t("./support");function d(u){this.files=[],this.loadOptions=u}d.prototype={checkSignature:function(u){if(!this.reader.readAndCheckSignature(u)){this.reader.index-=4;var g=this.reader.readString(4);throw new Error("Corrupted zip or bug: unexpected signature ("+a.pretty(g)+", expected "+a.pretty(u)+")")}},isSignature:function(u,g){var p=this.reader.index;this.reader.setIndex(u);var m=this.reader.readString(4)===g;return this.reader.setIndex(p),m},readBlockEndOfCentral:function(){this.diskNumber=this.reader.readInt(2),this.diskWithCentralDirStart=this.reader.readInt(2),this.centralDirRecordsOnThisDisk=this.reader.readInt(2),this.centralDirRecords=this.reader.readInt(2),this.centralDirSize=this.reader.readInt(4),this.centralDirOffset=this.reader.readInt(4),this.zipCommentLength=this.reader.readInt(2);var u=this.reader.readData(this.zipCommentLength),g=l.uint8array?"uint8array":"array",p=a.transformTo(g,u);this.zipComment=this.loadOptions.decodeFileName(p)},readBlockZip64EndOfCentral:function(){this.zip64EndOfCentralSize=this.reader.readInt(8),this.reader.skip(4),this.diskNumber=this.reader.readInt(4),this.diskWithCentralDirStart=this.reader.readInt(4),this.centralDirRecordsOnThisDisk=this.reader.readInt(8),this.centralDirRecords=this.reader.readInt(8),this.centralDirSize=this.reader.readInt(8),this.centralDirOffset=this.reader.readInt(8),this.zip64ExtensibleData={};for(var u,g,p,m=this.zip64EndOfCentralSize-44;0<m;)u=this.reader.readInt(2),g=this.reader.readInt(4),p=this.reader.readData(g),this.zip64ExtensibleData[u]={id:u,length:g,value:p}},readBlockZip64EndOfCentralLocator:function(){if(this.diskWithZip64CentralDirStart=this.reader.readInt(4),this.relativeOffsetEndOfZip64CentralDir=this.reader.readInt(8),this.disksCount=this.reader.readInt(4),1<this.disksCount)throw new Error("Multi-volumes zip are not supported")},readLocalFiles:function(){var u,g;for(u=0;u<this.files.length;u++)g=this.files[u],this.reader.setIndex(g.localHeaderOffset),this.checkSignature(s.LOCAL_FILE_HEADER),g.readLocalPart(this.reader),g.handleUTF8(),g.processAttributes()},readCentralDir:function(){var u;for(this.reader.setIndex(this.centralDirOffset);this.reader.readAndCheckSignature(s.CENTRAL_FILE_HEADER);)(u=new c({zip64:this.zip64},this.loadOptions)).readCentralPart(this.reader),this.files.push(u);if(this.centralDirRecords!==this.files.length&&this.centralDirRecords!==0&&this.files.length===0)throw new Error("Corrupted zip or bug: expected "+this.centralDirRecords+" records in central dir, got "+this.files.length)},readEndOfCentral:function(){var u=this.reader.lastIndexOfSignature(s.CENTRAL_DIRECTORY_END);if(u<0)throw this.isSignature(0,s.LOCAL_FILE_HEADER)?new Error("Corrupted zip: can't find end of central directory"):new Error("Can't find end of central directory : is this a zip file ? If it is, see https://stuk.github.io/jszip/documentation/howto/read_zip.html");this.reader.setIndex(u);var g=u;if(this.checkSignature(s.CENTRAL_DIRECTORY_END),this.readBlockEndOfCentral(),this.diskNumber===a.MAX_VALUE_16BITS||this.diskWithCentralDirStart===a.MAX_VALUE_16BITS||this.centralDirRecordsOnThisDisk===a.MAX_VALUE_16BITS||this.centralDirRecords===a.MAX_VALUE_16BITS||this.centralDirSize===a.MAX_VALUE_32BITS||this.centralDirOffset===a.MAX_VALUE_32BITS){if(this.zip64=!0,(u=this.reader.lastIndexOfSignature(s.ZIP64_CENTRAL_DIRECTORY_LOCATOR))<0)throw new Error("Corrupted zip: can't find the ZIP64 end of central directory locator");if(this.reader.setIndex(u),this.checkSignature(s.ZIP64_CENTRAL_DIRECTORY_LOCATOR),this.readBlockZip64EndOfCentralLocator(),!this.isSignature(this.relativeOffsetEndOfZip64CentralDir,s.ZIP64_CENTRAL_DIRECTORY_END)&&(this.relativeOffsetEndOfZip64CentralDir=this.reader.lastIndexOfSignature(s.ZIP64_CENTRAL_DIRECTORY_END),this.relativeOffsetEndOfZip64CentralDir<0))throw new Error("Corrupted zip: can't find the ZIP64 end of central directory");this.reader.setIndex(this.relativeOffsetEndOfZip64CentralDir),this.checkSignature(s.ZIP64_CENTRAL_DIRECTORY_END),this.readBlockZip64EndOfCentral()}var p=this.centralDirOffset+this.centralDirSize;this.zip64&&(p+=20,p+=12+this.zip64EndOfCentralSize);var m=g-p;if(0<m)this.isSignature(g,s.CENTRAL_FILE_HEADER)||(this.reader.zero=m);else if(m<0)throw new Error("Corrupted zip: missing "+Math.abs(m)+" bytes.")},prepareReader:function(u){this.reader=o(u)},load:function(u){this.prepareReader(u),this.readEndOfCentral(),this.readCentralDir(),this.readLocalFiles()}},r.exports=d},{"./reader/readerFor":22,"./signature":23,"./support":30,"./utils":32,"./zipEntry":34}],34:[function(t,r,i){var o=t("./reader/readerFor"),a=t("./utils"),s=t("./compressedObject"),c=t("./crc32"),l=t("./utf8"),d=t("./compressions"),u=t("./support");function g(p,m){this.options=p,this.loadOptions=m}g.prototype={isEncrypted:function(){return(1&this.bitFlag)==1},useUTF8:function(){return(2048&this.bitFlag)==2048},readLocalPart:function(p){var m,f;if(p.skip(22),this.fileNameLength=p.readInt(2),f=p.readInt(2),this.fileName=p.readData(this.fileNameLength),p.skip(f),this.compressedSize===-1||this.uncompressedSize===-1)throw new Error("Bug or corrupted zip : didn't get enough information from the central directory (compressedSize === -1 || uncompressedSize === -1)");if((m=(function(h){for(var _ in d)if(Object.prototype.hasOwnProperty.call(d,_)&&d[_].magic===h)return d[_];return null})(this.compressionMethod))===null)throw new Error("Corrupted zip : compression "+a.pretty(this.compressionMethod)+" unknown (inner file : "+a.transformTo("string",this.fileName)+")");this.decompressed=new s(this.compressedSize,this.uncompressedSize,this.crc32,m,p.readData(this.compressedSize))},readCentralPart:function(p){this.versionMadeBy=p.readInt(2),p.skip(2),this.bitFlag=p.readInt(2),this.compressionMethod=p.readString(2),this.date=p.readDate(),this.crc32=p.readInt(4),this.compressedSize=p.readInt(4),this.uncompressedSize=p.readInt(4);var m=p.readInt(2);if(this.extraFieldsLength=p.readInt(2),this.fileCommentLength=p.readInt(2),this.diskNumberStart=p.readInt(2),this.internalFileAttributes=p.readInt(2),this.externalFileAttributes=p.readInt(4),this.localHeaderOffset=p.readInt(4),this.isEncrypted())throw new Error("Encrypted zip are not supported");p.skip(m),this.readExtraFields(p),this.parseZIP64ExtraField(p),this.fileComment=p.readData(this.fileCommentLength)},processAttributes:function(){this.unixPermissions=null,this.dosPermissions=null;var p=this.versionMadeBy>>8;this.dir=!!(16&this.externalFileAttributes),p==0&&(this.dosPermissions=63&this.externalFileAttributes),p==3&&(this.unixPermissions=this.externalFileAttributes>>16&65535),this.dir||this.fileNameStr.slice(-1)!=="/"||(this.dir=!0)},parseZIP64ExtraField:function(){if(this.extraFields[1]){var p=o(this.extraFields[1].value);this.uncompressedSize===a.MAX_VALUE_32BITS&&(this.uncompressedSize=p.readInt(8)),this.compressedSize===a.MAX_VALUE_32BITS&&(this.compressedSize=p.readInt(8)),this.localHeaderOffset===a.MAX_VALUE_32BITS&&(this.localHeaderOffset=p.readInt(8)),this.diskNumberStart===a.MAX_VALUE_32BITS&&(this.diskNumberStart=p.readInt(4))}},readExtraFields:function(p){var m,f,h,_=p.index+this.extraFieldsLength;for(this.extraFields||(this.extraFields={});p.index+4<_;)m=p.readInt(2),f=p.readInt(2),h=p.readData(f),this.extraFields[m]={id:m,length:f,value:h};p.setIndex(_)},handleUTF8:function(){var p=u.uint8array?"uint8array":"array";if(this.useUTF8())this.fileNameStr=l.utf8decode(this.fileName),this.fileCommentStr=l.utf8decode(this.fileComment);else{var m=this.findExtraFieldUnicodePath();if(m!==null)this.fileNameStr=m;else{var f=a.transformTo(p,this.fileName);this.fileNameStr=this.loadOptions.decodeFileName(f)}var h=this.findExtraFieldUnicodeComment();if(h!==null)this.fileCommentStr=h;else{var _=a.transformTo(p,this.fileComment);this.fileCommentStr=this.loadOptions.decodeFileName(_)}}},findExtraFieldUnicodePath:function(){var p=this.extraFields[28789];if(p){var m=o(p.value);return m.readInt(1)!==1||c(this.fileName)!==m.readInt(4)?null:l.utf8decode(m.readData(p.length-5))}return null},findExtraFieldUnicodeComment:function(){var p=this.extraFields[25461];if(p){var m=o(p.value);return m.readInt(1)!==1||c(this.fileComment)!==m.readInt(4)?null:l.utf8decode(m.readData(p.length-5))}return null}},r.exports=g},{"./compressedObject":2,"./compressions":3,"./crc32":4,"./reader/readerFor":22,"./support":30,"./utf8":31,"./utils":32}],35:[function(t,r,i){function o(m,f,h){this.name=m,this.dir=h.dir,this.date=h.date,this.comment=h.comment,this.unixPermissions=h.unixPermissions,this.dosPermissions=h.dosPermissions,this._data=f,this._dataBinary=h.binary,this.options={compression:h.compression,compressionOptions:h.compressionOptions}}var a=t("./stream/StreamHelper"),s=t("./stream/DataWorker"),c=t("./utf8"),l=t("./compressedObject"),d=t("./stream/GenericWorker");o.prototype={internalStream:function(m){var f=null,h="string";try{if(!m)throw new Error("No output type specified.");var _=(h=m.toLowerCase())==="string"||h==="text";h!=="binarystring"&&h!=="text"||(h="string"),f=this._decompressWorker();var v=!this._dataBinary;v&&!_&&(f=f.pipe(new c.Utf8EncodeWorker)),!v&&_&&(f=f.pipe(new c.Utf8DecodeWorker))}catch(k){(f=new d("error")).error(k)}return new a(f,h,"")},async:function(m,f){return this.internalStream(m).accumulate(f)},nodeStream:function(m,f){return this.internalStream(m||"nodebuffer").toNodejsStream(f)},_compressWorker:function(m,f){if(this._data instanceof l&&this._data.compression.magic===m.magic)return this._data.getCompressedWorker();var h=this._decompressWorker();return this._dataBinary||(h=h.pipe(new c.Utf8EncodeWorker)),l.createWorkerFrom(h,m,f)},_decompressWorker:function(){return this._data instanceof l?this._data.getContentWorker():this._data instanceof d?this._data:new s(this._data)}};for(var u=["asText","asBinary","asNodeBuffer","asUint8Array","asArrayBuffer"],g=function(){throw new Error("This method has been removed in JSZip 3.0, please check the upgrade guide.")},p=0;p<u.length;p++)o.prototype[u[p]]=g;r.exports=o},{"./compressedObject":2,"./stream/DataWorker":27,"./stream/GenericWorker":28,"./stream/StreamHelper":29,"./utf8":31}],36:[function(t,r,i){(function(o){var a,s,c=o.MutationObserver||o.WebKitMutationObserver;if(c){var l=0,d=new c(m),u=o.document.createTextNode("");d.observe(u,{characterData:!0}),a=function(){u.data=l=++l%2}}else if(o.setImmediate||o.MessageChannel===void 0)a="document"in o&&"onreadystatechange"in o.document.createElement("script")?function(){var f=o.document.createElement("script");f.onreadystatechange=function(){m(),f.onreadystatechange=null,f.parentNode.removeChild(f),f=null},o.document.documentElement.appendChild(f)}:function(){setTimeout(m,0)};else{var g=new o.MessageChannel;g.port1.onmessage=m,a=function(){g.port2.postMessage(0)}}var p=[];function m(){var f,h;s=!0;for(var _=p.length;_;){for(h=p,p=[],f=-1;++f<_;)h[f]();_=p.length}s=!1}r.exports=function(f){p.push(f)!==1||s||a()}}).call(this,typeof Dn<"u"?Dn:typeof self<"u"?self:typeof window<"u"?window:{})},{}],37:[function(t,r,i){var o=t("immediate");function a(){}var s={},c=["REJECTED"],l=["FULFILLED"],d=["PENDING"];function u(_){if(typeof _!="function")throw new TypeError("resolver must be a function");this.state=d,this.queue=[],this.outcome=void 0,_!==a&&f(this,_)}function g(_,v,k){this.promise=_,typeof v=="function"&&(this.onFulfilled=v,this.callFulfilled=this.otherCallFulfilled),typeof k=="function"&&(this.onRejected=k,this.callRejected=this.otherCallRejected)}function p(_,v,k){o(function(){var E;try{E=v(k)}catch(D){return s.reject(_,D)}E===_?s.reject(_,new TypeError("Cannot resolve promise with itself")):s.resolve(_,E)})}function m(_){var v=_&&_.then;if(_&&(typeof _=="object"||typeof _=="function")&&typeof v=="function")return function(){v.apply(_,arguments)}}function f(_,v){var k=!1;function E($){k||(k=!0,s.reject(_,$))}function D($){k||(k=!0,s.resolve(_,$))}var P=h(function(){v(D,E)});P.status==="error"&&E(P.value)}function h(_,v){var k={};try{k.value=_(v),k.status="success"}catch(E){k.status="error",k.value=E}return k}(r.exports=u).prototype.finally=function(_){if(typeof _!="function")return this;var v=this.constructor;return this.then(function(k){return v.resolve(_()).then(function(){return k})},function(k){return v.resolve(_()).then(function(){throw k})})},u.prototype.catch=function(_){return this.then(null,_)},u.prototype.then=function(_,v){if(typeof _!="function"&&this.state===l||typeof v!="function"&&this.state===c)return this;var k=new this.constructor(a);return this.state!==d?p(k,this.state===l?_:v,this.outcome):this.queue.push(new g(k,_,v)),k},g.prototype.callFulfilled=function(_){s.resolve(this.promise,_)},g.prototype.otherCallFulfilled=function(_){p(this.promise,this.onFulfilled,_)},g.prototype.callRejected=function(_){s.reject(this.promise,_)},g.prototype.otherCallRejected=function(_){p(this.promise,this.onRejected,_)},s.resolve=function(_,v){var k=h(m,v);if(k.status==="error")return s.reject(_,k.value);var E=k.value;if(E)f(_,E);else{_.state=l,_.outcome=v;for(var D=-1,P=_.queue.length;++D<P;)_.queue[D].callFulfilled(v)}return _},s.reject=function(_,v){_.state=c,_.outcome=v;for(var k=-1,E=_.queue.length;++k<E;)_.queue[k].callRejected(v);return _},u.resolve=function(_){return _ instanceof this?_:s.resolve(new this(a),_)},u.reject=function(_){var v=new this(a);return s.reject(v,_)},u.all=function(_){var v=this;if(Object.prototype.toString.call(_)!=="[object Array]")return this.reject(new TypeError("must be an array"));var k=_.length,E=!1;if(!k)return this.resolve([]);for(var D=new Array(k),P=0,$=-1,x=new this(a);++$<k;)S(_[$],$);return x;function S(C,A){v.resolve(C).then(function(w){D[A]=w,++P!==k||E||(E=!0,s.resolve(x,D))},function(w){E||(E=!0,s.reject(x,w))})}},u.race=function(_){var v=this;if(Object.prototype.toString.call(_)!=="[object Array]")return this.reject(new TypeError("must be an array"));var k=_.length,E=!1;if(!k)return this.resolve([]);for(var D=-1,P=new this(a);++D<k;)$=_[D],v.resolve($).then(function(x){E||(E=!0,s.resolve(P,x))},function(x){E||(E=!0,s.reject(P,x))});var $;return P}},{immediate:36}],38:[function(t,r,i){var o={};(0,t("./lib/utils/common").assign)(o,t("./lib/deflate"),t("./lib/inflate"),t("./lib/zlib/constants")),r.exports=o},{"./lib/deflate":39,"./lib/inflate":40,"./lib/utils/common":41,"./lib/zlib/constants":44}],39:[function(t,r,i){var o=t("./zlib/deflate"),a=t("./utils/common"),s=t("./utils/strings"),c=t("./zlib/messages"),l=t("./zlib/zstream"),d=Object.prototype.toString,u=0,g=-1,p=0,m=8;function f(_){if(!(this instanceof f))return new f(_);this.options=a.assign({level:g,method:m,chunkSize:16384,windowBits:15,memLevel:8,strategy:p,to:""},_||{});var v=this.options;v.raw&&0<v.windowBits?v.windowBits=-v.windowBits:v.gzip&&0<v.windowBits&&v.windowBits<16&&(v.windowBits+=16),this.err=0,this.msg="",this.ended=!1,this.chunks=[],this.strm=new l,this.strm.avail_out=0;var k=o.deflateInit2(this.strm,v.level,v.method,v.windowBits,v.memLevel,v.strategy);if(k!==u)throw new Error(c[k]);if(v.header&&o.deflateSetHeader(this.strm,v.header),v.dictionary){var E;if(E=typeof v.dictionary=="string"?s.string2buf(v.dictionary):d.call(v.dictionary)==="[object ArrayBuffer]"?new Uint8Array(v.dictionary):v.dictionary,(k=o.deflateSetDictionary(this.strm,E))!==u)throw new Error(c[k]);this._dict_set=!0}}function h(_,v){var k=new f(v);if(k.push(_,!0),k.err)throw k.msg||c[k.err];return k.result}f.prototype.push=function(_,v){var k,E,D=this.strm,P=this.options.chunkSize;if(this.ended)return!1;E=v===~~v?v:v===!0?4:0,typeof _=="string"?D.input=s.string2buf(_):d.call(_)==="[object ArrayBuffer]"?D.input=new Uint8Array(_):D.input=_,D.next_in=0,D.avail_in=D.input.length;do{if(D.avail_out===0&&(D.output=new a.Buf8(P),D.next_out=0,D.avail_out=P),(k=o.deflate(D,E))!==1&&k!==u)return this.onEnd(k),!(this.ended=!0);D.avail_out!==0&&(D.avail_in!==0||E!==4&&E!==2)||(this.options.to==="string"?this.onData(s.buf2binstring(a.shrinkBuf(D.output,D.next_out))):this.onData(a.shrinkBuf(D.output,D.next_out)))}while((0<D.avail_in||D.avail_out===0)&&k!==1);return E===4?(k=o.deflateEnd(this.strm),this.onEnd(k),this.ended=!0,k===u):E!==2||(this.onEnd(u),!(D.avail_out=0))},f.prototype.onData=function(_){this.chunks.push(_)},f.prototype.onEnd=function(_){_===u&&(this.options.to==="string"?this.result=this.chunks.join(""):this.result=a.flattenChunks(this.chunks)),this.chunks=[],this.err=_,this.msg=this.strm.msg},i.Deflate=f,i.deflate=h,i.deflateRaw=function(_,v){return(v=v||{}).raw=!0,h(_,v)},i.gzip=function(_,v){return(v=v||{}).gzip=!0,h(_,v)}},{"./utils/common":41,"./utils/strings":42,"./zlib/deflate":46,"./zlib/messages":51,"./zlib/zstream":53}],40:[function(t,r,i){var o=t("./zlib/inflate"),a=t("./utils/common"),s=t("./utils/strings"),c=t("./zlib/constants"),l=t("./zlib/messages"),d=t("./zlib/zstream"),u=t("./zlib/gzheader"),g=Object.prototype.toString;function p(f){if(!(this instanceof p))return new p(f);this.options=a.assign({chunkSize:16384,windowBits:0,to:""},f||{});var h=this.options;h.raw&&0<=h.windowBits&&h.windowBits<16&&(h.windowBits=-h.windowBits,h.windowBits===0&&(h.windowBits=-15)),!(0<=h.windowBits&&h.windowBits<16)||f&&f.windowBits||(h.windowBits+=32),15<h.windowBits&&h.windowBits<48&&(15&h.windowBits)==0&&(h.windowBits|=15),this.err=0,this.msg="",this.ended=!1,this.chunks=[],this.strm=new d,this.strm.avail_out=0;var _=o.inflateInit2(this.strm,h.windowBits);if(_!==c.Z_OK)throw new Error(l[_]);this.header=new u,o.inflateGetHeader(this.strm,this.header)}function m(f,h){var _=new p(h);if(_.push(f,!0),_.err)throw _.msg||l[_.err];return _.result}p.prototype.push=function(f,h){var _,v,k,E,D,P,$=this.strm,x=this.options.chunkSize,S=this.options.dictionary,C=!1;if(this.ended)return!1;v=h===~~h?h:h===!0?c.Z_FINISH:c.Z_NO_FLUSH,typeof f=="string"?$.input=s.binstring2buf(f):g.call(f)==="[object ArrayBuffer]"?$.input=new Uint8Array(f):$.input=f,$.next_in=0,$.avail_in=$.input.length;do{if($.avail_out===0&&($.output=new a.Buf8(x),$.next_out=0,$.avail_out=x),(_=o.inflate($,c.Z_NO_FLUSH))===c.Z_NEED_DICT&&S&&(P=typeof S=="string"?s.string2buf(S):g.call(S)==="[object ArrayBuffer]"?new Uint8Array(S):S,_=o.inflateSetDictionary(this.strm,P)),_===c.Z_BUF_ERROR&&C===!0&&(_=c.Z_OK,C=!1),_!==c.Z_STREAM_END&&_!==c.Z_OK)return this.onEnd(_),!(this.ended=!0);$.next_out&&($.avail_out!==0&&_!==c.Z_STREAM_END&&($.avail_in!==0||v!==c.Z_FINISH&&v!==c.Z_SYNC_FLUSH)||(this.options.to==="string"?(k=s.utf8border($.output,$.next_out),E=$.next_out-k,D=s.buf2string($.output,k),$.next_out=E,$.avail_out=x-E,E&&a.arraySet($.output,$.output,k,E,0),this.onData(D)):this.onData(a.shrinkBuf($.output,$.next_out)))),$.avail_in===0&&$.avail_out===0&&(C=!0)}while((0<$.avail_in||$.avail_out===0)&&_!==c.Z_STREAM_END);return _===c.Z_STREAM_END&&(v=c.Z_FINISH),v===c.Z_FINISH?(_=o.inflateEnd(this.strm),this.onEnd(_),this.ended=!0,_===c.Z_OK):v!==c.Z_SYNC_FLUSH||(this.onEnd(c.Z_OK),!($.avail_out=0))},p.prototype.onData=function(f){this.chunks.push(f)},p.prototype.onEnd=function(f){f===c.Z_OK&&(this.options.to==="string"?this.result=this.chunks.join(""):this.result=a.flattenChunks(this.chunks)),this.chunks=[],this.err=f,this.msg=this.strm.msg},i.Inflate=p,i.inflate=m,i.inflateRaw=function(f,h){return(h=h||{}).raw=!0,m(f,h)},i.ungzip=m},{"./utils/common":41,"./utils/strings":42,"./zlib/constants":44,"./zlib/gzheader":47,"./zlib/inflate":49,"./zlib/messages":51,"./zlib/zstream":53}],41:[function(t,r,i){var o=typeof Uint8Array<"u"&&typeof Uint16Array<"u"&&typeof Int32Array<"u";i.assign=function(c){for(var l=Array.prototype.slice.call(arguments,1);l.length;){var d=l.shift();if(d){if(typeof d!="object")throw new TypeError(d+"must be non-object");for(var u in d)d.hasOwnProperty(u)&&(c[u]=d[u])}}return c},i.shrinkBuf=function(c,l){return c.length===l?c:c.subarray?c.subarray(0,l):(c.length=l,c)};var a={arraySet:function(c,l,d,u,g){if(l.subarray&&c.subarray)c.set(l.subarray(d,d+u),g);else for(var p=0;p<u;p++)c[g+p]=l[d+p]},flattenChunks:function(c){var l,d,u,g,p,m;for(l=u=0,d=c.length;l<d;l++)u+=c[l].length;for(m=new Uint8Array(u),l=g=0,d=c.length;l<d;l++)p=c[l],m.set(p,g),g+=p.length;return m}},s={arraySet:function(c,l,d,u,g){for(var p=0;p<u;p++)c[g+p]=l[d+p]},flattenChunks:function(c){return[].concat.apply([],c)}};i.setTyped=function(c){c?(i.Buf8=Uint8Array,i.Buf16=Uint16Array,i.Buf32=Int32Array,i.assign(i,a)):(i.Buf8=Array,i.Buf16=Array,i.Buf32=Array,i.assign(i,s))},i.setTyped(o)},{}],42:[function(t,r,i){var o=t("./common"),a=!0,s=!0;try{String.fromCharCode.apply(null,[0])}catch{a=!1}try{String.fromCharCode.apply(null,new Uint8Array(1))}catch{s=!1}for(var c=new o.Buf8(256),l=0;l<256;l++)c[l]=252<=l?6:248<=l?5:240<=l?4:224<=l?3:192<=l?2:1;function d(u,g){if(g<65537&&(u.subarray&&s||!u.subarray&&a))return String.fromCharCode.apply(null,o.shrinkBuf(u,g));for(var p="",m=0;m<g;m++)p+=String.fromCharCode(u[m]);return p}c[254]=c[254]=1,i.string2buf=function(u){var g,p,m,f,h,_=u.length,v=0;for(f=0;f<_;f++)(64512&(p=u.charCodeAt(f)))==55296&&f+1<_&&(64512&(m=u.charCodeAt(f+1)))==56320&&(p=65536+(p-55296<<10)+(m-56320),f++),v+=p<128?1:p<2048?2:p<65536?3:4;for(g=new o.Buf8(v),f=h=0;h<v;f++)(64512&(p=u.charCodeAt(f)))==55296&&f+1<_&&(64512&(m=u.charCodeAt(f+1)))==56320&&(p=65536+(p-55296<<10)+(m-56320),f++),p<128?g[h++]=p:(p<2048?g[h++]=192|p>>>6:(p<65536?g[h++]=224|p>>>12:(g[h++]=240|p>>>18,g[h++]=128|p>>>12&63),g[h++]=128|p>>>6&63),g[h++]=128|63&p);return g},i.buf2binstring=function(u){return d(u,u.length)},i.binstring2buf=function(u){for(var g=new o.Buf8(u.length),p=0,m=g.length;p<m;p++)g[p]=u.charCodeAt(p);return g},i.buf2string=function(u,g){var p,m,f,h,_=g||u.length,v=new Array(2*_);for(p=m=0;p<_;)if((f=u[p++])<128)v[m++]=f;else if(4<(h=c[f]))v[m++]=65533,p+=h-1;else{for(f&=h===2?31:h===3?15:7;1<h&&p<_;)f=f<<6|63&u[p++],h--;1<h?v[m++]=65533:f<65536?v[m++]=f:(f-=65536,v[m++]=55296|f>>10&1023,v[m++]=56320|1023&f)}return d(v,m)},i.utf8border=function(u,g){var p;for((g=g||u.length)>u.length&&(g=u.length),p=g-1;0<=p&&(192&u[p])==128;)p--;return p<0||p===0?g:p+c[u[p]]>g?p:g}},{"./common":41}],43:[function(t,r,i){r.exports=function(o,a,s,c){for(var l=65535&o|0,d=o>>>16&65535|0,u=0;s!==0;){for(s-=u=2e3<s?2e3:s;d=d+(l=l+a[c++]|0)|0,--u;);l%=65521,d%=65521}return l|d<<16|0}},{}],44:[function(t,r,i){r.exports={Z_NO_FLUSH:0,Z_PARTIAL_FLUSH:1,Z_SYNC_FLUSH:2,Z_FULL_FLUSH:3,Z_FINISH:4,Z_BLOCK:5,Z_TREES:6,Z_OK:0,Z_STREAM_END:1,Z_NEED_DICT:2,Z_ERRNO:-1,Z_STREAM_ERROR:-2,Z_DATA_ERROR:-3,Z_BUF_ERROR:-5,Z_NO_COMPRESSION:0,Z_BEST_SPEED:1,Z_BEST_COMPRESSION:9,Z_DEFAULT_COMPRESSION:-1,Z_FILTERED:1,Z_HUFFMAN_ONLY:2,Z_RLE:3,Z_FIXED:4,Z_DEFAULT_STRATEGY:0,Z_BINARY:0,Z_TEXT:1,Z_UNKNOWN:2,Z_DEFLATED:8}},{}],45:[function(t,r,i){var o=(function(){for(var a,s=[],c=0;c<256;c++){a=c;for(var l=0;l<8;l++)a=1&a?3988292384^a>>>1:a>>>1;s[c]=a}return s})();r.exports=function(a,s,c,l){var d=o,u=l+c;a^=-1;for(var g=l;g<u;g++)a=a>>>8^d[255&(a^s[g])];return-1^a}},{}],46:[function(t,r,i){var o,a=t("../utils/common"),s=t("./trees"),c=t("./adler32"),l=t("./crc32"),d=t("./messages"),u=0,g=4,p=0,m=-2,f=-1,h=4,_=2,v=8,k=9,E=286,D=30,P=19,$=2*E+1,x=15,S=3,C=258,A=C+S+1,w=42,R=113,y=1,O=2,J=3,M=4;function z(b,K){return b.msg=d[K],K}function U(b){return(b<<1)-(4<b?9:0)}function X(b){for(var K=b.length;0<=--K;)b[K]=0}function F(b){var K=b.state,q=K.pending;q>b.avail_out&&(q=b.avail_out),q!==0&&(a.arraySet(b.output,K.pending_buf,K.pending_out,q,b.next_out),b.next_out+=q,K.pending_out+=q,b.total_out+=q,b.avail_out-=q,K.pending-=q,K.pending===0&&(K.pending_out=0))}function N(b,K){s._tr_flush_block(b,0<=b.block_start?b.block_start:-1,b.strstart-b.block_start,K),b.block_start=b.strstart,F(b.strm)}function H(b,K){b.pending_buf[b.pending++]=K}function Q(b,K){b.pending_buf[b.pending++]=K>>>8&255,b.pending_buf[b.pending++]=255&K}function V(b,K){var q,L,I=b.max_chain_length,T=b.strstart,G=b.prev_length,W=b.nice_match,j=b.strstart>b.w_size-A?b.strstart-(b.w_size-A):0,Z=b.window,ne=b.w_mask,Y=b.prev,oe=b.strstart+C,he=Z[T+G-1],de=Z[T+G];b.prev_length>=b.good_match&&(I>>=2),W>b.lookahead&&(W=b.lookahead);do if(Z[(q=K)+G]===de&&Z[q+G-1]===he&&Z[q]===Z[T]&&Z[++q]===Z[T+1]){T+=2,q++;do;while(Z[++T]===Z[++q]&&Z[++T]===Z[++q]&&Z[++T]===Z[++q]&&Z[++T]===Z[++q]&&Z[++T]===Z[++q]&&Z[++T]===Z[++q]&&Z[++T]===Z[++q]&&Z[++T]===Z[++q]&&T<oe);if(L=C-(oe-T),T=oe-C,G<L){if(b.match_start=K,W<=(G=L))break;he=Z[T+G-1],de=Z[T+G]}}while((K=Y[K&ne])>j&&--I!=0);return G<=b.lookahead?G:b.lookahead}function ee(b){var K,q,L,I,T,G,W,j,Z,ne,Y=b.w_size;do{if(I=b.window_size-b.lookahead-b.strstart,b.strstart>=Y+(Y-A)){for(a.arraySet(b.window,b.window,Y,Y,0),b.match_start-=Y,b.strstart-=Y,b.block_start-=Y,K=q=b.hash_size;L=b.head[--K],b.head[K]=Y<=L?L-Y:0,--q;);for(K=q=Y;L=b.prev[--K],b.prev[K]=Y<=L?L-Y:0,--q;);I+=Y}if(b.strm.avail_in===0)break;if(G=b.strm,W=b.window,j=b.strstart+b.lookahead,Z=I,ne=void 0,ne=G.avail_in,Z<ne&&(ne=Z),q=ne===0?0:(G.avail_in-=ne,a.arraySet(W,G.input,G.next_in,ne,j),G.state.wrap===1?G.adler=c(G.adler,W,ne,j):G.state.wrap===2&&(G.adler=l(G.adler,W,ne,j)),G.next_in+=ne,G.total_in+=ne,ne),b.lookahead+=q,b.lookahead+b.insert>=S)for(T=b.strstart-b.insert,b.ins_h=b.window[T],b.ins_h=(b.ins_h<<b.hash_shift^b.window[T+1])&b.hash_mask;b.insert&&(b.ins_h=(b.ins_h<<b.hash_shift^b.window[T+S-1])&b.hash_mask,b.prev[T&b.w_mask]=b.head[b.ins_h],b.head[b.ins_h]=T,T++,b.insert--,!(b.lookahead+b.insert<S)););}while(b.lookahead<A&&b.strm.avail_in!==0)}function se(b,K){for(var q,L;;){if(b.lookahead<A){if(ee(b),b.lookahead<A&&K===u)return y;if(b.lookahead===0)break}if(q=0,b.lookahead>=S&&(b.ins_h=(b.ins_h<<b.hash_shift^b.window[b.strstart+S-1])&b.hash_mask,q=b.prev[b.strstart&b.w_mask]=b.head[b.ins_h],b.head[b.ins_h]=b.strstart),q!==0&&b.strstart-q<=b.w_size-A&&(b.match_length=V(b,q)),b.match_length>=S)if(L=s._tr_tally(b,b.strstart-b.match_start,b.match_length-S),b.lookahead-=b.match_length,b.match_length<=b.max_lazy_match&&b.lookahead>=S){for(b.match_length--;b.strstart++,b.ins_h=(b.ins_h<<b.hash_shift^b.window[b.strstart+S-1])&b.hash_mask,q=b.prev[b.strstart&b.w_mask]=b.head[b.ins_h],b.head[b.ins_h]=b.strstart,--b.match_length!=0;);b.strstart++}else b.strstart+=b.match_length,b.match_length=0,b.ins_h=b.window[b.strstart],b.ins_h=(b.ins_h<<b.hash_shift^b.window[b.strstart+1])&b.hash_mask;else L=s._tr_tally(b,0,b.window[b.strstart]),b.lookahead--,b.strstart++;if(L&&(N(b,!1),b.strm.avail_out===0))return y}return b.insert=b.strstart<S-1?b.strstart:S-1,K===g?(N(b,!0),b.strm.avail_out===0?J:M):b.last_lit&&(N(b,!1),b.strm.avail_out===0)?y:O}function te(b,K){for(var q,L,I;;){if(b.lookahead<A){if(ee(b),b.lookahead<A&&K===u)return y;if(b.lookahead===0)break}if(q=0,b.lookahead>=S&&(b.ins_h=(b.ins_h<<b.hash_shift^b.window[b.strstart+S-1])&b.hash_mask,q=b.prev[b.strstart&b.w_mask]=b.head[b.ins_h],b.head[b.ins_h]=b.strstart),b.prev_length=b.match_length,b.prev_match=b.match_start,b.match_length=S-1,q!==0&&b.prev_length<b.max_lazy_match&&b.strstart-q<=b.w_size-A&&(b.match_length=V(b,q),b.match_length<=5&&(b.strategy===1||b.match_length===S&&4096<b.strstart-b.match_start)&&(b.match_length=S-1)),b.prev_length>=S&&b.match_length<=b.prev_length){for(I=b.strstart+b.lookahead-S,L=s._tr_tally(b,b.strstart-1-b.prev_match,b.prev_length-S),b.lookahead-=b.prev_length-1,b.prev_length-=2;++b.strstart<=I&&(b.ins_h=(b.ins_h<<b.hash_shift^b.window[b.strstart+S-1])&b.hash_mask,q=b.prev[b.strstart&b.w_mask]=b.head[b.ins_h],b.head[b.ins_h]=b.strstart),--b.prev_length!=0;);if(b.match_available=0,b.match_length=S-1,b.strstart++,L&&(N(b,!1),b.strm.avail_out===0))return y}else if(b.match_available){if((L=s._tr_tally(b,0,b.window[b.strstart-1]))&&N(b,!1),b.strstart++,b.lookahead--,b.strm.avail_out===0)return y}else b.match_available=1,b.strstart++,b.lookahead--}return b.match_available&&(L=s._tr_tally(b,0,b.window[b.strstart-1]),b.match_available=0),b.insert=b.strstart<S-1?b.strstart:S-1,K===g?(N(b,!0),b.strm.avail_out===0?J:M):b.last_lit&&(N(b,!1),b.strm.avail_out===0)?y:O}function ae(b,K,q,L,I){this.good_length=b,this.max_lazy=K,this.nice_length=q,this.max_chain=L,this.func=I}function ie(){this.strm=null,this.status=0,this.pending_buf=null,this.pending_buf_size=0,this.pending_out=0,this.pending=0,this.wrap=0,this.gzhead=null,this.gzindex=0,this.method=v,this.last_flush=-1,this.w_size=0,this.w_bits=0,this.w_mask=0,this.window=null,this.window_size=0,this.prev=null,this.head=null,this.ins_h=0,this.hash_size=0,this.hash_bits=0,this.hash_mask=0,this.hash_shift=0,this.block_start=0,this.match_length=0,this.prev_match=0,this.match_available=0,this.strstart=0,this.match_start=0,this.lookahead=0,this.prev_length=0,this.max_chain_length=0,this.max_lazy_match=0,this.level=0,this.strategy=0,this.good_match=0,this.nice_match=0,this.dyn_ltree=new a.Buf16(2*$),this.dyn_dtree=new a.Buf16(2*(2*D+1)),this.bl_tree=new a.Buf16(2*(2*P+1)),X(this.dyn_ltree),X(this.dyn_dtree),X(this.bl_tree),this.l_desc=null,this.d_desc=null,this.bl_desc=null,this.bl_count=new a.Buf16(x+1),this.heap=new a.Buf16(2*E+1),X(this.heap),this.heap_len=0,this.heap_max=0,this.depth=new a.Buf16(2*E+1),X(this.depth),this.l_buf=0,this.lit_bufsize=0,this.last_lit=0,this.d_buf=0,this.opt_len=0,this.static_len=0,this.matches=0,this.insert=0,this.bi_buf=0,this.bi_valid=0}function fe(b){var K;return b&&b.state?(b.total_in=b.total_out=0,b.data_type=_,(K=b.state).pending=0,K.pending_out=0,K.wrap<0&&(K.wrap=-K.wrap),K.status=K.wrap?w:R,b.adler=K.wrap===2?0:1,K.last_flush=u,s._tr_init(K),p):z(b,m)}function _e(b){var K=fe(b);return K===p&&(function(q){q.window_size=2*q.w_size,X(q.head),q.max_lazy_match=o[q.level].max_lazy,q.good_match=o[q.level].good_length,q.nice_match=o[q.level].nice_length,q.max_chain_length=o[q.level].max_chain,q.strstart=0,q.block_start=0,q.lookahead=0,q.insert=0,q.match_length=q.prev_length=S-1,q.match_available=0,q.ins_h=0})(b.state),K}function ye(b,K,q,L,I,T){if(!b)return m;var G=1;if(K===f&&(K=6),L<0?(G=0,L=-L):15<L&&(G=2,L-=16),I<1||k<I||q!==v||L<8||15<L||K<0||9<K||T<0||h<T)return z(b,m);L===8&&(L=9);var W=new ie;return(b.state=W).strm=b,W.wrap=G,W.gzhead=null,W.w_bits=L,W.w_size=1<<W.w_bits,W.w_mask=W.w_size-1,W.hash_bits=I+7,W.hash_size=1<<W.hash_bits,W.hash_mask=W.hash_size-1,W.hash_shift=~~((W.hash_bits+S-1)/S),W.window=new a.Buf8(2*W.w_size),W.head=new a.Buf16(W.hash_size),W.prev=new a.Buf16(W.w_size),W.lit_bufsize=1<<I+6,W.pending_buf_size=4*W.lit_bufsize,W.pending_buf=new a.Buf8(W.pending_buf_size),W.d_buf=1*W.lit_bufsize,W.l_buf=3*W.lit_bufsize,W.level=K,W.strategy=T,W.method=q,_e(b)}o=[new ae(0,0,0,0,function(b,K){var q=65535;for(q>b.pending_buf_size-5&&(q=b.pending_buf_size-5);;){if(b.lookahead<=1){if(ee(b),b.lookahead===0&&K===u)return y;if(b.lookahead===0)break}b.strstart+=b.lookahead,b.lookahead=0;var L=b.block_start+q;if((b.strstart===0||b.strstart>=L)&&(b.lookahead=b.strstart-L,b.strstart=L,N(b,!1),b.strm.avail_out===0)||b.strstart-b.block_start>=b.w_size-A&&(N(b,!1),b.strm.avail_out===0))return y}return b.insert=0,K===g?(N(b,!0),b.strm.avail_out===0?J:M):(b.strstart>b.block_start&&(N(b,!1),b.strm.avail_out),y)}),new ae(4,4,8,4,se),new ae(4,5,16,8,se),new ae(4,6,32,32,se),new ae(4,4,16,16,te),new ae(8,16,32,32,te),new ae(8,16,128,128,te),new ae(8,32,128,256,te),new ae(32,128,258,1024,te),new ae(32,258,258,4096,te)],i.deflateInit=function(b,K){return ye(b,K,v,15,8,0)},i.deflateInit2=ye,i.deflateReset=_e,i.deflateResetKeep=fe,i.deflateSetHeader=function(b,K){return b&&b.state?b.state.wrap!==2?m:(b.state.gzhead=K,p):m},i.deflate=function(b,K){var q,L,I,T;if(!b||!b.state||5<K||K<0)return b?z(b,m):m;if(L=b.state,!b.output||!b.input&&b.avail_in!==0||L.status===666&&K!==g)return z(b,b.avail_out===0?-5:m);if(L.strm=b,q=L.last_flush,L.last_flush=K,L.status===w)if(L.wrap===2)b.adler=0,H(L,31),H(L,139),H(L,8),L.gzhead?(H(L,(L.gzhead.text?1:0)+(L.gzhead.hcrc?2:0)+(L.gzhead.extra?4:0)+(L.gzhead.name?8:0)+(L.gzhead.comment?16:0)),H(L,255&L.gzhead.time),H(L,L.gzhead.time>>8&255),H(L,L.gzhead.time>>16&255),H(L,L.gzhead.time>>24&255),H(L,L.level===9?2:2<=L.strategy||L.level<2?4:0),H(L,255&L.gzhead.os),L.gzhead.extra&&L.gzhead.extra.length&&(H(L,255&L.gzhead.extra.length),H(L,L.gzhead.extra.length>>8&255)),L.gzhead.hcrc&&(b.adler=l(b.adler,L.pending_buf,L.pending,0)),L.gzindex=0,L.status=69):(H(L,0),H(L,0),H(L,0),H(L,0),H(L,0),H(L,L.level===9?2:2<=L.strategy||L.level<2?4:0),H(L,3),L.status=R);else{var G=v+(L.w_bits-8<<4)<<8;G|=(2<=L.strategy||L.level<2?0:L.level<6?1:L.level===6?2:3)<<6,L.strstart!==0&&(G|=32),G+=31-G%31,L.status=R,Q(L,G),L.strstart!==0&&(Q(L,b.adler>>>16),Q(L,65535&b.adler)),b.adler=1}if(L.status===69)if(L.gzhead.extra){for(I=L.pending;L.gzindex<(65535&L.gzhead.extra.length)&&(L.pending!==L.pending_buf_size||(L.gzhead.hcrc&&L.pending>I&&(b.adler=l(b.adler,L.pending_buf,L.pending-I,I)),F(b),I=L.pending,L.pending!==L.pending_buf_size));)H(L,255&L.gzhead.extra[L.gzindex]),L.gzindex++;L.gzhead.hcrc&&L.pending>I&&(b.adler=l(b.adler,L.pending_buf,L.pending-I,I)),L.gzindex===L.gzhead.extra.length&&(L.gzindex=0,L.status=73)}else L.status=73;if(L.status===73)if(L.gzhead.name){I=L.pending;do{if(L.pending===L.pending_buf_size&&(L.gzhead.hcrc&&L.pending>I&&(b.adler=l(b.adler,L.pending_buf,L.pending-I,I)),F(b),I=L.pending,L.pending===L.pending_buf_size)){T=1;break}T=L.gzindex<L.gzhead.name.length?255&L.gzhead.name.charCodeAt(L.gzindex++):0,H(L,T)}while(T!==0);L.gzhead.hcrc&&L.pending>I&&(b.adler=l(b.adler,L.pending_buf,L.pending-I,I)),T===0&&(L.gzindex=0,L.status=91)}else L.status=91;if(L.status===91)if(L.gzhead.comment){I=L.pending;do{if(L.pending===L.pending_buf_size&&(L.gzhead.hcrc&&L.pending>I&&(b.adler=l(b.adler,L.pending_buf,L.pending-I,I)),F(b),I=L.pending,L.pending===L.pending_buf_size)){T=1;break}T=L.gzindex<L.gzhead.comment.length?255&L.gzhead.comment.charCodeAt(L.gzindex++):0,H(L,T)}while(T!==0);L.gzhead.hcrc&&L.pending>I&&(b.adler=l(b.adler,L.pending_buf,L.pending-I,I)),T===0&&(L.status=103)}else L.status=103;if(L.status===103&&(L.gzhead.hcrc?(L.pending+2>L.pending_buf_size&&F(b),L.pending+2<=L.pending_buf_size&&(H(L,255&b.adler),H(L,b.adler>>8&255),b.adler=0,L.status=R)):L.status=R),L.pending!==0){if(F(b),b.avail_out===0)return L.last_flush=-1,p}else if(b.avail_in===0&&U(K)<=U(q)&&K!==g)return z(b,-5);if(L.status===666&&b.avail_in!==0)return z(b,-5);if(b.avail_in!==0||L.lookahead!==0||K!==u&&L.status!==666){var W=L.strategy===2?(function(j,Z){for(var ne;;){if(j.lookahead===0&&(ee(j),j.lookahead===0)){if(Z===u)return y;break}if(j.match_length=0,ne=s._tr_tally(j,0,j.window[j.strstart]),j.lookahead--,j.strstart++,ne&&(N(j,!1),j.strm.avail_out===0))return y}return j.insert=0,Z===g?(N(j,!0),j.strm.avail_out===0?J:M):j.last_lit&&(N(j,!1),j.strm.avail_out===0)?y:O})(L,K):L.strategy===3?(function(j,Z){for(var ne,Y,oe,he,de=j.window;;){if(j.lookahead<=C){if(ee(j),j.lookahead<=C&&Z===u)return y;if(j.lookahead===0)break}if(j.match_length=0,j.lookahead>=S&&0<j.strstart&&(Y=de[oe=j.strstart-1])===de[++oe]&&Y===de[++oe]&&Y===de[++oe]){he=j.strstart+C;do;while(Y===de[++oe]&&Y===de[++oe]&&Y===de[++oe]&&Y===de[++oe]&&Y===de[++oe]&&Y===de[++oe]&&Y===de[++oe]&&Y===de[++oe]&&oe<he);j.match_length=C-(he-oe),j.match_length>j.lookahead&&(j.match_length=j.lookahead)}if(j.match_length>=S?(ne=s._tr_tally(j,1,j.match_length-S),j.lookahead-=j.match_length,j.strstart+=j.match_length,j.match_length=0):(ne=s._tr_tally(j,0,j.window[j.strstart]),j.lookahead--,j.strstart++),ne&&(N(j,!1),j.strm.avail_out===0))return y}return j.insert=0,Z===g?(N(j,!0),j.strm.avail_out===0?J:M):j.last_lit&&(N(j,!1),j.strm.avail_out===0)?y:O})(L,K):o[L.level].func(L,K);if(W!==J&&W!==M||(L.status=666),W===y||W===J)return b.avail_out===0&&(L.last_flush=-1),p;if(W===O&&(K===1?s._tr_align(L):K!==5&&(s._tr_stored_block(L,0,0,!1),K===3&&(X(L.head),L.lookahead===0&&(L.strstart=0,L.block_start=0,L.insert=0))),F(b),b.avail_out===0))return L.last_flush=-1,p}return K!==g?p:L.wrap<=0?1:(L.wrap===2?(H(L,255&b.adler),H(L,b.adler>>8&255),H(L,b.adler>>16&255),H(L,b.adler>>24&255),H(L,255&b.total_in),H(L,b.total_in>>8&255),H(L,b.total_in>>16&255),H(L,b.total_in>>24&255)):(Q(L,b.adler>>>16),Q(L,65535&b.adler)),F(b),0<L.wrap&&(L.wrap=-L.wrap),L.pending!==0?p:1)},i.deflateEnd=function(b){var K;return b&&b.state?(K=b.state.status)!==w&&K!==69&&K!==73&&K!==91&&K!==103&&K!==R&&K!==666?z(b,m):(b.state=null,K===R?z(b,-3):p):m},i.deflateSetDictionary=function(b,K){var q,L,I,T,G,W,j,Z,ne=K.length;if(!b||!b.state||(T=(q=b.state).wrap)===2||T===1&&q.status!==w||q.lookahead)return m;for(T===1&&(b.adler=c(b.adler,K,ne,0)),q.wrap=0,ne>=q.w_size&&(T===0&&(X(q.head),q.strstart=0,q.block_start=0,q.insert=0),Z=new a.Buf8(q.w_size),a.arraySet(Z,K,ne-q.w_size,q.w_size,0),K=Z,ne=q.w_size),G=b.avail_in,W=b.next_in,j=b.input,b.avail_in=ne,b.next_in=0,b.input=K,ee(q);q.lookahead>=S;){for(L=q.strstart,I=q.lookahead-(S-1);q.ins_h=(q.ins_h<<q.hash_shift^q.window[L+S-1])&q.hash_mask,q.prev[L&q.w_mask]=q.head[q.ins_h],q.head[q.ins_h]=L,L++,--I;);q.strstart=L,q.lookahead=S-1,ee(q)}return q.strstart+=q.lookahead,q.block_start=q.strstart,q.insert=q.lookahead,q.lookahead=0,q.match_length=q.prev_length=S-1,q.match_available=0,b.next_in=W,b.input=j,b.avail_in=G,q.wrap=T,p},i.deflateInfo="pako deflate (from Nodeca project)"},{"../utils/common":41,"./adler32":43,"./crc32":45,"./messages":51,"./trees":52}],47:[function(t,r,i){r.exports=function(){this.text=0,this.time=0,this.xflags=0,this.os=0,this.extra=null,this.extra_len=0,this.name="",this.comment="",this.hcrc=0,this.done=!1}},{}],48:[function(t,r,i){r.exports=function(o,a){var s,c,l,d,u,g,p,m,f,h,_,v,k,E,D,P,$,x,S,C,A,w,R,y,O;s=o.state,c=o.next_in,y=o.input,l=c+(o.avail_in-5),d=o.next_out,O=o.output,u=d-(a-o.avail_out),g=d+(o.avail_out-257),p=s.dmax,m=s.wsize,f=s.whave,h=s.wnext,_=s.window,v=s.hold,k=s.bits,E=s.lencode,D=s.distcode,P=(1<<s.lenbits)-1,$=(1<<s.distbits)-1;e:do{k<15&&(v+=y[c++]<<k,k+=8,v+=y[c++]<<k,k+=8),x=E[v&P];t:for(;;){if(v>>>=S=x>>>24,k-=S,(S=x>>>16&255)===0)O[d++]=65535&x;else{if(!(16&S)){if((64&S)==0){x=E[(65535&x)+(v&(1<<S)-1)];continue t}if(32&S){s.mode=12;break e}o.msg="invalid literal/length code",s.mode=30;break e}C=65535&x,(S&=15)&&(k<S&&(v+=y[c++]<<k,k+=8),C+=v&(1<<S)-1,v>>>=S,k-=S),k<15&&(v+=y[c++]<<k,k+=8,v+=y[c++]<<k,k+=8),x=D[v&$];n:for(;;){if(v>>>=S=x>>>24,k-=S,!(16&(S=x>>>16&255))){if((64&S)==0){x=D[(65535&x)+(v&(1<<S)-1)];continue n}o.msg="invalid distance code",s.mode=30;break e}if(A=65535&x,k<(S&=15)&&(v+=y[c++]<<k,(k+=8)<S&&(v+=y[c++]<<k,k+=8)),p<(A+=v&(1<<S)-1)){o.msg="invalid distance too far back",s.mode=30;break e}if(v>>>=S,k-=S,(S=d-u)<A){if(f<(S=A-S)&&s.sane){o.msg="invalid distance too far back",s.mode=30;break e}if(R=_,(w=0)===h){if(w+=m-S,S<C){for(C-=S;O[d++]=_[w++],--S;);w=d-A,R=O}}else if(h<S){if(w+=m+h-S,(S-=h)<C){for(C-=S;O[d++]=_[w++],--S;);if(w=0,h<C){for(C-=S=h;O[d++]=_[w++],--S;);w=d-A,R=O}}}else if(w+=h-S,S<C){for(C-=S;O[d++]=_[w++],--S;);w=d-A,R=O}for(;2<C;)O[d++]=R[w++],O[d++]=R[w++],O[d++]=R[w++],C-=3;C&&(O[d++]=R[w++],1<C&&(O[d++]=R[w++]))}else{for(w=d-A;O[d++]=O[w++],O[d++]=O[w++],O[d++]=O[w++],2<(C-=3););C&&(O[d++]=O[w++],1<C&&(O[d++]=O[w++]))}break}}break}}while(c<l&&d<g);c-=C=k>>3,v&=(1<<(k-=C<<3))-1,o.next_in=c,o.next_out=d,o.avail_in=c<l?l-c+5:5-(c-l),o.avail_out=d<g?g-d+257:257-(d-g),s.hold=v,s.bits=k}},{}],49:[function(t,r,i){var o=t("../utils/common"),a=t("./adler32"),s=t("./crc32"),c=t("./inffast"),l=t("./inftrees"),d=1,u=2,g=0,p=-2,m=1,f=852,h=592;function _(w){return(w>>>24&255)+(w>>>8&65280)+((65280&w)<<8)+((255&w)<<24)}function v(){this.mode=0,this.last=!1,this.wrap=0,this.havedict=!1,this.flags=0,this.dmax=0,this.check=0,this.total=0,this.head=null,this.wbits=0,this.wsize=0,this.whave=0,this.wnext=0,this.window=null,this.hold=0,this.bits=0,this.length=0,this.offset=0,this.extra=0,this.lencode=null,this.distcode=null,this.lenbits=0,this.distbits=0,this.ncode=0,this.nlen=0,this.ndist=0,this.have=0,this.next=null,this.lens=new o.Buf16(320),this.work=new o.Buf16(288),this.lendyn=null,this.distdyn=null,this.sane=0,this.back=0,this.was=0}function k(w){var R;return w&&w.state?(R=w.state,w.total_in=w.total_out=R.total=0,w.msg="",R.wrap&&(w.adler=1&R.wrap),R.mode=m,R.last=0,R.havedict=0,R.dmax=32768,R.head=null,R.hold=0,R.bits=0,R.lencode=R.lendyn=new o.Buf32(f),R.distcode=R.distdyn=new o.Buf32(h),R.sane=1,R.back=-1,g):p}function E(w){var R;return w&&w.state?((R=w.state).wsize=0,R.whave=0,R.wnext=0,k(w)):p}function D(w,R){var y,O;return w&&w.state?(O=w.state,R<0?(y=0,R=-R):(y=1+(R>>4),R<48&&(R&=15)),R&&(R<8||15<R)?p:(O.window!==null&&O.wbits!==R&&(O.window=null),O.wrap=y,O.wbits=R,E(w))):p}function P(w,R){var y,O;return w?(O=new v,(w.state=O).window=null,(y=D(w,R))!==g&&(w.state=null),y):p}var $,x,S=!0;function C(w){if(S){var R;for($=new o.Buf32(512),x=new o.Buf32(32),R=0;R<144;)w.lens[R++]=8;for(;R<256;)w.lens[R++]=9;for(;R<280;)w.lens[R++]=7;for(;R<288;)w.lens[R++]=8;for(l(d,w.lens,0,288,$,0,w.work,{bits:9}),R=0;R<32;)w.lens[R++]=5;l(u,w.lens,0,32,x,0,w.work,{bits:5}),S=!1}w.lencode=$,w.lenbits=9,w.distcode=x,w.distbits=5}function A(w,R,y,O){var J,M=w.state;return M.window===null&&(M.wsize=1<<M.wbits,M.wnext=0,M.whave=0,M.window=new o.Buf8(M.wsize)),O>=M.wsize?(o.arraySet(M.window,R,y-M.wsize,M.wsize,0),M.wnext=0,M.whave=M.wsize):(O<(J=M.wsize-M.wnext)&&(J=O),o.arraySet(M.window,R,y-O,J,M.wnext),(O-=J)?(o.arraySet(M.window,R,y-O,O,0),M.wnext=O,M.whave=M.wsize):(M.wnext+=J,M.wnext===M.wsize&&(M.wnext=0),M.whave<M.wsize&&(M.whave+=J))),0}i.inflateReset=E,i.inflateReset2=D,i.inflateResetKeep=k,i.inflateInit=function(w){return P(w,15)},i.inflateInit2=P,i.inflate=function(w,R){var y,O,J,M,z,U,X,F,N,H,Q,V,ee,se,te,ae,ie,fe,_e,ye,b,K,q,L,I=0,T=new o.Buf8(4),G=[16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15];if(!w||!w.state||!w.output||!w.input&&w.avail_in!==0)return p;(y=w.state).mode===12&&(y.mode=13),z=w.next_out,J=w.output,X=w.avail_out,M=w.next_in,O=w.input,U=w.avail_in,F=y.hold,N=y.bits,H=U,Q=X,K=g;e:for(;;)switch(y.mode){case m:if(y.wrap===0){y.mode=13;break}for(;N<16;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if(2&y.wrap&&F===35615){T[y.check=0]=255&F,T[1]=F>>>8&255,y.check=s(y.check,T,2,0),N=F=0,y.mode=2;break}if(y.flags=0,y.head&&(y.head.done=!1),!(1&y.wrap)||(((255&F)<<8)+(F>>8))%31){w.msg="incorrect header check",y.mode=30;break}if((15&F)!=8){w.msg="unknown compression method",y.mode=30;break}if(N-=4,b=8+(15&(F>>>=4)),y.wbits===0)y.wbits=b;else if(b>y.wbits){w.msg="invalid window size",y.mode=30;break}y.dmax=1<<b,w.adler=y.check=1,y.mode=512&F?10:12,N=F=0;break;case 2:for(;N<16;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if(y.flags=F,(255&y.flags)!=8){w.msg="unknown compression method",y.mode=30;break}if(57344&y.flags){w.msg="unknown header flags set",y.mode=30;break}y.head&&(y.head.text=F>>8&1),512&y.flags&&(T[0]=255&F,T[1]=F>>>8&255,y.check=s(y.check,T,2,0)),N=F=0,y.mode=3;case 3:for(;N<32;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}y.head&&(y.head.time=F),512&y.flags&&(T[0]=255&F,T[1]=F>>>8&255,T[2]=F>>>16&255,T[3]=F>>>24&255,y.check=s(y.check,T,4,0)),N=F=0,y.mode=4;case 4:for(;N<16;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}y.head&&(y.head.xflags=255&F,y.head.os=F>>8),512&y.flags&&(T[0]=255&F,T[1]=F>>>8&255,y.check=s(y.check,T,2,0)),N=F=0,y.mode=5;case 5:if(1024&y.flags){for(;N<16;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}y.length=F,y.head&&(y.head.extra_len=F),512&y.flags&&(T[0]=255&F,T[1]=F>>>8&255,y.check=s(y.check,T,2,0)),N=F=0}else y.head&&(y.head.extra=null);y.mode=6;case 6:if(1024&y.flags&&(U<(V=y.length)&&(V=U),V&&(y.head&&(b=y.head.extra_len-y.length,y.head.extra||(y.head.extra=new Array(y.head.extra_len)),o.arraySet(y.head.extra,O,M,V,b)),512&y.flags&&(y.check=s(y.check,O,V,M)),U-=V,M+=V,y.length-=V),y.length))break e;y.length=0,y.mode=7;case 7:if(2048&y.flags){if(U===0)break e;for(V=0;b=O[M+V++],y.head&&b&&y.length<65536&&(y.head.name+=String.fromCharCode(b)),b&&V<U;);if(512&y.flags&&(y.check=s(y.check,O,V,M)),U-=V,M+=V,b)break e}else y.head&&(y.head.name=null);y.length=0,y.mode=8;case 8:if(4096&y.flags){if(U===0)break e;for(V=0;b=O[M+V++],y.head&&b&&y.length<65536&&(y.head.comment+=String.fromCharCode(b)),b&&V<U;);if(512&y.flags&&(y.check=s(y.check,O,V,M)),U-=V,M+=V,b)break e}else y.head&&(y.head.comment=null);y.mode=9;case 9:if(512&y.flags){for(;N<16;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if(F!==(65535&y.check)){w.msg="header crc mismatch",y.mode=30;break}N=F=0}y.head&&(y.head.hcrc=y.flags>>9&1,y.head.done=!0),w.adler=y.check=0,y.mode=12;break;case 10:for(;N<32;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}w.adler=y.check=_(F),N=F=0,y.mode=11;case 11:if(y.havedict===0)return w.next_out=z,w.avail_out=X,w.next_in=M,w.avail_in=U,y.hold=F,y.bits=N,2;w.adler=y.check=1,y.mode=12;case 12:if(R===5||R===6)break e;case 13:if(y.last){F>>>=7&N,N-=7&N,y.mode=27;break}for(;N<3;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}switch(y.last=1&F,N-=1,3&(F>>>=1)){case 0:y.mode=14;break;case 1:if(C(y),y.mode=20,R!==6)break;F>>>=2,N-=2;break e;case 2:y.mode=17;break;case 3:w.msg="invalid block type",y.mode=30}F>>>=2,N-=2;break;case 14:for(F>>>=7&N,N-=7&N;N<32;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if((65535&F)!=(F>>>16^65535)){w.msg="invalid stored block lengths",y.mode=30;break}if(y.length=65535&F,N=F=0,y.mode=15,R===6)break e;case 15:y.mode=16;case 16:if(V=y.length){if(U<V&&(V=U),X<V&&(V=X),V===0)break e;o.arraySet(J,O,M,V,z),U-=V,M+=V,X-=V,z+=V,y.length-=V;break}y.mode=12;break;case 17:for(;N<14;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if(y.nlen=257+(31&F),F>>>=5,N-=5,y.ndist=1+(31&F),F>>>=5,N-=5,y.ncode=4+(15&F),F>>>=4,N-=4,286<y.nlen||30<y.ndist){w.msg="too many length or distance symbols",y.mode=30;break}y.have=0,y.mode=18;case 18:for(;y.have<y.ncode;){for(;N<3;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}y.lens[G[y.have++]]=7&F,F>>>=3,N-=3}for(;y.have<19;)y.lens[G[y.have++]]=0;if(y.lencode=y.lendyn,y.lenbits=7,q={bits:y.lenbits},K=l(0,y.lens,0,19,y.lencode,0,y.work,q),y.lenbits=q.bits,K){w.msg="invalid code lengths set",y.mode=30;break}y.have=0,y.mode=19;case 19:for(;y.have<y.nlen+y.ndist;){for(;ae=(I=y.lencode[F&(1<<y.lenbits)-1])>>>16&255,ie=65535&I,!((te=I>>>24)<=N);){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if(ie<16)F>>>=te,N-=te,y.lens[y.have++]=ie;else{if(ie===16){for(L=te+2;N<L;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if(F>>>=te,N-=te,y.have===0){w.msg="invalid bit length repeat",y.mode=30;break}b=y.lens[y.have-1],V=3+(3&F),F>>>=2,N-=2}else if(ie===17){for(L=te+3;N<L;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}N-=te,b=0,V=3+(7&(F>>>=te)),F>>>=3,N-=3}else{for(L=te+7;N<L;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}N-=te,b=0,V=11+(127&(F>>>=te)),F>>>=7,N-=7}if(y.have+V>y.nlen+y.ndist){w.msg="invalid bit length repeat",y.mode=30;break}for(;V--;)y.lens[y.have++]=b}}if(y.mode===30)break;if(y.lens[256]===0){w.msg="invalid code -- missing end-of-block",y.mode=30;break}if(y.lenbits=9,q={bits:y.lenbits},K=l(d,y.lens,0,y.nlen,y.lencode,0,y.work,q),y.lenbits=q.bits,K){w.msg="invalid literal/lengths set",y.mode=30;break}if(y.distbits=6,y.distcode=y.distdyn,q={bits:y.distbits},K=l(u,y.lens,y.nlen,y.ndist,y.distcode,0,y.work,q),y.distbits=q.bits,K){w.msg="invalid distances set",y.mode=30;break}if(y.mode=20,R===6)break e;case 20:y.mode=21;case 21:if(6<=U&&258<=X){w.next_out=z,w.avail_out=X,w.next_in=M,w.avail_in=U,y.hold=F,y.bits=N,c(w,Q),z=w.next_out,J=w.output,X=w.avail_out,M=w.next_in,O=w.input,U=w.avail_in,F=y.hold,N=y.bits,y.mode===12&&(y.back=-1);break}for(y.back=0;ae=(I=y.lencode[F&(1<<y.lenbits)-1])>>>16&255,ie=65535&I,!((te=I>>>24)<=N);){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if(ae&&(240&ae)==0){for(fe=te,_e=ae,ye=ie;ae=(I=y.lencode[ye+((F&(1<<fe+_e)-1)>>fe)])>>>16&255,ie=65535&I,!(fe+(te=I>>>24)<=N);){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}F>>>=fe,N-=fe,y.back+=fe}if(F>>>=te,N-=te,y.back+=te,y.length=ie,ae===0){y.mode=26;break}if(32&ae){y.back=-1,y.mode=12;break}if(64&ae){w.msg="invalid literal/length code",y.mode=30;break}y.extra=15&ae,y.mode=22;case 22:if(y.extra){for(L=y.extra;N<L;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}y.length+=F&(1<<y.extra)-1,F>>>=y.extra,N-=y.extra,y.back+=y.extra}y.was=y.length,y.mode=23;case 23:for(;ae=(I=y.distcode[F&(1<<y.distbits)-1])>>>16&255,ie=65535&I,!((te=I>>>24)<=N);){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if((240&ae)==0){for(fe=te,_e=ae,ye=ie;ae=(I=y.distcode[ye+((F&(1<<fe+_e)-1)>>fe)])>>>16&255,ie=65535&I,!(fe+(te=I>>>24)<=N);){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}F>>>=fe,N-=fe,y.back+=fe}if(F>>>=te,N-=te,y.back+=te,64&ae){w.msg="invalid distance code",y.mode=30;break}y.offset=ie,y.extra=15&ae,y.mode=24;case 24:if(y.extra){for(L=y.extra;N<L;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}y.offset+=F&(1<<y.extra)-1,F>>>=y.extra,N-=y.extra,y.back+=y.extra}if(y.offset>y.dmax){w.msg="invalid distance too far back",y.mode=30;break}y.mode=25;case 25:if(X===0)break e;if(V=Q-X,y.offset>V){if((V=y.offset-V)>y.whave&&y.sane){w.msg="invalid distance too far back",y.mode=30;break}ee=V>y.wnext?(V-=y.wnext,y.wsize-V):y.wnext-V,V>y.length&&(V=y.length),se=y.window}else se=J,ee=z-y.offset,V=y.length;for(X<V&&(V=X),X-=V,y.length-=V;J[z++]=se[ee++],--V;);y.length===0&&(y.mode=21);break;case 26:if(X===0)break e;J[z++]=y.length,X--,y.mode=21;break;case 27:if(y.wrap){for(;N<32;){if(U===0)break e;U--,F|=O[M++]<<N,N+=8}if(Q-=X,w.total_out+=Q,y.total+=Q,Q&&(w.adler=y.check=y.flags?s(y.check,J,Q,z-Q):a(y.check,J,Q,z-Q)),Q=X,(y.flags?F:_(F))!==y.check){w.msg="incorrect data check",y.mode=30;break}N=F=0}y.mode=28;case 28:if(y.wrap&&y.flags){for(;N<32;){if(U===0)break e;U--,F+=O[M++]<<N,N+=8}if(F!==(4294967295&y.total)){w.msg="incorrect length check",y.mode=30;break}N=F=0}y.mode=29;case 29:K=1;break e;case 30:K=-3;break e;case 31:return-4;case 32:default:return p}return w.next_out=z,w.avail_out=X,w.next_in=M,w.avail_in=U,y.hold=F,y.bits=N,(y.wsize||Q!==w.avail_out&&y.mode<30&&(y.mode<27||R!==4))&&A(w,w.output,w.next_out,Q-w.avail_out)?(y.mode=31,-4):(H-=w.avail_in,Q-=w.avail_out,w.total_in+=H,w.total_out+=Q,y.total+=Q,y.wrap&&Q&&(w.adler=y.check=y.flags?s(y.check,J,Q,w.next_out-Q):a(y.check,J,Q,w.next_out-Q)),w.data_type=y.bits+(y.last?64:0)+(y.mode===12?128:0)+(y.mode===20||y.mode===15?256:0),(H==0&&Q===0||R===4)&&K===g&&(K=-5),K)},i.inflateEnd=function(w){if(!w||!w.state)return p;var R=w.state;return R.window&&(R.window=null),w.state=null,g},i.inflateGetHeader=function(w,R){var y;return w&&w.state?(2&(y=w.state).wrap)==0?p:((y.head=R).done=!1,g):p},i.inflateSetDictionary=function(w,R){var y,O=R.length;return w&&w.state?(y=w.state).wrap!==0&&y.mode!==11?p:y.mode===11&&a(1,R,O,0)!==y.check?-3:A(w,R,O,O)?(y.mode=31,-4):(y.havedict=1,g):p},i.inflateInfo="pako inflate (from Nodeca project)"},{"../utils/common":41,"./adler32":43,"./crc32":45,"./inffast":48,"./inftrees":50}],50:[function(t,r,i){var o=t("../utils/common"),a=[3,4,5,6,7,8,9,10,11,13,15,17,19,23,27,31,35,43,51,59,67,83,99,115,131,163,195,227,258,0,0],s=[16,16,16,16,16,16,16,16,17,17,17,17,18,18,18,18,19,19,19,19,20,20,20,20,21,21,21,21,16,72,78],c=[1,2,3,4,5,7,9,13,17,25,33,49,65,97,129,193,257,385,513,769,1025,1537,2049,3073,4097,6145,8193,12289,16385,24577,0,0],l=[16,16,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25,25,26,26,27,27,28,28,29,29,64,64];r.exports=function(d,u,g,p,m,f,h,_){var v,k,E,D,P,$,x,S,C,A=_.bits,w=0,R=0,y=0,O=0,J=0,M=0,z=0,U=0,X=0,F=0,N=null,H=0,Q=new o.Buf16(16),V=new o.Buf16(16),ee=null,se=0;for(w=0;w<=15;w++)Q[w]=0;for(R=0;R<p;R++)Q[u[g+R]]++;for(J=A,O=15;1<=O&&Q[O]===0;O--);if(O<J&&(J=O),O===0)return m[f++]=20971520,m[f++]=20971520,_.bits=1,0;for(y=1;y<O&&Q[y]===0;y++);for(J<y&&(J=y),w=U=1;w<=15;w++)if(U<<=1,(U-=Q[w])<0)return-1;if(0<U&&(d===0||O!==1))return-1;for(V[1]=0,w=1;w<15;w++)V[w+1]=V[w]+Q[w];for(R=0;R<p;R++)u[g+R]!==0&&(h[V[u[g+R]]++]=R);if($=d===0?(N=ee=h,19):d===1?(N=a,H-=257,ee=s,se-=257,256):(N=c,ee=l,-1),w=y,P=f,z=R=F=0,E=-1,D=(X=1<<(M=J))-1,d===1&&852<X||d===2&&592<X)return 1;for(;;){for(x=w-z,C=h[R]<$?(S=0,h[R]):h[R]>$?(S=ee[se+h[R]],N[H+h[R]]):(S=96,0),v=1<<w-z,y=k=1<<M;m[P+(F>>z)+(k-=v)]=x<<24|S<<16|C|0,k!==0;);for(v=1<<w-1;F&v;)v>>=1;if(v!==0?(F&=v-1,F+=v):F=0,R++,--Q[w]==0){if(w===O)break;w=u[g+h[R]]}if(J<w&&(F&D)!==E){for(z===0&&(z=J),P+=y,U=1<<(M=w-z);M+z<O&&!((U-=Q[M+z])<=0);)M++,U<<=1;if(X+=1<<M,d===1&&852<X||d===2&&592<X)return 1;m[E=F&D]=J<<24|M<<16|P-f|0}}return F!==0&&(m[P+F]=w-z<<24|64<<16|0),_.bits=J,0}},{"../utils/common":41}],51:[function(t,r,i){r.exports={2:"need dictionary",1:"stream end",0:"","-1":"file error","-2":"stream error","-3":"data error","-4":"insufficient memory","-5":"buffer error","-6":"incompatible version"}},{}],52:[function(t,r,i){var o=t("../utils/common"),a=0,s=1;function c(I){for(var T=I.length;0<=--T;)I[T]=0}var l=0,d=29,u=256,g=u+1+d,p=30,m=19,f=2*g+1,h=15,_=16,v=7,k=256,E=16,D=17,P=18,$=[0,0,0,0,0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,0],x=[0,0,0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13],S=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,3,7],C=[16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15],A=new Array(2*(g+2));c(A);var w=new Array(2*p);c(w);var R=new Array(512);c(R);var y=new Array(256);c(y);var O=new Array(d);c(O);var J,M,z,U=new Array(p);function X(I,T,G,W,j){this.static_tree=I,this.extra_bits=T,this.extra_base=G,this.elems=W,this.max_length=j,this.has_stree=I&&I.length}function F(I,T){this.dyn_tree=I,this.max_code=0,this.stat_desc=T}function N(I){return I<256?R[I]:R[256+(I>>>7)]}function H(I,T){I.pending_buf[I.pending++]=255&T,I.pending_buf[I.pending++]=T>>>8&255}function Q(I,T,G){I.bi_valid>_-G?(I.bi_buf|=T<<I.bi_valid&65535,H(I,I.bi_buf),I.bi_buf=T>>_-I.bi_valid,I.bi_valid+=G-_):(I.bi_buf|=T<<I.bi_valid&65535,I.bi_valid+=G)}function V(I,T,G){Q(I,G[2*T],G[2*T+1])}function ee(I,T){for(var G=0;G|=1&I,I>>>=1,G<<=1,0<--T;);return G>>>1}function se(I,T,G){var W,j,Z=new Array(h+1),ne=0;for(W=1;W<=h;W++)Z[W]=ne=ne+G[W-1]<<1;for(j=0;j<=T;j++){var Y=I[2*j+1];Y!==0&&(I[2*j]=ee(Z[Y]++,Y))}}function te(I){var T;for(T=0;T<g;T++)I.dyn_ltree[2*T]=0;for(T=0;T<p;T++)I.dyn_dtree[2*T]=0;for(T=0;T<m;T++)I.bl_tree[2*T]=0;I.dyn_ltree[2*k]=1,I.opt_len=I.static_len=0,I.last_lit=I.matches=0}function ae(I){8<I.bi_valid?H(I,I.bi_buf):0<I.bi_valid&&(I.pending_buf[I.pending++]=I.bi_buf),I.bi_buf=0,I.bi_valid=0}function ie(I,T,G,W){var j=2*T,Z=2*G;return I[j]<I[Z]||I[j]===I[Z]&&W[T]<=W[G]}function fe(I,T,G){for(var W=I.heap[G],j=G<<1;j<=I.heap_len&&(j<I.heap_len&&ie(T,I.heap[j+1],I.heap[j],I.depth)&&j++,!ie(T,W,I.heap[j],I.depth));)I.heap[G]=I.heap[j],G=j,j<<=1;I.heap[G]=W}function _e(I,T,G){var W,j,Z,ne,Y=0;if(I.last_lit!==0)for(;W=I.pending_buf[I.d_buf+2*Y]<<8|I.pending_buf[I.d_buf+2*Y+1],j=I.pending_buf[I.l_buf+Y],Y++,W===0?V(I,j,T):(V(I,(Z=y[j])+u+1,T),(ne=$[Z])!==0&&Q(I,j-=O[Z],ne),V(I,Z=N(--W),G),(ne=x[Z])!==0&&Q(I,W-=U[Z],ne)),Y<I.last_lit;);V(I,k,T)}function ye(I,T){var G,W,j,Z=T.dyn_tree,ne=T.stat_desc.static_tree,Y=T.stat_desc.has_stree,oe=T.stat_desc.elems,he=-1;for(I.heap_len=0,I.heap_max=f,G=0;G<oe;G++)Z[2*G]!==0?(I.heap[++I.heap_len]=he=G,I.depth[G]=0):Z[2*G+1]=0;for(;I.heap_len<2;)Z[2*(j=I.heap[++I.heap_len]=he<2?++he:0)]=1,I.depth[j]=0,I.opt_len--,Y&&(I.static_len-=ne[2*j+1]);for(T.max_code=he,G=I.heap_len>>1;1<=G;G--)fe(I,Z,G);for(j=oe;G=I.heap[1],I.heap[1]=I.heap[I.heap_len--],fe(I,Z,1),W=I.heap[1],I.heap[--I.heap_max]=G,I.heap[--I.heap_max]=W,Z[2*j]=Z[2*G]+Z[2*W],I.depth[j]=(I.depth[G]>=I.depth[W]?I.depth[G]:I.depth[W])+1,Z[2*G+1]=Z[2*W+1]=j,I.heap[1]=j++,fe(I,Z,1),2<=I.heap_len;);I.heap[--I.heap_max]=I.heap[1],(function(de,Fe){var Mt,je,Bt,ke,mn,Zn,Ke=Fe.dyn_tree,Vr=Fe.max_code,Da=Fe.stat_desc.static_tree,Ta=Fe.stat_desc.has_stree,Aa=Fe.stat_desc.extra_bits,Kr=Fe.stat_desc.extra_base,jt=Fe.stat_desc.max_length,hn=0;for(ke=0;ke<=h;ke++)de.bl_count[ke]=0;for(Ke[2*de.heap[de.heap_max]+1]=0,Mt=de.heap_max+1;Mt<f;Mt++)jt<(ke=Ke[2*Ke[2*(je=de.heap[Mt])+1]+1]+1)&&(ke=jt,hn++),Ke[2*je+1]=ke,Vr<je||(de.bl_count[ke]++,mn=0,Kr<=je&&(mn=Aa[je-Kr]),Zn=Ke[2*je],de.opt_len+=Zn*(ke+mn),Ta&&(de.static_len+=Zn*(Da[2*je+1]+mn)));if(hn!==0){do{for(ke=jt-1;de.bl_count[ke]===0;)ke--;de.bl_count[ke]--,de.bl_count[ke+1]+=2,de.bl_count[jt]--,hn-=2}while(0<hn);for(ke=jt;ke!==0;ke--)for(je=de.bl_count[ke];je!==0;)Vr<(Bt=de.heap[--Mt])||(Ke[2*Bt+1]!==ke&&(de.opt_len+=(ke-Ke[2*Bt+1])*Ke[2*Bt],Ke[2*Bt+1]=ke),je--)}})(I,T),se(Z,he,I.bl_count)}function b(I,T,G){var W,j,Z=-1,ne=T[1],Y=0,oe=7,he=4;for(ne===0&&(oe=138,he=3),T[2*(G+1)+1]=65535,W=0;W<=G;W++)j=ne,ne=T[2*(W+1)+1],++Y<oe&&j===ne||(Y<he?I.bl_tree[2*j]+=Y:j!==0?(j!==Z&&I.bl_tree[2*j]++,I.bl_tree[2*E]++):Y<=10?I.bl_tree[2*D]++:I.bl_tree[2*P]++,Z=j,he=(Y=0)===ne?(oe=138,3):j===ne?(oe=6,3):(oe=7,4))}function K(I,T,G){var W,j,Z=-1,ne=T[1],Y=0,oe=7,he=4;for(ne===0&&(oe=138,he=3),W=0;W<=G;W++)if(j=ne,ne=T[2*(W+1)+1],!(++Y<oe&&j===ne)){if(Y<he)for(;V(I,j,I.bl_tree),--Y!=0;);else j!==0?(j!==Z&&(V(I,j,I.bl_tree),Y--),V(I,E,I.bl_tree),Q(I,Y-3,2)):Y<=10?(V(I,D,I.bl_tree),Q(I,Y-3,3)):(V(I,P,I.bl_tree),Q(I,Y-11,7));Z=j,he=(Y=0)===ne?(oe=138,3):j===ne?(oe=6,3):(oe=7,4)}}c(U);var q=!1;function L(I,T,G,W){Q(I,(l<<1)+(W?1:0),3),(function(j,Z,ne,Y){ae(j),H(j,ne),H(j,~ne),o.arraySet(j.pending_buf,j.window,Z,ne,j.pending),j.pending+=ne})(I,T,G)}i._tr_init=function(I){q||((function(){var T,G,W,j,Z,ne=new Array(h+1);for(j=W=0;j<d-1;j++)for(O[j]=W,T=0;T<1<<$[j];T++)y[W++]=j;for(y[W-1]=j,j=Z=0;j<16;j++)for(U[j]=Z,T=0;T<1<<x[j];T++)R[Z++]=j;for(Z>>=7;j<p;j++)for(U[j]=Z<<7,T=0;T<1<<x[j]-7;T++)R[256+Z++]=j;for(G=0;G<=h;G++)ne[G]=0;for(T=0;T<=143;)A[2*T+1]=8,T++,ne[8]++;for(;T<=255;)A[2*T+1]=9,T++,ne[9]++;for(;T<=279;)A[2*T+1]=7,T++,ne[7]++;for(;T<=287;)A[2*T+1]=8,T++,ne[8]++;for(se(A,g+1,ne),T=0;T<p;T++)w[2*T+1]=5,w[2*T]=ee(T,5);J=new X(A,$,u+1,g,h),M=new X(w,x,0,p,h),z=new X(new Array(0),S,0,m,v)})(),q=!0),I.l_desc=new F(I.dyn_ltree,J),I.d_desc=new F(I.dyn_dtree,M),I.bl_desc=new F(I.bl_tree,z),I.bi_buf=0,I.bi_valid=0,te(I)},i._tr_stored_block=L,i._tr_flush_block=function(I,T,G,W){var j,Z,ne=0;0<I.level?(I.strm.data_type===2&&(I.strm.data_type=(function(Y){var oe,he=4093624447;for(oe=0;oe<=31;oe++,he>>>=1)if(1&he&&Y.dyn_ltree[2*oe]!==0)return a;if(Y.dyn_ltree[18]!==0||Y.dyn_ltree[20]!==0||Y.dyn_ltree[26]!==0)return s;for(oe=32;oe<u;oe++)if(Y.dyn_ltree[2*oe]!==0)return s;return a})(I)),ye(I,I.l_desc),ye(I,I.d_desc),ne=(function(Y){var oe;for(b(Y,Y.dyn_ltree,Y.l_desc.max_code),b(Y,Y.dyn_dtree,Y.d_desc.max_code),ye(Y,Y.bl_desc),oe=m-1;3<=oe&&Y.bl_tree[2*C[oe]+1]===0;oe--);return Y.opt_len+=3*(oe+1)+5+5+4,oe})(I),j=I.opt_len+3+7>>>3,(Z=I.static_len+3+7>>>3)<=j&&(j=Z)):j=Z=G+5,G+4<=j&&T!==-1?L(I,T,G,W):I.strategy===4||Z===j?(Q(I,2+(W?1:0),3),_e(I,A,w)):(Q(I,4+(W?1:0),3),(function(Y,oe,he,de){var Fe;for(Q(Y,oe-257,5),Q(Y,he-1,5),Q(Y,de-4,4),Fe=0;Fe<de;Fe++)Q(Y,Y.bl_tree[2*C[Fe]+1],3);K(Y,Y.dyn_ltree,oe-1),K(Y,Y.dyn_dtree,he-1)})(I,I.l_desc.max_code+1,I.d_desc.max_code+1,ne+1),_e(I,I.dyn_ltree,I.dyn_dtree)),te(I),W&&ae(I)},i._tr_tally=function(I,T,G){return I.pending_buf[I.d_buf+2*I.last_lit]=T>>>8&255,I.pending_buf[I.d_buf+2*I.last_lit+1]=255&T,I.pending_buf[I.l_buf+I.last_lit]=255&G,I.last_lit++,T===0?I.dyn_ltree[2*G]++:(I.matches++,T--,I.dyn_ltree[2*(y[G]+u+1)]++,I.dyn_dtree[2*N(T)]++),I.last_lit===I.lit_bufsize-1},i._tr_align=function(I){Q(I,2,3),V(I,k,A),(function(T){T.bi_valid===16?(H(T,T.bi_buf),T.bi_buf=0,T.bi_valid=0):8<=T.bi_valid&&(T.pending_buf[T.pending++]=255&T.bi_buf,T.bi_buf>>=8,T.bi_valid-=8)})(I)}},{"../utils/common":41}],53:[function(t,r,i){r.exports=function(){this.input=null,this.next_in=0,this.avail_in=0,this.total_in=0,this.output=null,this.next_out=0,this.avail_out=0,this.total_out=0,this.msg="",this.state=null,this.data_type=2,this.adler=0}},{}],54:[function(t,r,i){(function(o){(function(a,s){if(!a.setImmediate){var c,l,d,u,g=1,p={},m=!1,f=a.document,h=Object.getPrototypeOf&&Object.getPrototypeOf(a);h=h&&h.setTimeout?h:a,c={}.toString.call(a.process)==="[object process]"?function(E){process.nextTick(function(){v(E)})}:(function(){if(a.postMessage&&!a.importScripts){var E=!0,D=a.onmessage;return a.onmessage=function(){E=!1},a.postMessage("","*"),a.onmessage=D,E}})()?(u="setImmediate$"+Math.random()+"$",a.addEventListener?a.addEventListener("message",k,!1):a.attachEvent("onmessage",k),function(E){a.postMessage(u+E,"*")}):a.MessageChannel?((d=new MessageChannel).port1.onmessage=function(E){v(E.data)},function(E){d.port2.postMessage(E)}):f&&"onreadystatechange"in f.createElement("script")?(l=f.documentElement,function(E){var D=f.createElement("script");D.onreadystatechange=function(){v(E),D.onreadystatechange=null,l.removeChild(D),D=null},l.appendChild(D)}):function(E){setTimeout(v,0,E)},h.setImmediate=function(E){typeof E!="function"&&(E=new Function(""+E));for(var D=new Array(arguments.length-1),P=0;P<D.length;P++)D[P]=arguments[P+1];var $={callback:E,args:D};return p[g]=$,c(g),g++},h.clearImmediate=_}function _(E){delete p[E]}function v(E){if(m)setTimeout(v,0,E);else{var D=p[E];if(D){m=!0;try{(function(P){var $=P.callback,x=P.args;switch(x.length){case 0:$();break;case 1:$(x[0]);break;case 2:$(x[0],x[1]);break;case 3:$(x[0],x[1],x[2]);break;default:$.apply(s,x)}})(D)}finally{_(E),m=!1}}}}function k(E){E.source===a&&typeof E.data=="string"&&E.data.indexOf(u)===0&&v(+E.data.slice(u.length))}})(typeof self>"u"?o===void 0?this:o:self)}).call(this,typeof Dn<"u"?Dn:typeof self<"u"?self:typeof window<"u"?window:{})},{}]},{},[10])(10)})})(_r)),_r.exports}var Ra=Bu();const ju=Mu(Ra),Wr=Ia({__proto__:null,default:ju},[Ra]);</script>
  <style rel="stylesheet" crossorigin>:root{--bg: #0e1117;--surface: #1a1d27;--surface2: #262730;--border: #363842;--text: #fafafa;--text2: #808495;--primary: #ff4b4b;--primary-hover: #ff6b6b;--green: #21c354;--amber: #faca15;--red: #ff4b4b;--blue: #1d4ed8;--font: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;--mono: "SF Mono", "Fira Code", monospace}*{box-sizing:border-box;margin:0;padding:0}body{background:var(--bg);color:var(--text);font-family:var(--font);line-height:1.6}h1{font-size:2rem;margin-bottom:4px}h2{font-size:1.4rem;margin:24px 0 12px;border-bottom:1px solid var(--border);padding-bottom:8px}h3{font-size:1.1rem;margin:16px 0 8px}.caption{color:var(--text2);font-size:.9rem;margin-bottom:20px}code{background:var(--surface2);padding:2px 6px;border-radius:4px;font-family:var(--mono);font-size:.85rem}pre{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px;overflow-x:auto;font-family:var(--mono);font-size:.85rem;margin:12px 0}.hidden{display:none}.divider{border:none;border-top:1px solid var(--border);margin:24px 0}.container{max-width:1200px;margin:0 auto;padding:20px}.tabs{display:grid;grid-template-columns:repeat(6,1fr);gap:0;margin-bottom:24px}.tab{padding:10px 8px;cursor:pointer;color:var(--text2);border-bottom:2px solid var(--border);white-space:nowrap;font-size:.88rem;transition:all .2s;text-align:center}.tab:hover{color:var(--text)}.tab.active{color:var(--primary);border-bottom-color:var(--primary);font-weight:600}.tab{position:relative}.tab .check{display:none;margin-left:6px;color:var(--green);font-size:.85rem}.tab.done .check{display:inline-block;animation:pop .4s ease}.tab.locked{opacity:.4;pointer-events:none}.tab .spinner-sm{display:none;width:12px;height:12px;border:2px solid var(--border);border-top:2px solid var(--primary);border-radius:50%;animation:spin .6s linear infinite;margin-left:6px;vertical-align:middle}.panel{display:none}.panel.active{display:block}.row{display:flex;gap:20px;flex-wrap:wrap}.col{flex:1;min-width:280px}.col-3{flex:1;min-width:200px}@media(max-width:768px){.row{flex-direction:column}.col,.col-3{min-width:100%}}.btn{padding:10px 24px;border:none;border-radius:6px;cursor:pointer;font-size:.95rem;font-weight:600;transition:all .2s;display:inline-flex;align-items:center;gap:8px}.btn-primary{background:var(--primary);color:#fff}.btn-primary:hover{background:var(--primary-hover)}.btn-secondary{background:var(--surface2);color:var(--text);border:1px solid var(--border)}.btn-secondary:hover{background:var(--border)}.btn:disabled{opacity:.5;cursor:not-allowed}.alert{padding:12px 16px;border-radius:6px;margin:12px 0;font-size:.9rem}.alert-info{background:#1e3a5f;border:1px solid #2563eb;color:#93c5fd}.alert-success{background:#14532d;border:1px solid var(--green);color:#86efac}.alert-warn{background:#713f12;border:1px solid var(--amber);color:#fde68a}.alert-error{background:#450a0a;border:1px solid var(--red);color:#fca5a5}.badge{display:inline-block;padding:2px 10px;border-radius:12px;font-size:.8rem;font-weight:600}.badge-green{background:#14532d;color:#86efac}.badge-amber{background:#713f12;color:#fde68a}.badge-red{background:#450a0a;color:#fca5a5}.conf-badge{padding:1px 6px;border-radius:3px;font-size:.65rem;font-weight:700}.conf-high{background:#21c35433;color:#86efac}.conf-med{background:#eab30833;color:#fde68a}.conf-low{background:#ef444433;color:#fca5a5}.conf-none{background:#80849526;color:#9ca3af}.conf-dot{display:inline-block;width:8px;height:8px;border-radius:50%;margin-right:4px;vertical-align:middle}.conf-dot.high{background:var(--green)}.conf-dot.med{background:var(--amber)}.conf-dot.low{background:var(--red)}.conf-dot.none{background:var(--text2)}.progress-bar{height:8px;background:var(--surface2);border-radius:4px;overflow:hidden;margin:6px 0}.progress-fill{height:100%;border-radius:4px;transition:width .3s}.progress-fill.green{background:var(--green)}.progress-fill.amber{background:var(--amber)}.progress-fill.red{background:var(--red)}.file-upload{border:2px dashed var(--border);border-radius:8px;padding:40px 20px;text-align:center;cursor:pointer;transition:border-color .2s}.file-upload:hover{border-color:var(--primary)}.file-upload input{display:none}.sources-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(140px,1fr));gap:8px;margin:16px 0}.source-badge{background:var(--surface);border:1px solid var(--border);border-radius:6px;padding:8px 12px;text-align:center;border-top:3px solid var(--border);transition:border-color .2s}.source-badge:hover{border-color:var(--primary)}.source-badge .src-name{font-weight:600;font-size:.85rem}.source-badge .src-type{color:var(--text2);font-size:.7rem;text-transform:uppercase;letter-spacing:.3px}.detected-source{display:inline-flex;align-items:center;gap:8px;padding:6px 14px;background:var(--surface);border:1px solid var(--green);border-radius:6px;margin:8px 0;font-size:.9rem}.detected-source .dot{width:8px;height:8px;border-radius:50%;background:var(--green)}.gap-card{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:10px 14px;margin:6px 0;border-left:3px solid var(--red)}.gap-card .gap-title{font-weight:600;font-size:.85rem;margin-bottom:2px}.gap-card .gap-meta{color:var(--text2);font-size:.75rem}.gap-card .gap-rec{color:var(--amber);font-size:.8rem;margin-top:4px}.sys-card{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:12px;margin:6px 0}.sys-card-header{display:flex;align-items:center;gap:8px;margin-bottom:8px;flex-wrap:wrap}.sys-card-header .sys-name{font-weight:700;font-size:.95rem}.sys-card-header .sys-badge{font-size:.7rem;padding:2px 8px;border-radius:10px}.sys-detail-row{display:flex;gap:16px;flex-wrap:wrap;font-size:.82rem;padding:3px 0}.sys-detail-row .sys-label{color:var(--text2);min-width:90px}.sys-detail-row .sys-value{color:var(--text);word-break:break-all}.el-highlight{background:#3b82f626;color:#60a5fa;padding:1px 3px;border-radius:2px;font-family:var(--mono);font-size:.75rem}.effort-bar{display:flex;height:28px;border-radius:6px;overflow:hidden;margin:8px 0}.effort-bar .effort-seg{display:flex;align-items:center;justify-content:center;font-size:.7rem;font-weight:700;color:#fff;transition:width .4s ease}.node-upstream{box-shadow:0 0 0 3px #3b82f699!important}.node-downstream{box-shadow:0 0 0 3px #21c35499!important}.node-selected{box-shadow:0 0 0 3px #ff4b4bcc!important}.conform-check{padding:4px 0;font-size:.82rem}.conform-check .check-icon{margin-right:6px}.resource-dot{display:inline-block;width:8px;height:8px;border-radius:50%;margin-right:4px;vertical-align:middle}.resource-dot.used{background:#21c354}.resource-dot.orphaned{background:#faca15}.resource-dot.missing{background:#ff4b4b}.match-badge{display:inline-block;padding:1px 8px;border-radius:3px;font-size:.7rem;font-weight:700}.match-exact{background:#21c35433;color:#86efac}.match-functional{background:#eab30833;color:#fde68a}.match-gap{background:#ef444433;color:#fca5a5}.sample-card{display:flex;gap:10px;align-items:center;padding:10px 14px;border:1px solid var(--border);border-radius:8px;cursor:pointer;transition:all .15s;background:var(--bg1)}.sample-card:hover{border-color:var(--accent);background:#ff6b350d;transform:translateY(-1px);box-shadow:0 2px 8px #0000001a}.sample-icon{font-size:1.5rem;flex-shrink:0;width:32px;text-align:center}.sample-info{display:flex;flex-direction:column;gap:1px;min-width:0}.sample-info strong{font-size:.85rem;color:var(--text1)}.sample-info span{font-size:.75rem;color:var(--text2)}.sample-tags{font-size:.7rem!important;color:var(--accent)!important;opacity:.8}.manifest-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(120px,1fr));gap:8px;margin:8px 0}.manifest-stat{text-align:center;padding:8px;border-radius:8px;background:var(--surface2)}.manifest-stat .num{font-size:1.3rem;font-weight:700;color:var(--accent)}.manifest-stat .lbl{font-size:.72rem;color:var(--text2)}.venv-tree{font-family:var(--mono);font-size:.82rem;line-height:1.7;padding:8px 0}.venv-tree .dir{color:var(--text2);font-weight:600}.venv-tree .file{color:var(--accent)}.venv-tree .venv-badge{display:inline-block;background:var(--surface2);border-radius:4px;padding:1px 6px;font-size:.72rem;margin-left:6px}.venv-summary{display:flex;gap:16px;flex-wrap:wrap;margin:12px 0}.venv-stat{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:12px 16px;text-align:center;flex:1;min-width:100px}.venv-stat .venv-stat-num{font-size:1.5rem;font-weight:700;color:var(--primary)}.venv-stat .venv-stat-label{font-size:.75rem;color:var(--text2);margin-top:4px}.format-badge{display:inline-block;padding:2px 10px;border-radius:12px;font-size:.75rem;font-weight:700;text-transform:uppercase;letter-spacing:.5px;margin-left:8px;vertical-align:middle}.format-badge.fmt-xml{background:#3b82f633;color:#60a5fa}.format-badge.fmt-json{background:#eab30833;color:#fde68a}.format-badge.fmt-sql{background:#a855f733;color:#c084fc}.format-badge.fmt-archive{background:#21c35433;color:#86efac}.format-badge.fmt-document{background:#fb923c33;color:#fdba74}.format-badge.fmt-unknown{background:#80849526;color:#9ca3af}.file-size-warn{display:inline-block;padding:2px 8px;border-radius:4px;font-size:.75rem;font-weight:600;background:#ef444426;color:#fca5a5;margin-left:8px}.file-size-info{display:inline-block;padding:2px 8px;border-radius:4px;font-size:.75rem;color:var(--text2);margin-left:8px}.flow-summary-card{background:var(--surface);border:1px solid var(--border);border-radius:10px;padding:16px;margin:12px 0}.flow-summary-card .fsc-header{display:flex;align-items:center;justify-content:space-between;margin-bottom:12px;flex-wrap:wrap;gap:8px}.flow-summary-card .fsc-title{font-weight:700;font-size:1rem}.flow-summary-card .fsc-version{font-size:.78rem;color:var(--text2);background:var(--surface2);padding:2px 8px;border-radius:4px}.flow-summary-card .fsc-roles{display:flex;gap:6px;flex-wrap:wrap;margin:8px 0}.flow-summary-card .fsc-role-bar{display:flex;align-items:center;gap:4px;font-size:.78rem;min-width:90px}.flow-summary-card .fsc-role-fill{height:6px;border-radius:3px;min-width:2px;transition:width .3s}.flow-summary-card .fsc-chips{display:flex;gap:6px;flex-wrap:wrap;margin-top:8px}.flow-summary-card .fsc-chip{display:inline-block;padding:3px 10px;border-radius:14px;font-size:.75rem;font-weight:600;background:var(--surface2);border:1px solid var(--border)}.parse-warning-group{margin:8px 0;border-radius:6px;overflow:hidden}.parse-warning-header{display:flex;align-items:center;gap:8px;padding:8px 12px;cursor:pointer;font-size:.85rem;font-weight:600;user-select:none}.parse-warning-header.severity-critical{background:#ef444426;color:#fca5a5;border-left:3px solid var(--red)}.parse-warning-header.severity-warning{background:#eab30826;color:#fde68a;border-left:3px solid var(--amber)}.parse-warning-header.severity-info{background:#3b82f626;color:#93c5fd;border-left:3px solid #2563eb}.parse-warning-header .pw-count{font-size:.72rem;opacity:.7;margin-left:auto}.parse-warning-body{padding:0;max-height:0;overflow:hidden;transition:max-height .3s ease}.parse-warning-group.open .parse-warning-body{max-height:2000px}.parse-warning-item{padding:4px 12px 4px 24px;font-size:.82rem;color:var(--text);border-bottom:1px solid rgba(128,132,149,.1)}.parse-warning-item:last-child{border-bottom:none}.parse-error-detail{background:var(--surface);border:1px solid var(--red);border-radius:8px;padding:12px;margin:8px 0}.parse-error-detail .error-line{font-family:var(--mono);font-size:.8rem;color:#fca5a5;margin:4px 0}.parse-error-detail .error-suggestion{font-size:.82rem;color:var(--amber);margin-top:8px;padding-top:8px;border-top:1px solid var(--border)}.info-tooltip-trigger{display:inline-block;cursor:help;position:relative;margin-left:4px;width:14px;height:14px;border-radius:50%;background:var(--surface2);text-align:center;line-height:14px;font-size:.65rem;font-weight:700;color:var(--text2);vertical-align:middle}.info-tooltip-trigger:hover .info-tooltip-content{display:block}.info-tooltip-content{display:none;position:absolute;bottom:calc(100% + 6px);left:50%;transform:translate(-50%);background:var(--surface);border:1px solid var(--border);border-radius:6px;padding:8px 12px;font-size:.75rem;font-weight:400;white-space:nowrap;z-index:100;box-shadow:0 4px 12px #0000004d;color:var(--text);min-width:180px;text-align:left}.info-tooltip-content:after{content:"";position:absolute;top:100%;left:50%;transform:translate(-50%);border:5px solid transparent;border-top-color:var(--border)}.risk-badge{display:inline-block;padding:1px 8px;border-radius:3px;font-size:.7rem;font-weight:700}.risk-low{background:#21c35426;color:#86efac}.risk-med{background:#eab30826;color:#fde68a}.risk-high{background:#ef444426;color:#fca5a5}.risk-critical{background:#ef44444d;color:#ff8a8a}.phase-badge{display:inline-block;padding:1px 8px;border-radius:3px;font-size:.7rem;font-weight:600}.phase-1{background:#3b82f626;color:#60a5fa}.phase-2{background:#a855f726;color:#c084fc}.phase-3{background:#21c35426;color:#86efac}.pkg-warn{background:#eab3081a;border:1px solid rgba(234,179,8,.3);border-radius:6px;padding:8px 12px;margin:8px 0;font-size:.82rem;color:#fde68a}.pkg-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(250px,1fr));gap:8px;margin:8px 0}.pkg-card{background:var(--surface);border:1px solid var(--border);border-radius:6px;padding:8px 12px;font-size:.8rem}.pkg-card .pkg-name{font-weight:700;font-family:var(--mono);font-size:.82rem}.pkg-card .pkg-procs{color:var(--text2);font-size:.72rem;margin-top:4px}.pkg-card .pkg-dbx{color:var(--green);font-size:.72rem;margin-top:2px}.metrics{display:flex;gap:16px;flex-wrap:wrap;margin:16px 0}.metric{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px 20px;min-width:140px;flex:1}.metric .label{font-size:.8rem;color:var(--text2);text-transform:uppercase;letter-spacing:.5px}.metric .value{font-size:1.8rem;font-weight:700;margin-top:4px}.metric .delta{font-size:.85rem;color:var(--red)}.score-big{font-size:3rem;font-weight:800;text-align:center;padding:20px}table{width:100%;border-collapse:collapse;margin:12px 0;font-size:.85rem}th{text-align:left;padding:8px 12px;background:var(--surface2);color:var(--text2);font-weight:600;text-transform:uppercase;font-size:.75rem;letter-spacing:.5px;border-bottom:2px solid var(--border)}td{padding:8px 12px;border-bottom:1px solid var(--border)}tr:hover td{background:var(--surface)}.table-scroll{overflow-x:auto}.mapping-table{width:100%;border-collapse:collapse;font-size:.78rem;margin:12px 0}.mapping-table th{text-align:left;padding:6px 10px;border-bottom:2px solid var(--border);font-size:.7rem;text-transform:uppercase;color:var(--text2)}.mapping-table td{padding:5px 10px;border-bottom:1px solid var(--border)}.mapping-table tr.unmapped{opacity:.5}.comparison-detail{margin-top:16px}.comparison-detail summary{cursor:pointer;font-weight:600;font-size:.95rem;padding:10px 0;color:var(--text)}.comparison-detail summary:hover{color:var(--primary)}.comparison-detail table{width:100%;font-size:.8rem}.comparison-detail td,.comparison-detail th{padding:6px 8px}.comparison-detail tr:hover{background:var(--surface2)}.ops-log{max-height:400px;overflow-y:auto;font-family:var(--mono);font-size:.78rem}.ops-log-row{display:grid;grid-template-columns:50px 180px 100px 1fr 80px;gap:4px;padding:3px 0;border-bottom:1px solid var(--border);align-items:center}.ops-log-row:hover{background:#ff6b350d}.ops-log-row.header{font-weight:700;color:var(--text2);font-size:.72rem;text-transform:uppercase;border-bottom:2px solid var(--border)}.ops-action{font-weight:600;font-size:.75rem}.ops-action.file{color:#4285f4}.ops-action.sql{color:#21c354}.ops-action.token{color:#faca15}.ops-action.signal{color:#ff6d70}.ops-action.counter{color:#8b5cf6}.ops-action.queue{color:#29b5e8}.action-log{max-height:300px;overflow-y:auto;border:1px solid var(--border);border-radius:6px;margin:8px 0}.action-log-entry{display:grid;grid-template-columns:100px 1fr 1fr;gap:8px;padding:6px 10px;border-bottom:1px solid var(--border);font-size:.78rem;align-items:start}.action-log-entry:last-child{border-bottom:none}.action-log-entry .action-type{font-weight:600;font-size:.75rem;text-transform:uppercase}.action-log-entry .action-type.file-op{color:#21c354}.action-log-entry .action-type.sql-op{color:#6366f1}.action-log-entry .action-type.token-op{color:#eab308}.action-log-entry .action-type.signal-op{color:#f97316}.action-log-entry .action-type.queue-op{color:#06b6d4}.expander{border:1px solid var(--border);border-radius:8px;margin:12px 0;overflow:hidden}.expander-header{padding:12px 16px;cursor:pointer;display:flex;justify-content:space-between;align-items:center;background:var(--surface);font-weight:500}.expander-header:hover{background:var(--surface2)}.expander-body{padding:16px;display:none;border-top:1px solid var(--border)}.expander.open .expander-body{display:block}.expander-arrow{transition:transform .2s}.expander.open .expander-arrow{transform:rotate(90deg)}.tier-diagram{position:relative;overflow:auto;border:1px solid var(--border);border-radius:8px;background:var(--bg);min-height:200px;margin:16px 0}.tier-diagram svg.tier-svg{position:absolute;top:0;left:0;width:100%;height:100%;pointer-events:none;z-index:1}.tier-band{padding:16px 20px;border-bottom:1px solid var(--border);position:relative;z-index:2}.tier-band:last-child{border-bottom:none}.tier-band-label{font-size:.7rem;text-transform:uppercase;letter-spacing:1px;font-weight:700;margin-bottom:10px;opacity:.8}.tier-nodes{display:flex;flex-wrap:wrap;gap:12px;justify-content:center}.tier-node{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:10px 16px;min-width:120px;max-width:200px;text-align:center;cursor:pointer;transition:all .2s;position:relative;z-index:3}.tier-node:hover{border-color:var(--primary);transform:translateY(-2px);box-shadow:0 4px 12px #0000004d}.tier-node.selected{border-color:var(--primary);box-shadow:0 0 0 2px #ff4b4b4d}.tier-node.highlighted{border-color:var(--green)}.tier-node.dimmed{opacity:.3}.tier-node .node-name{font-weight:600;font-size:.85rem;word-break:break-all}.tier-node .node-meta{font-size:.7rem;color:var(--text2);margin-top:4px}.tier-node .node-badge{position:absolute;top:-6px;right:-6px;background:var(--primary);color:#fff;border-radius:10px;padding:1px 6px;font-size:.65rem;font-weight:700}.tier-node .node-badge.green{background:var(--green)}.tier-node .node-badge.amber{background:var(--amber);color:#000}.tier-node .node-badge.red{background:var(--red)}.tier-node .node-seq{display:inline-flex;align-items:center;justify-content:center;width:22px;height:22px;border-radius:50%;background:var(--primary);color:#fff;font-size:.7rem;font-weight:700;margin-bottom:4px}.tier-node .node-stats{display:flex;gap:4px;justify-content:center;margin-top:6px;flex-wrap:wrap}.tier-node .node-stats .ns{padding:1px 5px;border-radius:3px;font-size:.6rem;font-weight:700}.ns-tx{background:#3b82f6;color:#fff}.ns-ext{background:#21c354;color:#fff}.ns-lkp{background:#eab308;color:#000}.tier-node.table-output{background:var(--surface2);border-style:dashed;min-width:100px;max-width:180px;padding:8px 12px}.tier-node.table-output .node-name{font-size:.75rem}.tier-node.table-output .node-class{font-size:.6rem;text-transform:uppercase;letter-spacing:.5px;margin-top:2px}.tier-node.conflict-gate{border-color:var(--red);background:#1a0a0a;min-width:140px}.tier-node.conflict-gate .node-name{color:var(--red)}.tier-diagram-wrapper{display:flex;gap:0}.tier-diagram-main{flex:1;min-width:0}.tier-density-sidebar{width:220px;flex-shrink:0;border-left:1px solid var(--border);background:var(--surface);padding:12px;overflow-y:auto;max-height:800px;font-size:.7rem}.tier-density-sidebar h4{font-size:.75rem;margin:0 0 8px;color:var(--text2);text-transform:uppercase;letter-spacing:.5px}.density-row{display:flex;align-items:center;gap:6px;margin:3px 0}.density-bar{height:6px;border-radius:3px;flex-shrink:0;min-width:2px}.density-label{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;color:var(--text2);max-width:120px}@media(max-width:900px){.tier-density-sidebar{display:none}.tier-diagram-wrapper{flex-direction:column}}.node-detail{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px;margin-top:12px;font-size:.85rem}.node-detail h4{margin:0 0 8px}.diagram-legend{display:flex;gap:16px;flex-wrap:wrap;margin:8px 0;font-size:.75rem;color:var(--text2)}.diagram-legend span{display:flex;align-items:center;gap:4px}.diagram-legend .leg-line{width:20px;height:2px;border-radius:1px}.tier-node.in-cycle{border-color:#ef4444;animation:cyclePulse 2s ease-in-out infinite}.cycle-badge{position:absolute;top:-6px;left:-6px;background:#ef4444;color:#fff;border-radius:50%;width:18px;height:18px;font-size:.6rem;display:flex;align-items:center;justify-content:center;z-index:4}.tier-node .expand-indicator{font-size:.6rem;color:var(--text2);margin-top:4px}.tier-node.expanded{border-color:var(--primary);box-shadow:0 0 0 2px #ff4b4b33}.tier-sub-band{padding:8px 20px 12px 40px;border-bottom:1px solid var(--border);background:#ffffff05;position:relative;z-index:2}.tier-sub-band .tier-band-label{font-size:.6rem;opacity:.6;margin-bottom:6px}.tier-sub-band .tier-nodes{gap:8px}.tier-sub-band .tier-node{min-width:100px;max-width:160px;padding:6px 10px}.tier-sub-band .tier-node .node-name{font-size:.75rem}.tier-node.path-selected{border-color:#faca15;box-shadow:0 0 0 3px #faca1566;z-index:10}.tier-node.path-selected:before{content:"âœ“";position:absolute;top:-8px;left:-8px;width:18px;height:18px;border-radius:50%;background:#faca15;color:#000;font-size:.65rem;font-weight:700;display:flex;align-items:center;justify-content:center;z-index:5}.tier-node.path-member{border-color:#faca15;background:#faca1514}.tier-node.path-dimmed{opacity:.12}.path-trace-toast{position:fixed;bottom:24px;left:50%;transform:translate(-50%);background:var(--surface2);border:1px solid var(--border);border-radius:8px;padding:10px 20px;font-size:.85rem;color:var(--text);z-index:1000;display:none;align-items:center;gap:12px;box-shadow:0 4px 24px #00000080}.path-trace-toast .toast-hint{color:var(--text2);font-size:.75rem}.path-trace-toast .toast-clear{cursor:pointer;color:var(--primary);font-weight:600;font-size:.8rem}.node-stage-label{font-size:.55rem;text-transform:uppercase;letter-spacing:.5px;font-weight:700;border:1px solid;border-radius:3px;padding:1px 4px;display:inline-block;margin-bottom:4px;opacity:.85}.node-subcategories{display:flex;gap:3px;flex-wrap:wrap;justify-content:center;margin-top:4px}.ns-sub{background:#6366f126;color:#a5b4fc;font-size:.55rem;padding:1px 4px;border-radius:3px}.ns-sub-more{font-size:.55rem;color:var(--text2);padding:1px 3px}.node-attr-indicators{display:flex;gap:4px;justify-content:center;margin-top:4px}.ns-attr-create{background:#06b6d426;color:#22d3ee;font-size:.55rem}.ns-attr-read{background:#f59e0b26;color:#fbbf24;font-size:.55rem}.attr-chip{font-size:.65rem;padding:1px 5px;border-radius:3px;cursor:pointer;transition:background .15s}.attr-chip:hover{opacity:.8}.attr-chip-create{background:#06b6d426;color:#22d3ee}.attr-chip-read{background:#f59e0b26;color:#fbbf24}.attr-trace-banner{display:none;align-items:center;gap:8px;padding:8px 16px;background:#06b6d414;border-bottom:1px solid rgba(6,182,212,.3);font-size:.8rem;z-index:10;position:relative}.attr-trace-banner code{background:#ffffff1a;padding:1px 4px;border-radius:3px}.attr-badge{padding:2px 8px;border-radius:10px;font-size:.7rem;font-weight:600}.attr-badge.attr-create{background:#06b6d433;color:#22d3ee}.attr-badge.attr-read{background:#f59e0b33;color:#fbbf24}.attr-badge.attr-modify{background:#a855f733;color:#c084fc}.tier-node.attr-creator{border-color:#06b6d4!important;box-shadow:0 0 0 2px #06b6d44d}.tier-node.attr-reader{border-color:#f59e0b!important;box-shadow:0 0 0 2px #f59e0b4d}.tier-node.attr-modifier{border-color:#a855f7!important;box-shadow:0 0 0 2px #a855f74d}.tier-node.attr-dimmed{opacity:.15!important}.tier-node.attr-involved{border-color:#06b6d4;opacity:.7}.filter-action-select{background:var(--surface);color:var(--text);border:1px solid var(--border);border-radius:4px;padding:3px 6px;font-size:.75rem;cursor:pointer}.filter-action-select:focus{outline:none;border-color:var(--primary)}.severity-critical{background:#dc2626;color:#fff}.severity-high{background:#ea580c;color:#fff}.severity-medium{background:#d97706;color:#000}.severity-low{background:#65a30d;color:#fff}.severity-badge{padding:1px 6px;border-radius:3px;font-size:.6rem;font-weight:700;text-transform:uppercase}.notebook-preview{background:var(--surface);border:1px solid var(--border);border-radius:8px;max-height:600px;overflow-y:auto;margin:12px 0}.notebook-cell{padding:10px 16px;border-bottom:1px solid var(--border);font-family:SF Mono,Monaco,Consolas,monospace;font-size:.78rem;line-height:1.5;white-space:pre-wrap;word-break:break-word}.notebook-cell:last-child{border-bottom:none}.notebook-cell.cell-md{background:#3b82f60f;color:#93c5fd}.notebook-cell.cell-sql{background:#a855f70f}.notebook-cell.cell-code{background:var(--surface)}.notebook-cell .cell-label{display:inline-block;padding:1px 8px;border-radius:3px;font-size:.65rem;font-weight:700;margin-bottom:4px;text-transform:uppercase;letter-spacing:.5px}.cell-label.lb-source{background:#1d3557;color:#93c5fd}.cell-label.lb-transform{background:#2d1b4e;color:#c4b5fd}.cell-label.lb-sink{background:#14532d;color:#86efac}.cell-label.lb-route{background:#422006;color:#fde68a}.cell-label.lb-process{background:#1e1b4b;color:#a5b4fc}.cell-label.lb-utility{background:#1f2937;color:#9ca3af}.cell-label.lb-config{background:#0c4a6e;color:#7dd3fc}.cell-label.lb-manual{background:#450a0a;color:#fca5a5}textarea,input[type=text],input[type=number],select{width:100%;padding:10px 14px;background:var(--surface);border:1px solid var(--border);border-radius:6px;color:var(--text);font-family:var(--mono);font-size:.9rem;resize:vertical}textarea:focus,input:focus{outline:none;border-color:var(--primary)}textarea{min-height:200px}label{display:block;font-size:.85rem;color:var(--text2);margin-bottom:6px;font-weight:500}.coverage-ring{width:120px;height:120px;border-radius:50%;position:relative;display:inline-flex;align-items:center;justify-content:center}.coverage-ring .ring-text{font-size:1.6rem;font-weight:800;z-index:1}.comparison-donuts{display:flex;gap:32px;justify-content:center;flex-wrap:wrap;margin:24px 0}.donut-chart{text-align:center;flex:0 0 160px}.donut-chart svg{filter:drop-shadow(0 2px 8px rgba(0,0,0,.3))}.donut-label{font-size:.85rem;font-weight:600;margin-top:8px;color:var(--text2);text-transform:uppercase;letter-spacing:.5px}.donut-sub{font-size:.75rem;color:var(--text2);margin-top:2px}.val-section{margin:16px 0;padding:16px;background:var(--surface2,#1e2030);border-radius:8px;border-left:4px solid var(--primary)}.val-section h4{margin:0 0 10px;color:var(--primary)}.val-score-ring{display:inline-flex;align-items:center;gap:8px;margin:4px 8px 4px 0}.val-score-ring svg{vertical-align:middle}.val-gap{background:var(--red)11;border:1px solid var(--red)44;border-radius:6px;padding:8px 12px;margin:4px 0;font-size:.85rem}.val-gap .gap-label{color:var(--red);font-weight:600}.val-ok{background:var(--green)11;border:1px solid var(--green)44;border-radius:6px;padding:8px 12px;margin:4px 0;font-size:.85rem}.val-warn{background:var(--amber)11;border:1px solid var(--amber)44;border-radius:6px;padding:8px 12px;margin:4px 0;font-size:.85rem}.val-matrix{display:grid;grid-template-columns:1fr 1fr;gap:12px}@media(max-width:768px){.val-matrix{grid-template-columns:1fr}}.val-item{display:flex;align-items:flex-start;gap:8px;padding:6px 0;border-bottom:1px solid var(--border);font-size:.85rem}.val-item:last-child{border-bottom:none}.val-dot{width:10px;height:10px;border-radius:50%;flex-shrink:0;margin-top:4px}.val-accel-card{background:var(--primary)11;border:1px solid var(--primary)44;border-radius:8px;padding:12px;margin:6px 0}.val-accel-card h5{margin:0 0 6px;color:var(--primary);font-size:.9rem}.val-accel-card pre{font-size:.78rem;margin:6px 0 0;padding:8px;background:var(--bg);border-radius:4px;overflow-x:auto}.sim-progress{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px 20px;margin:16px 0}.sim-progress-bar{height:10px;background:var(--surface2);border-radius:5px;overflow:hidden;margin:8px 0}.sim-progress-fill{height:100%;border-radius:5px;background:linear-gradient(90deg,var(--primary),#ff8a4b);transition:width .4s ease}.sim-status{font-size:.85rem;color:var(--text2);display:flex;justify-content:space-between;align-items:center}.sim-status .engine-label{font-weight:600}.sim-donuts{display:flex;gap:32px;justify-content:center;flex-wrap:wrap;margin:24px 0}.sim-split{display:grid;grid-template-columns:1fr 1fr;gap:0;margin:8px 0;border:1px solid var(--border);border-radius:6px;overflow:hidden}.sim-split-header{background:var(--surface2);padding:8px 12px;font-weight:600;font-size:.8rem;text-transform:uppercase;letter-spacing:.5px;text-align:center}.sim-split-header.nifi-side{border-right:1px solid var(--border);color:#728e9b}.sim-split-header.dbx-side{color:var(--primary)}.sim-split-cell{padding:8px 12px;font-size:.78rem;font-family:var(--mono);background:var(--surface);border-top:1px solid var(--border);overflow-x:auto;max-height:200px;overflow-y:auto}.sim-split-cell.nifi-side{border-right:1px solid var(--border)}.state-diff{display:grid;grid-template-columns:1fr 1fr;gap:16px;margin:12px 0}.state-diff-panel{background:var(--bg2);border-radius:8px;padding:12px}.state-diff-panel h4{margin:0 0 8px;font-size:.85rem;color:var(--text2)}.state-diff-panel pre{font-size:.72rem;max-height:200px;overflow-y:auto}.filter-toolbar{display:flex;gap:8px;flex-wrap:wrap;padding:12px;background:var(--surface);border:1px solid var(--border);border-radius:8px;margin-bottom:12px;align-items:center}.filter-toolbar .filter-group{display:flex;gap:4px;align-items:center}.filter-toolbar label{font-size:.75rem;color:var(--text2);margin-right:4px;white-space:nowrap}.filter-btn{padding:4px 10px;font-size:.75rem;border:1px solid var(--border);border-radius:4px;background:var(--surface2);color:var(--text2);cursor:pointer;transition:all .15s}.filter-btn:hover{border-color:var(--primary);color:var(--text)}.filter-btn.active{background:var(--primary);color:#fff;border-color:var(--primary)}.filter-search{padding:4px 8px;font-size:.75rem;border:1px solid var(--border);border-radius:4px;background:var(--bg);color:var(--text);width:160px}.filter-search::placeholder{color:var(--text2)}.density-row{cursor:pointer;padding:2px 4px;border-radius:4px;transition:background .15s}.density-row:hover{background:#ffffff0f}.density-row.filter-active{background:#faca1526;border:1px solid rgba(250,202,21,.4)}.density-row.filter-dimmed{opacity:.3}.sidebar-filter-hint{font-size:.6rem;color:var(--text2);margin-bottom:6px;font-style:italic}.sidebar-clear-btn{font-size:.65rem;color:var(--primary);cursor:pointer;margin-top:6px;display:none;text-align:center;padding:4px;border-radius:4px}.sidebar-clear-btn:hover{background:#ff4b4b1a}@keyframes pop{0%{transform:scale(0)}60%{transform:scale(1.3)}to{transform:scale(1)}}@keyframes spin{to{transform:rotate(360deg)}}@keyframes cyclePulse{0%,to{box-shadow:0 0 #ef444466}50%{box-shadow:0 0 0 6px #ef444400}}@keyframes simPulse{0%,to{opacity:1}50%{opacity:.5}}.sim-running{animation:simPulse 1s ease-in-out infinite}.error-panel{position:fixed;bottom:0;left:0;right:0;z-index:1000;background:var(--bg, #0e1117);border-top:2px solid var(--red, #EF4444);max-height:40vh;transition:max-height .3s}.error-panel.collapsed{max-height:36px;overflow:hidden}.error-panel-header{display:flex;align-items:center;gap:8px;padding:8px 16px;cursor:pointer;font-weight:600;font-size:.85rem}.error-panel-arrow{margin-left:auto;transition:transform .3s}.error-panel.collapsed .error-panel-arrow{transform:rotate(180deg)}.error-badge{background:var(--red, #EF4444);color:#fff;border-radius:10px;padding:1px 7px;font-size:.75rem;font-weight:700}.error-badge.hidden{display:none}.error-panel-body{overflow-y:auto;max-height:calc(40vh - 36px);padding:0 16px 12px}.error-panel-toolbar{display:flex;gap:6px;margin-bottom:8px}.error-list{font-size:.8rem}.error-item{padding:6px 8px;border-bottom:1px solid var(--surface2, #262730);display:flex;gap:8px;align-items:flex-start;cursor:pointer}.error-item:hover{background:var(--surface, #1a1d27)}.error-severity{flex-shrink:0}.error-phase{font-size:.7rem;padding:1px 5px;border-radius:3px;background:var(--surface2, #262730);color:var(--text2, #808495);flex-shrink:0}.error-message{flex:1}.error-timestamp{font-size:.7rem;color:var(--text2, #808495);flex-shrink:0}.error-fix{font-size:.75rem;color:var(--text2, #808495);margin-top:3px;font-style:italic}.error-stack{font-size:.7rem;color:var(--text2, #808495);margin-top:4px;white-space:pre-wrap;font-family:var(--mono, monospace);display:none}.error-item.expanded .error-stack{display:block}.btn-sm{font-size:.75rem;padding:3px 8px}.json-explorer{position:relative;background:var(--bg, #0d1117);border:1px solid var(--border);border-radius:8px;overflow:hidden}.json-explorer-toolbar{display:flex;align-items:center;gap:8px;padding:8px 12px;background:var(--surface, #161b22);border-bottom:1px solid var(--border)}.json-search-input{flex:1;padding:4px 10px;border:1px solid var(--border);border-radius:4px;background:var(--bg, #0d1117);color:var(--text);font-size:.8rem;outline:none}.json-search-input:focus{border-color:var(--primary)}.json-search-count{font-size:.75rem;color:var(--text2);white-space:nowrap}.json-tree-container{max-height:500px;overflow:auto;padding:12px;font-family:var(--mono, "Fira Code", monospace);font-size:.8rem;line-height:1.6}.json-toggle{cursor:pointer;user-select:none}.json-toggle .json-arrow{display:inline-block;transition:transform .15s;font-size:.65rem;margin-right:4px;color:var(--text2)}.json-toggle.open .json-arrow{transform:rotate(90deg)}.json-toggle .json-count{font-size:.7rem;color:var(--text2);margin-left:6px;font-style:italic}.json-toggle.open .json-count,.json-children.hidden,.json-close-bracket.hidden{display:none}.json-entry{padding:1px 0}.json-key{color:#79c0ff;cursor:pointer}.json-key:hover{text-decoration:underline}.json-string{color:#a5d6ff;cursor:pointer}.json-string:hover{opacity:.8}.json-number{color:#56d4dd;cursor:pointer}.json-boolean{color:#ff7b72;cursor:pointer}.json-null{color:#8b949e;font-style:italic;cursor:pointer}.json-bracket,.json-colon{color:#c9d1d9}.json-comma{color:#8b949e}.json-index{color:#8b949e;font-size:.75rem}.json-path-toast{position:absolute;bottom:8px;left:50%;transform:translate(-50%) translateY(10px);background:var(--primary);color:#fff;padding:6px 14px;border-radius:6px;font-size:.78rem;font-family:var(--mono, monospace);opacity:0;transition:opacity .2s,transform .2s;pointer-events:none;white-space:nowrap;max-width:90%;overflow:hidden;text-overflow:ellipsis}.json-path-toast.visible{opacity:1;transform:translate(-50%) translateY(0)}.json-highlight{background:#eab30840;border-radius:2px}.step-progress-bar{display:flex;align-items:center;gap:6px;padding:8px 16px;background:var(--surface, #161b22);border:1px solid var(--border);border-radius:0 0 8px 8px;margin-top:-1px;font-size:.78rem}.step-progress-label{color:var(--text2);white-space:nowrap;font-weight:600}.step-progress-track{flex:1;height:6px;background:var(--surface2, #21262d);border-radius:3px;overflow:hidden}.step-progress-fill{height:100%;background:var(--primary);border-radius:3px;transition:width .4s ease}.step-progress-pct{color:var(--primary);font-weight:700;min-width:36px;text-align:right}.tab .check{display:none;margin-left:4px;color:var(--green);font-weight:700}.tab .spinner-sm{display:none;margin-left:4px}.tab.done .check{display:inline}.tab.processing .spinner-sm{display:inline-block}.severity-badge{display:inline-block;padding:2px 8px;border-radius:4px;font-size:.7rem;font-weight:700;text-transform:uppercase;letter-spacing:.3px}.severity-critical{background:#7f1d1d;color:#fca5a5}.severity-high{background:#713f12;color:#fde68a}.severity-medium{background:#1e3a5f;color:#93c5fd}.severity-low{background:#14532d;color:#86efac}.info-tooltip{position:relative;display:inline-block;cursor:help;margin-left:4px}.info-tooltip .tooltip-icon{display:inline-flex;align-items:center;justify-content:center;width:16px;height:16px;border-radius:50%;background:var(--primary)33;color:var(--primary);font-size:.65rem;font-weight:700}.info-tooltip .tooltip-text{visibility:hidden;opacity:0;position:absolute;bottom:125%;left:50%;transform:translate(-50%);background:var(--surface);border:1px solid var(--border);color:var(--text);padding:8px 12px;border-radius:6px;font-size:.78rem;font-weight:400;white-space:normal;width:250px;z-index:100;transition:opacity .15s;box-shadow:0 4px 12px #0000004d;line-height:1.4}.info-tooltip:hover .tooltip-text{visibility:visible;opacity:1}.remediation-block{position:relative;background:var(--bg, #0d1117);border:1px solid var(--border);border-radius:6px;margin:6px 0;overflow:hidden}.remediation-header{display:flex;justify-content:space-between;align-items:center;padding:6px 10px;background:var(--surface, #161b22);border-bottom:1px solid var(--border);font-size:.78rem;color:var(--text2)}.remediation-copy-btn{padding:2px 8px;border:1px solid var(--border);border-radius:4px;background:transparent;color:var(--text2);cursor:pointer;font-size:.7rem;transition:all .15s}.remediation-copy-btn:hover{background:var(--primary)22;border-color:var(--primary);color:var(--primary)}.remediation-code{padding:8px 12px;font-family:var(--mono, monospace);font-size:.78rem;color:var(--text);white-space:pre-wrap;word-break:break-all}.roi-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:12px;margin:12px 0}.roi-card{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:14px;text-align:center}.roi-value{font-size:1.8rem;font-weight:800;margin-bottom:4px}.roi-label{font-size:.82rem;color:var(--text2)}.roi-detail{font-size:.75rem;color:var(--text2);margin-top:4px}.roadmap-timeline{position:relative;padding-left:24px;margin:12px 0}.roadmap-timeline:before{content:"";position:absolute;left:8px;top:0;bottom:0;width:2px;background:var(--border)}.roadmap-item{position:relative;padding:10px 14px;margin-bottom:8px;background:var(--surface);border:1px solid var(--border);border-radius:8px}.roadmap-item:before{content:"";position:absolute;left:-20px;top:14px;width:10px;height:10px;border-radius:50%;background:var(--primary);border:2px solid var(--bg, #0d1117)}.roadmap-phase{font-size:.72rem;font-weight:700;text-transform:uppercase;letter-spacing:.5px;color:var(--primary);margin-bottom:4px}.roadmap-title{font-weight:600;font-size:.9rem;margin-bottom:4px}.roadmap-effort{font-size:.78rem;color:var(--text2)}.format-selector{display:inline-flex;align-items:center;gap:4px}.format-selector select{padding:6px 10px;border:1px solid var(--border);border-radius:6px;background:var(--surface);color:var(--text);font-size:.85rem;cursor:pointer}.exec-summary{background:linear-gradient(135deg,var(--surface) 0%,var(--bg, #0d1117) 100%);border:1px solid var(--border);border-radius:12px;padding:20px;margin:16px 0}.exec-summary h4{margin:0 0 16px;font-size:1rem;color:var(--primary)}.exec-recommendation{display:inline-block;padding:6px 16px;border-radius:6px;font-weight:700;font-size:.95rem}.exec-proceed{background:#14532d;color:#86efac}.exec-caution{background:#713f12;color:#fde68a}.exec-planning{background:#450a0a;color:#fca5a5}.stacked-bar-container{margin:12px 0}.stacked-bar{display:flex;height:32px;border-radius:6px;overflow:hidden;margin:4px 0}.stacked-bar-seg{display:flex;align-items:center;justify-content:center;font-size:.7rem;font-weight:700;color:#fff;transition:width .4s ease;min-width:2px}.stacked-bar-legend{display:flex;flex-wrap:wrap;gap:12px;margin-top:6px;font-size:.78rem}.stacked-bar-legend-item{display:inline-flex;align-items:center;gap:4px}.stacked-bar-legend-dot{width:10px;height:10px;border-radius:2px;flex-shrink:0}</style>
</head>
<body>
<div class="container">
  <h1>NiFi Flow Analyzer</h1>
  <p class="caption">Upload a NiFi flow XML &mdash; analyze processors, assess migration readiness, convert to Databricks, and generate migration reports</p>

  <div class="tabs" id="tabs">
    <div class="tab active" data-tab="load">1. Load Flow<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="analyze">2. Analyze<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="assess">3. Assess<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="convert">4. Convert<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="report">5. Report<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="reportFinal">6. Final Report<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="validate">7. Validate<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="value">8. Value Analysis<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
  </div>

  <!-- STEP 1: LOAD NIFI FLOW -->
  <div class="panel active" id="panel-load">
    <h2>Step 1: Load NiFi Flow</h2>
    <div class="row">
      <div class="col">
        <h3>Upload NiFi XML</h3>
        <div class="file-upload" id="fileDropZone">
          <p>Drop a NiFi flow XML here or click to browse</p>
          <p style="color:var(--text2);font-size:0.85rem">Supports NiFi templates, flow definitions, and registry exports</p>
          <input type="file" id="fileInput" accept=".xml,.json">
        </div>
        <div id="fileName" class="alert alert-info hidden" style="margin-top:12px"></div>
      </div>
      <div class="col">
        <h3>Paste NiFi XML</h3>
        <textarea id="pasteInput" placeholder="&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;template&gt;
  &lt;snippet&gt;
    &lt;processors&gt;...&lt;/processors&gt;
  &lt;/snippet&gt;
&lt;/template&gt;"></textarea>
      </div>
    </div>
    <div style="margin-top:16px;display:flex;gap:8px;flex-wrap:wrap;align-items:center">
      <button class="btn btn-primary" id="parseBtn">Analyze Flow</button>
      <div id="parseProgress" style="display:none;flex:1;min-width:300px;align-items:center;gap:10px">
        <div style="flex:0 0 200px;height:8px;background:var(--surface2);border-radius:4px;overflow:hidden"><div id="parsePBar" style="height:100%;width:0%;background:var(--primary);border-radius:4px;transition:width 0.15s"></div></div>
        <span id="parsePPct" style="font-size:0.8rem;font-weight:700;color:var(--primary);min-width:36px">0%</span>
        <span id="parsePStatus" style="font-size:0.78rem;color:var(--text2);white-space:nowrap;overflow:hidden;text-overflow:ellipsis;max-width:500px"></span>
      </div>
    </div>
    <div style="margin-top:20px;border:1px solid var(--border);border-radius:12px;padding:16px;background:var(--bg2)">
      <h3 style="margin:0 0 4px 0;font-size:0.95rem">Sample NiFi Flows &mdash; Click to Load &amp; Analyze</h3>
      <p style="color:var(--text2);font-size:0.8rem;margin:0 0 12px 0">Pre-built NiFi flow samples that demonstrate the full analysis pipeline.</p>
      <div style="display:grid;grid-template-columns:repeat(auto-fill,minmax(220px,1fr));gap:8px">
        <div class="sample-card" data-sample-flow="etl">
          <div class="sample-icon">&#128640;</div>
          <div class="sample-info"><strong>ETL Pipeline</strong><span>NiFi XML &bull; 9 processors</span><span class="sample-tags">GetFile, SQL, Route, PutFile</span></div>
        </div>
        <div class="sample-card" data-sample-flow="streaming">
          <div class="sample-icon">&#9889;</div>
          <div class="sample-info"><strong>Streaming IoT</strong><span>NiFi XML &bull; 10 processors</span><span class="sample-tags">Kafka, JSON, Merge, HDFS</span></div>
        </div>
        <div class="sample-card" data-sample-flow="full">
          <div class="sample-icon">&#127981;</div>
          <div class="sample-info"><strong>Manufacturing Migration</strong><span>NiFi XML &bull; 17 processors</span><span class="sample-tags">ListFile, SFTP, Wait/Notify, SQL</span></div>
        </div>
      </div>
    </div>
    <div id="parseResults"></div>
  </div>

  <!-- STEP 2: ANALYZE -->
  <div class="panel" id="panel-analyze">
    <h2>Step 2: Flow Analysis</h2>
    <div id="analyzeNotReady" class="alert alert-info">Load a NiFi flow first (Step 1).</div>
    <div id="analyzeReady" class="hidden">
      <button class="btn btn-primary" id="analyzeBtn">Run Deep Analysis</button>
      <div id="analyzeResults"></div>
    </div>
  </div>

  <!-- STEP 3: ASSESS -->
  <div class="panel" id="panel-assess">
    <h2>Step 3: Migration Assessment</h2>
    <div id="assessNotReady" class="alert alert-info">Complete the flow analysis first (Step 2).</div>
    <div id="assessReady" class="hidden">
      <button class="btn btn-primary" id="assessBtn">Run Assessment</button>
      <div id="assessResults"></div>
    </div>
  </div>

  <!-- STEP 4: CONVERT -->
  <div class="panel" id="panel-convert">
    <h2>Step 4: NiFi &rarr; Databricks Notebook</h2>
    <div id="convertNotReady" class="alert alert-info">Complete the assessment first (Step 3).</div>
    <div id="convertReady" class="hidden">
      <p style="color:var(--text2);font-size:0.9rem;margin-bottom:12px">
        Reverse-engineer your NiFi flow into a Databricks Python notebook with PySpark equivalents,
        Unity Catalog definitions, and a Databricks workflow.
      </p>
      <div class="expander" id="dbxConfigExpander"><div class="expander-header" data-expander-toggle id="dbxConfigExpanderHeader"><span>Databricks Configuration</span><span class="expander-arrow">&#9654;</span></div><div class="expander-body">
        <p style="font-size:0.82rem;color:var(--text2);margin-bottom:12px">Configure these to generate a runnable notebook with resolved placeholders. Leave blank for generic templates.</p>
        <div class="row">
          <div class="col"><label>Unity Catalog</label><input type="text" id="cfgCatalog" placeholder="e.g. main"></div>
          <div class="col"><label>Schema</label><input type="text" id="cfgSchema" placeholder="e.g. nifi_migration"></div>
          <div class="col"><label>Secret Scope</label><input type="text" id="cfgScope" placeholder="e.g. migration_secrets"></div>
        </div>
        <div class="row" style="margin-top:8px">
          <div class="col"><label>Cloud Provider</label><select id="cfgCloud"><option value="azure">Azure</option><option value="aws">AWS</option><option value="gcp">GCP</option></select></div>
          <div class="col"><label>Compute Type</label><select id="cfgComputeType"><option value="cluster">All-Purpose Cluster</option><option value="serverless">Serverless Compute</option><option value="sql-warehouse">SQL Warehouse</option></select></div>
          <div class="col"><label>Runtime Version</label><select id="cfgRuntimeVersion"><option value="14.3">14.3 LTS</option><option value="15.4">15.4 LTS</option><option value="16.0">16.0</option></select></div>
        </div>
        <div class="row" style="margin-top:8px">
          <div class="col"><label>Node Type</label><select id="cfgNodeType"><option value="Standard_DS3_v2">Standard_DS3_v2 (4 vCPU, 14 GB)</option><option value="Standard_DS4_v2">Standard_DS4_v2 (8 vCPU, 28 GB)</option><option value="Standard_DS5_v2">Standard_DS5_v2 (16 vCPU, 56 GB)</option><option value="Standard_E4ds_v5">Standard_E4ds_v5 (4 vCPU, 32 GB, memory-opt)</option><option value="Standard_NC6s_v3">Standard_NC6s_v3 (6 vCPU, 112 GB, GPU)</option></select></div>
          <div class="col"><label>Workers</label><input type="number" id="cfgWorkers" value="2" min="1" max="100"></div>
          <div class="col"><label>Workspace Path</label><input type="text" id="cfgWorkspacePath" value="/Workspace/Migrations/NiFi"></div>
        </div>
        <div style="margin-top:8px;display:flex;gap:8px"><button class="btn" id="cfgSaveBtn">Save Configuration</button><button class="btn btn-secondary" id="cfgResetBtn">Reset to Defaults</button></div>
      </div></div>
      <div style="margin-top:12px"><button class="btn btn-primary" id="generateNotebookBtn">Generate Notebook</button></div>
      <div id="notebookResults"></div>
    </div>
  </div>

  <!-- STEP 5: MIGRATION REPORT -->
  <div class="panel" id="panel-report">
    <h2>Step 5: Migration Report</h2>
    <div id="reportNotReady" class="alert alert-info">Generate the notebook first (Step 4).</div>
    <div id="reportReady" class="hidden">
      <button class="btn btn-primary" id="reportBtn">Generate Report</button>
      <div id="reportResults"></div>
    </div>
  </div>

  <!-- STEP 6: FINAL REPORT -->
  <div class="panel" id="panel-reportFinal">
    <h2>Step 6: Final Report</h2>
    <div id="reportFinalNotReady" class="alert alert-info">Generate the migration report first (Step 5).</div>
    <div id="reportFinalReady" class="hidden">
      <p style="color:var(--text2);font-size:0.9rem;margin-bottom:12px">
        Comprehensive end-to-end analysis &mdash; every processor, every gap, every recommendation. Download as JSON.
      </p>
      <button class="btn btn-primary" id="finalReportBtn">Generate Final Report</button>
      <div id="reportFinalResults"></div>
    </div>
  </div>

  <!-- STEP 7: VALIDATE -->
  <div class="panel" id="panel-validate">
    <h2>Step 7: Notebook &harr; Flow Validation</h2>
    <div id="validateNotReady" class="alert alert-info">Generate the final report first (Step 6).</div>
    <div id="validateReady" class="hidden">
      <p style="color:var(--text2);font-size:0.9rem;margin-bottom:12px">
        Four-angle comparison between the generated PySpark notebook and the original NiFi flow.
        Identifies gaps, manual items, and feeds corrections back into the accelerator.
      </p>
      <button class="btn btn-primary" id="validateBtn">Run Validation</button>
      <div id="validateResults"></div>
    </div>
  </div>

  <!-- STEP 8: VALUE ANALYSIS -->
  <div class="panel" id="panel-value">
    <h2>Step 8: Workflow Value Analysis</h2>
    <div id="valueNotReady" class="alert alert-info">Complete validation first (Step 7).</div>
    <div id="valueReady" class="hidden">
      <p style="color:var(--text2);font-size:0.9rem;margin-bottom:12px">
        Holistic migration value assessment &mdash; what the flow does, how to build it better in Databricks,
        what steps can be dropped, and quantified migration ROI.
      </p>
      <button class="btn btn-primary" id="valueBtn">Run Value Analysis</button>
      <div id="valueResults"></div>
    </div>
  </div>
</div>
<div class="path-trace-toast" id="pathTraceToast">
  <span id="pathTraceText"></span>
  <span class="toast-hint">Click another node to extend path</span>
  <span class="toast-clear" id="pathTraceClear">Clear</span>
</div>

</body>
</html>
