<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>NiFi Flow Analyzer</title>
  <script type="module" crossorigin>(function(){const t=document.createElement("link").relList;if(t&&t.supports&&t.supports("modulepreload"))return;for(const a of document.querySelectorAll('link[rel="modulepreload"]'))s(a);new MutationObserver(a=>{for(const i of a)if(i.type==="childList")for(const c of i.addedNodes)c.tagName==="LINK"&&c.rel==="modulepreload"&&s(c)}).observe(document,{childList:!0,subtree:!0});function n(a){const i={};return a.integrity&&(i.integrity=a.integrity),a.referrerPolicy&&(i.referrerPolicy=a.referrerPolicy),a.crossOrigin==="use-credentials"?i.credentials="include":a.crossOrigin==="anonymous"?i.credentials="omit":i.credentials="same-origin",i}function s(a){if(a.ep)return;a.ep=!0;const i=n(a);fetch(a.href,i)}})();const zt="modulepreload",Wt=function(e,t){return new URL(e,t).href},et={},ht=function(t,n,s){let a=Promise.resolve();if(n&&n.length>0){let c=function(p){return Promise.all(p.map(d=>Promise.resolve(d).then(u=>({status:"fulfilled",value:u}),u=>({status:"rejected",reason:u}))))};const o=document.getElementsByTagName("link"),r=document.querySelector("meta[property=csp-nonce]"),l=r?.nonce||r?.getAttribute("nonce");a=c(n.map(p=>{if(p=Wt(p,s),p in et)return;et[p]=!0;const d=p.endsWith(".css"),u=d?'[rel="stylesheet"]':"";if(!!s)for(let _=o.length-1;_>=0;_--){const b=o[_];if(b.href===p&&(!d||b.rel==="stylesheet"))return}else if(document.querySelector(`link[href="${p}"]${u}`))return;const y=document.createElement("link");if(y.rel=d?"stylesheet":zt,d||(y.as="script"),y.crossOrigin="",y.href=p,l&&y.setAttribute("nonce",l),document.head.appendChild(y),d)return new Promise((_,b)=>{y.addEventListener("load",_),y.addEventListener("error",()=>b(new Error(`Unable to preload CSS for ${p}`)))})}))}function i(c){const o=new Event("vite:preloadError",{cancelable:!0});if(o.payload=c,window.dispatchEvent(o),!o.defaultPrevented)throw c}return a.then(c=>{for(const o of c||[])o.status==="rejected"&&i(o.reason);return t().catch(i)})},Ue=Object.freeze({parsed:null,analysis:null,assessment:null,notebook:null,migrationReport:null,finalReport:null,manifest:null,validation:null,valueAnalysis:null});let ae={...Ue};const we=new Set;function Qt(e={}){return ae={...Ue,...e},we.clear(),{getState:O,setState:K,subscribe:Jt,resetState:de}}function O(e){return e!==void 0?ae[e]:{...ae}}function K(e){if(!e||typeof e!="object")return;const t={...ae};Object.assign(ae,e),_t(t)}function Jt(e){if(typeof e!="function")throw new TypeError("subscribe() expects a function");return we.add(e),()=>we.delete(e)}function de(){const e={...ae};ae={...Ue},_t(e)}function _t(e){const t={...ae};for(const n of we)try{n(t,e)}catch(s){console.error("[state] subscriber threw:",s)}}class Kt{constructor(){this._handlers=new Map}on(t,n){return this._handlers.has(t)||this._handlers.set(t,new Set),this._handlers.get(t).add(n),()=>this.off(t,n)}once(t,n){const s=(...a)=>{this.off(t,s),n(...a)};return this.on(t,s)}off(t,n){const s=this._handlers.get(t);s&&(s.delete(n),s.size===0&&this._handlers.delete(t))}emit(t,...n){const s=this._handlers.get(t);if(s)for(const a of s)try{a(...n)}catch(i){console.error(`[EventBus] handler for "${t}" threw:`,i)}}clear(t){t?this._handlers.delete(t):this._handlers.clear()}}const q=new Kt,Pe=new Map;let ge=null;function bt(){if(ge!==null)return ge;try{const e="__ls_probe__";localStorage.setItem(e,"1"),localStorage.removeItem(e),ge=!0}catch{ge=!1}return ge}function Vt(e,t=null){try{if(bt()){const n=localStorage.getItem(e);if(n===null)return t;try{return JSON.parse(n)}catch{return n}}return Pe.has(e)?Pe.get(e):t}catch{return t}}function Xt(e,t){const n=typeof t=="string"?t:JSON.stringify(t);try{return bt()?(localStorage.setItem(e,n),!0):(Pe.set(e,t),!0)}catch{return Pe.set(e,t),!1}}const kt="dbx_config",ee=Object.freeze({catalog:"",schema:"",secretScope:"",cloudProvider:"azure",sparkVersion:"14.3.x-scala2.12",nodeType:"Standard_DS3_v2",numWorkers:2,workspacePath:"/Workspace/Migrations/NiFi"});function tt(){try{const e=Vt(kt);return e?{...ee,...JSON.parse(e)}:{...ee}}catch{return{...ee}}}function Yt(e){try{Xt(kt,JSON.stringify(e))}catch{}}function St(){return{catalog:document.getElementById("cfgCatalog")?.value||"",schema:document.getElementById("cfgSchema")?.value||"",secretScope:document.getElementById("cfgScope")?.value||"",cloudProvider:document.getElementById("cfgCloud")?.value||ee.cloudProvider,sparkVersion:document.getElementById("cfgSparkVersion")?.value||ee.sparkVersion,nodeType:document.getElementById("cfgNodeType")?.value||ee.nodeType,numWorkers:parseInt(document.getElementById("cfgWorkers")?.value)||ee.numWorkers,workspacePath:document.getElementById("cfgWorkspacePath")?.value||ee.workspacePath}}function Zt(e,t){return!t||!t.catalog?e:e.replace(/<catalog>|\{catalog\}/g,t.catalog).replace(/<schema>|\{schema\}/g,t.schema||"default").replace(/<scope>|\{scope\}/g,t.secretScope||"migration_secrets").replace(/<workspace_path>/g,t.workspacePath||ee.workspacePath).replace(/<spark_version>/g,t.sparkVersion||ee.sparkVersion).replace(/<node_type>/g,t.nodeType||ee.nodeType)}class ue extends Error{constructor(t,{code:n,phase:s,severity:a,cause:i,context:c}={}){super(t),this.name="AppError",this.code=n||"UNKNOWN",this.phase=s||"",this.severity=a||"medium",this.cause=i||null,this.context=c||{},this.timestamp=Date.now()}}function $e(e,t={}){const n=e instanceof ue?e:new ue(e.message||String(e),{code:"UNHANDLED",cause:e,...t});console.error(`[${n.code}] ${n.message}`,n)}function re(e,t={}){return async function(...s){try{return await e.apply(this,s)}catch(a){throw $e(a,t),a}}}class en{constructor(){this._steps=[],this._runId=0,this._running=!1,this._locks=new Set}register(t){this._steps=t.map((n,s)=>({...n,order:s}))}get running(){return this._running}async execute({delayMs:t=150}={}){const n=++this._runId;this._running=!0,q.emit("pipeline:start",{runId:n});try{for(const s of this._steps){if(this._runId!==n){q.emit("pipeline:cancelled",{runId:n,at:s.name});return}if(this._locks.has(s.name)){console.warn(`[pipeline] skipping locked step: ${s.name}`);continue}this._locks.add(s.name),q.emit("pipeline:step:start",{runId:n,step:s.name});try{await s.run()}catch(a){$e(a instanceof ue?a:new ue(`Pipeline step "${s.name}" failed`,{code:"PIPELINE_STEP_FAILED",phase:s.name,severity:"high",cause:a})),q.emit("pipeline:step:error",{runId:n,step:s.name,error:a})}finally{this._locks.delete(s.name),q.emit("pipeline:step:done",{runId:n,step:s.name})}t>0&&this._runId===n&&await new Promise(a=>setTimeout(a,t))}}finally{this._runId===n&&(this._running=!1,q.emit("pipeline:done",{runId:n}))}}cancel(){this._running&&(this._runId++,this._running=!1,q.emit("pipeline:cancelled",{runId:this._runId}))}}function tn(){document.querySelectorAll(".tab").forEach(e=>{e.addEventListener("click",()=>{if(e.classList.contains("locked"))return;document.querySelectorAll(".tab").forEach(n=>n.classList.remove("active")),document.querySelectorAll(".panel").forEach(n=>n.classList.remove("active")),e.classList.add("active");const t=document.getElementById("panel-"+e.dataset.tab);t&&t.classList.add("active")})})}function V(e){document.querySelectorAll(".tab").forEach(t=>{t.classList.toggle("active",t.dataset.tab===e)}),document.querySelectorAll(".panel").forEach(t=>{t.classList.toggle("active",t.id==="panel-"+e)})}function H(e,t){const n=document.querySelector(`.tab[data-tab="${e}"]`);n&&(n.classList.remove("locked","processing","done"),t==="locked"?n.classList.add("locked"):t==="processing"?n.classList.add("processing"):t==="done"&&n.classList.add("done"),t!=="locked"&&(n.style.pointerEvents="",n.style.opacity=""))}function ce(e){H(e,"ready")}function x(e){return e==null?"":String(e).replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;").replace(/"/g,"&quot;").replace(/'/g,"&#39;")}let Ge="",He="";function vt(){return Ge}function ze(e,t){Ge=e,He=t}function Ct(){return He}function wt(){const e=document.getElementById("fileInput");if(!e)return Promise.resolve();const t=e.files[0];if(!t)return Promise.resolve();He=t.name;const n=document.getElementById("fileName");return n&&(n.textContent="Loaded: "+t.name,n.classList.remove("hidden")),new Promise(s=>{const a=new FileReader;a.onload=i=>{Ge=i.target.result,s()},a.onerror=()=>s(),a.readAsText(t)})}function Pt(){const e=document.getElementById("fileInput"),t=document.getElementById("fileDropZone");!e||!t||(t.addEventListener("click",()=>e.click()),t.addEventListener("dragover",n=>{n.preventDefault(),t.style.borderColor="var(--primary)"}),t.addEventListener("dragleave",()=>{t.style.borderColor="var(--border)"}),t.addEventListener("drop",n=>{n.preventDefault(),t.style.borderColor="var(--border)",n.dataTransfer.files.length&&(e.files=n.dataTransfer.files,wt())}))}const nn=Object.freeze(Object.defineProperty({__proto__:null,getUploadedContent:vt,getUploadedName:Ct,handleFile:wt,initFileUpload:Pt,setUploadedContent:ze},Symbol.toStringTag,{value:"Module"})),sn={etl:`<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<flowController encoding-version="1.4">
  <rootGroup><name>ETL_Demo_Pipeline</name>
    <processor><id>p1</id><name>Read Source CSV</name><class>org.apache.nifi.processors.standard.GetFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>5 min</schedulingPeriod><state>RUNNING</state>
      <property><name>Input Directory</name><value>/data/input/sales</value></property>
      <property><name>File Filter</name><value>[^\\\\.].*\\\\.csv</value></property>
      <autoTerminatedRelationship>failure</autoTerminatedRelationship>
    </processor>
    <processor><id>p2</id><name>Validate Schema</name><class>org.apache.nifi.processors.standard.ValidateRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Record Reader</name><value>CSVReader</value></property>
      <property><name>Record Writer</name><value>CSVWriter</value></property>
    </processor>
    <processor><id>p3</id><name>Route by Region</name><class>org.apache.nifi.processors.standard.RouteOnAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Routing Strategy</name><value>Route to Property name</value></property>
      <property><name>us_east</name><value>\${region:equals("US-East")}</value></property>
      <property><name>us_west</name><value>\${region:equals("US-West")}</value></property>
      <property><name>europe</name><value>\${region:equals("EU")}</value></property>
    </processor>
    <processor><id>p4</id><name>Transform Sales Data</name><class>org.apache.nifi.processors.standard.ReplaceText</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Search Value</name><value>"amount":"(\\d+)"</value></property>
      <property><name>Replacement Value</name><value>"amount_cents":"\${1}00"</value></property>
      <property><name>Replacement Strategy</name><value>Regex Replace</value></property>
    </processor>
    <processor><id>p5</id><name>Query Sales Summary</name><class>org.apache.nifi.processors.standard.ExecuteSQL</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Database Connection Pooling Service</name><value>DBCPService</value></property>
      <property><name>SQL select query</name><value>SELECT region, product, SUM(amount) as total, COUNT(*) as cnt FROM sales.transactions WHERE trade_date >= CURRENT_DATE - 7 GROUP BY region, product</value></property>
    </processor>
    <processor><id>p6</id><name>Update Attributes</name><class>org.apache.nifi.processors.standard.UpdateAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>output.filename</name><value>\${filename:substringBefore('.')}_processed_\${now():format('yyyyMMdd')}.csv</value></property>
      <property><name>batch.id</name><value>\${UUID()}</value></property>
    </processor>
    <processor><id>p7</id><name>Write to Data Lake</name><class>org.apache.nifi.processors.standard.PutFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Directory</name><value>/data/output/processed_sales</value></property>
      <property><name>Conflict Resolution Strategy</name><value>replace</value></property>
    </processor>
    <processor><id>p8</id><name>Insert to Warehouse</name><class>org.apache.nifi.processors.standard.PutDatabaseRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Database Connection Pooling Service</name><value>DBCPService</value></property>
      <property><name>Table Name</name><value>warehouse.sales_processed</value></property>
      <property><name>Statement Type</name><value>INSERT</value></property>
    </processor>
    <processor><id>p9</id><name>Log Completion</name><class>org.apache.nifi.processors.standard.LogMessage</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
      <property><name>Log Level</name><value>info</value></property>
      <property><name>Log Message</name><value>ETL batch complete: \${batch.id}</value></property>
    </processor>
    <connection><id>c1</id><source><id>p1</id><type>PROCESSOR</type></source><destination><id>p2</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c2</id><source><id>p2</id><type>PROCESSOR</type></source><destination><id>p3</id><type>PROCESSOR</type></destination><relationship>valid</relationship></connection>
    <connection><id>c3</id><source><id>p3</id><type>PROCESSOR</type></source><destination><id>p4</id><type>PROCESSOR</type></destination><relationship>us_east</relationship></connection>
    <connection><id>c4</id><source><id>p3</id><type>PROCESSOR</type></source><destination><id>p4</id><type>PROCESSOR</type></destination><relationship>us_west</relationship></connection>
    <connection><id>c5</id><source><id>p3</id><type>PROCESSOR</type></source><destination><id>p4</id><type>PROCESSOR</type></destination><relationship>europe</relationship></connection>
    <connection><id>c6</id><source><id>p4</id><type>PROCESSOR</type></source><destination><id>p5</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c7</id><source><id>p5</id><type>PROCESSOR</type></source><destination><id>p6</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c8</id><source><id>p6</id><type>PROCESSOR</type></source><destination><id>p7</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c9</id><source><id>p6</id><type>PROCESSOR</type></source><destination><id>p8</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>c10</id><source><id>p8</id><type>PROCESSOR</type></source><destination><id>p9</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
  </rootGroup>
  <controllerServices>
    <controllerService><id>cs1</id><name>DBCPService</name><class>org.apache.nifi.dbcp.DBCPConnectionPool</class><property><name>Database Connection URL</name><value>jdbc:postgresql://db.example.com:5432/analytics</value></property><property><name>Database User</name><value>etl_user</value></property><property><name>Password</name><value>s3cur3_p4ss</value></property></controllerService>
  </controllerServices>
</flowController>`,streaming:`<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<flowController encoding-version="1.4">
  <rootGroup><name>Streaming_IoT_Pipeline</name>
    <processGroup><name>IoT Ingestion</name>
      <processor><id>s1</id><name>Consume Kafka Events</name><class>org.apache.nifi.processors.kafka.pubsub.ConsumeKafka_2_6</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>100 ms</schedulingPeriod><state>RUNNING</state>
        <property><name>Kafka Brokers</name><value>kafka-broker-1:9092,kafka-broker-2:9092</value></property>
        <property><name>Topic Name(s)</name><value>iot.sensor.readings</value></property>
        <property><name>Group ID</name><value>nifi-iot-consumer</value></property>
      </processor>
      <processor><id>s2</id><name>Parse JSON Payload</name><class>org.apache.nifi.processors.standard.EvaluateJsonPath</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Destination</name><value>flowfile-attribute</value></property>
        <property><name>sensor_id</name><value>$.sensor_id</value></property>
        <property><name>temperature</name><value>$.readings.temperature</value></property>
        <property><name>humidity</name><value>$.readings.humidity</value></property>
        <property><name>timestamp</name><value>$.event_time</value></property>
      </processor>
      <processor><id>s3</id><name>Route by Threshold</name><class>org.apache.nifi.processors.standard.RouteOnAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Routing Strategy</name><value>Route to Property name</value></property>
        <property><name>alert</name><value>\${temperature:gt(100):or(\${humidity:gt(95)})}</value></property>
        <property><name>normal</name><value>\${temperature:le(100):and(\${humidity:le(95)})}</value></property>
      </processor>
      <connection><id>sc1</id><source><id>s1</id><type>PROCESSOR</type></source><destination><id>s2</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>sc2</id><source><id>s2</id><type>PROCESSOR</type></source><destination><id>s3</id><type>PROCESSOR</type></destination><relationship>matched</relationship></connection>
    </processGroup>
    <processGroup><name>Alert Processing</name>
      <processor><id>s4</id><name>Enrich Alert Data</name><class>org.apache.nifi.processors.standard.LookupAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Lookup Service</name><value>DeviceRegistry</value></property>
        <property><name>device.name</name><value>\${sensor_id}</value></property>
      </processor>
      <processor><id>s5</id><name>Format Alert Notification</name><class>org.apache.nifi.processors.standard.ReplaceText</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Replacement Value</name><value>{"alert":"THRESHOLD_EXCEEDED","sensor":"\${sensor_id}","temp":"\${temperature}","humidity":"\${humidity}","device":"\${device.name}","time":"\${timestamp}"}</value></property>
        <property><name>Replacement Strategy</name><value>Always Replace</value></property>
      </processor>
      <processor><id>s6</id><name>Send Alert to API</name><class>org.apache.nifi.processors.standard.InvokeHTTP</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Remote URL</name><value>https://alerts.example.com/api/v2/notify</value></property>
        <property><name>HTTP Method</name><value>POST</value></property>
        <property><name>Content-Type</name><value>application/json</value></property>
      </processor>
      <connection><id>sc3</id><source><id>s4</id><type>PROCESSOR</type></source><destination><id>s5</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>sc4</id><source><id>s5</id><type>PROCESSOR</type></source><destination><id>s6</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <processGroup><name>Data Storage</name>
      <processor><id>s7</id><name>Batch Readings</name><class>org.apache.nifi.processors.standard.MergeContent</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Merge Strategy</name><value>Bin-Packing Algorithm</value></property>
        <property><name>Minimum Number of Entries</name><value>100</value></property>
        <property><name>Maximum Number of Entries</name><value>1000</value></property>
        <property><name>Max Bin Age</name><value>30 sec</value></property>
      </processor>
      <processor><id>s8</id><name>Convert to Parquet</name><class>org.apache.nifi.processors.standard.ConvertRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Record Reader</name><value>JsonTreeReader</value></property>
        <property><name>Record Writer</name><value>ParquetRecordSetWriter</value></property>
      </processor>
      <processor><id>s9</id><name>Write to Delta Lake</name><class>org.apache.nifi.processors.standard.PutHDFS</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Directory</name><value>/data/iot/sensor_readings/\${now():format('yyyy/MM/dd')}</value></property>
        <property><name>Conflict Resolution Strategy</name><value>replace</value></property>
      </processor>
      <processor><id>s10</id><name>Insert to Timeseries DB</name><class>org.apache.nifi.processors.standard.PutDatabaseRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Database Connection Pooling Service</name><value>TimeseriesDBCP</value></property>
        <property><name>Table Name</name><value>iot.sensor_readings</value></property>
        <property><name>Statement Type</name><value>INSERT</value></property>
      </processor>
      <connection><id>sc5</id><source><id>s7</id><type>PROCESSOR</type></source><destination><id>s8</id><type>PROCESSOR</type></destination><relationship>merged</relationship></connection>
      <connection><id>sc6</id><source><id>s8</id><type>PROCESSOR</type></source><destination><id>s9</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>sc7</id><source><id>s8</id><type>PROCESSOR</type></source><destination><id>s10</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <connection><id>sc_g1</id><source><id>s3</id><type>PROCESSOR</type></source><destination><id>s4</id><type>PROCESSOR</type></destination><relationship>alert</relationship></connection>
    <connection><id>sc_g2</id><source><id>s3</id><type>PROCESSOR</type></source><destination><id>s7</id><type>PROCESSOR</type></destination><relationship>normal</relationship></connection>
    <connection><id>sc_g3</id><source><id>s3</id><type>PROCESSOR</type></source><destination><id>s7</id><type>PROCESSOR</type></destination><relationship>alert</relationship></connection>
  </rootGroup>
</flowController>`,full:`<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<flowController encoding-version="1.4">
  <rootGroup><name>Manufacturing_Data_Pipeline</name>
    <processGroup><name>Data Ingestion</name>
      <processor><id>f1</id><name>Scan Input Directory</name><class>org.apache.nifi.processors.standard.GetFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>1 min</schedulingPeriod><state>RUNNING</state>
        <property><name>Input Directory</name><value>/data/mfg/incoming</value></property>
        <property><name>File Filter</name><value>.*\\.(csv|json|xml)</value></property>
        <property><name>Keep Source File</name><value>false</value></property>
      </processor>
      <processor><id>f2</id><name>List SFTP Uploads</name><class>org.apache.nifi.processors.standard.ListFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>5 min</schedulingPeriod><state>RUNNING</state>
        <property><name>Input Directory</name><value>/sftp/uploads/mfg_data</value></property>
        <property><name>File Filter</name><value>production_.*\\.csv</value></property>
      </processor>
      <processor><id>f3</id><name>Fetch Upload Contents</name><class>org.apache.nifi.processors.standard.FetchFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>File to Fetch</name><value>\${absolute.path}/\${filename}</value></property>
      </processor>
      <processor><id>f4</id><name>Query Production Metrics</name><class>org.apache.nifi.processors.standard.ExecuteSQL</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>10 min</schedulingPeriod><state>RUNNING</state>
        <property><name>Database Connection Pooling Service</name><value>MfgDBCP</value></property>
        <property><name>SQL select query</name><value>SELECT lot_id, wafer_id, step_name, measurement, result, operator, meas_time FROM mfg_data.production_steps WHERE meas_time >= CURRENT_TIMESTAMP - INTERVAL '1' HOUR ORDER BY meas_time</value></property>
      </processor>
      <connection><id>fc1</id><source><id>f2</id><type>PROCESSOR</type></source><destination><id>f3</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <processGroup><name>Data Transformation</name>
      <processor><id>f5</id><name>Route by File Type</name><class>org.apache.nifi.processors.standard.RouteOnAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Routing Strategy</name><value>Route to Property name</value></property>
        <property><name>csv_files</name><value>\${filename:endsWith('.csv')}</value></property>
        <property><name>json_files</name><value>\${filename:endsWith('.json')}</value></property>
        <property><name>xml_files</name><value>\${filename:endsWith('.xml')}</value></property>
      </processor>
      <processor><id>f6</id><name>Parse JSON Metrics</name><class>org.apache.nifi.processors.standard.EvaluateJsonPath</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Destination</name><value>flowfile-attribute</value></property>
        <property><name>lot_id</name><value>$.lot_id</value></property>
        <property><name>status</name><value>$.quality_status</value></property>
        <property><name>yield_pct</name><value>$.yield_percentage</value></property>
      </processor>
      <processor><id>f7</id><name>Normalize Data Format</name><class>org.apache.nifi.processors.standard.ConvertRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Record Reader</name><value>InferAvroReader</value></property>
        <property><name>Record Writer</name><value>CSVRecordSetWriter</value></property>
      </processor>
      <processor><id>f8</id><name>Add Processing Metadata</name><class>org.apache.nifi.processors.standard.UpdateAttribute</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>processing.timestamp</name><value>\${now():format('yyyy-MM-dd HH:mm:ss')}</value></property>
        <property><name>source.system</name><value>nifi_mfg_pipeline</value></property>
        <property><name>batch.id</name><value>\${UUID()}</value></property>
        <property><name>output.filename</name><value>\${filename:substringBefore('.')}_enriched_\${now():format('yyyyMMdd_HHmmss')}.csv</value></property>
      </processor>
      <connection><id>fc2</id><source><id>f5</id><type>PROCESSOR</type></source><destination><id>f6</id><type>PROCESSOR</type></destination><relationship>json_files</relationship></connection>
      <connection><id>fc3</id><source><id>f5</id><type>PROCESSOR</type></source><destination><id>f7</id><type>PROCESSOR</type></destination><relationship>csv_files</relationship></connection>
      <connection><id>fc4</id><source><id>f6</id><type>PROCESSOR</type></source><destination><id>f8</id><type>PROCESSOR</type></destination><relationship>matched</relationship></connection>
      <connection><id>fc5</id><source><id>f7</id><type>PROCESSOR</type></source><destination><id>f8</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <processGroup><name>Data Loading</name>
      <processor><id>f9</id><name>Write to Staging</name><class>org.apache.nifi.processors.standard.PutFile</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Directory</name><value>/data/mfg/staging</value></property>
        <property><name>Conflict Resolution Strategy</name><value>replace</value></property>
      </processor>
      <processor><id>f10</id><name>Upload to HDFS</name><class>org.apache.nifi.processors.hadoop.PutHDFS</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Directory</name><value>/data/warehouse/mfg_production</value></property>
        <property><name>Conflict Resolution Strategy</name><value>replace</value></property>
      </processor>
      <processor><id>f11</id><name>Insert Production Records</name><class>org.apache.nifi.processors.standard.PutDatabaseRecord</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Database Connection Pooling Service</name><value>MfgDBCP</value></property>
        <property><name>Table Name</name><value>mfg_data.production_processed</value></property>
        <property><name>Statement Type</name><value>INSERT</value></property>
      </processor>
      <processor><id>f12</id><name>Transfer to Partner SFTP</name><class>org.apache.nifi.processors.standard.PutSFTP</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Hostname</name><value>sftp.partner.example.com</value></property>
        <property><name>Port</name><value>22</value></property>
        <property><name>Username</name><value>mfg_data_xfer</value></property>
        <property><name>Password</name><value>xfer_s3cret!</value></property>
        <property><name>Remote Path</name><value>/incoming/mfg/\${now():format('yyyyMMdd')}</value></property>
      </processor>
      <connection><id>fc6</id><source><id>f9</id><type>PROCESSOR</type></source><destination><id>f10</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>fc7</id><source><id>f9</id><type>PROCESSOR</type></source><destination><id>f11</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>fc8</id><source><id>f10</id><type>PROCESSOR</type></source><destination><id>f12</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <processGroup><name>Orchestration</name>
      <processor><id>f13</id><name>Signal Data Ready</name><class>org.apache.nifi.processors.standard.Notify</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Signal Counter Name</name><value>mfg_data_ready</value></property>
        <property><name>Signal Counter Delta</name><value>1</value></property>
      </processor>
      <processor><id>f14</id><name>Wait for All Sources</name><class>org.apache.nifi.processors.standard.Wait</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>5 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Signal Counter Name</name><value>mfg_data_ready</value></property>
        <property><name>Target Signal Count</name><value>3</value></property>
      </processor>
      <processor><id>f15</id><name>Run Aggregation Script</name><class>org.apache.nifi.processors.standard.ExecuteStreamCommand</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Command</name><value>/opt/scripts/aggregate_mfg.sh</value></property>
        <property><name>Command Arguments</name><value>/data/mfg/staging /data/mfg/aggregated</value></property>
      </processor>
      <processor><id>f16</id><name>Refresh Impala Tables</name><class>org.apache.nifi.processors.standard.ExecuteStreamCommand</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Command</name><value>impala-shell</value></property>
        <property><name>Command Arguments</name><value>-q INVALIDATE METADATA mfg_data.production_processed; COMPUTE STATS mfg_data.production_processed;</value></property>
      </processor>
      <processor><id>f17</id><name>Log Pipeline Status</name><class>org.apache.nifi.processors.standard.LogMessage</class><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><schedulingPeriod>0 sec</schedulingPeriod><state>RUNNING</state>
        <property><name>Log Level</name><value>info</value></property>
        <property><name>Log Message</name><value>Manufacturing pipeline complete: batch=\${batch.id}, files=\${file.count}, timestamp=\${processing.timestamp}</value></property>
      </processor>
      <connection><id>fc9</id><source><id>f14</id><type>PROCESSOR</type></source><destination><id>f15</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>fc10</id><source><id>f15</id><type>PROCESSOR</type></source><destination><id>f16</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
      <connection><id>fc11</id><source><id>f16</id><type>PROCESSOR</type></source><destination><id>f17</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    </processGroup>
    <connection><id>fc_g1</id><source><id>f1</id><type>PROCESSOR</type></source><destination><id>f5</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g2</id><source><id>f3</id><type>PROCESSOR</type></source><destination><id>f5</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g3</id><source><id>f4</id><type>PROCESSOR</type></source><destination><id>f8</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g4</id><source><id>f8</id><type>PROCESSOR</type></source><destination><id>f9</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g5</id><source><id>f11</id><type>PROCESSOR</type></source><destination><id>f13</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
    <connection><id>fc_g6</id><source><id>f12</id><type>PROCESSOR</type></source><destination><id>f13</id><type>PROCESSOR</type></destination><relationship>success</relationship></connection>
  </rootGroup>
  <controllerServices>
    <controllerService><id>cs_mfg</id><name>MfgDBCP</name><class>org.apache.nifi.dbcp.DBCPConnectionPool</class>
      <property><name>Database Connection URL</name><value>jdbc:oracle:thin:@mfg-db.example.com:1521/MFGPRD</value></property>
      <property><name>Database User</name><value>mfg_reader</value></property>
      <property><name>Password</name><value>mfg_r34d3r!</value></property>
      <property><name>Database Driver Class Name</name><value>oracle.jdbc.driver.OracleDriver</value></property>
    </controllerService>
  </controllerServices>
</flowController>`};function on(e,t){const n=sn[e];if(!n)return;const s={etl:"ETL Pipeline (9 processors)",streaming:"Streaming IoT (10 processors)",full:"Manufacturing Migration (17 processors)"};ze(n,`sample_${e}_flow.xml`);const a=document.getElementById("fileName");a&&(a.textContent="Sample: "+(s[e]||e),a.classList.remove("hidden"));const i=document.getElementById("pasteInput");i&&(i.value=""),typeof t=="function"&&t()}async function rn(e,t,n){const s=document.getElementById("fileName");s&&(s.textContent="Loading: "+t+"...",s.classList.remove("hidden"));try{const a=await fetch(e);if(!a.ok)throw new Error("HTTP "+a.status);const i=await a.text();ze(i,t),s&&(s.textContent="Sample: "+t);const c=document.getElementById("pasteInput");c&&(c.value=""),typeof n=="function"&&n()}catch(a){s&&(s.textContent="Failed to load "+t+" — "+a.message,s.style.color="var(--red)",setTimeout(()=>{s.style.color=""},3e3))}}function Te(e,t,n){const s=document.getElementById("parseProgress"),a=document.getElementById("parsePBar"),i=document.getElementById("parsePPct"),c=document.getElementById("parsePStatus");s&&(s.style.display="flex",a&&(a.style.width=e+"%",a.style.background=e>=100?"var(--green)":"var(--primary)"),i&&(i.textContent=Math.round(e)+"%"),c&&(c.textContent=t+""))}function an(){const e=document.getElementById("parseProgress");e&&(e.style.display="none")}function $t(e){let t=document.getElementById("pathTraceToast");t||(t=document.createElement("div"),t.id="pathTraceToast",t.className="path-trace-toast",document.body.appendChild(t));const n=e.selected.length,s=e.pathNodes.size,a=n===1?"1 node selected — click another to trace route":`${n} nodes selected — ${s} in path`;t.innerHTML=`<span>${a}</span><span class="toast-hint">Click nodes to build route</span><span class="toast-clear" id="pathTraceToastClear">✕ Clear</span>`,t.style.display="flex";const i=document.getElementById("pathTraceToastClear");i&&(i.onclick=()=>{t.style.display="none"})}function cn(){const e=document.getElementById("pathTraceToast");e&&(e.style.display="none")}function nt(){const e=document.getElementById("pathTraceToast");if(e){const t=document.createElement("span");t.style.cssText="color:var(--red);margin-left:8px",t.textContent="No direct path",e.appendChild(t),setTimeout(()=>{t.parentNode&&t.remove()},2500)}}const Et=/password|secret|token|key|auth|credential|cert|private|keytab|passphrase/i;function ln(e,t){return Et.test(e)?"********":t}function Dt(e){return Et.test(e)}function Tt(e){let t=0;const n=[],s=new Set,a={},i={},c=[];function o(r){a[r]=i[r]=t++,n.push(r),s.add(r);for(const l of e[r]||[])a[l]===void 0?(o(l),i[r]=Math.min(i[r],i[l])):s.has(l)&&(i[r]=Math.min(i[r],a[l]));if(i[r]===a[r]){const l=[];let p;do p=n.pop(),s.delete(p),l.push(p);while(p!==r);l.length>1&&c.push(l)}}for(const r of Object.keys(e))a[r]===void 0&&o(r);return c}const pn={GetFile:"source",GetHTTP:"source",ConsumeKafka:"source",ConsumeKafka_2_6:"source",ConsumeKafkaRecord_2_6:"source",QueryDatabaseTable:"source",QueryDatabaseTableRecord:"source",ListenHTTP:"source",GetSFTP:"source",GetFTP:"source",GenerateFlowFile:"source",ListS3:"source",FetchS3Object:"source",ListFile:"source",ListSFTP:"source",ListFTP:"source",TailFile:"source",GetMongo:"source",FetchFile:"source",GetElasticsearch:"source",GetSQS:"source",ConsumeJMS:"source",ConsumeMQTT:"source",ConsumeAMQP:"source",GetHDFS:"source",FetchHDFS:"source",ListHDFS:"source",ConsumeAzureEventHub:"source",FetchAzureBlobStorage:"source",HandleHttpRequest:"source",ConsumeGCPubSub:"source",ConsumeKinesisStream:"source",GetDynamoDB:"source",FetchGCS:"source",ListGCSBucket:"source",GetHBase:"source",GetCouchbaseKey:"source",GetCypher:"source",ReplaceText:"transform",EvaluateJsonPath:"transform",ConvertRecord:"transform",UpdateAttribute:"transform",FlattenJson:"transform",JoltTransformJSON:"transform",SplitJson:"transform",SplitContent:"transform",MergeContent:"transform",MergeRecord:"transform",CompressContent:"transform",ConvertAvroToJSON:"transform",ConvertAvroToORC:"transform",ConvertCSVToAvro:"transform",ConvertJSONToAvro:"transform",ConvertJSONToSQL:"transform",ExtractText:"transform",TransformXml:"transform",EvaluateXPath:"transform",EvaluateXQuery:"transform",EncryptContent:"transform",HashAttribute:"transform",SplitXml:"transform",SplitText:"transform",SplitAvro:"transform",SplitRecord:"transform",ExtractGrok:"transform",ExtractHL7Attributes:"transform",ExecuteScript:"transform",ExecuteGroovyScript:"transform",AttributesToJSON:"transform",ForkRecord:"transform",SampleRecord:"transform",JoltTransformRecord:"transform",ConvertCharacterSet:"transform",ParseCEF:"transform",ParseEvtx:"transform",ParseNetflowv5:"transform",ParseSyslog5424:"transform",UnpackContent:"transform",IdentifyMimeType:"transform",ModifyBytes:"transform",SegmentContent:"transform",DuplicateFlowFile:"transform",RouteOnAttribute:"route",RouteOnContent:"route",RouteText:"route",RouteHL7:"route",ValidateRecord:"route",DistributeLoad:"route",DetectDuplicate:"route",ExecuteSQL:"process",InvokeHTTP:"process",ExecuteStreamCommand:"process",LookupRecord:"process",LookupAttribute:"process",HandleHttpResponse:"process",ExecuteProcess:"process",InvokeAWSGatewayApi:"process",PutFile:"sink",PutHDFS:"sink",PutDatabaseRecord:"sink",PutSQL:"sink",PutS3Object:"sink",PutSFTP:"sink",PutFTP:"sink",PutEmail:"sink",PublishKafka:"sink",PublishKafka_2_6:"sink",PublishKafkaRecord_2_6:"sink",PutAzureBlobStorage:"sink",PutAzureDataLakeStorage:"sink",PutElasticsearch:"sink",PutMongo:"sink",PutHBaseJSON:"sink",PutHBaseCell:"sink",PutSNS:"sink",PutDynamoDB:"sink",PutKinesisStream:"sink",PutGCSObject:"sink",PublishGCPubSub:"sink",PutDatabaseRecord:"sink",PutSyslog:"sink",PutTCP:"sink",LogMessage:"utility",LogAttribute:"utility",Wait:"utility",Notify:"utility",DebugFlow:"utility",CountText:"utility",AttributesToJSON:"utility"};function X(e){return pn[e]||(/^(Get|List|Consume|Listen|Fetch|Tail|Query)/i.test(e)?"source":/^(Put|Publish|Send|Post)/i.test(e)?"sink":/^(Route|Distribute|Control|Validate|Detect)/i.test(e)?"route":/^(Convert|Split|Merge|Replace|Transform|Extract|Evaluate|Flatten|Compress|Encrypt|Hash)/i.test(e)?"transform":/^(Execute|Invoke|Lookup|Handle)/i.test(e)?"process":/^(Log|Debug|Count|Wait|Notify)/i.test(e)?"utility":"process")}const he=["source","route","transform","process","sink","utility"],dn={source:"#3B82F6",route:"#EAB308",transform:"#A855F7",process:"#6366F1",sink:"#21C354",utility:"#808495"},un={source:"SOURCES",route:"ROUTING",transform:"TRANSFORMS",process:"PROCESSING",sink:"SINKS",utility:"UTILITY"};function mn(e){const t=[["source",e.sources],["route",e.routes],["transform",e.transforms],["process",e.processes],["sink",e.sinks],["utility",e.utilities]];return t.sort((n,s)=>s[1]!==n[1]?s[1]-n[1]:he.indexOf(n[0])-he.indexOf(s[0])),t[0][1]>0?t[0][0]:"process"}function fn(e,t){return t&&t._nifi?gn(t._nifi):{nodes:[],connections:[],tierLabels:{},tierColors:{},cycles:[]}}function gn(e,t){const n=[],s=[],a={},i=e.processors||[],c=e.connections||[];e.processGroups;const o={};i.forEach(f=>{const g=f.group||"(root)";o[g]||(o[g]={sources:0,sinks:0,routes:0,transforms:0,processes:0,utilities:0,total:0,processors:[],typeCount:{}});const h=X(f.type);o[g][h+"s"]=(o[g][h+"s"]||0)+1,o[g].total++,o[g].processors.push(f),o[g].typeCount[f.type]=(o[g].typeCount[f.type]||0)+1});const r={};i.forEach(f=>{r[f.name]=f.group||"(root)"});const l={},p={};c.forEach(f=>{const g=r[f.sourceName]||"(root)",h=r[f.destinationName]||"(root)";if(g!==h){const v=g+"|"+h;l[v]=(l[v]||0)+1}else p[g]=(p[g]||0)+1});const d=Object.keys(o),u={};Object.keys(l).forEach(f=>{const[g,h]=f.split("|");u[g]||(u[g]=new Set),u[g].add(h)}),d.forEach(f=>{u[f]||(u[f]=new Set)});const m=Tt(u),y=new Set,_={};m.forEach((f,g)=>{f.forEach(h=>{y.add(h),_[h]=g})});const b={};d.forEach(f=>{b[f]=mn(o[f])});const S={};he.forEach(f=>{S[f]=[]}),d.forEach(f=>{S[b[f]].push(f)}),he.forEach(f=>{const g=f+"s";S[f].sort((h,v)=>{const $=(o[h][g]||0)/(o[h].total||1),E=(o[v][g]||0)/(o[v].total||1);return E!==$?E-$:o[v].total-o[h].total})});let P=0;he.forEach(f=>{const g=S[f];if(!g.length)return;P++;const h=dn[f],v=h.replace("#","").match(/.{2}/g).map($=>parseInt($,16));a[P]={label:un[f],color:h,bg:`rgba(${v[0]},${v[1]},${v[2]},0.06)`,role:f},g.forEach($=>{const E=o[$],R=Object.entries(E.typeCount).sort((z,Q)=>Q[1]-z[1]).slice(0,3).map(([z,Q])=>`${z}(${Q})`).join(", "),A=y.has($),T=_[$],N=A?m[T]:[],U=A?Object.entries(l).filter(([z])=>{const[Q,w]=z.split("|");return m[T].includes(Q)&&m[T].includes(w)}).map(([z,Q])=>{const[w,F]=z.split("|");return{from:w,to:F,count:Q}}):[];n.push({id:"pg_"+$,name:$,tier:P,type:"process_group",dominantRole:b[$],subtype:b[$]+"s",procCount:E.total,srcCount:E.sources,sinkCount:E.sinks,routeCount:E.routes,transformCount:E.transforms,processCount:E.processes,utilityCount:E.utilities,intraConns:p[$]||0,topTypes:R,inCycle:A,sccMembers:N,cycleEdges:U,expandable:!0,detail:{processors:E.processors,typeCount:E.typeCount,intraConns:p[$]||0}})})}),Object.entries(l).forEach(([f,g])=>{const[h,v]=f.split("|"),$=y.has(h)&&y.has(v)&&_[h]===_[v];s.push({from:"pg_"+h,to:"pg_"+v,label:g>1?g+" flows":"1 flow",type:"flow",color:$?"#EF4444":"#4B5563",width:Math.min(1+g*.3,4),inCycle:$})});const k=[],D={};i.forEach(f=>{D[f.type]=(D[f.type]||0)+1}),Object.entries(D).sort((f,g)=>g[1]-f[1]).forEach(([f,g])=>{const h=X(f);k.push({name:f,writers:g,readers:0,lookups:0,total:g,role:h})});const C=m.map((f,g)=>({id:g,groups:f,edgeCount:Object.keys(l).filter(h=>{const[v,$]=h.split("|");return f.includes(v)&&f.includes($)}).length}));return{nodes:n,connections:s,tierLabels:a,diagramType:"nifi_flow",densityData:k,cycleData:C}}function Rt(e,t=250){let n=null,s=null,a=null;function i(...c){s=c,a=this,clearTimeout(n),n=setTimeout(()=>{n=null,e.apply(a,s),s=a=null},t)}return i.cancel=()=>{clearTimeout(n),n=null,s=a=null},i.flush=()=>{n!==null&&(clearTimeout(n),n=null,e.apply(a,s),s=a=null)},i}function yn(e,t,n,s,a){const i=new Set([e]),c=new Set;function o(p){n.forEach(d=>{const u=d.from+"|"+d.to;d.from===p&&!c.has(u)&&(c.add(u),i.add(d.to),o(d.to))})}function r(p){n.forEach(d=>{const u=d.from+"|"+d.to;d.to===p&&!c.has(u)&&(c.add(u),i.add(d.from),r(d.from))})}o(e),r(e),Object.entries(s).forEach(([p,d])=>{i.has(p)?(d.classList.add("highlighted"),d.classList.remove("dimmed"),p===e&&d.classList.add("selected")):(d.classList.add("dimmed"),d.classList.remove("highlighted","selected"))});const l=a.querySelector("svg.tier-svg");l&&l.querySelectorAll("path[data-from]").forEach(p=>{const d=p.dataset.from+"|"+p.dataset.to;c.has(d)?(p.setAttribute("opacity","1"),p.setAttribute("stroke","#FAFAFA"),p.setAttribute("stroke-width","3"),p.setAttribute("filter","url(#glow)"),p.setAttribute("marker-end","url(#arrow-white)"),p.style.transition="all 0.2s ease"):(p.setAttribute("opacity","0.08"),p.removeAttribute("filter"))})}const hn=Rt(yn,30);function _n(e,t){Object.values(e).forEach(s=>{s.classList.remove("highlighted","dimmed","selected")});const n=t.querySelector("svg.tier-svg");n&&n.querySelectorAll("path[data-from]").forEach(s=>{s.setAttribute("opacity","0.35"),s.setAttribute("stroke",s.dataset.origColor||"#4B5563"),s.setAttribute("stroke-width",s.dataset.origWidth||"1.5"),s.removeAttribute("filter");const a=s.dataset.origColor||"",i=a.includes("EF44")?"arrow-red":a.includes("F59E")||a.includes("F5")?"arrow-amber":a.includes("6366")?"arrow-purple":a.includes("3B82")?"arrow-blue":a.includes("21C3")?"arrow-green":"arrow-default";s.setAttribute("marker-end",`url(#${i})`),s.style.transition=""})}function le(e,t,n){const s={};e.forEach(o=>{s[o.from]||(s[o.from]=[]),s[o.from].push({to:o.to,key:o.from+"|"+o.to})});const a=new Set([t]),i={},c=[t];for(;c.length;){const o=c.shift();if(o===n){const r=[],l=[];let p=n;for(;p!==t;)r.unshift(p),l.unshift(i[p].key),p=i[p].from;return r.unshift(t),{pathNodes:r,pathEdgeKeys:l,found:!0}}for(const r of s[o]||[])a.has(r.to)||(a.add(r.to),i[r.to]={from:o,key:r.key},c.push(r.to))}return{pathNodes:[],pathEdgeKeys:[],found:!1}}function bn(e,t){const n=new Set([e]),s=new Set,a=[e],i=new Set([e]);for(;a.length;){const r=a.shift();t.forEach(l=>{l.from===r&&!i.has(l.to)&&(i.add(l.to),n.add(l.to),s.add(l.from+"|"+l.to),a.push(l.to))})}const c=[e],o=new Set([e]);for(;c.length;){const r=c.shift();t.forEach(l=>{l.to===r&&!o.has(l.from)&&(o.add(l.from),n.add(l.from),s.add(l.from+"|"+l.to),c.push(l.from))})}return{reachNodes:n,reachEdges:s}}function xt(e,t,n){Object.entries(t).forEach(([a,i])=>{i.classList.remove("path-selected","path-member","path-dimmed","highlighted","dimmed","selected"),e.selected.includes(a)?i.classList.add("path-selected"):e.pathNodes.has(a)?i.classList.add("path-member"):i.classList.add("path-dimmed")});const s=n.querySelector("svg.tier-svg");s&&s.querySelectorAll("path[data-from]").forEach(a=>{const i=a.dataset.from+"|"+a.dataset.to,c=a.dataset.to+"|"+a.dataset.from;e.pathEdgeKeys.has(i)||e.pathEdgeKeys.has(c)?(a.setAttribute("opacity","1"),a.setAttribute("stroke","#FACA15"),a.setAttribute("stroke-width","3"),a.setAttribute("filter","url(#glow)"),a.setAttribute("marker-end","url(#arrow-white)"),a.style.transition="all 0.2s ease"):(a.setAttribute("opacity","0.04"),a.removeAttribute("filter"),a.style.transition="all 0.2s ease")})}function kn(e,t,n,s,a){t.selected=[e],t.active=!0;const{reachNodes:i,reachEdges:c}=bn(e,n);t.pathNodes=i,t.pathEdgeKeys=c,xt(t,s,a),$t(t)}function Sn(e,t,n,s,a){if(!t.selected.includes(e)){if(t.selected.push(e),t.selected.length===2){const i=t.selected[0],c=t.selected[1];let o=le(n,i,c);if(o.found||(o=le(n,c,i)),!o.found){const r=n.flatMap(l=>[l,{from:l.to,to:l.from,label:l.label,type:l.type,color:l.color,width:l.width}]);o=le(r,i,c)}o.found?(t.pathNodes=new Set(o.pathNodes),t.pathEdgeKeys=new Set(o.pathEdgeKeys)):(t.pathNodes=new Set(t.selected),t.pathEdgeKeys=new Set,nt())}else{const i=t.selected[t.selected.length-2];let c=le(n,i,e);if(c.found||(c=le(n,e,i)),!c.found){const o=n.flatMap(r=>[r,{from:r.to,to:r.from,label:r.label,type:r.type,color:r.color,width:r.width}]);c=le(o,i,e)}c.found?(c.pathNodes.forEach(o=>t.pathNodes.add(o)),c.pathEdgeKeys.forEach(o=>t.pathEdgeKeys.add(o))):(t.pathNodes.add(e),nt())}xt(t,s,a),$t(t)}}function Re(e,t,n){e.selected=[],e.pathNodes=new Set,e.pathEdgeKeys=new Set,e.active=!1,Object.values(t).forEach(a=>{a.classList.remove("path-selected","path-member","path-dimmed")});const s=n.querySelector("svg.tier-svg");s&&s.querySelectorAll("path[data-from]").forEach(a=>{a.setAttribute("opacity","0.35"),a.setAttribute("stroke",a.dataset.origColor||"#4B5563"),a.setAttribute("stroke-width",a.dataset.origWidth||"1.5"),a.removeAttribute("filter");const i=a.dataset.origColor||"",c=i.includes("EF44")?"arrow-red":i.includes("F59E")||i.includes("F5")?"arrow-amber":i.includes("6366")?"arrow-purple":i.includes("3B82")?"arrow-blue":i.includes("21C3")?"arrow-green":"arrow-default";a.setAttribute("marker-end",`url(#${c})`),a.style.transition=""}),cn()}function Ne(e,t,n){const s=e.querySelector("svg.tier-svg");if(s&&s.remove(),!t.length)return;const a=document.createElementNS("http://www.w3.org/2000/svg","svg");a.classList.add("tier-svg"),a.style.position="absolute",a.style.top="0",a.style.left="0",a.style.width=e.scrollWidth+"px",a.style.height=e.scrollHeight+"px",a.style.pointerEvents="none",a.style.zIndex="1",a.setAttribute("viewBox",`0 0 ${e.scrollWidth} ${e.scrollHeight}`);const i=document.createElementNS("http://www.w3.org/2000/svg","defs"),c=document.createElementNS("http://www.w3.org/2000/svg","filter");c.setAttribute("id","glow"),c.setAttribute("x","-50%"),c.setAttribute("y","-50%"),c.setAttribute("width","200%"),c.setAttribute("height","200%");const o=document.createElementNS("http://www.w3.org/2000/svg","feGaussianBlur");o.setAttribute("stdDeviation","3"),o.setAttribute("result","blur"),c.appendChild(o);const r=document.createElementNS("http://www.w3.org/2000/svg","feMerge"),l=document.createElementNS("http://www.w3.org/2000/svg","feMergeNode");l.setAttribute("in","blur");const p=document.createElementNS("http://www.w3.org/2000/svg","feMergeNode");p.setAttribute("in","SourceGraphic"),r.appendChild(l),r.appendChild(p),c.appendChild(r),i.appendChild(c),Object.entries({default:"#4B5563",blue:"#3B82F6",purple:"#6366F1",red:"#EF4444",amber:"#F59E0B",green:"#21C354",white:"#FAFAFA"}).forEach(([m,y])=>{const _=document.createElementNS("http://www.w3.org/2000/svg","marker");_.setAttribute("id","arrow-"+m),_.setAttribute("viewBox","0 0 10 8"),_.setAttribute("refX","10"),_.setAttribute("refY","4"),_.setAttribute("markerWidth","8"),_.setAttribute("markerHeight","6"),_.setAttribute("orient","auto");const b=document.createElementNS("http://www.w3.org/2000/svg","path");b.setAttribute("d","M0,0 L10,4 L0,8 Z"),b.setAttribute("fill",y),_.appendChild(b),i.appendChild(_)}),a.appendChild(i);const u=e.getBoundingClientRect();t.forEach(m=>{const y=n[m.from],_=n[m.to];if(!y||!_)return;const b=y.getBoundingClientRect(),S=_.getBoundingClientRect(),P=b.left+b.width/2-u.left+e.scrollLeft,k=b.top+b.height-u.top+e.scrollTop,D=S.left+S.width/2-u.left+e.scrollLeft,C=S.top-u.top+e.scrollTop,f=C-k,g=Math.max(Math.abs(f)*.35,30),h=(D-P)*.15,v=document.createElementNS("http://www.w3.org/2000/svg","path");v.setAttribute("d",`M${P},${k} C${P+h},${k+g} ${D-h},${C-g} ${D},${C}`);const $=m.color||"#4B5563";v.setAttribute("stroke",$),v.setAttribute("stroke-width",String(m.width||1.5)),v.setAttribute("fill","none");const E=$.includes("EF44")?"arrow-red":$.includes("F59E")||$.includes("F5")?"arrow-amber":$.includes("6366")?"arrow-purple":$.includes("3B82")?"arrow-blue":$.includes("21C3")?"arrow-green":"arrow-default";v.setAttribute("marker-end",`url(#${E})`),v.setAttribute("opacity","0.35"),v.dataset.from=m.from,v.dataset.to=m.to,v.dataset.origColor=$,v.dataset.origWidth=String(m.width||1.5),m.dash&&v.setAttribute("stroke-dasharray","6,4"),m.inCycle&&v.setAttribute("stroke-dasharray","8,4"),a.appendChild(v)}),e.style.position="relative",e.insertBefore(a,e.firstChild)}const st={source:"#3B82F6",route:"#EAB308",transform:"#A855F7",process:"#6366F1",sink:"#21C354",utility:"#808495"};function ot(e,t,n,s,a,i,c,o){const r="sub_"+e.id,l=i.querySelector(`[data-sub-band="${r}"]`);if(l){l.remove(),t.classList.remove("expanded");const _=t.querySelector(".expand-indicator");_&&(_.textContent="▶ expand"),Object.keys(a).forEach(b=>{b.startsWith("proc_"+e.name+"|")&&delete a[b]}),requestAnimationFrame(()=>Ne(i,s.connections,a));return}t.classList.add("expanded");const p=t.querySelector(".expand-indicator");p&&(p.textContent="▼ collapse");const d=e.detail&&e.detail.processors||[],u=["source","route","transform","process","sink","utility"],m={source:"Sources",route:"Routing",transform:"Transforms",process:"Processing",sink:"Sinks",utility:"Utility"},y=document.createElement("div");y.className="tier-sub-band",y.dataset.subBand=r,u.forEach(_=>{const b=d.filter(k=>X(k.type)===_);if(!b.length)return;const S=document.createElement("div");S.className="tier-band-label",S.style.color=st[_]||"#808495",S.textContent=`${e.name} → ${m[_]} (${b.length})`,y.appendChild(S);const P=document.createElement("div");P.className="tier-nodes",b.forEach(k=>{const D=document.createElement("div");D.className="tier-node";const C="proc_"+e.name+"|"+k.name;D.dataset.nodeId=C,(k.state==="DISABLED"||k.state==="STOPPED")&&(D.style.opacity="0.5"),D.style.borderTopColor=st[_]||"#808495",D.style.borderTopWidth="3px";const f=document.createElement("div");f.className="node-name",f.textContent=k.name.length>20?k.name.substring(0,17)+"...":k.name,f.title=k.name,D.appendChild(f);const g=document.createElement("div");g.className="node-meta",g.textContent=k.type,D.appendChild(g),D.addEventListener("click",h=>{h.stopPropagation(),ht(()=>Promise.resolve().then(()=>vn),void 0,import.meta.url).then(v=>{v.showNodeDetail({name:k.name,type:"processor",subtype:_,meta:k.type,group:e.name,state:k.state,propCount:Object.keys(k.properties||{}).length,detail:k},c,o)})}),P.appendChild(D),a[C]=D}),y.appendChild(P)}),n.after(y),requestAnimationFrame(()=>Ne(i,s.connections,a))}function Ce(e,t,n){if(!t)return;let s='<div class="node-detail">';if(s+=`<h4>${x(e.name)}</h4>`,n==="nifi_flow"&&e.type==="process_group"&&e.detail){const a=e.detail;if(s+=`<p><strong>Processors:</strong> ${e.procCount} &middot; <strong>Internal Connections:</strong> ${a.intraConns||0}</p>`,s+='<div style="display:flex;gap:6px;flex-wrap:wrap;margin:8px 0">',e.srcCount&&(s+=`<span class="ns ns-tx">${e.srcCount} sources</span>`),e.routeCount&&(s+=`<span class="ns" style="background:#EAB308;color:#000">${e.routeCount} routes</span>`),e.transformCount&&(s+=`<span class="ns" style="background:#A855F7;color:white">${e.transformCount} transforms</span>`),e.processCount&&(s+=`<span class="ns ns-ext">${e.processCount} processors</span>`),e.sinkCount&&(s+=`<span class="ns" style="background:#21C354;color:white">${e.sinkCount} sinks</span>`),e.utilityCount&&(s+=`<span class="ns" style="background:#808495;color:white">${e.utilityCount} utility</span>`),s+="</div>",a.typeCount){const i=Object.entries(a.typeCount).sort((c,o)=>o[1]-c[1]);s+='<table style="font-size:0.75rem"><thead><tr><th>Processor Type</th><th>Count</th></tr></thead><tbody>',i.slice(0,15).forEach(([c,o])=>{s+=`<tr><td>${x(c)}</td><td>${o}</td></tr>`}),i.length>15&&(s+=`<tr><td colspan="2" style="color:var(--text2)">+${i.length-15} more types</td></tr>`),s+="</tbody></table>"}a.processors&&a.processors.length&&(s+='<p style="margin-top:8px"><strong>Processors (first 10):</strong></p>',s+='<ul style="font-size:0.75rem;margin:4px 0 4px 16px">',a.processors.slice(0,10).forEach(i=>{s+=`<li>${x(i.name)} <code style="font-size:0.65rem">${x(i.type)}</code></li>`}),a.processors.length>10&&(s+=`<li style="color:var(--text2)">+${a.processors.length-10} more</li>`),s+="</ul>"),e.inCycle&&e.sccMembers&&(s+='<div style="margin:8px 0;padding:8px 12px;border:1px solid #EF4444;border-radius:6px;background:rgba(239,68,68,0.08);font-size:0.8rem">',s+='<strong style="color:#EF4444">Circular Dependency Detected</strong><br>',s+="Cycle with: "+e.sccMembers.filter(i=>i!==e.name).map(x).join(", "),e.cycleEdges&&e.cycleEdges.length&&(s+="<br><br><strong>Cycle edges:</strong><br>",e.cycleEdges.forEach(i=>{s+=`${x(i.from)} → ${x(i.to)} (${i.count} flow${i.count>1?"s":""})<br>`})),s+="</div>")}else if(n==="nifi_flow"&&e.detail){const a=e.detail;s+=`<p><strong>Type:</strong> ${x(a.type)} <code style="font-size:0.7rem">${x(a.fullType||"")}</code></p>`,s+=`<p><strong>Group:</strong> ${x(a.group||"(root)")}</p>`,s+=`<p><strong>State:</strong> ${x(a.state||"N/A")}</p>`,a.schedulingStrategy&&(s+=`<p><strong>Scheduling:</strong> ${x(a.schedulingStrategy)} / ${x(a.schedulingPeriod)}</p>`);const i=Object.keys(a.properties||{}),c=i.filter(o=>Dt(o)).length;i.length&&(s+=`<p><strong>Properties (${i.length}):</strong>${c?` <span style="color:#EAB308;font-size:0.75rem">&#x26A0; ${c} sensitive masked</span>`:""}</p><pre style="max-height:200px;overflow:auto;font-size:0.75rem">`,i.slice(0,20).forEach(o=>{s+=`${x(o)}: ${x(ln(o,(a.properties[o]||"").substring(0,100)))}
`}),i.length>20&&(s+=`... +${i.length-20} more
`),s+="</pre>")}else if(n==="dependency_graph"&&e.type==="session"&&e.detail){const a=e.detail;s+=`<p><strong>Sources:</strong> ${a.sources} &middot; <strong>Targets:</strong> ${a.targets} &middot; <strong>Lookups:</strong> ${a.lookups}</p>`,e.seq&&(s+=`<p><strong>Execution Order:</strong> #${e.seq}</p>`),a.source_tables&&a.source_tables.length&&(s+='<p style="margin-top:6px"><strong>Source Tables:</strong></p><ul style="font-size:0.8rem;margin:4px 0 4px 16px">',a.source_tables.slice(0,10).forEach(i=>{s+=`<li>${x(i.name)}</li>`}),a.source_tables.length>10&&(s+=`<li style="color:var(--text2)">+${a.source_tables.length-10} more</li>`),s+="</ul>"),a.target_tables&&a.target_tables.length&&(s+='<p><strong>Target Tables:</strong></p><ul style="font-size:0.8rem;margin:4px 0 4px 16px">',a.target_tables.forEach(i=>{s+=`<li>${x(i.name)}${i.load_type?' <code style="font-size:0.7rem">'+x(i.load_type)+"</code>":""}</li>`}),s+="</ul>"),e.hasConflict&&e.conflictDetails&&e.conflictDetails.length&&(s+='<div class="alert alert-warn" style="margin:8px 0;padding:8px 12px;font-size:0.8rem"><strong>Conflicts:</strong><br>',e.conflictDetails.forEach(i=>{s+=`${x(i.table_name)} &mdash; ${x(i.conflict_type)}<br>`}),s+="</div>")}else if(n==="dependency_graph"&&(e.type==="table_output"||e.type==="conflict_gate")&&e.detail){const a=e.detail;a.writers&&a.writers.length&&(s+=`<p><strong>Writers:</strong> ${a.writers.map(x).join(", ")}</p>`),a.readers&&a.readers.length&&(s+=`<p><strong>Readers:</strong> ${a.readers.map(x).join(", ")}</p>`),a.lookups&&a.lookups.length&&(s+=`<p><strong>Lookup Readers:</strong> ${a.lookups.map(x).join(", ")}</p>`),a.conflicts&&a.conflicts.length&&(s+='<div class="alert alert-warn" style="margin:8px 0;padding:8px 12px;font-size:0.8rem"><strong>Conflicts:</strong><br>',a.conflicts.forEach(i=>{s+=`${x(i.conflict_type)}${i.writers?" &mdash; Writers: "+i.writers.map(x).join(", "):""}<br>`}),s+="</div>")}else if(e.detail&&e.detail.columns){const a=e.detail;s+=`<p><strong>Schema:</strong> ${x(a.schema||"dbo")} &middot; <strong>Rows:</strong> ${a.row_count}</p>`,s+='<table style="font-size:0.75rem"><thead><tr><th>Column</th><th>Type</th><th>PK</th><th>Null</th></tr></thead><tbody>',a.columns.slice(0,15).forEach(i=>{s+=`<tr><td>${x(i.name)}</td><td>${x(i.data_type)}</td><td>${i.is_primary_key?"Y":""}</td><td>${i.nullable?"Y":"N"}</td></tr>`}),a.columns.length>15&&(s+=`<tr><td colspan="4" style="color:var(--text2)">+${a.columns.length-15} more columns</td></tr>`),s+="</tbody></table>",a.foreign_keys&&a.foreign_keys.length&&(s+='<p style="margin-top:8px"><strong>Foreign Keys:</strong></p>',a.foreign_keys.forEach(i=>{s+=`<p style="font-size:0.8rem"><code>${x(i.column||i.fk_column)}</code> &rarr; <code>${x(i.references_table)}(${x(i.references_column)})</code></p>`}))}s+="</div>",t.innerHTML=s}const vn=Object.freeze(Object.defineProperty({__proto__:null,showNodeDetail:Ce},Symbol.toStringTag,{value:"Module"})),Cn={source:"#3B82F6",route:"#EAB308",transform:"#A855F7",process:"#6366F1",sink:"#21C354",utility:"#808495"};function wn(e,t,n,s,a,i){const{nodes:c,connections:o,tierLabels:r}=t,l={};c.forEach(d=>{l[d.tier]||(l[d.tier]=[]),l[d.tier].push(d)});const p=Object.keys(l).map(Number).sort((d,u)=>d-u);return p.forEach(d=>{const u=r[d]||{label:`TIER ${d}`,color:"#808495",bg:"rgba(128,132,149,0.06)"},m=document.createElement("div");m.className="tier-band",m.style.background=u.bg,m.style.borderLeft=`3px solid ${u.color}`;const y=document.createElement("div");y.className="tier-band-label",y.style.color=u.color,y.textContent=u.label,m.appendChild(y);const _=document.createElement("div");_.className="tier-nodes",l[d].forEach(b=>{const S=document.createElement("div");if(S.dataset.nodeId=b.id,S.dataset.role=b.subtype||b.dominantRole||"",S.dataset.conf=String(b.conf||0),S.dataset.name=b.name||"",S.dataset.type=b.procType||b.type||"",b.type==="session"){if(S.className="tier-node",b.hasConflict?(S.style.borderColor="#EF4444",S.style.borderTopColor="#EF4444"):b.subtype==="root"?S.style.borderTopColor="#3B82F6":S.style.borderTopColor="#6366F1",S.style.borderTopWidth="3px",b.seq){const C=document.createElement("div");C.className="node-seq",C.textContent=b.seq,b.hasConflict&&(C.style.background="#EF4444"),S.appendChild(C)}const P=document.createElement("div");P.className="node-name";const k=(b.name||"").replace(/^s_m_(?:Load_|LOAD_)?/i,"");P.textContent=k.length>22?k.substring(0,19)+"...":k,P.title=b.name||"",S.appendChild(P);const D=document.createElement("div");if(D.className="node-stats",D.innerHTML=`<span class="ns ns-tx">${b.srcCount||0} tx</span><span class="ns ns-ext">${b.tgtCount||0} ext</span>`+(b.lkpCount?`<span class="ns ns-lkp">${b.lkpCount} lkp</span>`:""),S.appendChild(D),b.hasConflict){const C=document.createElement("div");C.className="node-badge red",C.textContent="!",C.title=(b.conflictDetails||[]).map(f=>(f.table_name||"")+": "+(f.conflict_type||"")).join(", "),S.appendChild(C)}}else if(b.type==="table_output"){S.className="tier-node table-output",b.isConflict?S.style.borderColor="#EF4444":b.isChain?S.style.borderColor="#F59E0B":S.style.borderColor="#21C354";const P=document.createElement("div");P.style.cssText="font-size:0.8rem;margin-bottom:2px",P.textContent=b.isConflict?"⚠":b.isChain?"Ⅱ":"✓",S.appendChild(P);const k=document.createElement("div");k.className="node-name",k.textContent=(b.name||"").length>20?(b.name||"").substring(0,17)+"...":b.name||"",k.title=b.name||"",S.appendChild(k);const D=document.createElement("div");D.className="node-class",D.style.color=b.isConflict?"#FCA5A5":b.isChain?"#FDE68A":"#86EFAC",D.textContent=b.isConflict?"CONFLICT":b.isChain?"CHAIN":"INDEPENDENT",S.appendChild(D)}else if(b.type==="conflict_gate"){S.className="tier-node conflict-gate";const P=document.createElement("div");P.style.cssText="font-size:1.2rem;margin-bottom:2px",P.textContent="⚠",S.appendChild(P);const k=document.createElement("div");k.className="node-name",k.textContent=(b.name||"").length>25?(b.name||"").substring(0,22)+"...":b.name||"",k.title=b.name||"",S.appendChild(k);const D=document.createElement("div");D.className="node-meta",D.textContent=`${b.writerCount||0}W / ${b.readerCount||0}R / ${b.lookupCount||0}L`,S.appendChild(D);const C=document.createElement("div");C.className="node-class",C.style.color="#FCA5A5",C.textContent="CONFLICT",S.appendChild(C)}else if(b.type==="process_group"){S.className="tier-node expandable",b.inCycle&&S.classList.add("in-cycle");const P=Cn[b.dominantRole]||"#6366F1";if(S.style.borderTopColor=P,S.style.borderTopWidth="3px",S.style.minWidth="160px",S.style.maxWidth="240px",b.inCycle){const g=document.createElement("div");g.className="cycle-badge",g.textContent="↻",g.title="Cycle: "+(b.sccMembers||[]).filter(h=>h!==b.name).join(", "),S.appendChild(g)}const k=document.createElement("div");k.className="node-name",k.textContent=(b.name||"").length>28?(b.name||"").substring(0,25)+"...":b.name||"",k.title=b.name||"",S.appendChild(k);const D=document.createElement("div");D.className="node-badge",D.textContent=b.procCount||0,D.title=(b.procCount||0)+" processors",S.appendChild(D);const C=document.createElement("div");C.className="node-stats",b.srcCount&&(C.innerHTML+=`<span class="ns ns-tx">${b.srcCount} src</span>`),(b.transformCount||0)+(b.routeCount||0)&&(C.innerHTML+=`<span class="ns" style="background:#A855F7;color:white">${(b.transformCount||0)+(b.routeCount||0)} xfm</span>`),b.processCount&&(C.innerHTML+=`<span class="ns ns-ext">${b.processCount} proc</span>`),b.sinkCount&&(C.innerHTML+=`<span class="ns" style="background:#21C354;color:white">${b.sinkCount} sink</span>`),b.utilityCount&&(C.innerHTML+=`<span class="ns" style="background:#808495;color:white">${b.utilityCount} util</span>`),S.appendChild(C);const f=document.createElement("div");f.className="expand-indicator",f.textContent="▶ expand",S.appendChild(f)}else{S.className="tier-node",(b.state==="DISABLED"||b.state==="STOPPED")&&(S.style.opacity="0.5"),b.subtype==="source"?S.style.borderTopColor="#3B82F6":b.subtype==="sink"?S.style.borderTopColor="#21C354":b.subtype==="route"?S.style.borderTopColor="#EAB308":b.subtype==="transform"?S.style.borderTopColor="#A855F7":b.type==="table"&&(S.style.borderTopColor="#3B82F6"),S.style.borderTopWidth="3px";const P=document.createElement("div");if(P.className="node-name",P.textContent=(b.name||"").length>25?(b.name||"").substring(0,22)+"...":b.name||"",P.title=b.name||"",S.appendChild(P),b.meta){const k=document.createElement("div");k.className="node-meta",k.textContent=b.meta,S.appendChild(k)}if(b.rows){const k=document.createElement("div");k.className="node-badge",k.textContent=b.rows>=1e3?Math.round(b.rows/1e3)+"K":b.rows,S.appendChild(k)}}S.addEventListener("mouseenter",()=>{s.active||hn(b.id,c,o,n,e)}),S.addEventListener("mouseleave",()=>{s.active||_n(n,e)}),S.addEventListener("click",P=>{P.stopPropagation(),s.active&&!s.selected.includes(b.id)?(Sn(b.id,s,o,n,e),Ce(b,a,i)):s.active&&s.selected.includes(b.id)?(b.type==="process_group"&&b.expandable&&ot(b,S,m,t,n,e,a,i),Ce(b,a,i)):(kn(b.id,s,o,n,e),b.type==="process_group"&&b.expandable&&ot(b,S,m,t,n,e,a,i),Ce(b,a,i))}),_.appendChild(S),n[b.id]=S}),m.appendChild(_),e.appendChild(m)}),{sortedTiers:p}}function Pn(e,t,n,s,a,i,c){const o=document.getElementById("sidebarClearBtn");if(!e.activeTypes.size){At(a,i),c.querySelectorAll(".density-row").forEach(d=>d.classList.remove("filter-dimmed")),o&&(o.style.display="none");return}o&&(o.style.display="block");const r=new Set;e.activeTypes.forEach(d=>{t[d]&&t[d].forEach(u=>r.add(u))});const l=new Set;s.forEach(d=>{r.has(d.from)&&r.has(d.to)&&l.add(d.from+"|"+d.to)}),Object.entries(a).forEach(([d,u])=>{u.classList.remove("path-selected","path-member","path-dimmed","highlighted","dimmed","selected"),r.has(d)?u.classList.add("path-member"):u.classList.add("path-dimmed")});const p=i.querySelector("svg.tier-svg");p&&p.querySelectorAll("path[data-from]").forEach(d=>{const u=d.dataset.from+"|"+d.dataset.to;l.has(u)?(d.setAttribute("opacity","0.8"),d.setAttribute("stroke",d.dataset.origColor||"#4B5563"),d.setAttribute("stroke-width",d.dataset.origWidth||"1.5")):d.setAttribute("opacity","0.04")}),c.querySelectorAll(".density-row").forEach(d=>{e.activeTypes.has(d.dataset.typeName)?d.classList.remove("filter-dimmed"):d.classList.add("filter-dimmed")})}function At(e,t){Object.values(e).forEach(s=>{s.classList.remove("path-selected","path-member","path-dimmed")});const n=t.querySelector("svg.tier-svg");n&&n.querySelectorAll("path[data-from]").forEach(s=>{s.setAttribute("opacity","0.35"),s.setAttribute("stroke",s.dataset.origColor||"#4B5563"),s.setAttribute("stroke-width",s.dataset.origWidth||"1.5"),s.removeAttribute("filter");const a=s.dataset.origColor||"",i=a.includes("EF44")?"arrow-red":a.includes("F59E")||a.includes("F5")?"arrow-amber":a.includes("6366")?"arrow-purple":a.includes("3B82")?"arrow-blue":a.includes("21C3")?"arrow-green":"arrow-default";s.setAttribute("marker-end",`url(#${i})`),s.style.transition=""})}let oe={role:"all",conf:"all",search:""};function It(e,t,n){oe[t]=n;const s=e.parentElement;s&&s.querySelectorAll("[data-node-id]").forEach(a=>{const i=a.dataset.role||"",c=parseFloat(a.dataset.conf||0),o=(a.dataset.name||"").toLowerCase(),r=(a.dataset.type||"").toLowerCase();let l=!0;oe.role!=="all"&&i!==oe.role&&(l=!1),oe.conf==="high"&&c<.7&&(l=!1),oe.conf==="med"&&(c<.3||c>=.7)&&(l=!1),oe.conf==="low"&&c>=.3&&(l=!1),oe.search&&!o.includes(oe.search.toLowerCase())&&!r.includes(oe.search.toLowerCase())&&(l=!1),a.style.opacity=l?"1":"0.15",a.style.pointerEvents=l?"":"none"})}const $n=Rt((e,t)=>{It(e,"search",t)},150);function xe(e,t,n){t==="search"?$n(e,n):It(e,t,n)}function En(e,t,n,s){const a=document.getElementById(t),i=document.getElementById(n),c=document.getElementById(s);if(!a)return;a.innerHTML="",a.style.minHeight="200px";const{nodes:o,connections:r,tierLabels:l,diagramType:p,densityData:d}=e;if(!o.length){a.innerHTML='<p style="text-align:center;padding:20px;color:var(--text2)">No nodes to display</p>';return}const u=document.createElement("div");u.className="filter-toolbar";let m='<div class="filter-group"><label>Role:</label>';["all","source","transform","route","process","sink","utility"].forEach(C=>{const f={all:"",source:"#3B82F6",transform:"#A855F7",route:"#EAB308",process:"#6366F1",sink:"#21C354",utility:"#808495"},g=f[C]?' style="border-color:'+f[C]+";color:"+f[C]+'"':"";m+='<button class="filter-btn'+(C==="all"?" active":"")+'"'+g+' data-filter-role="'+C+'">'+(C==="all"?"All":C.charAt(0).toUpperCase()+C.slice(1))+"</button>"}),m+='</div><div class="filter-group"><label>Confidence:</label>',[{k:"all",l:"All",s:""},{k:"high",l:"High",s:"border-color:var(--green);color:var(--green)"},{k:"med",l:"Med",s:"border-color:var(--amber);color:var(--amber)"},{k:"low",l:"Low",s:"border-color:var(--red);color:var(--red)"}].forEach(C=>{m+='<button class="filter-btn'+(C.k==="all"?" active":"")+'"'+(C.s?' style="'+C.s+'"':"")+' data-filter-conf="'+C.k+'">'+C.l+"</button>"}),m+='</div><div class="filter-group"><input class="filter-search" type="text" placeholder="Search processors..."></div>',u.innerHTML=m,u.querySelectorAll("[data-filter-role]").forEach(C=>{C.addEventListener("click",()=>{C.parentElement.querySelectorAll(".filter-btn").forEach(f=>f.classList.remove("active")),C.classList.add("active"),xe(u,"role",C.dataset.filterRole)})}),u.querySelectorAll("[data-filter-conf]").forEach(C=>{C.addEventListener("click",()=>{C.parentElement.querySelectorAll(".filter-btn").forEach(f=>f.classList.remove("active")),C.classList.add("active"),xe(u,"conf",C.dataset.filterConf)})});const y=u.querySelector(".filter-search");y&&y.addEventListener("input",()=>{xe(u,"search",y.value)}),a.appendChild(u);const _={selected:[],pathNodes:new Set,pathEdgeKeys:new Set,active:!1},b={};wn(a,e,b,_,i,p),requestAnimationFrame(()=>{Ne(a,r,b)}),document.addEventListener("keydown",C=>{C.key==="Escape"&&_.active&&Re(_,b,a)}),a.addEventListener("click",C=>{(C.target===a||C.target.classList.contains("tier-band")||C.target.classList.contains("tier-band-label"))&&_.active&&Re(_,b,a)});const S=document.getElementById("tierDensitySidebar"),P=document.getElementById("densityBars"),k={activeTypes:new Set};if(S&&P&&d&&d.length){S.classList.remove("hidden");const C=S.querySelector("h4");if(C&&(C.textContent=p==="nifi_flow"?"Filter by Type":"Connection Density"),P.innerHTML="",p==="nifi_flow"){const h=document.createElement("div");h.className="sidebar-filter-hint",h.textContent="Click to filter diagram",P.appendChild(h)}const f=Math.max(...d.map(h=>h.total)),g={};if(p==="nifi_flow"&&o.forEach(h=>{h.type==="process_group"&&h.detail&&h.detail.typeCount&&Object.keys(h.detail.typeCount).forEach(v=>{g[v]||(g[v]=new Set),g[v].add(h.id)})}),d.forEach(h=>{const v=document.createElement("div");if(v.className="density-row",v.dataset.typeName=h.name,h.role){const $={source:"#3B82F6",sink:"#21C354",route:"#EAB308",transform:"#A855F7",process:"#6366F1",utility:"#808495"},E=Math.max(4,h.total/f*80);v.innerHTML=`<span class="density-bar" style="width:${E}px;background:${$[h.role]||"#808495"}" title="${h.total}x"></span><span class="density-label" title="${h.name}">${h.name} (${h.total})</span>`}else{const $=Math.max(2,h.writers/f*60),E=Math.max(0,h.readers/f*60),R=Math.max(0,h.lookups/f*60);let A=`<span class="density-bar" style="width:${$}px;background:#EF4444" title="${h.writers} writer(s)"></span>`;h.readers&&(A+=`<span class="density-bar" style="width:${E}px;background:#3B82F6" title="${h.readers} reader(s)"></span>`),h.lookups&&(A+=`<span class="density-bar" style="width:${R}px;background:#F59E0B" title="${h.lookups} lookup(s)"></span>`),v.innerHTML=A+`<span class="density-label" title="${h.name}">${h.name}</span>`}p==="nifi_flow"&&v.addEventListener("click",()=>{_.active&&Re(_,b,a);const $=h.name;k.activeTypes.has($)?(k.activeTypes.delete($),v.classList.remove("filter-active")):(k.activeTypes.add($),v.classList.add("filter-active")),Pn(k,g,o,r,b,a,P)}),P.appendChild(v)}),p==="nifi_flow"){const h=document.createElement("div");h.className="sidebar-clear-btn",h.id="sidebarClearBtn",h.textContent="✕ Clear filter",h.addEventListener("click",()=>{k.activeTypes.clear(),P.querySelectorAll(".density-row").forEach(v=>v.classList.remove("filter-active","filter-dimmed")),At(b,a),h.style.display="none"}),P.appendChild(h)}}else S&&S.classList.add("hidden");if(c){c.innerHTML="";const C=o.filter(g=>g.type==="session").length,f=o.filter(g=>g.type==="table_output"||g.type==="conflict_gate").length;if(p==="dependency_graph")c.innerHTML=[`<span>${C} Sessions</span>`,`<span>${f} Tables</span>`,`<span style="color:#EF4444">${o.filter(g=>g.hasConflict||g.type==="conflict_gate").length} Conflicts</span>`,'<span><span class="leg-line" style="background:#6366F1"></span> Dependency</span>','<span><span class="leg-line" style="background:#F59E0B;border-top:2px dashed #F59E0B"></span> Lookup</span>','<span><span class="leg-line" style="background:#EF4444"></span> Conflict</span>','<span><span class="leg-line" style="background:#21C354"></span> Independent</span>','<span><span class="leg-line" style="background:#F59E0B"></span> Chain</span>'].join("");else if(p==="nifi_flow"){const g=o.filter($=>$.type==="process_group").length,h=o.filter($=>$.type==="processor").length,v=e.cycleData?e.cycleData.length:0;c.innerHTML=[`<span>${g} Process Groups</span>`,h?`<span>${h} Processors</span>`:"",`<span>${r.length} Connections</span>`,v?`<span style="color:#EF4444">${v} Cycle(s)</span>`:"",'<span><span class="leg-line" style="background:#3B82F6"></span> Source</span>','<span><span class="leg-line" style="background:#EAB308"></span> Route</span>','<span><span class="leg-line" style="background:#A855F7"></span> Transform</span>','<span><span class="leg-line" style="background:#6366F1"></span> Process</span>','<span><span class="leg-line" style="background:#21C354"></span> Sink</span>','<span><span class="leg-line" style="background:#EF4444;border-top:2px dashed #EF4444"></span> Cycle Edge</span>','<span style="color:var(--text2);font-size:0.7rem">Click nodes to trace route &middot; Esc to clear</span>'].join("")}else p==="sql_tables"?c.innerHTML=['<span><span class="leg-line" style="background:#3B82F6;border-top:2px solid #3B82F6"></span> Foreign Key</span>',`<span>${o.length} tables &middot; ${r.length} relationships</span>`].join(""):c.innerHTML=[`<span>${o.length} objects</span>`,r.length?'<span><span class="leg-line" style="background:#4B5563;border-top:2px dashed #4B5563"></span> Shared columns</span>':""].join("")}const D=document.getElementById("tierDiagramContainer");D&&D.classList.remove("hidden")}function M(e,t){const n=e.querySelector(":scope > "+t);return n?n.textContent.trim():""}function rt(e){const t={};return e.querySelectorAll("config > properties > entry").forEach(n=>{const s=n.querySelector(":scope > key")?.textContent||"",a=n.querySelector(":scope > value");s&&a&&(t[s]=a.textContent||"")}),Object.keys(t).length||e.querySelectorAll(":scope > properties > entry").forEach(n=>{const s=n.querySelector(":scope > key")?.textContent||"",a=n.querySelector(":scope > value");s&&a&&(t[s]=a.textContent||"")}),Object.keys(t).length||e.querySelectorAll(":scope > property").forEach(n=>{const s=n.querySelector(":scope > name")?.textContent||"",a=n.querySelector(":scope > value")?.textContent||"";s&&(t[s]=a)}),t}function at(e,t){const n=[],s=[],a=[],i=[],c=[],o={};function r(g,h){const v=g.querySelector(":scope > contents")||g,$=v.querySelectorAll(":scope > processors"),E=$.length===0?v.querySelectorAll(":scope > processor"):[];($.length>0?$:E).forEach(w=>{const F=M(w,"name"),I=M(w,"type")||M(w,"class"),L=I.split(".").pop(),G=M(w,"state"),J=rt(w),Y=w.querySelector("config > schedulingPeriod")?.textContent||M(w,"schedulingPeriod")||"",W=w.querySelector("config > schedulingStrategy")?.textContent||M(w,"schedulingStrategy")||"",te=M(w,"id");te&&(o[te]=F||L),s.push({name:F,type:L,fullType:I,state:G,properties:J,group:h,schedulingPeriod:Y,schedulingStrategy:W,_id:te||"gen_"+s.length})});const A=v.querySelectorAll(":scope > connections"),T=A.length===0?v.querySelectorAll(":scope > connection"):[];(A.length>0?A:T).forEach(w=>{const F=w.querySelector("source > id")?.textContent||M(w,"sourceId")||"",I=w.querySelector("destination > id")?.textContent||M(w,"destinationId")||"",L=w.querySelector("source > type")?.textContent||"",G=w.querySelector("destination > type")?.textContent||"",J=[];w.querySelectorAll(":scope > selectedRelationships").forEach(W=>{W.textContent&&J.push(W.textContent)}),J.length||w.querySelectorAll(":scope > relationship").forEach(W=>{W.textContent&&J.push(W.textContent)});const Y=M(w,"backPressureObjectThreshold");a.push({sourceId:F,destinationId:I,sourceType:L,destinationType:G,relationships:J,backPressure:Y})}),v.querySelectorAll(":scope > inputPorts").forEach(w=>{const F=M(w,"id"),I=M(w,"name");F&&(o[F]=I||"input_port")}),v.querySelectorAll(":scope > outputPorts").forEach(w=>{const F=M(w,"id"),I=M(w,"name");F&&(o[F]=I||"output_port")});const U=v.querySelectorAll(":scope > processGroups"),z=U.length===0?v.querySelectorAll(":scope > processGroup"):[];(U.length>0?U:z).forEach(w=>{const F=M(w,"name"),I=M(w,"id");I&&(o[I]=F),c.push({name:F,parentGroup:h}),r(w,F)})}const l=e.querySelector("template > snippet")||e.querySelector("snippet")||e.querySelector("flowController > rootGroup")||e.querySelector("rootGroup")||e.querySelector("processGroupFlow > flow")||e.documentElement,p=e.querySelector("controllerServices"),d=l.querySelectorAll(":scope > controllerServices"),u=p?p.querySelectorAll(":scope > controllerService"):[];(d.length>0?d:u).forEach(g=>{const h=M(g,"name"),v=M(g,"type")||M(g,"class"),$=M(g,"state"),E={};g.querySelectorAll(":scope > properties > entry").forEach(R=>{const A=R.querySelector(":scope > key")?.textContent||"",T=R.querySelector(":scope > value");A&&T&&(E[A]=T.textContent||"")}),g.querySelectorAll(":scope > property").forEach(R=>{const A=R.querySelector(":scope > name")?.textContent||"",T=R.querySelector(":scope > value")?.textContent||"";A&&(E[A]=T)}),i.push({name:h,type:v.split(".").pop(),fullType:v,state:$,properties:E})});const y=l.querySelectorAll(":scope > processGroups"),_=y.length===0?l.querySelectorAll(":scope > processGroup"):[];(y.length>0?y:_).forEach(g=>{const h=M(g,"name"),v=M(g,"id");v&&(o[v]=h),c.push({name:h,parentGroup:"(root)"}),r(g,h)});const S=l.querySelectorAll(":scope > processors"),P=S.length===0?l.querySelectorAll(":scope > processor"):[];(S.length>0?S:P).forEach(g=>{const h=M(g,"name"),v=M(g,"type")||M(g,"class"),$=M(g,"id");$&&(o[$]=h||v.split(".").pop());const E=rt(g),R=g.querySelector("config > schedulingPeriod")?.textContent||M(g,"schedulingPeriod")||"",A=g.querySelector("config > schedulingStrategy")?.textContent||M(g,"schedulingStrategy")||"";s.push({name:h,type:v.split(".").pop(),fullType:v,state:M(g,"state"),properties:E,group:"(root)",schedulingPeriod:R,schedulingStrategy:A,_id:$||"gen_"+s.length})});const D=l.querySelectorAll(":scope > connections"),C=D.length===0?l.querySelectorAll(":scope > connection"):[];return(D.length>0?D:C).forEach(g=>{const h=g.querySelector("source > id")?.textContent||M(g,"sourceId")||"",v=g.querySelector("destination > id")?.textContent||M(g,"destinationId")||"",$=g.querySelector("source > type")?.textContent||"",E=g.querySelector("destination > type")?.textContent||"",R=[];g.querySelectorAll(":scope > selectedRelationships").forEach(T=>{T.textContent&&R.push(T.textContent)}),R.length||g.querySelectorAll(":scope > relationship").forEach(T=>{T.textContent&&R.push(T.textContent)});const A=M(g,"backPressureObjectThreshold");a.some(T=>T.sourceId===h&&T.destinationId===v&&T.relationships.join(",")===R.join(","))||a.push({sourceId:h,destinationId:v,sourceType:$,destinationType:E,relationships:R,backPressure:A})}),a.forEach(g=>{g.sourceName=o[g.sourceId]||g.sourceId.substring(0,12)+"...",g.destinationName=o[g.destinationId]||g.destinationId.substring(0,12)+"..."}),{tables:n,processors:s,connections:a,controllerServices:i,processGroups:c,idToName:o}}function it(e,t){const n=[],s=[],a=[],i=[];function c(p,d){const u=p.name||d||"root";p.name&&a.push({name:u,id:p.identifier||u}),(p.processors||[]).forEach(m=>{const y={};Array.isArray(m.properties)?m.properties.forEach(_=>{_.name&&_.value&&(y[_.name]=_.value)}):m.properties&&Object.entries(m.properties).forEach(([_,b])=>{b!==null&&(y[_]=String(b))}),n.push({id:m.identifier||m.id||"p_"+n.length,name:m.name||m.type||"Unknown",type:(m.type||"").replace(/^org\.apache\.nifi\.processors?\.\w+\./,""),class:m.type||"",group:u,state:m.scheduledState||m.state||"STOPPED",schedulingStrategy:m.schedulingStrategy||"TIMER_DRIVEN",schedulingPeriod:m.schedulingPeriod||"0 sec",properties:y,relationships:m.autoTerminatedRelationships||[]})}),(p.connections||[]).forEach(m=>{s.push({sourceId:m.source?.id||m.sourceId||"",destinationId:m.destination?.id||m.destinationId||"",sourceName:m.source?.name||m.sourceName||"",destinationName:m.destination?.name||m.destinationName||"",relationships:m.selectedRelationships||[],backPressure:m.backPressureObjectThreshold||""})}),(p.controllerServices||[]).forEach(m=>{i.push({name:m.name||m.type||"Unknown",type:(m.type||"").replace(/^org\.apache\.nifi\.\w+\./,""),state:m.scheduledState||"ENABLED",properties:m.properties||{}})}),(p.processGroups||[]).forEach(m=>c(m,m.name||u))}c(e,"root");const o={};n.forEach(p=>{o[p.id]=p.name}),s.forEach(p=>{!p.sourceName&&p.sourceId&&(p.sourceName=o[p.sourceId]||p.sourceId),!p.destinationName&&p.destinationId&&(p.destinationName=o[p.destinationId]||p.destinationId)});const r=/password|secret|token|key|auth|credential|cert|private|keytab|passphrase/i,l={processors:n,connections:s,processGroups:a,controllerServices:i,clouderaTools:[],deepPropertyInventory:{filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set},sqlTables:[],sqlTableMeta:{}};return n.forEach(p=>{Object.entries(p.properties).forEach(([d,u])=>{u&&(/\$\{/.test(u)&&(l.deepPropertyInventory.nifiEL[u]||(l.deepPropertyInventory.nifiEL[u]=[]),l.deepPropertyInventory.nifiEL[u].push(p.name)),/jdbc:/i.test(u)&&(l.deepPropertyInventory.jdbcUrls[u]||(l.deepPropertyInventory.jdbcUrls[u]=[]),l.deepPropertyInventory.jdbcUrls[u].push(p.name)),r.test(d)&&(l.deepPropertyInventory.credentialRefs[d]||(l.deepPropertyInventory.credentialRefs[d]=[]),l.deepPropertyInventory.credentialRefs[d].push(p.name)))})}),{source_name:t,source_type:"nifi_registry_json",_nifi:l,parse_warnings:[],_deferredProcessorWork:null}}function Dn(e){if(!e)return"";let t=e;return t.charCodeAt(0)===65279&&(t=t.substring(1)),t=t.replace(/\r\n/g,`
`).replace(/\r/g,`
`),t=t.replace(/\x00/g,""),t=t.replace(/\u00A0/g," "),t=t.replace(/[\u201C\u201D]/g,'"').replace(/[\u2018\u2019]/g,"'"),t.trim()}function Tn(e){for(var t=[],n="",s=0,a=!1,i="",c=0;c<e.length;c++){var o=e[c];a?(n+=o,o===i&&(a=!1)):o==="'"||o==='"'?(a=!0,i=o,n+=o):o==="("?(s++,n+=o):o===")"?(s--,n+=o):o===":"&&s===0?(t.push(n),n=""):n+=o}return n&&t.push(n),t}function Ft(e){const t=e.toLowerCase();return/password|token|secret|credential|api[_.]?key|private[_.]?key|passphrase/i.test(t)?{type:"secret",code:`dbutils.secrets.get(scope="${t.includes("kafka")?"kafka":t.includes("jdbc")?"jdbc":t.includes("s3")||t.includes("aws")?"aws":t.includes("es")?"es":"app"}", key="${e}")`}:/^(s3|kafka|jdbc|nifi|aws|azure|gcp|http|ftp|smtp|ssl|sasl)\./i.test(e)||/\.url$|\.host$|\.port$|\.path$|\.bucket$|\.region$/i.test(t)?{type:"config",code:`dbutils.widgets.get("${e}")`}:{type:"column",code:`col("${e}")`}}function Z(e,t){if(e=e.trim(),/^'([^']*)'$/.test(e))return t==="col"?'lit("'+e.slice(1,-1)+'")':'"'+e.slice(1,-1)+'"';if(/^\d+$/.test(e))return t==="col"?"lit("+e+")":e;if(e.includes(":"))return Lt(e,t);var n=Ft(e);return t==="col"?n.type==="column"?'col("'+e+'")':n.code:n.type==="column"?'_attrs.get("'+e+'", "")':n.code}function Rn(e){var t=e.match(/^(\w+)\((.*)\)$/s);if(!t)return{name:e,args:[]};var n=t[1],s=t[2].trim();if(!s)return{name:n,args:[]};for(var a=[],i="",c=0,o=!1,r="",l=0;l<s.length;l++){var p=s[l];o?(i+=p,p===r&&(o=!1)):p==="'"||p==='"'?(o=!0,r=p,i+=p):p==="("?(c++,i+=p):p===")"?(c--,i+=p):p===","&&c===0?(a.push(i.trim()),i=""):i+=p}return i.trim()&&a.push(i.trim()),{name:n,args:a}}function j(e){return/^'([^']*)'$/.test(e)||/^"([^"]*)"$/.test(e)?e.slice(1,-1):e}function ct(e){return e.replace(/yyyy/g,"%Y").replace(/MM/g,"%m").replace(/dd/g,"%d").replace(/HH/g,"%H").replace(/mm/g,"%M").replace(/ss/g,"%S").replace(/SSS/g,"%f").replace(/E+/g,"%a").replace(/Z/g,"%z")}const lt={COMBINEDAPACHELOG:'(?<client>\\S+) \\S+ (?<user>\\S+) \\[(?<timestamp>[^\\]]+)\\] "(?<method>\\S+) (?<request>[^"]+) HTTP/\\S+" (?<status>\\d+) (?<size>\\S+) "(?<referrer>[^"]*)" "(?<agent>[^"]*)"',COMMONAPACHELOG:'(?<client>\\S+) \\S+ (?<user>\\S+) \\[(?<timestamp>[^\\]]+)\\] "(?<method>\\S+) (?<request>[^"]+) HTTP/\\S+" (?<status>\\d+) (?<size>\\S+)',SYSLOGLINE:"(?<timestamp>\\w{3}\\s+\\d{1,2} \\d{2}:\\d{2}:\\d{2}) (?<host>\\S+) (?<program>[^\\[]+)(?:\\[(?<pid>\\d+)\\])?: (?<message>.*)",TIMESTAMP_ISO8601:"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?(?:Z|[+-]\\d{2}:?\\d{2})?",IP:"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}",NUMBER:"(?:-?\\d+(?:\\.\\d+)?)",WORD:"\\w+",DATA:".*?",GREEDYDATA:".*"};function xn(e){return lt[e]||lt[e.toUpperCase()]||e}function An(e,t,n){var s=Rn(t),a=s.name.toLowerCase(),i=s.args;if(a==="toupper")return n==="col"?"upper("+e+")":e+".upper()";if(a==="tolower")return n==="col"?"lower("+e+")":e+".lower()";if(a==="trim")return n==="col"?"trim("+e+")":e+".strip()";if(a==="length")return n==="col"?"length("+e+")":"len("+e+")";if(a==="substring"){var c=i[0]||"0",o=i[1];return n==="col"?o?"substring("+e+", "+(parseInt(c)+1)+", "+o+")":"substring("+e+", "+(parseInt(c)+1)+", 9999)":o?e+"["+c+":"+(parseInt(c)+parseInt(o))+"]":e+"["+c+":]"}if(a==="replace"){var r=j(i[0]||""),l=j(i[1]||"");return n==="col"?"regexp_replace("+e+', "'+r+'", "'+l+'")':e+'.replace("'+r+'", "'+l+'")'}if(a==="replaceall"){var p=j(i[0]||""),d=j(i[1]||"");return n==="col"?"regexp_replace("+e+', "'+p+'", "'+d+'")':'re.sub(r"'+p+'", "'+d+'", '+e+")"}if(a==="replacefirst"){var u=j(i[0]||""),m=j(i[1]||"");return n==="col"?"regexp_replace("+e+', "('+u+')", "'+m+'")':'re.sub(r"'+u+'", "'+m+'", '+e+", count=1)"}if(a==="startswith"){var y=j(i[0]||"");return e+'.startswith("'+y+'")'}if(a==="endswith"){var _=j(i[0]||"");return e+'.endswith("'+_+'")'}if(a==="contains"){var b=j(i[0]||"");return n==="col"?e+'.contains("'+b+'")':'"'+b+'" in '+e}if(a==="matches"){var S=j(i[0]||"");return n==="col"?e+'.rlike("'+S+'")':'re.match(r"'+S+'", '+e+")"}if(a==="find"){var P=j(i[0]||"");return n==="col"?"regexp_extract("+e+', "'+P+'", 0)':'re.search(r"'+P+'", '+e+").group(0)"}if(a==="split"){var k=j(i[0]||",");return n==="col"?"split("+e+', "'+k+'")':e+'.split("'+k+'")'}if(a==="substringbefore"){var D=j(i[0]||"");return n==="col"?"substring_index("+e+', "'+D+'", 1)':e+'.split("'+D+'")[0]'}if(a==="substringafter"){var C=j(i[0]||"");return n==="col"?"substring_index("+e+', "'+C+'", -1)':'"'+C+'".join('+e+'.split("'+C+'")[1:])'}if(a==="padleft"||a==="leftpad"){var f=i[0]||"10",g=j(i[1]||" ");return n==="col"?"lpad("+e+", "+f+', "'+g+'")':e+".rjust("+f+', "'+g+'")'}if(a==="padright"||a==="rightpad"){var h=i[0]||"10",v=j(i[1]||" ");return n==="col"?"rpad("+e+", "+h+', "'+v+'")':e+".ljust("+h+', "'+v+'")'}if(a==="indexof"){var $=j(i[0]||"");return n==="col"?'locate("'+$+'", '+e+")":e+'.find("'+$+'")'}if(a==="getdelimitedfield"){var E=i[0]||"1",R=j(i[1]||",");return n==="col"?"split("+e+', "'+R+'")['+(parseInt(E)-1)+"]":e+'.split("'+R+'")['+(parseInt(E)-1)+"]"}if(a==="append"){var A=j(i[0]||"");return n==="col"?"concat("+e+', lit("'+A+'"))':e+' + "'+A+'"'}if(a==="prepend"){var T=j(i[0]||"");return n==="col"?'concat(lit("'+T+'"), '+e+")":'"'+T+'" + '+e}if(a==="equals"){var N=j(i[0]||"");return n==="col"?"("+e+' == lit("'+N+'"))':"("+e+' == "'+N+'")'}if(a==="equalsignorecase"){var U=j(i[0]||"");return n==="col"?"(lower("+e+') == lit("'+U.toLowerCase()+'"))':"("+e+'.lower() == "'+U.toLowerCase()+'")'}if(a==="plus"){var z=i[0]||"0";return n==="col"?"("+e+" + lit("+z+"))":"("+e+" + "+z+")"}if(a==="minus"){var Q=i[0]||"0";return n==="col"?"("+e+" - lit("+Q+"))":"("+e+" - "+Q+")"}if(a==="multiply"){var w=i[0]||"1";return n==="col"?"("+e+" * lit("+w+"))":"("+e+" * "+w+")"}if(a==="divide"){var F=i[0]||"1";return n==="col"?"("+e+" / lit("+F+"))":"("+e+" / "+F+")"}if(a==="mod"){var I=i[0]||"1";return n==="col"?"("+e+" % lit("+I+"))":"("+e+" % "+I+")"}if(a==="gt"){var L=i[0]||"0";return n==="col"?"("+e+" > lit("+L+"))":"("+e+" > "+L+")"}if(a==="lt"){var G=i[0]||"0";return n==="col"?"("+e+" < lit("+G+"))":"("+e+" < "+G+")"}if(a==="ge"){var J=i[0]||"0";return n==="col"?"("+e+" >= lit("+J+"))":"("+e+" >= "+J+")"}if(a==="le"){var Y=i[0]||"0";return n==="col"?"("+e+" <= lit("+Y+"))":"("+e+" <= "+Y+")"}if(a==="isempty")return n==="col"?"("+e+".isNull() | ("+e+' == lit("")))':"(not "+e+")";if(a==="isnull")return n==="col"?e+".isNull()":"("+e+" is None)";if(a==="notnull")return n==="col"?e+".isNotNull()":"("+e+" is not None)";if(a==="not")return n==="col"?"(~"+e+")":"(not "+e+")";if(a==="and"){var W=i[0]?Z(i[0],n):"lit(True)";return n==="col"?"("+e+" & "+W+")":"("+e+" and "+W+")"}if(a==="or"){var te=i[0]?Z(i[0],n):"lit(False)";return n==="col"?"("+e+" | "+te+")":"("+e+" or "+te+")"}if(a==="ifelse"){var ie=i[0]?Z(i[0],n):"lit(True)",Ve=i[1]?Z(i[1],n):"lit(None)";return n==="col"?"when("+e+", "+ie+").otherwise("+Ve+")":"("+ie+" if "+e+" else "+Ve+")"}if(a==="format"){var Xe=j(i[0]||"yyyy-MM-dd");return n==="col"?"date_format("+e+', "'+Xe+'")':e+'.strftime("'+ct(Xe)+'")'}if(a==="todate"){var Ye=j(i[0]||"yyyy-MM-dd");return n==="col"?"to_timestamp("+e+', "'+Ye+'")':"datetime.strptime("+e+', "'+ct(Ye)+'")'}if(a==="tonumber")return n==="col"?e+'.cast("long")':"int("+e+")";if(a==="tostring")return n==="col"?e+'.cast("string")':"str("+e+")";if(a==="grok"){var Ht=j(i[0]||""),Ze=xn(Ht);return n==="col"?"regexp_extract("+e+', "'+Ze+'", 0)':'re.search(r"'+Ze+'", '+e+")"}return e+"  /* NEL: "+t+" */"}function Lt(e,t){var n=Tn(e);if(n.length===0)return'""';var s=n[0].trim(),a=n.slice(1),i;if(/^now\(\)$/i.test(s))i=t==="col"?"current_timestamp()":"datetime.now()";else if(/^UUID\(\)$/i.test(s))i=t==="col"?'expr("uuid()")':"str(uuid.uuid4())";else if(/^hostname\(\)$/i.test(s))i=t==="col"?'lit(spark.conf.get("spark.databricks.clusterUsageTags.clusterName", "unknown"))':"socket.gethostname()";else if(/^nextInt\(\)$/i.test(s)||/^random\(\)$/i.test(s))i=t==="col"?'(rand() * 2147483647).cast("int")':"random.randint(0, 2147483647)";else if(/^literal\('([^']*)'\)$/i.test(s)){var c=s.match(/^literal\('([^']*)'\)$/i)[1];i=t==="col"?'lit("'+c+'")':'"'+c+'"'}else if(/^literal\((\d+)\)$/i.test(s)){var o=s.match(/^literal\((\d+)\)$/i)[1];i=t==="col"?"lit("+o+")":o}else if(/^math:abs\((.+)\)$/i.test(s)){var r=s.match(/^math:abs\((.+)\)$/i)[1];i="abs("+Z(r,t)+")"}else if(/^math:ceil\((.+)\)$/i.test(s))i=t==="col"?"ceil("+Z(s.match(/^math:ceil\((.+)\)$/i)[1],t)+")":"math.ceil("+Z(s.match(/^math:ceil\((.+)\)$/i)[1],t)+")";else if(/^math:floor\((.+)\)$/i.test(s))i=t==="col"?"floor("+Z(s.match(/^math:floor\((.+)\)$/i)[1],t)+")":"math.floor("+Z(s.match(/^math:floor\((.+)\)$/i)[1],t)+")";else if(/^math:round\((.+)\)$/i.test(s))i=t==="col"?"round("+Z(s.match(/^math:round\((.+)\)$/i)[1],t)+")":"round("+Z(s.match(/^math:round\((.+)\)$/i)[1],t)+")";else{var l=Ft(s);t==="col"?i=l.type==="column"?'col("'+s+'")':l.code:i=l.type==="column"?'_attrs.get("'+s+'", "")':(l.type==="secret",l.code)}for(var p=0;p<a.length;p++)i=An(i,a[p].trim(),t);return i}function In(e){const t={};return e._rawXml&&((e._rawXml.match(/<variable\s+name="([^"]+)"\s+value="([^"]*)"\s*\/>/g)||[]).forEach(a=>{const i=a.match(/name="([^"]+)"/)[1],c=a.match(/value="([^"]*)"/)[1];t[i]=c}),(e._rawXml.match(/<parameter>\s*<name>([^<]+)<\/name>\s*<value>([^<]*)<\/value>/g)||[]).forEach(a=>{const i=a.match(/<name>([^<]+)<\/name>/)[1],c=a.match(/<value>([^<]*)<\/value>/);c&&(t[i]=c[1])})),t}function Fn(e,t){if(!e||!t||Object.keys(t).length===0)return e;let n=e;for(const[s,a]of Object.entries(t))n=n.replace(new RegExp("\\$\\{"+s.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")+"\\}","g"),a),n=n.replace(new RegExp("#\\{"+s.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")+"\\}","g"),a);return n}function Ln(e,t){return!e||typeof e!="string"||(t=t||"col",!e.includes("${"))?e:e.replace(/\$\{([^}]+)\}/g,function(n,s){return Lt(s.trim(),t)})}function Nt(e,t){const n=Dn(e);if(!n)throw new Error("Empty or invalid file content");const s=n.trimStart();if(s.startsWith("<")||s.startsWith("<?xml")){const i=new DOMParser().parseFromString(n,"text/xml"),c=i.querySelector("parsererror");if(c)throw new Error("XML parse error: "+c.textContent.substring(0,200));const o=at(i);return{source_name:t,source_type:"nifi_xml",_nifi:{processors:o.processors,connections:o.connections,controllerServices:o.controllerServices,processGroups:o.processGroups,idToName:o.idToName,clouderaTools:[],deepPropertyInventory:{filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set},sqlTables:[],sqlTableMeta:{}},tables:o.tables,parse_warnings:[],_deferredProcessorWork:null,_rawXml:n}}if(s.startsWith("{")||s.startsWith("[")){let a;try{a=JSON.parse(n)}catch(c){throw new Error("JSON parse error: "+c.message)}const i=a.flowContents||a.flow?.flowContents||a.processGroupFlow?.flow||a;return it(i,t)}try{const i=new DOMParser().parseFromString(n,"text/xml");if(!i.querySelector("parsererror")){const c=at(i,t);return{source_name:t,source_type:"nifi_xml",_nifi:{processors:c.processors,connections:c.connections,controllerServices:c.controllerServices,processGroups:c.processGroups,idToName:c.idToName,clouderaTools:[],deepPropertyInventory:{filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set},sqlTables:[],sqlTableMeta:{}},tables:c.tables,parse_warnings:[],_deferredProcessorWork:null,_rawXml:n}}}catch{}try{const a=JSON.parse(n),i=a.flowContents||a.flow?.flowContents||a.processGroupFlow?.flow||a;return it(i,t)}catch{throw new Error("Unable to parse file as XML or JSON. Ensure the file is a valid NiFi template, flow definition, or registry export.")}}const ne={filePath:/(?:\/[\w${}._-]+){2,}/g,urlPattern:/https?:\/\/[^\s"',;]+/gi,jdbcUrl:/jdbc:[a-z0-9]+:[^\s"',;]+/gi,nifiEL:/\$\{[^}]+\}/g,cronExpr:/(?:^|\s)((?:\*|[0-9]+(?:[-/,][0-9]+)*)\s+){4,5}(?:\*|[0-9]+(?:[-/,][0-9]+)*)/,hostPort:/([a-zA-Z0-9][-a-zA-Z0-9.]+\.[a-zA-Z]{2,})(?::(\d{2,5}))?/g,credentialKey:/password|secret|token|api[_-]?key|credential|private[_-]?key|auth/i,dataFormat:/avro|parquet|orc|json|csv|tsv|xml|protobuf|thrift|msgpack|yaml|excel|xlsx/i},pt=new Set(["now","nextInt","UUID","hostname","IP","literal","thread","format","toDate","substring","substringBefore","substringAfter","replace","replaceAll","replaceFirst","replaceEmpty","replaceNull","toUpper","toLower","trim","length","isEmpty","equals","equalsIgnoreCase","contains","startsWith","endsWith","append","prepend","plus","minus","multiply","divide","mod","gt","ge","lt","le","and","or","not","ifElse","toString","toNumber","math","getStateValue","count","padLeft","padRight","escapeJson","escapeXml","escapeCsv","unescapeJson","unescapeXml","urlEncode","urlDecode","base64Encode","base64Decode","toRadix","jsonPath","jsonPathDelete","jsonPathAdd","jsonPathSet","jsonPathPut"]);function Ot(e){const t={},n={},s={},a={},i={},c=[],o=[],r=[],l=[],p=[],d=[],u=new Set;function m(f,g,h){!f||f.includes("${")||(t[f]||(t[f]={type:g,processors:[]}),t[f].processors.push(h))}function y(f,g,h,v){f&&(n[f]||(n[f]={producers:[],consumers:[],format:v||"unknown"}),g==="read"?n[f].consumers.push(h):n[f].producers.push(h))}function _(f,g,h){if(!f||f==="dual"||f.length<2)return;const v=f.replace(/^["'`]+|["'`]+$/g,"").trim();!v||/^(waiting|because|account|log|Dates|LeadLag|startGrouping|grps|Temptation)$/i.test(v)||(s[v]||(s[v]={readers:[],writers:[]}),g==="read"?s[v].readers.push(h):s[v].writers.push(h))}function b(f,g){if(!f||typeof f!="string")return;const h=f.match(/\$\{([^}]+)\}/g);h&&h.forEach(v=>{const E=v.slice(2,-1).split(":")[0].trim(),R=E.replace(/\(.*$/,"");!pt.has(E)&&!pt.has(R)&&!E.includes(".")&&!/^(filename|path|absolute\.path|uuid|fileSize|file\.size|entryDate|lineageStartDate|flowfile)/.test(E)&&p.push({expr:v,processor:g,attrName:E,resolved:!1})})}(e.connections||[]).forEach(f=>{f.sourceType==="PROCESSOR"&&u.add(f.sourceId||f.sourceName),f.destinationType==="PROCESSOR"&&u.add(f.destinationId||f.destinationName),u.add(f.sourceName),u.add(f.destinationName)}),(e.processors||[]).forEach(f=>{const g=f.properties||{},h=f.name,v=f.type;if(Object.entries(g).forEach(([$,E])=>b(E,h)),v==="GetFile"){const $=g["Input Directory"];m($,"input",h),$&&y($+"/*.csv","read",h,"csv")}else if(v==="ListFile"){const $=g["Input Directory"];m($,"input",h)}else if(v==="FetchFile"){const $=g["File to Fetch"]||g.Filename;y($||"(dynamic)","read",h)}else if(v==="PutFile"){const $=g.Directory;m($,"output",h)}else if(v==="PutSFTP"){const $=g.Hostname||"sftp-host",E=g["Remote Path"]||"/";m(`sftp://${$}${E}`,"output",h),y(`sftp://${$}${E}/(dynamic)`,"write",h)}else if(v==="PutHDFS"||v==="PutParquet"){const $=g.Directory||g.directory;m($,"output",h)}else if(v==="ExecuteSQL"||v==="ExecuteSQLRecord"){const E=(g["SQL select query"]||g["sql-select-query"]||"").match(/(?:FROM|JOIN)\s+([\w$.{}"]+)/gi);E&&E.forEach(A=>{const T=A.replace(/^(FROM|JOIN)\s+/i,"").trim();_(T,"read",h)});const R=g["Database Connection Pooling Service"]||g["dbcp-service"];R&&r.push({name:R,processor:h,type:"read"})}else if(v==="PutDatabaseRecord"||v==="PutSQL"){const $=g["Table Name"]||g["table-name"]||g["put-db-record-table-name"];_($,"write",h);const E=g["Database Connection Pooling Service"]||g["dbcp-service"];E&&r.push({name:E,processor:h,type:"write"})}else if(v==="ExecuteStreamCommand"){const $=g.Command||"",E=g["Command Arguments"]||"";$&&l.push({path:$,args:E,processor:h,_cloudera:null});const R=($+" "+E).toLowerCase();if(/hdfs\s+dfs|dfs;/.test(R)||/^dfs$/.test($.trim())){const A=E.match(/(?:-cp|-mv|-put|-get|-ls|-rm|-mkdir|-cat|-chmod|-chown|-touchz)\s*;?\s*([^\s;]+)/);A&&m(A[1],/(-ls|-cat|-get)/.test(E)?"input":"output",h);const T=E.match(/(?:-put|-cp)\s*;?\s*[^\s;]+\s*;?\s*([^\s;]+)/);T&&m(T[1],"output",h)}if(/impala-shell|impala/.test(R)){const A=E.match(/(?:refresh|invalidate\s+metadata|compute\s+stats|from|join|into|insert\s+into|insert\s+overwrite)\s+[;]?\s*([\w.${}]+)/gi);A&&A.forEach(T=>{const N=T.replace(/^(refresh|invalidate\s+metadata|compute\s+stats|from|join|into|insert\s+into|insert\s+overwrite)\s+;?\s*/i,"").trim().replace(/[";]/g,"");N&&N.length>2&&_(N,/insert|into/i.test(T)?"write":"read",h)})}if(/hive|beeline/.test(R)){const A=E.match(/(?:from|join|into|table)\s+([\w.]+)/gi);A&&A.forEach(T=>{const N=T.replace(/^(from|join|into|table)\s+/i,"").trim();N&&N.length>2&&_(N,/into/i.test(T)?"write":"read",h)})}if(/kinit|keytab|kerberos|klist/.test(R)&&l.length&&$&&(l[l.length-1]._cloudera="kerberos"),/sqoop/.test(R)){const A=E.match(/--table\s+(\S+)/);A&&_(A[1],/export/.test(R)?"write":"read",h)}if(/sqlline\.py|phoenix/.test(R)){const A=E.match(/(?:from|into|table|upsert\s+into)\s+([\w.]+)/gi);A&&A.forEach(T=>{const N=T.replace(/^(from|into|table|upsert\s+into)\s+/i,"").trim();N&&N.length>2&&_(N,/into|upsert/i.test(T)?"write":"read",h)})}if(/presto|trino/.test(R)){const A=E.match(/(?:from|join|into)\s+([\w.]+)/gi);A&&A.forEach(T=>{const N=T.replace(/^(from|join|into)\s+/i,"").trim();N&&N.length>2&&_(N,/into/i.test(T)?"write":"read",h)})}}else if(v==="InvokeHTTP"){const $=g["Remote URL"]||g["remote-url"]||"",E=g["HTTP Method"]||g["http-method"]||"GET";$&&c.push({url:$,method:E,processor:h})}else if(v==="ConsumeKafka_2_6"||v==="ConsumeKafka"||v==="ConsumeKafkaRecord_2_6"){const $=g["Topic Name(s)"]||g.topic||"",E=g["Kafka Brokers"]||g["bootstrap.servers"]||"";$&&o.push({topic:$,brokers:E,processor:h,direction:"consume"})}else if(v==="PublishKafka_2_6"||v==="PublishKafka"||v==="PublishKafkaRecord_2_6"){const $=g["Topic Name(s)"]||g.topic||"",E=g["Kafka Brokers"]||g["bootstrap.servers"]||"";$&&o.push({topic:$,brokers:E,processor:h,direction:"produce"})}else if(v==="Wait"){const $=g["Signal Counter Name"]||"",E=parseInt(g["Target Signal Count"])||1;$&&(i[$]||(i[$]={senders:[],waiters:[],target:E}),i[$].waiters.push(h))}else if(v==="Notify"){const $=g["Signal Counter Name"]||"";$&&(i[$]||(i[$]={senders:[],waiters:[],target:1}),i[$].senders.push(h))}else if(v==="ControlRate"){const $="rate_"+h.replace(/\s+/g,"_");a[$]={acquirers:[h],releasers:[h]}}else v==="LogMessage"?d.push({name:"log_"+h,processor:h}):(v==="LookupAttribute"||v==="LookupRecord")&&_("lookup_"+h.replace(/\s+/g,"_").toLowerCase(),"read",h)}),(e.controllerServices||[]).forEach(f=>{const g=f.properties||{},h=g["Database Connection URL"]||g["database-connection-url"]||"";(h||f.type.includes("DBCP")||f.type.includes("ConnectionPool"))&&r.push({name:f.name,url:h?h.replace(/password=[^&;]+/gi,"password=***"):"",processor:"(controller service)",type:"service"})});const S=(e.processors||[]).filter(f=>!u.has(f.name)&&!u.has(f.id)),P=[],k=new Set;p.forEach(f=>{const g=f.expr+"|"+f.processor;k.has(g)||(k.add(g),P.push(f))});const D=Object.keys(t).length+Object.keys(n).length+Object.keys(s).length+Object.keys(a).length+Object.keys(i).length+c.length+o.length+l.length+r.length+d.length,C=e.clouderaTools||[];return{directories:t,files:n,sqlTables:s,tokens:a,signals:i,httpEndpoints:c,kafkaTopics:o,dbConnections:r,scripts:l,clouderaTools:C,parameters:P,counters:d,disconnected:S,disconnectedProcessors:S.map(f=>({name:f.name,type:f.type,group:f.group||"(root)",noInbound:!0,noOutbound:!0})),totalResources:D,warnings:[...S.map(f=>`Processor "${f.name}" (${f.type}) has no connections — may never execute`),...P.filter(f=>!f.resolved).slice(0,10).map(f=>`Unresolved expression ${f.expr} in "${f.processor}"`),...r.filter(f=>f.type!=="service").length>0&&r.filter(f=>f.type==="service").length===0?["Processors reference DB connections but no DBCP controller service found"]:[],...C.length?[`${C.length} external system references detected — see External Systems & Tools inventory`]:[]]}}function Nn(e,t){const n=e.data_type.toLowerCase(),s={null_ratio:e.nullable?.05:0};if(e.check_constraints&&e.check_constraints.length){const a=e.check_constraints.length;return s.top_values=e.check_constraints.map(i=>({value:i,frequency:Math.round(1/a*1e4)/1e4})),s.distinct_count=a,s}if(["int","integer","smallint","tinyint"].includes(n))e.is_primary_key?Object.assign(s,{min:1,max:t,mean:t/2,stddev:t/6,distinct_count:t}):Object.assign(s,{min:1,max:1e3,mean:500,stddev:300,distinct_count:Math.min(500,t)});else if(["bigint","long"].includes(n))Object.assign(s,{min:1,max:1e5,mean:5e4,stddev:3e4,distinct_count:Math.min(1e4,t)});else if(["float","double"].includes(n))Object.assign(s,{min:0,max:1e4,mean:100,stddev:50,distinct_count:t});else if(["decimal","numeric"].includes(n)){const a=Math.pow(10,(e.precision||10)-(e.scale||2))-1;Object.assign(s,{min:0,max:a,mean:a/10,stddev:a/20,distinct_count:t})}else["varchar","char","text","string"].includes(n)?Object.assign(s,{min_length:3,max_length:Math.min(e.max_length||50,100),distinct_count:t}):n==="date"?Object.assign(s,{min:"2020-01-01",max:"2025-12-31",distinct_count:Math.min(t,2e3)}):["timestamp","datetime"].includes(n)?Object.assign(s,{min:"2020-01-01",max:"2025-12-31",distinct_count:t}):["boolean","bool"].includes(n)?(s.top_values=[{value:!0,frequency:.5},{value:!1,frequency:.5}],s.distinct_count=2):Object.assign(s,{min_length:5,max_length:20,distinct_count:t});return s}function We(e){const t=typeof crypto<"u"&&crypto.randomUUID?crypto.randomUUID():"bp-"+Math.random().toString(36).substring(2,10),n=e.tables.map(a=>{const i=a.row_count||1e3,c=a.columns.map(r=>({name:r.name,data_type:r.data_type,nullable:r.nullable,is_primary_key:r.is_primary_key,stats:Nn(r,i)})),o=a.foreign_keys.map(r=>({column:r.fk_column,references_table:r.referenced_table,references_column:r.referenced_column}));return{name:a.name,schema:a.schema,row_count:i,columns:c,foreign_keys:o}}),s=[];return e.tables.forEach(a=>a.foreign_keys.forEach(i=>s.push({from_table:`${a.schema}.${a.name}`,to_table:`${a.schema}.${i.referenced_table}`,relationship_type:"one_to_many",join_columns:[{from_column:i.fk_column,to_column:i.referenced_column}]}))),{blueprint_id:t,source_system:{name:e.source_name,type:e.source_type},tables:n,relationships:s}}function Qe(e){const t=e.processors||[],n=e.connections||[],s={},a={};t.forEach(r=>{s[r.name]=[],a[r.name]=[]}),n.forEach(r=>{const l=r.sourceName,p=r.destinationName;l&&p&&(s[l]||(s[l]=[]),a[p]||(a[p]=[]),s[l].includes(p)||s[l].push(p),a[p].includes(l)||a[p].push(l))});function i(r,l){const p=[],d=new Set,u=[...r[l]||[]];for(;u.length;){const m=u.shift();d.has(m)||(d.add(m),p.push(m),(r[m]||[]).forEach(y=>{d.has(y)||u.push(y)}))}return p}const c={},o={};return t.forEach(r=>{o[r.name]=i(a,r.name),c[r.name]=i(s,r.name)}),{downstream:s,upstream:a,fullDownstream:c,fullUpstream:o}}const Ae={kafka:{pip:["confluent-kafka"],dbx:"Pre-installed on DBR",desc:"Kafka"},oracle:{pip:["oracledb"],dbx:"JDBC driver via cluster library",desc:"Oracle Database"},mysql:{pip:["mysql-connector-python"],dbx:"JDBC driver via cluster library",desc:"MySQL"},postgresql:{pip:["psycopg2-binary"],dbx:"JDBC driver via cluster library",desc:"PostgreSQL"},sqlserver:{pip:["pymssql"],dbx:"JDBC driver via cluster library",desc:"SQL Server"},mongodb:{pip:["pymongo"],dbx:"MongoDB Spark Connector",desc:"MongoDB"},elasticsearch:{pip:["elasticsearch"],dbx:"Elasticsearch Spark library",desc:"Elasticsearch"},cassandra:{pip:["cassandra-driver"],dbx:"Spark Cassandra Connector",desc:"Cassandra"},redis:{pip:["redis"],dbx:"Custom library install",desc:"Redis"},hbase:{pip:[],dbx:"Delta Lake migration",desc:"HBase"},kudu:{pip:[],dbx:"Delta Lake (direct replacement)",desc:"Kudu"},s3:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS S3"},azure_blob:{pip:["azure-storage-blob"],dbx:"Pre-installed on DBR",desc:"Azure Blob"},azure_adls:{pip:["azure-storage-file-datalake"],dbx:"Pre-installed on DBR",desc:"Azure Data Lake"},gcs:{pip:["google-cloud-storage"],dbx:"GCS Spark connector",desc:"GCS"},hdfs:{pip:[],dbx:"dbutils.fs (pre-installed)",desc:"HDFS"},sftp:{pip:["paramiko"],dbx:"Volumes-based staging",desc:"SFTP/FTP"},http:{pip:["requests"],dbx:"Pre-installed on DBR",desc:"HTTP/REST"},email:{pip:["sendgrid"],dbx:"Webhook notification",desc:"Email"},mqtt:{pip:["paho-mqtt"],dbx:"Custom library",desc:"MQTT"},jms:{pip:["stomp.py"],dbx:"Custom library",desc:"JMS/AMQP"},snowflake:{pip:["snowflake-connector-python"],dbx:"Snowflake Spark Connector",desc:"Snowflake"},neo4j:{pip:["neo4j"],dbx:"Neo4j Spark Connector",desc:"Neo4j"},splunk:{pip:["splunklib"],dbx:"Splunk Spark Add-on",desc:"Splunk"},influxdb:{pip:["influxdb-client"],dbx:"Custom library",desc:"InfluxDB"},solr:{pip:["pysolr"],dbx:"Solr Spark library",desc:"Solr"},hive:{pip:[],dbx:"Pre-installed (Spark SQL)",desc:"Hive"},iceberg:{pip:[],dbx:"Pre-installed on DBR 13+",desc:"Iceberg"},teradata:{pip:["teradatasql"],dbx:"JDBC driver",desc:"Teradata"},slack:{pip:["slack-sdk"],dbx:"Webhook integration",desc:"Slack"},kerberos:{pip:[],dbx:"Unity Catalog identity federation",desc:"Kerberos"},azure_eventhub:{pip:["azure-eventhub"],dbx:"Pre-installed on DBR",desc:"Azure Event Hubs"},azure_servicebus:{pip:["azure-servicebus"],dbx:"Custom library install",desc:"Azure Service Bus"},azure_cosmos:{pip:[],dbx:"Pre-installed (Cosmos Spark connector)",desc:"Azure Cosmos DB"},azure_queue:{pip:["azure-storage-queue"],dbx:"Custom library install",desc:"Azure Queue Storage"},gcp_pubsub:{pip:["google-cloud-pubsub"],dbx:"Custom library install",desc:"GCP Pub/Sub"},gcp_bigquery:{pip:["google-cloud-bigquery"],dbx:"BigQuery Spark connector",desc:"GCP BigQuery"},clickhouse:{pip:["clickhouse-driver"],dbx:"ClickHouse JDBC driver",desc:"ClickHouse"},druid:{pip:[],dbx:"Druid JDBC driver",desc:"Apache Druid"},hudi:{pip:[],dbx:"Pre-installed on DBR 13+",desc:"Apache Hudi"},kinesis:{pip:["boto3"],dbx:"Kinesis Spark connector",desc:"AWS Kinesis"},cloudwatch:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS CloudWatch"},sqs:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS SQS"},sns:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS SNS"},dynamodb:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS DynamoDB"},lambda_aws:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS Lambda"},pagerduty:{pip:["pdpyras"],dbx:"Custom library install",desc:"PagerDuty"},opsgenie:{pip:["opsgenie-sdk"],dbx:"Custom library install",desc:"OpsGenie"},telegram:{pip:[],dbx:"HTTP API (no SDK needed)",desc:"Telegram"},geoip:{pip:["geoip2"],dbx:"Custom library install",desc:"GeoIP"},exchange:{pip:["exchangelib"],dbx:"Custom library install",desc:"Microsoft Exchange"},whois:{pip:["python-whois"],dbx:"Custom library install",desc:"WHOIS"},snmp:{pip:["pysnmp"],dbx:"Custom library install",desc:"SNMP"},datadog:{pip:["datadog-api-client"],dbx:"Datadog integration",desc:"Datadog"},prometheus:{pip:["prometheus-client"],dbx:"Custom library install",desc:"Prometheus"},grafana:{pip:[],dbx:"Grafana REST API",desc:"Grafana"},phoenix:{pip:[],dbx:"Phoenix JDBC driver",desc:"Apache Phoenix"},cockroachdb:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"CockroachDB"},timescaledb:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"TimescaleDB"},greenplum:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"Greenplum"},vertica:{pip:["vertica-python"],dbx:"Vertica JDBC driver",desc:"Vertica"},saphana:{pip:["hdbcli"],dbx:"SAP HANA JDBC driver",desc:"SAP HANA"},presto:{pip:[],dbx:"Presto JDBC driver",desc:"Presto"},trino:{pip:["trino"],dbx:"Trino JDBC driver",desc:"Trino"}};function De(e){const t={},n=e.processors||[],s=e.deepPropertyInventory||{},a=[[/Kafka/i,"Apache Kafka","kafka"],[/Oracle/i,"Oracle Database","oracle"],[/MySQL/i,"MySQL","mysql"],[/Postgres/i,"PostgreSQL","postgresql"],[/Mongo/i,"MongoDB","mongodb"],[/Elastic/i,"Elasticsearch","elasticsearch"],[/Cassandra/i,"Cassandra","cassandra"],[/HBase/i,"HBase","hbase"],[/Kudu/i,"Apache Kudu","kudu"],[/Hive/i,"Hive","hive"],[/HDFS|Hadoop/i,"HDFS","hdfs"],[/S3/i,"AWS S3","s3"],[/Azure.*Blob/i,"Azure Blob","azure_blob"],[/Azure.*Lake|ADLS/i,"Azure Data Lake","azure_adls"],[/Azure.*Event/i,"Azure Event Hubs","azure_eventhub"],[/GCS|BigQuery/i,"Google Cloud","gcs"],[/Snowflake/i,"Snowflake","snowflake"],[/Redis/i,"Redis","redis"],[/Solr/i,"Solr","solr"],[/SFTP|FTP/i,"SFTP/FTP","sftp"],[/HTTP|REST/i,"HTTP/REST","http"],[/JMS/i,"JMS","jms"],[/AMQP/i,"AMQP","amqp"],[/MQTT/i,"MQTT","mqtt"],[/Email|SMTP/i,"Email","email"],[/Syslog/i,"Syslog","syslog"],[/Slack/i,"Slack","slack"],[/Splunk/i,"Splunk","splunk"],[/InfluxDB/i,"InfluxDB","influxdb"],[/Neo4j/i,"Neo4j","neo4j"],[/Teradata/i,"Teradata","teradata"],[/Iceberg/i,"Iceberg","iceberg"],[/SQL|Database|JDBC/i,"SQL/JDBC","sql_jdbc"]];return n.forEach(i=>{const c=/^(Get|List|Consume|Listen|Fetch|Query|Scan|Select)/i.test(i.type)?"READ":/^(Put|Publish|Send|Post|Insert|Write|Delete)/i.test(i.type)?"WRITE":"READ/WRITE";a.forEach(([o,r,l])=>{o.test(i.type)&&(t[l]||(t[l]={name:r,key:l,processors:[],jdbcUrls:[],credentials:[],packages:[]}),t[l].processors.push({name:i.name,type:i.type,direction:c,group:i.group}))})}),s.jdbcUrls&&Object.keys(s.jdbcUrls).forEach(i=>{const c=i.match(/jdbc:(\w+):/);if(c){const o=c[1].toLowerCase(),r={oracle:"oracle",mysql:"mysql",postgresql:"postgresql",sqlserver:"sqlserver",hive2:"hive",teradata:"teradata",snowflake:"snowflake"}[o]||"sql_jdbc";t[r]||(t[r]={name:Ae[r]?Ae[r].desc:r,key:r,processors:[],jdbcUrls:[],credentials:[],packages:[]}),t[r].jdbcUrls.push(i)}}),s.credentialRefs&&Object.keys(s.credentialRefs).forEach(i=>{Object.values(t).forEach(c=>{const o=c.processors.map(l=>l.name),r=s.credentialRefs[i];Array.isArray(r)&&r.some(l=>o.includes(l))&&c.credentials.push(i)})}),Object.values(t).forEach(i=>{const c=Ae[i.key];i.dbxApproach=c?c.dbx:"Custom implementation",i.packages=c?c.pip:[]}),t}function B(e){return e?e.replace(/[^a-zA-Z0-9_]/g,"_").replace(/^(\d)/,"_$1").toLowerCase().substring(0,40):"_empty"}function On(e,t,n,s,a,i,c,o){let r=e.tpl.replace(/\{v\}/g,t).replace(/\{in\}/g,n).replace(/\{in1\}/g,n).replace(/\{in2\}/g,s[1]?B(s[1]):"input2");return Object.entries(a).forEach(([l,p])=>{const d=l.replace(/\s+/g,"_").toLowerCase();r=r.replace(new RegExp("\\{"+d+"\\}","gi"),p)}),r.includes("${")&&(r=i(r,"python")),r.includes("${")&&(r=c(r,o)),{code:r,conf:e.conf}}const Mn=/^(ConsumeKafka|ConsumeKafkaRecord|ListenHTTP|ListenTCP|ListenUDP|ListenSyslog|ListenRELP|ListenSMTP|ListenGRPC|ListenWebSocket|ConsumeJMS|ConsumeMQTT|ConsumeAMQP|ConsumeGCPubSub|ConsumeAzureEventHub|ConsumeAzureServiceBus|ConsumeKinesisStream|TailFile|GetHTTP|GenerateFlowFile)/,qn=/\.toPandas\(\)|for\s+row\s+in\s+df_\w+\.(?:limit\(\d+\)\.)?collect\(\)|df_\w+\.limit\(\d+\)\.toPandas|df_\w+\.count\(\)|\.write\.format\(|\.saveAsTable\(|\.save\(|\.show\(|\.display\(/,jn=/^(ExecuteSQL|PutDatabaseRecord|PutFile|PutFTP|PutSFTP|PutHDFS|FetchFile|QueryDatabaseTable|ListDatabaseTables|GenerateTableFetch)$/;function Bn(e,t){const n=new Set;e.forEach(i=>{Mn.test(i.type)&&n.add(i.name)});const s={};e.forEach(i=>{s[i.name]=i.type});let a=!0;for(;a;)a=!1,t.forEach(i=>{if(n.has(i.sourceName)&&!n.has(i.destinationName)){const c=s[i.destinationName]||"";if(jn.test(c))return;n.add(i.destinationName),a=!0}});return n}function Un(e,t,n,s,a){if(!a||!qn.test(e))return e;let i=e;if(/for\s+row\s+in\s+df_\w+/.test(i)){const c=i.indexOf("for row");if(c>=0){const o=i.substring(0,c),l=i.substring(c).split(`
`).map(p=>"    "+p).join(`
`);i=o+`# Streaming-safe sink using foreachBatch
def _process_batch_`+n+`(batch_df, batch_id):
    """Process each micro-batch (streaming-safe)"""
`+l.replace(/df_\w+\.(?:limit\(\d+\)\.)?collect\(\)/g,"batch_df.collect()")+`

(df_`+s+`.writeStream
    .foreachBatch(_process_batch_`+n+`)
    .option("checkpointLocation", "/tmp/checkpoints/`+n+`")
    .trigger(processingTime="10 seconds")
    .start()
)`}}return/\.toPandas\(\)\.to_dict/.test(i)&&(i=i.replace(/df_(\w+)(?:\.limit\(\d+\))?\.toPandas\(\)\.to_dict\(orient="records"\)/g,"None  # Resolved in foreachBatch below"),i=`# Streaming-safe: collect via foreachBatch, not .toPandas()
def _process_batch_`+n+`(batch_df, batch_id):
    _records = batch_df.toPandas().to_dict(orient="records")
    # Process _records here
    return _records

`+i),i=i.replace(/df_(\w+)\.count\(\)/g,"0  # Cannot call .count() on streaming DataFrame; use watermark/window aggregation"),i=i.replace(/df_(\w+)\.show\([^)]*\)/g,"# Cannot call .show() on streaming DataFrame — use display() in notebook"),i=i.replace(/display\(df_(\w+)\)/g,"# display() streams automatically in Databricks notebooks"),/\.write\.format\(/.test(i)&&!/\.writeStream/.test(i)&&(i=i.replace(/(\w+)\.write\.format\(([^)]+)\)((?:\s*\.option\([^)]+\))*)\s*\.(?:save|mode)\(/g,function(c,o,r,l){return o+".writeStream.format("+r+")"+l+`
    .option("checkpointLocation", "/Volumes/<catalog>/<schema>/checkpoints/`+n+`")
    .trigger(availableNow=True)
    .start(`})),/\.write\.(?:mode\([^)]+\)\.)?saveAsTable\(/.test(i)&&!/\.writeStream/.test(i)&&(i=i.replace(/\.write\.(?:mode\([^)]+\)\.)?saveAsTable\(([^)]+)\)/g,`.writeStream
    .option("checkpointLocation", "/Volumes/<catalog>/<schema>/checkpoints/`+n+`")
    .trigger(availableNow=True)
    .toTable($1)`)),/\.writeStream/.test(i)&&!/checkpointLocation/.test(i)&&(i=i.replace(/\.writeStream/g,`.writeStream
    .option("checkpointLocation", "/Volumes/<catalog>/<schema>/checkpoints/`+n+'")')),i}function Gn(e,t){if(!t||!e)return null;const n=t.find(i=>i.name===e||i.type.includes(e));if(!n)return null;const s=n.properties||{},a={name:n.name,type:n.type,props:{}};return/DBCP|ConnectionPool/i.test(n.type)&&(a.jdbcUrl=s["Database Connection URL"]||"",a.driver=s["Database Driver Class Name"]||"",a.user=s["Database User"]||"",a.maxConns=s["Max Total Connections"]||"10",/oracle/i.test(a.jdbcUrl)?a.dbType="oracle":/postgresql/i.test(a.jdbcUrl)?a.dbType="postgresql":/mysql/i.test(a.jdbcUrl)?a.dbType="mysql":/sqlserver/i.test(a.jdbcUrl)?a.dbType="sqlserver":/hive/i.test(a.jdbcUrl)?a.dbType="hive":a.dbType="jdbc"),/SSL/i.test(n.type)&&(a.keystore=s["Keystore Filename"]||"",a.truststore=s["Truststore Filename"]||"",a.protocol=s["SSL Protocol"]||"TLS"),/Cache/i.test(n.type)&&(a.cacheHost=s["Server Hostname"]||"localhost",a.cachePort=s["Server Port"]||"4557"),/Reader|Writer/i.test(n.type)&&(a.schemaStrategy=s["Schema Access Strategy"]||"Infer Schema",/CSV/i.test(n.type)?a.format="csv":/Json/i.test(n.type)?a.format="json":/Avro/i.test(n.type)?a.format="avro":/Parquet/i.test(n.type)&&(a.format="parquet")),a}function Hn(e,t,n){if(!e||!e.includes("${"))return e;var s=e.split(`
`),a=s.map(function(o){if(o.trim().charAt(0)==="#"||!o.includes("${"))return o;var r=n(o,"python");return r}),i=a.join(`
`);if(i.indexOf("_attrs.get(")>=0&&i.indexOf("_attrs =")<0&&i.indexOf("_attrs=")<0){var c=t.replace(/[^a-zA-Z0-9_]/g,"_");i=`# Resolve NiFi FlowFile attributes from upstream DataFrame
_attrs = {}
try:
    _first_row = df_`+c+`_input.first()
    if _first_row: _attrs = _first_row.asDict()
except: pass

`+i}return i}function zn(e,t,n,s,a,i){let c=a,o=i;if(e.type==="ListenHTTP"||e.type==="HandleHttpRequest"){const r=t["Listening Port"]||t.Port||"8080",l=t["Base Path"]||"/api/v1";return c="# HTTP Endpoint: "+e.name+`
# NiFi HTTP listener on port `+r+", path: "+l+`
#
# DO NOT run a blocking web server (Flask/FastAPI) in a notebook cell.
# It will hang indefinitely and block all downstream execution.
#
# OPTION 1 (RECOMMENDED): Databricks Model Serving Endpoint
# Deploy as an MLflow model serving endpoint that writes to Delta table.
# See: https://docs.databricks.com/machine-learning/model-serving/
#
# OPTION 2: Databricks Apps (Gradio/Streamlit on port 8080)
# See: databricks.yml app deployment
#
# OPTION 3: External API Gateway -> Databricks Job trigger
# AWS API Gateway / Azure APIM -> triggers Databricks Job via REST API
#
# Implementation: Read from Delta landing table (populated by serving endpoint)
df_`+n+` = (spark.readStream
    .format("delta")
    .table("`+n+`_incoming")
)
print(f"[HTTP] Endpoint: streaming from `+n+'_incoming Delta table")',o=.92,{code:c,conf:o}}if(/^Consume(Kafka|KafkaRecord)/.test(e.type)){const r=t["Kafka Brokers"]||t["bootstrap.servers"]||"kafka:9092",l=t["Topic Name(s)"]||t.topic||"default_topic",p=t["Group ID"]||t["group.id"]||"consumer_group",d=t["Offset Reset"]||t["auto.offset.reset"]||"earliest",u=t["Security Protocol"]||"";let m="";return u.includes("SASL")&&(m=`
  .option("kafka.security.protocol", "${u}")
  .option("kafka.sasl.mechanism", "PLAIN")
  .option("kafka.sasl.jaas.config", f"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\\"{dbutils.secrets.get(scope='kafka', key='user')}\\\\" password=\\\\"{dbutils.secrets.get(scope='kafka', key='pass')}\\\\";")`),c=`# Kafka Consumer: ${e.name}
# Topic: ${l} | Group: ${p} | Brokers: ${r}
df_${n} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "${r}")
  .option("subscribe", "${l}")
  .option("kafka.group.id", "${p}")
  .option("startingOffsets", "${d}")
  .option("maxOffsetsPerTrigger", 10000)${m}
  .load()
  .selectExpr("CAST(key AS STRING) as key", "CAST(value AS STRING) as value", "topic", "partition", "offset", "timestamp")
)
print(f"[KAFKA] Consuming from ${l} with group ${p}")`,o=.95,{code:c,conf:o}}if(/^(Get|Fetch|List)(SFTP|FTP)$/.test(e.type)){const r=t.Hostname||"sftp.example.com",l=t.Port||"22",p=t.Username||"sftp_user",d=t["Remote Path"]||"/",u=t["File Filter Regex"]||t["File Filter"]||"*";return c=`# ${e.type}: ${e.name}
# Host: ${r}:${l} | Path: ${d} | Filter: ${u}
import paramiko
_ssh = paramiko.SSHClient()
_ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
_ssh.connect("${r}", port=int("${l}"), username="${p}",
    password=dbutils.secrets.get(scope="sftp", key="password"))
_sftp = _ssh.open_sftp()

import re as _re
_files = [f for f in _sftp.listdir("${d}") if _re.match(r"${u}", f)]
_data = []
for _fname in _files:
    with _sftp.open(f"${d}/{_fname}") as _f:
        _data.append({"filename": _fname, "content": _f.read().decode("utf-8", errors="replace")})

df_${n} = spark.createDataFrame(_data) if _data else spark.createDataFrame([], "filename STRING, content STRING")
_sftp.close()
_ssh.close()
print(f"[SFTP] Fetched {len(_files)} files from ${r}:${d}")`,o=.9,{code:c,conf:o}}if(/^(ExecuteSQL|QueryDatabase|GenerateTableFetch)/.test(e.type)){const r=t["Database Connection Pooling Service"]||t["JDBC Connection Pool"]||"",l=t["SQL select query"]||t["SQL Statement"]||"",p=t["Table Name"]||"",d=t["Max Rows Per Flow File"]||"0";let u="jdbc:database://host:port/db",m="com.database.Driver";if(/oracle/i.test(r)?(u="jdbc:oracle:thin:@db_host:1521:db_sid",m="oracle.jdbc.driver.OracleDriver"):/postgres/i.test(r)?(u="jdbc:postgresql://pg_host:5432/pg_db",m="org.postgresql.Driver"):/mysql/i.test(r)?(u="jdbc:mysql://mysql_host:3306/mysql_db",m="com.mysql.cj.jdbc.Driver"):/hive/i.test(r)&&(u="",m=""),u){const y=l?'"('+l.replace(/"/g,'\\"').substring(0,200)+') AS subq"':`"${p}"`;c=`# SQL Query: ${e.name}
# Pool: ${r} | Table: ${p||"(custom query)"}
df_${n} = (spark.read
  .format("jdbc")
  .option("url", "${u}")
  .option("dbtable", ${y})
  .option("driver", "${m}")
  .option("user", dbutils.secrets.get(scope="db", key="user"))
  .option("password", dbutils.secrets.get(scope="db", key="pass"))`+(d!=="0"?`
  .option("fetchsize", "${d}")`:"")+`
  .load()
)
print(f"[SQL] Read from ${p||"query"}")`}else{const y=l||"SELECT * FROM "+p;c=`# SQL Query: ${e.name} (via Hive/Spark SQL)
df_${n} = spark.sql(f"${y}")
print(f"[SQL] Read from Hive/Spark SQL")`}return o=.92,{code:c,conf:o}}if(e.type==="GenerateFlowFile"){const r=t["Batch Size"]||"1";return c=`# Generate Test Data: ${e.name}
from pyspark.sql.functions import current_timestamp, lit
df_${n} = spark.range(${r}).toDF("id")
df_${n} = df_${n}.withColumn("_generated_at", current_timestamp())
print(f"[GEN] Generated ${r} test records")`,o=.95,{code:c,conf:o}}if(e.type==="TailFile"){const r=t["File(s) to Tail"]||t["File to Tail"]||"/var/log/app.log";return c=`# TailFile: ${e.name}
# File: ${r}
# In Databricks, use Auto Loader for continuous file ingestion
df_${n} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "text")
  .load("${r.replace(/[^/]*$/,"")}")
)
print(f"[TAIL] Streaming from ${r}")`,o=.92,{code:c,conf:o}}if(e.type==="ConsumeKinesisStream"){const r=t["Kinesis Stream Name"]||t["Amazon Kinesis Stream Name"]||"stream",l=t.Region||"us-east-1";return c=`# Kinesis: ${e.name}
df_${n} = (spark.readStream
  .format("kinesis")
  .option("streamName", "${r}")
  .option("region", "${l}")
  .option("initialPosition", "TRIM_HORIZON")
  .load())`,o=.92,{code:c,conf:o}}if(e.type==="CaptureChangeMySQL"){const r=t["MySQL Hostname"]||"mysql_host",l=t["Database/Schema"]||"source_db",p=t.Table||"source_table";return c=`# MySQL CDC: ${e.name}
# Host: ${r}
df_${n} = (spark.readStream
  .format("delta")
  .option("readChangeFeed", "true")
  .table("${l}.${p}")
)
print(f"[CDC] Streaming changes via DLT")`,o=.92,{code:c,conf:o}}return null}function Wn(e,t,n,s,a,i,c){let o=a,r=i;if(e.type==="UpdateAttribute"){const l=new Set(["Delete Attributes Expression","Store State","Stateful Variables Initial Value","canonical-value-lookup-cache-size"]),p=Object.entries(t).filter(([d])=>!l.has(d));if(p.length){const d=["from pyspark.sql.functions import col, lit, upper, lower, trim, length, substring, regexp_replace, concat, when, current_timestamp, date_format, to_timestamp, expr, rand, substring_index, lpad, rpad, locate, split, regexp_extract, round, abs, ceil, floor",`# UpdateAttribute: ${e.name} — set DataFrame columns via NEL expressions`,`df_${n} = df_${s}`];return p.forEach(([u,m])=>{const y=u.replace(/[^a-zA-Z0-9_]/g,"_").toLowerCase();if(m.includes("${")){const _=c(m,"col");d.push(`df_${n} = df_${n}.withColumn("${y}", ${_})  # NEL: ${m.substring(0,80).replace(/"/g,"'")}`)}else d.push(`df_${n} = df_${n}.withColumn("${y}", lit("${m.replace(/"/g,'\\"')}"))  # ${u}`)}),o=d.join(`
`),r=.92,{code:o,conf:r}}}if(e.type==="EvaluateJsonPath"){t.Destination;const l=Object.entries(t).filter(([p])=>!["Destination","Return Type","Null Value Representation","Path Not Found Behavior"].includes(p));if(l.length){const p=[`# JSON Path Evaluation: ${e.name}`,"from pyspark.sql.functions import col, get_json_object"];return p.push(`df_${n} = df_${s}`),l.forEach(([d,u])=>{const m=d.replace(/[^a-zA-Z0-9_]/g,"_"),y=u.replace(/^\$/,"$");p.push(`df_${n} = df_${n}.withColumn("${m}", get_json_object(col("value"), "${y}"))`)}),p.push(`print(f"[JSON] Extracted ${l.length} fields from JSON")`),o=p.join(`
`),r=.93,{code:o,conf:r}}}if(e.type==="JoltTransformJSON"){const l=t["Jolt Specification"]||"[]",p=t["Jolt Transformation DSL"]||"Chain";return o=`# Jolt Transform: ${e.name}
# DSL: ${p}
# Spec: ${l.substring(0,100)}...
from pyspark.sql.functions import col, from_json, to_json, struct, lit
import json

_jolt_spec = json.loads('${l.substring(0,500).replace(/'/g,"\\'").replace(/\n/g," ")}')
df_${n} = df_${s}
for op in (_jolt_spec if isinstance(_jolt_spec, list) else [_jolt_spec]):
    _operation = op.get("operation", "")
    _spec = op.get("spec", {})
    if _operation == "shift":
        for src, dst in _spec.items():
            if src != "*" and isinstance(dst, str):
                df_${n} = df_${n}.withColumnRenamed(src, dst)
            elif src == "*" and isinstance(dst, str):
                # Wildcard shift: rename all to nested path
                for c in df_${n}.columns:
                    df_${n} = df_${n}.withColumnRenamed(c, f"{dst}.{c}")
    elif _operation == "default":
        for k, v in _spec.items():
            if isinstance(v, (str, int, float)):
                df_${n} = df_${n}.withColumn(k, lit(v))
    elif _operation == "remove":
        for k in _spec.keys():
            if k in df_${n}.columns:
                df_${n} = df_${n}.drop(k)
    elif _operation == "modify-overwrite-beta":
        for k, v in _spec.items():
            if isinstance(v, str) and v.startswith("="):
                df_${n} = df_${n}.withColumn(k, lit(v[1:]))
print(f"[JOLT] Applied {len(_jolt_spec) if isinstance(_jolt_spec, list) else 1} transformation(s)")`,r=.9,{code:o,conf:r}}if(e.type==="JoltTransformRecord")return o=`# Jolt Record: ${e.name}
df_${n} = df_${s}
# Apply Jolt-equivalent column renames/transforms
print(f"[JOLT] Record transformation applied")`,r=.9,{code:o,conf:r};if(e.type==="ConvertRecord"){const l=t["Record Reader"]||"CSVReader",p=t["Record Writer"]||"JsonRecordSetWriter",d=/CSV/i.test(l)?"csv":/Avro/i.test(l)?"avro":/Json/i.test(l)?"json":"csv",u=/CSV/i.test(p)?"csv":/Avro/i.test(p)?"avro":(/Json/i.test(p),"json");return o=`# Format Conversion: ${e.name}
# ${l} -> ${p} (${d} -> ${u})
df_${n} = df_${s}  # Spark DataFrames are format-agnostic
# Write example: df_${n}.write.format("${u}").save("/path/to/output")
print(f"[CONVERT] ${d} -> ${u}")`,r=.93,{code:o,conf:r}}if(/^Merge(Content|Record)$/.test(e.type)){const l=t["Merge Strategy"]||"Bin-Packing",p=t["Minimum Number of Entries"]||"1",d=t["Maximum Number of Entries"]||"1000",u=t["Merge Format"]||"Binary Concatenation";return o=`# Merge: ${e.name}
# Strategy: ${l} | Entries: ${p}-${d} | Format: ${u}
# Enable Delta write optimizations for small file compaction
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.minNumFiles", ${p})

# Coalesce partitions to reduce file count
_num_parts = max(1, df_${s}.rdd.getNumPartitions() // 4)
df_${n} = df_${s}.coalesce(_num_parts)
# Post-write: run OPTIMIZE on target table for best performance
# spark.sql("OPTIMIZE <catalog>.<schema>.<table> ZORDER BY (<key_column>)")
print(f"[MERGE] Coalesced to {_num_parts} partitions — Delta Auto Optimize + Auto Compaction enabled")`,r=.93,{code:o,conf:r}}if(e.type==="SplitJson"){const l=t["JsonPath Expression"]||"$.*",p=l.replace(/^\$\.?\*?/,"$"),d=["from pyspark.sql.functions import explode, col, from_json, get_json_object","from pyspark.sql.types import ArrayType, StringType",`# SplitJson: ${e.name}`,`# JsonPath: ${l}`];return l==="$"||l==="$.*"||l==="$[*]"?(d.push("# Top-level array — explode directly"),d.push(`df_${n} = df_${s}.withColumn("_items", from_json(col("value"), ArrayType(StringType())))`),d.push(`df_${n} = df_${n}.withColumn("_item", explode(col("_items"))).drop("_items")`)):(d.push("# Extract nested array then explode"),d.push(`df_${n} = df_${s}.withColumn("_nested", get_json_object(col("value"), "${p||"$"}"))`),d.push(`df_${n} = df_${n}.withColumn("_items", from_json(col("_nested"), ArrayType(StringType())))`),d.push(`df_${n} = df_${n}.withColumn("value", explode(col("_items"))).drop("_nested", "_items")`)),d.push("# Note: For complex nested structures, define explicit schema instead of StringType()"),d.push('print(f"[SPLIT] JSON array exploded into individual rows")'),o=d.join(`
`),r=.92,{code:o,conf:r}}if(e.type==="SplitContent"){const l=t["Byte Sequence"]||t["Line Split Count"]||"",p=t["Byte Sequence Format"]||"UTF-8",d=["from pyspark.sql.functions import explode, split, col",`# SplitContent: ${e.name}`,`# Byte Sequence: ${l||"(newline)"} | Format: ${p}`],m=(l||"\\n").replace(/\\/g,"\\\\").replace(/"/g,'\\"');return d.push(`df_${n} = df_${s}.withColumn("_parts", split(col("value"), "${m}"))`),d.push(`df_${n} = df_${n}.withColumn("value", explode(col("_parts"))).drop("_parts")`),d.push(`df_${n} = df_${n}.filter(col("value") != lit(""))  # Remove empty splits`),d.push('print(f"[SPLIT] Content split into individual rows by delimiter")'),o=d.join(`
`),r=.92,{code:o,conf:r}}if(/^Split(Text|Xml|Record|Avro)$/.test(e.type)){const l=e.type.replace("Split","").toLowerCase();return o=`# Split: ${e.name}
# In Databricks, Spark reads entire ${l} datasets as DataFrames.
df_${n} = df_${s}  # Already partitioned across Spark executors
from pyspark.sql.functions import explode, col
print(f"[SPLIT] ${l} data already distributed across partitions")`,r=.92,{code:o,conf:r}}if(/^(Compress|Unpack)Content$/.test(e.type)){const l=t.Mode||(e.type==="CompressContent"?"compress":"decompress"),p=t["Compression Format"]||t["Compression Level"]||"gzip",u={gzip:"gzip",bzip2:"bzip2",snappy:"snappy",lz4:"lz4",zstd:"zstd",deflate:"deflate",lzo:"lzo",none:"none"}[p.toLowerCase()]||"snappy";return e.type==="CompressContent"?o=`# CompressContent: ${e.name}
# Mode: ${l} | Format: ${p}
# Set Spark compression codec for Parquet/Delta writes
spark.conf.set("spark.sql.parquet.compression.codec", "${u}")
spark.conf.set("spark.sql.orc.compression.codec", "${u==="gzip"?"zlib":u}")
df_${n} = df_${s}
print(f"[COMPRESS] Codec set to ${u} for downstream writes")`:o=`# UnpackContent: ${e.name}
# Mode: ${l} | Format: ${p}
# Spark auto-detects compression when reading (gzip, snappy, bzip2, etc.)
df_${n} = df_${s}
print(f"[DECOMPRESS] Spark auto-detects ${p} compression on read")`,r=.95,{code:o,conf:r}}if(e.type==="EncryptContent"){const l=t["Encryption Algorithm"]||"AES/GCM/NoPadding";return/AES/i.test(l)?o=`# Encryption: ${e.name}
# Algorithm: ${l}
from pyspark.sql.functions import col, expr, lit

# Use Spark built-in aes_encrypt with secret scope key
_enc_key = dbutils.secrets.get(scope="encryption", key="aes-key")

df_${n} = df_${s}
for _col in df_${s}.columns:
    if _col not in ["id", "key", "timestamp"]:
        df_${n} = df_${n}.withColumn(_col,
            expr(f"base64(aes_encrypt(CAST({_col} AS STRING), '{_enc_key}', 'GCM', 'DEFAULT', ''))"))
print(f"[ENCRYPT] AES-GCM encryption applied via aes_encrypt()")`:o=`# Encryption: ${e.name}
# Algorithm: ${l}
from cryptography.fernet import Fernet
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType

_key = dbutils.secrets.get(scope="encryption", key="fernet-key")
_fernet = Fernet(_key.encode() if isinstance(_key, str) else _key)

@udf(StringType())
def encrypt_value(val):
    if val is None: return None
    return _fernet.encrypt(val.encode()).decode()

df_${n} = df_${s}
for _col in df_${s}.columns:
    if _col not in ["id", "key", "timestamp"]:
        df_${n} = df_${n}.withColumn(_col, encrypt_value(col(_col)))
print(f"[ENCRYPT] ${l} encryption applied")`,r=.9,{code:o,conf:r}}if(e.type==="ExecuteScript"){const l=t["Script Engine"]||"python",p=t["Script Body"]||"",d=p.substring(0,200).replace(/\n/g," ").replace(/"/g,"'"),u=/groovy/i.test(l),m=/flowFile|session\.get|session\.transfer/i.test(p);return u&&m?o=`# ExecuteScript (Groovy->Python): ${e.name}
from pyspark.sql.functions import udf, col, struct
from pyspark.sql.types import StringType
import json

# Migrated from Groovy NiFi script
# Original pattern: session.get() -> process -> session.transfer()
# Groovy: ${d}
def _nifi_groovy_logic(row_dict):
    """Migrated from Groovy ExecuteScript.
    Common patterns:
    - session.get() -> DataFrame row
    - flowFile.getAttribute() -> row_dict[key]
    - session.transfer(flowFile, REL_SUCCESS) -> return
    """
    try:
        data = row_dict
        # TODO: Port Groovy logic to Python
        data["_migrated_from"] = "groovy"
        return json.dumps(data)
    except Exception as e:
        return json.dumps({"_error": str(e), **row_dict})

_script_udf = udf(lambda row: _nifi_groovy_logic(row.asDict()), StringType())
df_${n} = df_${s}.withColumn("_result", _script_udf(struct("*")))
print(f"[SCRIPT] Executed migrated Groovy logic")`:o=`# ExecuteScript (${l}): ${e.name}
from pyspark.sql.functions import udf, col, struct
from pyspark.sql.types import StringType
import json

def _nifi_script_logic(row_dict):
    """Migrated from NiFi ExecuteScript. Engine: ${l}
    Original: ${d}"""
    try:
        data = row_dict
        data["_processed"] = True
        return json.dumps(data)
    except Exception as e:
        return json.dumps({"_error": str(e), **row_dict})

_script_udf = udf(lambda row: _nifi_script_logic(row.asDict()), StringType())
df_${n} = df_${s}.withColumn("_result", _script_udf(struct("*")))
print(f"[SCRIPT] Executed migrated ${l} logic")`,r=.9,{code:o,conf:r}}if(e.type==="ExecuteGroovyScript"){const l=(t["Script Body"]||"").substring(0,200).replace(/"/g,"'").replace(/\n/g," ");return o=`# Groovy->Python: ${e.name}
from pyspark.sql.functions import udf, col, struct
from pyspark.sql.types import StringType
import json
@udf(StringType())
def groovy_migrated(row_json):
    """Migrated from Groovy: ${l}"""
    data = json.loads(row_json)
    data["_migrated"] = True
    return json.dumps(data)
df_${n} = df_${s}.withColumn("_result", groovy_migrated(col("value")))`,r=.9,{code:o,conf:r}}if(e.type==="ReplaceText"&&!o.includes("regexp_replace")){const l=t["Search Value"]||"",p=t["Replacement Value"]||"";return t["Evaluation Mode"],o=`# ReplaceText: ${e.name}
from pyspark.sql.functions import regexp_replace, col
df_${n} = df_${s}.withColumn("value", regexp_replace(col("value"), "${l.replace(/\\/g,"\\\\").replace(/"/g,'\\"').substring(0,200)}", "${p.replace(/\\/g,"\\\\").replace(/"/g,'\\"').substring(0,200)}"))
print(f"[REPLACE] Text replacement applied")`,r=.9,{code:o,conf:r}}if(e.type==="FlattenJson"&&!o.includes("flatten"))return o=`# FlattenJson: ${e.name}
from pyspark.sql.functions import col
# Flatten nested JSON structure
def _flatten_df(df, prefix=""):
    cols = []
    for field in df.schema.fields:
        name = f"{prefix}{field.name}" if prefix else field.name
        if hasattr(field.dataType, "fields"):
            cols += _flatten_df(df.select(f"{prefix}{field.name}.*"), f"{name}_")
        else:
            cols.append(col(f"{prefix}{field.name}").alias(name.replace(".", "_")))
    return cols
df_${n} = df_${s}.select(_flatten_df(df_${s}))`,r=.92,{code:o,conf:r};if(e.type==="EvaluateXPath"){const l=Object.entries(t).filter(([p])=>!["Destination","Return Type"].includes(p));if(l.length){const p=[`# XPath Evaluation: ${e.name}`,"from pyspark.sql.functions import xpath_string, col"];return p.push(`df_${n} = df_${s}`),l.forEach(([d,u])=>{const m=d.replace(/[^a-zA-Z0-9_]/g,"_");p.push(`df_${n} = df_${n}.withColumn("${m}", xpath_string(col("xml"), "${u}"))`)}),o=p.join(`
`),r=.9,{code:o,conf:r}}}if(e.type==="EvaluateXQuery")return o=`# XQuery: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
@udf(StringType())
def eval_xquery(xml_str):
    import lxml.etree as ET
    doc = ET.fromstring(xml_str.encode())
    return str(doc.xpath("${t["XQuery Expression"]||"//*"}"))
df_${n} = df_${s}.withColumn("_xquery_result", eval_xquery(col("value")))`,r=.9,{code:o,conf:r};if(e.type==="SplitXml"){const l=t["Record Tag"]||"record";return o=`# Split XML: ${e.name}
df_${n} = spark.read.format("xml").option("rowTag", "${l}").load("/mnt/data/*.xml")
print(f"[XML] Split XML by <${l}>")`,r=.92,{code:o,conf:r}}if(e.type==="ExtractGrok"){const l=t["Grok Expression"]||"%{COMBINEDAPACHELOG}";return o=`# Grok: ${e.name}
# Pattern: ${l}
from pyspark.sql.functions import regexp_extract, col
df_${n} = df_${s}
print(f"[GROK] Extracted fields")`,r=.9,{code:o,conf:r}}if(e.type==="ExtractText"){const l=new Set(["Character Set","Enable Canonical Equivalence","Enable Case Insensitive Flag","Enable Comments","Enable DOTALL Mode","Enable Literal Flag","Enable Multiline Mode","Enable Unicode Case","Enable Unicode Predefined Character Classes","Include Capture Group 0","Maximum Buffer Size","Maximum Capture Group Length","Permit Whitespace and Comments in Pattern"]),p=Object.entries(t).filter(([d])=>!l.has(d));if(p.length){const d=["from pyspark.sql.functions import regexp_extract, col",`# ExtractText: ${e.name} — regex extraction to columns`,`df_${n} = df_${s}`];return p.forEach(([u,m])=>{const y=u.replace(/[^a-zA-Z0-9_]/g,"_"),_=(m.match(/\((?!\?)/g)||[]).length,b=m.replace(/\\/g,"\\\\").replace(/"/g,'\\"');if(_>1)for(let S=1;S<=_;S++)d.push(`df_${n} = df_${n}.withColumn("${y}_${S}", regexp_extract(col("value"), "${b}", ${S}))`);else{const S=_>=1?1:0;d.push(`df_${n} = df_${n}.withColumn("${y}", regexp_extract(col("value"), "${b}", ${S}))`)}}),d.push(`print(f"[EXTRACT] Extracted ${p.length} regex patterns into columns")`),o=d.join(`
`),r=.92,{code:o,conf:r}}}if(e.type==="ExtractHL7Attributes"||e.type==="RouteHL7")return o=`# HL7: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType, StructType, StructField

@udf(MapType(StringType(), StringType()))
def parse_hl7(msg):
    """Parse HL7 v2 message into segment fields.
    Handles MSH (header), PID (patient), OBX (observation) segments."""
    if not msg: return {}
    segs = msg.split("\\r") if "\\r" in msg else msg.split("\\n")
    result = {}
    for s in segs:
        fields = s.split("|")
        seg_type = fields[0] if fields else ""
        if seg_type == "MSH" and len(fields) > 9:
            result["msh_sending_app"] = fields[2] if len(fields) > 2 else ""
            result["msh_message_type"] = fields[8] if len(fields) > 8 else ""
            result["msh_version"] = fields[11] if len(fields) > 11 else ""
        elif seg_type == "PID" and len(fields) > 5:
            result["pid_patient_id"] = fields[3] if len(fields) > 3 else ""
            result["pid_patient_name"] = fields[5] if len(fields) > 5 else ""
            result["pid_dob"] = fields[7] if len(fields) > 7 else ""
            result["pid_sex"] = fields[8] if len(fields) > 8 else ""
        elif seg_type == "OBX" and len(fields) > 5:
            result[f"obx_{fields[3]}"] = fields[5] if len(fields) > 5 else ""
        else:
            result[seg_type] = "|".join(fields[1:4]) if len(fields) > 1 else ""
    return result

df_${n} = df_${s}.withColumn("hl7_attrs", parse_hl7(col("value")))
print(f"[HL7] Parsed MSH/PID/OBX segments")`,r=.9,{code:o,conf:r};if(e.type==="ParseCEF")return o=`# CEF: ${e.name}
from pyspark.sql.functions import regexp_extract, col
df_${n} = df_${s}.withColumn("cef_vendor", regexp_extract(col("value"), "CEF:\\\\d+\\\\|([^|]+)", 1)).withColumn("cef_severity", regexp_extract(col("value"), "CEF:\\\\d+(?:\\\\|[^|]*){6}\\\\|([^|]+)", 1))`,r=.9,{code:o,conf:r};if(e.type==="ParseEvtx")return o=`# EVTX: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_evtx(xml):
    import lxml.etree as ET
    doc = ET.fromstring(xml.encode())
    return {"EventID": doc.findtext(".//{*}EventID", default="")}
df_${n} = df_${s}.withColumn("event_data", parse_evtx(col("value")))`,r=.9,{code:o,conf:r};if(e.type==="ParseNetflowv5")return o=`# NetFlow v5: ${e.name}
from pyspark.sql.functions import col
df_${n} = df_${s}.selectExpr("*", "substring(value,1,4) as src_ip", "substring(value,5,4) as dst_ip")`,r=.9,{code:o,conf:r};if(e.type==="ParseSyslog5424")return o=`# Syslog 5424: ${e.name}
from pyspark.sql.functions import regexp_extract, col
df_${n} = df_${s}.withColumn("priority", regexp_extract(col("value"), "<(\\\\d+)>", 1)).withColumn("hostname", regexp_extract(col("value"), "<\\\\d+>\\\\d+ [\\\\S]+ ([\\\\S]+)", 1))`,r=.92,{code:o,conf:r};if(e.type==="ForkRecord"){const l=t["Record Path"]||"records";return o=`# Fork Record: ${e.name}
from pyspark.sql.functions import explode, col
df_${n} = df_${s}.select(explode(col("${l}")).alias("record"), "*")`,r=.93,{code:o,conf:r}}if(e.type==="SampleRecord"){const l=t["Sampling Rate"]||"0.1";return o=`# Sample: ${e.name}
df_${n} = df_${s}.sample(fraction=${parseFloat(l)||.1}, seed=42)`,r=.95,{code:o,conf:r}}if(e.type==="ScriptedTransformRecord"||e.type==="InvokeScriptedProcessor"){const l=t["Script Engine"]||"python";return o=`# Scripted Transform: ${e.name} (${l})
from pyspark.sql.functions import udf, col, struct
from pyspark.sql.types import StringType
import json
@udf(StringType())
def transform_record(row_json):
    data = json.loads(row_json)
    data["_processed"] = True
    return json.dumps(data)
df_${n} = df_${s}.withColumn("_result", transform_record(col("value")))`,r=.9,{code:o,conf:r}}if(e.type==="PutRecord"){const l=t["Record Writer"]||"",p=/CSV/i.test(l)?"csv":/Avro/i.test(l)?"avro":"delta";return o=`# Put Record: ${e.name}
df_${s}.write.format("${p}").mode("append").saveAsTable("${t["Table Name"]||n+"_output"}")`,r=.93,{code:o,conf:r}}if(e.type==="CryptographicHashAttribute"||e.type==="HashAttribute"){const l=t["Hash Algorithm"]||"SHA-256",p=t["Attribute Name"]||Object.keys(t)[0]||"value",d=l.includes("512")?`sha2(col("${p}"), 512)`:l.includes("MD5")?`md5(col("${p}"))`:`sha2(col("${p}"), 256)`;return o=`# Hash: ${e.name} (${l})
from pyspark.sql.functions import sha2, md5, col
df_${n} = df_${s}.withColumn("_hash", ${d})`,r=.95,{code:o,conf:r}}if(e.type==="EncryptContentPGP")return o=`# PGP Encrypt: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import BinaryType
import gnupg
_gpg = gnupg.GPG()
@udf(BinaryType())
def pgp_encrypt(data):
    return bytes(str(_gpg.encrypt(data, "${t.Recipient||"recipient"}")), "utf-8")
df_${n} = df_${s}.withColumn("_encrypted", pgp_encrypt(col("value")))`,r=.9,{code:o,conf:r};if(e.type==="DecryptContentPGP")return o=`# PGP Decrypt: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import gnupg
_gpg = gnupg.GPG()
@udf(StringType())
def pgp_decrypt(data):
    return str(_gpg.decrypt(data, passphrase=dbutils.secrets.get(scope="pgp", key="passphrase")))
df_${n} = df_${s}.withColumn("_decrypted", pgp_decrypt(col("value")))`,r=.9,{code:o,conf:r};if(e.type==="IdentifyMimeType")return o=`# MIME: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import mimetypes
@udf(StringType())
def detect_mime(fname):
    mime, _ = mimetypes.guess_type(fname or "")
    return mime or "application/octet-stream"
df_${n} = df_${s}.withColumn("mime_type", detect_mime(col("filename")))`,r=.93,{code:o,conf:r};if(e.type==="ModifyBytes")return o=`# ModifyBytes: ${e.name}
from pyspark.sql.functions import substring, col
df_${n} = df_${s}.withColumn("_modified", substring(col("content"), 1, 100))`,r=.9,{code:o,conf:r};if(e.type==="SegmentContent")return o=`# Segment: ${e.name}
from pyspark.sql.functions import explode, split, col
df_${n} = df_${s}.withColumn("_segment", explode(split(col("value"), "\\n")))`,r=.92,{code:o,conf:r};if(e.type==="DuplicateFlowFile"){const l=t["Number of Copies"]||"2";return o=`# Duplicate: ${e.name}
from functools import reduce
from pyspark.sql import DataFrame
df_${n} = reduce(DataFrame.union, [df_${s}] * ${parseInt(l)||2})`,r=.93,{code:o,conf:r}}if(e.type==="GeoEnrichIP"||e.type==="ISPEnrichIP"){const l=t["IP Address Attribute"]||"ip_address";return o=`# GeoIP: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StructType, StructField, StringType, FloatType
import geoip2.database
_reader = geoip2.database.Reader("/dbfs/geo/GeoLite2-City.mmdb")
@udf(StructType([StructField("city",StringType()),StructField("country",StringType())]))
def geo_lookup(ip):
    try:
        r = _reader.city(ip)
        return (r.city.name, r.country.name)
    except: return (None, None)
df_${n} = df_${s}.withColumn("_geo", geo_lookup(col("${l}")))`,r=.9,{code:o,conf:r}}if(e.type==="QueryDNS")return o=`# DNS: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import socket
@udf(StringType())
def dns_lookup(hostname):
    try: return socket.gethostbyname(hostname)
    except: return None
df_${n} = df_${s}.withColumn("_ip", dns_lookup(col("${t["DNS Query Attribute"]||"hostname"}")))`,r=.93,{code:o,conf:r};if(e.type==="AttributesToJSON"){const l=t["Attributes List"]||"",p=l?l.split(",").map(u=>u.trim()):[],d=p.length?p.map(u=>'col("'+u+'")').join(", "):'"*"';return o=`# Attributes to JSON: ${e.name}
from pyspark.sql.functions import to_json, struct, col
df_${n} = df_${s}.withColumn("_json", to_json(struct(${d})))
print(f"[JSON] Converted attributes to JSON")`,r=.93,{code:o,conf:r}}if(e.type==="AttributesToCSV")return o=`# Attrs to CSV: ${e.name}
from pyspark.sql.functions import concat_ws, col
df_${n} = df_${s}.withColumn("_csv", concat_ws(",", *[col(c) for c in df_${s}.columns]))`,r=.93,{code:o,conf:r};if(e.type==="AttributeRollingWindow"){const l=t["Time Window"]||"5 minutes";return o=`# Rolling Window: ${e.name}
from pyspark.sql.functions import col, avg, window
df_${n} = df_${s}.groupBy(window("timestamp", "${l}")).agg(avg("value").alias("rolling_avg"))`,r=.92,{code:o,conf:r}}if(e.type==="CompareFuzzyHash"||e.type==="FuzzyHashContent")return o=`# Fuzzy Hash: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import hashlib
@udf(StringType())
def fuzzy_hash(content):
    return hashlib.sha256(content.encode()).hexdigest()[:16]
df_${n} = df_${s}.withColumn("_fuzzy_hash", fuzzy_hash(col("value")))`,r=.9,{code:o,conf:r};if(e.type==="ExtractCCDAAttributes")return o=`# CCDA: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_ccda(xml):
    import lxml.etree as ET
    doc = ET.fromstring(xml.encode())
    return {"patient": doc.findtext(".//{urn:hl7-org:v3}patient/{urn:hl7-org:v3}name", default="")}
df_${n} = df_${s}.withColumn("ccda_attrs", parse_ccda(col("value")))`,r=.9,{code:o,conf:r};if(e.type==="ExtractTNEFAttachments")return o=`# TNEF: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import ArrayType, StringType
@udf(ArrayType(StringType()))
def extract_tnef(data):
    return ["attachment_extracted"]
df_${n} = df_${s}.withColumn("tnef_attachments", extract_tnef(col("content")))`,r=.9,{code:o,conf:r};if(e.type==="ValidateCsv")return o=`# Validate CSV: ${e.name}
from pyspark.sql.functions import col
df_${n} = spark.read.option("header", "true").option("mode", "PERMISSIVE").csv("/mnt/data/*.csv")
_corrupt = df_${n}.filter(col("_corrupt_record").isNotNull())`,r=.93,{code:o,conf:r};if(e.type==="GetHTMLElement"||e.type==="ModifyHTMLElement"||e.type==="PutHTMLElement"){const l=t["CSS Selector"]||"body";return o=`# HTML ${e.type}: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
@udf(StringType())
def process_html(html_content):
    soup = BeautifulSoup(html_content, "html.parser")
    el = soup.select_one("${l}")
    return el.get_text() if el else None
df_${n} = df_${s}.withColumn("_html_result", process_html(col("value")))`,r=.9,{code:o,conf:r}}if(/^Convert(AvroToJSON|AvroToParquet|AvroToORC|CSVToAvro|JSONToAvro|JSONToSQL|ParquetToAvro|CharacterSet|ExcelToCSVProcessor)$/.test(e.type)&&!o.includes("format")){const l={AvroToJSON:"json",AvroToParquet:"parquet",AvroToORC:"orc",CSVToAvro:"avro",JSONToAvro:"avro",ParquetToAvro:"avro"},p=e.type.replace("Convert",""),d=l[p]||"delta";return o=`# ${e.type}: ${e.name}
# Spark DataFrames are format-agnostic — conversion happens at write time
df_${n} = df_${s}
# Write as ${d}: df_${n}.write.format("${d}").save("/path")
print(f"[CONVERT] Format conversion -> ${d}")`,r=.93,{code:o,conf:r}}return e.type==="ExtractAvroMetadata"?(o=`# Avro Metadata: ${e.name}
df_${n} = spark.read.format("avro").load("${t.Path||"/mnt/data/*.avro"}")
print(f"[AVRO] Schema: {df_${n}.schema.simpleString()}")`,r=.93,{code:o,conf:r}):e.type==="FetchParquet"?(o=`# Parquet: ${e.name}
df_${n} = spark.read.format("parquet").load("${t.Path||"/mnt/data/*.parquet"}")`,r=.95,{code:o,conf:r}):null}function Qn(e,t,n,s,a,i,c){let o=a,r=i;if(e.type==="RouteOnAttribute"){const l=t["Routing Strategy"]||"Route to Property name",p=Object.entries(t).filter(([d])=>d!=="Routing Strategy");if(p.length){const d=["from pyspark.sql.functions import col, lit, upper, lower, trim, length, substring, regexp_replace, concat, when, current_timestamp, date_format, to_timestamp, expr, rand, substring_index, lpad, rpad, locate, split, regexp_extract, round, abs, ceil, floor",`# RouteOnAttribute: ${e.name} — ${l}`,"# Generates named DataFrames per route with NEL-parsed filter conditions"],u=[];return p.forEach(([m,y])=>{const _=m.replace(/[^a-zA-Z0-9_]/g,"_").toLowerCase(),b=`df_${n}_${_}`;if(u.push(b),y.includes("${")){const S=c(y,"col");d.push(`# Route "${m}": ${y.substring(0,80).replace(/"/g,"'")}`),d.push(`${b} = df_${s}.filter(${S})`)}else d.push(`# Route "${m}": ${y}`),/^true$/i.test(y)?d.push(`${b} = df_${s}  # Always matches`):/^false$/i.test(y)?d.push(`${b} = df_${s}.limit(0)  # Never matches`):d.push(`${b} = df_${s}.filter(lit(True))  # Static: ${y.substring(0,60)}`)}),u.length===1?d.push(`df_${n}_unmatched = df_${s}.subtract(${u[0]})`):u.length>1&&(d.push("# Unmatched: rows not matching any route"),d.push(`df_${n}_unmatched = df_${s}`),u.forEach(m=>{d.push(`df_${n}_unmatched = df_${n}_unmatched.subtract(${m})`)})),d.push(`df_${n} = df_${s}  # Pass-through for default routing`),o=d.join(`
`),r=.92,{code:o,conf:r}}}if(e.type==="RouteOnContent"){const l=t["Content Requirement"]||".*",p=t["Match Requirement"]||"content must contain match",d=Object.entries(t).filter(([u])=>!["Content Requirement","Match Requirement","Character Set","Buffer Size"].includes(u));if(d.length>0){const u=["from pyspark.sql.functions import col, regexp_extract, when, lit",`# RouteOnContent: ${e.name}`,`# Match: ${p}`,`df_${n} = df_${s}`];d.forEach(([m,y])=>{const _=m.replace(/[^a-zA-Z0-9_]/g,"_").toLowerCase();u.push(`# Route "${m}" — pattern: ${y.substring(0,60)}`),u.push(`df_${n}_${_} = df_${s}.filter(col("value").rlike("${y.replace(/"/g,'\\"')}"))`)}),u.push("# GAP FIX: MIME-type routing — extract Content-Type from headers if present"),u.push(`if "content_type" in df_${s}.columns:`),u.push(`    df_${n}_json = df_${s}.filter(col("content_type").rlike("application/json"))`),u.push(`    df_${n}_xml = df_${s}.filter(col("content_type").rlike("(application|text)/xml"))`),u.push(`    df_${n}_csv = df_${s}.filter(col("content_type").rlike("text/csv"))`),u.push(`df_${n}_unmatched = df_${s}.subtract(df_${n})`),o=u.join(`
`)}else o=`# RouteOnContent: ${e.name}
from pyspark.sql.functions import col
# Route based on content matching
df_${n}_matched = df_${s}.filter(col("value").rlike("${l}"))
df_${n}_unmatched = df_${s}.subtract(df_${n}_matched)
df_${n} = df_${s}`;return r=.9,{code:o,conf:r}}if(e.type==="RouteText")return o=`# RouteText: ${e.name}
from pyspark.sql.functions import col
df_${n} = df_${s}
# Route text lines by pattern matching
print(f"[ROUTE] Text routing applied")`,r=.9,{code:o,conf:r};if(e.type==="ValidateRecord"){const l=t["Schema Name"]||"";t["Schema Text"];const p=t["Schema Access Strategy"]||"Inherit Record Schema",d=t["Invalid Record Strategy"]||"route",u=Object.entries(t).filter(([y])=>!["Schema Name","Schema Text","Schema Access Strategy","Record Reader","Record Writer","Invalid Record Strategy","Allow Extra Fields","Strict Type Checking"].includes(y)),m=["from pyspark.sql.functions import col, lit, when, current_timestamp",`# ValidateRecord: ${e.name}`,`# Schema: ${l||"inferred"} | Strategy: ${p} | On Invalid: ${d}`];if(m.push("# DLT Expectations (use when running as Delta Live Table):"),u.length>0?u.forEach(([y,_])=>{const b=y.replace(/[^a-zA-Z0-9_]/g,"_").toLowerCase();if(_.includes("${")){const S=c(_,"col");m.push(`# @dlt.expect_or_drop("${b}", "${S.replace(/"/g,"'").substring(0,100)}")`)}else m.push(`# @dlt.expect_or_drop("${b}", "${_.replace(/"/g,"'").substring(0,100)}")`)}):m.push(`# @dlt.expect_or_drop("not_null_check", "col('${l||"id"}') IS NOT NULL")`),m.push(""),m.push("# Inline validation — split valid/invalid records"),u.length>0){const _=u.map(([b,S])=>S.includes("${")?c(S,"col"):`col("${b}").isNotNull()`).join(" & ");m.push(`_valid_cond = ${_}`),m.push(`df_${n}_valid = df_${s}.filter(_valid_cond)`),m.push(`df_${n}_invalid = df_${s}.filter(~(_valid_cond))`)}else{const y=l||"id";m.push(`df_${n}_valid = df_${s}.filter(col("${y}").isNotNull())`),m.push(`df_${n}_invalid = df_${s}.filter(col("${y}").isNull())`)}return m.push(`df_${n} = df_${n}_valid`),m.push(""),m.push("# Dead-letter queue for invalid records"),m.push(`if df_${n}_invalid.count() > 0:`),m.push(`    df_${n}_invalid.withColumn("_rejected_at", current_timestamp()).withColumn("_rejection_source", lit("${e.name.replace(/"/g,'\\"')}")).write.mode("append").saveAsTable("${l?l+".":""}_dead_letter_queue")`),m.push(`print(f"[VALIDATE] {df_${n}_valid.count()} valid, {df_${n}_invalid.count()} invalid records")`),o=m.join(`
`),r=.92,{code:o,conf:r}}if(e.type==="DistributeLoad"){const l=t["Number of Relationships"]||"4";return o=`# Distribute Load: ${e.name}
df_${n} = df_${s}.repartition(${l})
print(f"[DISTRIBUTE] Repartitioned to ${l} partitions")`,r=.93,{code:o,conf:r}}return e.type==="DetectDuplicate"?(o=`# Dedup: ${e.name}
df_${n} = df_${s}.dropDuplicates()
print(f"[DEDUP] Removed duplicates")`,r=.93,{code:o,conf:r}):null}function Jn(e,t,n,s,a,i){let c=a,o=i;if(e.type==="HandleHttpResponse"){const r=t["HTTP Status Code"]||"200";return c=`# HTTP Response: ${e.name}
# NiFi sends HTTP response with status ${r}
print(f"[HTTP] Response: status=${r} for request")`,o=.92,{code:c,conf:o}}if(/^(Publish|Put)(Kafka|KafkaRecord)/.test(e.type)){const r=t["Kafka Brokers"]||t["bootstrap.servers"]||"kafka:9092",l=t["Topic Name"]||t.topic||"output_topic",p=t["Compression Type"]||"snappy";return c=`# Kafka Producer: ${e.name}
# Topic: ${l} | Brokers: ${r}
(df_${s}
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "${r}")
  .option("topic", "${l}")
  .option("kafka.compression.type", "${p}")
  .save()
)
print(f"[KAFKA] Published to ${l}")`,o=.95,{code:c,conf:o}}if(/^Put(SFTP|FTP)$/.test(e.type)){const r=t.Hostname||"sftp.target.com",l=t.Port||"22",p=t.Username||"sftp_user",d=t["Remote Path"]||"/exports/";return c=`# ${e.type}: ${e.name}
# Host: ${r}:${l} | Path: ${d}
import paramiko
_ssh = paramiko.SSHClient()
_ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
_ssh.connect("${r}", port=int("${l}"), username="${p}",
    password=dbutils.secrets.get(scope="sftp", key="password"))
_sftp = _ssh.open_sftp()

_pdf = df_${s}.toPandas()
_output_path = f"${d}/{_pdf.shape[0]}_records.csv"
_pdf.to_csv(f"/tmp/_sftp_out.csv", index=False)
_sftp.put("/tmp/_sftp_out.csv", _output_path)

_sftp.close()
_ssh.close()
print(f"[SFTP] Uploaded {_pdf.shape[0]} records to ${r}:${d}")`,o=.9,{code:c,conf:o}}if(/^(PutDatabaseRecord|PutSQL)$/.test(e.type)&&!c.includes("spark.read")){const r=t["Database Connection Pooling Service"]||t["JDBC Connection Pool"]||"",l=t["Table Name"]||"target_table",p=t["Schema Name"]||"",d=t["Statement Type"]||"INSERT";let u="jdbc:database://host:port/db",m="com.database.Driver";/oracle/i.test(r)?(u="jdbc:oracle:thin:@db_host:1521:db_sid",m="oracle.jdbc.driver.OracleDriver"):/postgres/i.test(r)?(u="jdbc:postgresql://pg_host:5432/pg_db",m="org.postgresql.Driver"):/mysql/i.test(r)&&(u="jdbc:mysql://mysql_host:3306/mysql_db",m="com.mysql.cj.jdbc.Driver");const y=p?`${p}.${l}`:l;return c=`# DB Write: ${e.name} (${d})
# Pool: ${r} | Table: ${y}
(df_${s}.write
  .format("jdbc")
  .option("url", "${u}")
  .option("dbtable", "${y}")
  .option("driver", "${m}")
  .option("user", dbutils.secrets.get(scope="db", key="user"))
  .option("password", dbutils.secrets.get(scope="db", key="pass"))
  .option("batchsize", 1000)
  .mode("append")
  .save()
)
print(f"[DB] Wrote to ${y}")`,o=.92,{code:c,conf:o}}if(e.type==="PutEmail"){const r=t["SMTP Hostname"]||"smtp.example.com",l=t["SMTP Port"]||"587",p=t.From||"noreply@example.com",d=t.To||"alerts@example.com",u=t.Subject||"Pipeline notification";return c=`# Email: ${e.name}
# SMTP: ${r}:${l} | From: ${p} | To: ${d}
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

_msg = MIMEMultipart()
_msg["From"] = "${p}"
_msg["To"] = "${d}"
_msg["Subject"] = f"${u}"
_msg.attach(MIMEText("Pipeline completed successfully.", "plain"))

with smtplib.SMTP("${r}", ${l}) as _smtp:
    _smtp.starttls()
    _smtp.login(dbutils.secrets.get(scope="email", key="user"),
               dbutils.secrets.get(scope="email", key="pass"))
    _smtp.send_message(_msg)
print(f"[EMAIL] Sent notification to ${d}")`,o=.9,{code:c,conf:o}}return null}function Kn(e,t,n,s,a,i){let c=a,o=i;if(e.type==="ExecuteStreamCommand"){const r=t.Command||t.command||t["Command Path"]||"",l=t["Command Arguments"]||t.command_arguments||"",p=(r+" "+l).trim(),d=p.toLowerCase();if(/hdfs\s+dfs|hadoop\s+fs|^dfs;/i.test(p)){const u=/-cp\b/.test(p)?"cp":/-mv\b/.test(p)?"mv":/-mkdir/.test(p)?"mkdirs":/-rm/.test(p)?"rm":/-ls/.test(p)?"ls":/-put/.test(p)||/-get/.test(p)?"cp":(/-cat/.test(p),"head"),m=p.match(/\/[\w${}./-]+/g)||[],y=m[0]?m[0].replace(/\$\{[^}]*\}/g,"<param>"):"/Volumes/<catalog>/<schema>/<path>",_=m[1]?m[1].replace(/\$\{[^}]*\}/g,"<param>"):"";return _&&(u==="cp"||u==="mv")?c=`# HDFS -> dbutils.fs.${u}
# Original: ${p.substring(0,120)}
dbutils.fs.${u}("${y}", "${_}")`:c=`# HDFS -> dbutils.fs.${u}
# Original: ${p.substring(0,120)}
dbutils.fs.${u}("${y}")`,o=.9,{code:c,conf:o}}if(/kinit|klist|kdestroy|keytab/i.test(d))return c=`# Kerberos -> Unity Catalog (no kinit needed)
# Original: ${p.substring(0,120)}
# Unity Catalog handles identity federation natively
print("[AUTH] Kerberos auth handled by Unity Catalog identity federation")`,o=.95,{code:c,conf:o};if(/impala-shell|impala/i.test(d)){let u=l.match(/-q\s*;?\s*"([^"]+)"/i)||l.match(/--query\s*=\s*"([^"]+)"/i)||l.match(/-q\s*;?\s*([^;]+(?:;[^;]+)*)\s*$/i);const m=u?u[1].trim().replace(/^"|"$/g,""):"";if(/refresh\s+/i.test(m)){const y=m.match(/refresh\s+([\w.]+)/i);c=`# Impala REFRESH -> Spark SQL
spark.catalog.refreshTable("${y?y[1]:"<table>"}")
# Original: ${m.substring(0,100)}`}else if(/invalidate\s+metadata/i.test(m)){const y=m.match(/invalidate\s+metadata\s+([\w.]+)/i);c=`# Impala INVALIDATE METADATA -> Spark SQL
spark.catalog.refreshTable("${y?y[1]:"<table>"}")
spark.sql("REFRESH TABLE ${y?y[1]:"<table>"}")
# Original: ${m.substring(0,100)}`}else if(/compute\s+stats/i.test(m)){const y=m.match(/compute\s+stats\s+([\w.]+)/i);c=`# Impala COMPUTE STATS -> Spark SQL ANALYZE TABLE
spark.sql("ANALYZE TABLE ${y?y[1]:"<table>"} COMPUTE STATISTICS")
# Original: ${m.substring(0,100)}`}else/select/i.test(m)?c=`# Impala SELECT -> Spark SQL
df_${n} = spark.sql("""
${m.replace(/"/g,'\\"').substring(0,200)}
""")
# Note: Impala SQL is mostly Spark-compatible`:/insert/i.test(m)?c=`# Impala INSERT -> Spark SQL
spark.sql("""
${m.replace(/"/g,'\\"').substring(0,200)}
""")
# Note: Ensure target table exists in Unity Catalog`:c=`# Impala -> Spark SQL
# Original: ${p.substring(0,150)}
spark.sql("${m.replace(/"/g,'\\"').substring(0,150)||"REFRESH TABLE <table>"}")`;return o=.9,{code:c,conf:o}}if(/hive|beeline/i.test(d)){const u=l.match(/-e\s*;?\s*"([^"]+)"/i)||l.match(/--query\s*=\s*"([^"]+)"/i);return c=`# Hive/Beeline -> Spark SQL
spark.sql("""
${u?u[1].replace(/"/g,'\\"').substring(0,200):"<hive_query>"}
""")
# Original: ${p.substring(0,100)}`,o=.9,{code:c,conf:o}}if(/sqoop/i.test(d)){const u=l.match(/--table\s+(\S+)/);return c=`# Sqoop -> Spark JDBC
df_${n} = (spark.read.format("jdbc")
  .option("url", dbutils.secrets.get(scope="<scope>", key="jdbc-url"))
  .option("dbtable", "${u?u[1]:"<table>"}")
  .load())
# Original: ${p.substring(0,100)}`,o=.9,{code:c,conf:o}}if(/\.jar\b/i.test(d))return c=`# JAR execution -> Spark Submit or cluster library
# Original: ${p.substring(0,120)}
# Upload JAR to /Volumes/<catalog>/<schema>/jars/ and add to cluster libraries
# spark._jvm.com.example.MainClass.run(args)`,o=.9,{code:c,conf:o};if(/\b(mv|cp|copy|move|rename)\b/i.test(p)||/\/[\w/.-]+/.test(p)){const u=p.match(/\/[\w${}./-]+/g)||[],m=u[0]?u[0].replace(/\$\{[^}]*\}/g,"<param>"):"/Volumes/<catalog>/<schema>/<src>",y=u[1]?u[1].replace(/\$\{[^}]*\}/g,"<param>"):"",_=/\brm\b|\bdelete\b/i.test(p)?"rm":/\bmv\b|\bmove\b|\brename\b/i.test(p)?"mv":/\bmkdir/i.test(p)?"mkdirs":/\bls\b|\bdir\b/i.test(p)?"ls":"cp";return y&&(_==="cp"||_==="mv")?c=`# Shell -> dbutils.fs.${_}
# Original: ${p.substring(0,120)}
dbutils.fs.${_}("${m}", "${y}")`:c=`# Shell -> dbutils.fs.${_}
# Original: ${p.substring(0,120)}
dbutils.fs.${_}("${m}")`,o=.9,{code:c,conf:o}}if(/\bwc\b.*-l|line.?count|count.*lines/i.test(p)){const u=p.match(/\/[\w${}./-]+/),m=u?u[0].replace(/\$\{[^}]*\}/g,"<param>"):"<file_path>";return c=`# Line count -> Spark
# Original: ${p.substring(0,120)}
_line_count = spark.read.text("${m}").count()
print(f"Line count: {_line_count}")`,o=.9,{code:c,conf:o}}if(p&&!c.includes("dbutils.fs")){const u=t["Working Directory"]||"/opt/scripts",m=l?", "+l.split(";").map(y=>'"'+y.trim()+'"').join(", "):"";return c=`# Shell Command: ${e.name}
# Command: ${r} ${l}
import subprocess
_result = subprocess.run(
    ["${r}"${m}],
    capture_output=True, text=True, timeout=300,
    cwd="${u}"
)
if _result.returncode != 0:
    print(f"[CMD ERROR] Return code: {_result.returncode}")
    raise RuntimeError(f"Command failed: ${r}")
else:
    print(f"[CMD OK] {_result.stdout[:200]}")
    _lines = [l for l in _result.stdout.strip().split("\\\\n") if l]
    if _lines:
        df_${n} = spark.createDataFrame([{"output": l} for l in _lines])
    else:
        df_${n} = df_${s}`,o=.9,{code:c,conf:o}}}if(e.type==="InvokeHTTP"){const r=t["Remote URL"]||t["HTTP URL"]||"https://api.example.com/endpoint",l=t["HTTP Method"]||"GET",p=t["Content-Type"]||"application/json",d=t["Connection Timeout"]||"30 secs",u=t["Read Timeout"]||"60 secs",m=t["Basic Authentication Username"]||"",y=m?`
_auth = (dbutils.secrets.get(scope="api", key="user"), dbutils.secrets.get(scope="api", key="pass"))`:"",_=m?", auth=_auth":"";return l==="GET"?c=`# HTTP ${l}: ${e.name}
# URL: ${r}${y}
import requests
_response = requests.${l.toLowerCase()}("${r}",
    headers={"Content-Type": "${p}", "Accept": "application/json"},
    timeout=(${parseInt(d)||30}, ${parseInt(u)||60})${_})
_response.raise_for_status()
_json = _response.json()
df_${n} = spark.createDataFrame([_json] if isinstance(_json, dict) else _json)
print(f"[HTTP] ${l} ${r} -> {_response.status_code}")`:c=`# HTTP ${l}: ${e.name}
# URL: ${r}${y}
import requests
_payload = df_${s}.limit(1000).toPandas().to_dict(orient="records")
_response = requests.${l.toLowerCase()}("${r}",
    json=_payload,
    headers={"Content-Type": "${p}"},
    timeout=(${parseInt(d)||30}, ${parseInt(u)||60})${_})
_response.raise_for_status()
print(f"[HTTP] ${l} ${r} -> {_response.status_code}, sent {len(_payload)} records")`,o=.92,{code:c,conf:o}}if(e.type==="ExecuteProcess"||e.type==="ExecuteProcessBash"){const r=t.Command||t["Command Path"]||"/bin/echo",l=t["Command Arguments"]||"";return c=`# ${e.type}: ${e.name}
import subprocess
_result = subprocess.run(["${r}", "${l}"], capture_output=True, text=True, timeout=300)
if _result.returncode != 0:
    raise RuntimeError(f"Command failed: {_result.stderr[:200]}")
df_${n} = df_${s}
print(f"[CMD] ${r} -> exit {_result.returncode}")`,o=.9,{code:c,conf:o}}if(e.type==="LookupRecord"){const r=t["Lookup Service"]||t["lookup-service"]||"",l=t.Key||t.key||"id";return c=`# Lookup Record: ${e.name}
_lookup_df = spark.table("${r||"lookup_table"}")
df_${n} = df_${s}.join(_lookup_df, "${l}", "left")
print(f"[LOOKUP] Joined with ${r||"lookup_table"} on ${l}")`,o=.92,{code:c,conf:o}}return e.type==="LookupAttribute"?(c=`# Lookup: ${e.name}
_lookup_df = spark.table("${t["Lookup Service"]||"lookup_table"}")
df_${n} = df_${s}.join(_lookup_df, "${t.Key||"id"}", "left")`,o=.92,{code:c,conf:o}):null}function Vn(e,t,n,s,a,i){let c=a,o=i;if(e.type==="Wait"){const r=t["Release Signal Identifier"]||"batch_signal";return t["Expiration Duration"],c="# Wait: "+e.name+" | Signal: "+r+`
#
# DO NOT use while/sleep polling loops in notebooks.
# Use Databricks Workflow task dependencies or Delta CDF streaming.
#
# OPTION 1 (RECOMMENDED): Databricks Workflow Task Dependency
# In workflow YAML, set: depends_on: [{task_key: "notify_`+r+`"}]
# Zero-cost, natively supported, no compute wasted.
#
# OPTION 2: Delta Change Data Feed (streaming trigger)
df_`+n+` = (spark.readStream
    .format("delta")
    .option("readChangeFeed", "true")
    .option("startingVersion", "latest")
    .table("workflow_signals")
    .filter("signal_id = '`+r+`' AND status = 'ready'")
)

def _on_signal_`+n+`(signal_batch, batch_id):
    if signal_batch.count() > 0:
        print(f"[WAIT] Signal `+r+` received in batch {batch_id}")
        spark.sql("UPDATE workflow_signals SET status = 'consumed' WHERE signal_id = '`+r+`'")

(df_`+n+`.writeStream
    .foreachBatch(_on_signal_`+n+`)
    .option("checkpointLocation", "/tmp/checkpoints/wait_`+r+`")
    .trigger(processingTime="5 seconds")
    .start()
    .awaitTermination(timeout=300)
)

# After signal received, continue with original data
df_`+n+" = df_"+s+`
print(f"[WAIT] `+r+' — proceeding")',o=.92,{code:c,conf:o}}if(e.type==="Notify"){const r=t["Release Signal Identifier"]||"batch_signal";return c="# Notify: "+e.name+" | Signal: "+r+`
# Ensure signals table exists with CDF enabled
spark.sql("""
CREATE TABLE IF NOT EXISTS workflow_signals (
    signal_id STRING, status STRING, payload STRING, ts TIMESTAMP
) USING DELTA
TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')
""")

# Emit signal for downstream Wait processors
spark.sql(f"""
MERGE INTO workflow_signals t
USING (SELECT '`+r+`' AS signal_id, 'ready' AS status, NULL AS payload, current_timestamp() AS ts) s
ON t.signal_id = s.signal_id
WHEN MATCHED THEN UPDATE SET status = 'ready', ts = current_timestamp()
WHEN NOT MATCHED THEN INSERT *
""")
df_`+n+" = df_"+s+`
print(f"[NOTIFY] Signal `+r+' sent")',o=.93,{code:c,conf:o}}if(e.type==="LogMessage"){const r=t["log-level"]||"info",l=t["log-prefix"]||"",d=(t["log-message"]||"").replace(/"/g,"'").substring(0,200);return c=`# Log: ${e.name}
import logging
_logger = logging.getLogger("nifi_migration")
_logger.${r}(f"${l}${d}")
df_${n} = df_${s}  # Pass through`,o=.95,{code:c,conf:o}}if(e.type==="LogAttribute")return c=`# Log Attributes: ${e.name}
import logging
_logger = logging.getLogger("nifi_migration")
_logger.info(f"Schema: {df_${s}.schema.simpleString()}")
_logger.info(f"Count: {df_${s}.count()}")
df_${n} = df_${s}  # Pass through`,o=.95,{code:c,conf:o};if(e.type==="CountText")return c=`# Count: ${e.name}
from pyspark.sql.functions import lit
_count = df_${s}.count()
df_${n} = df_${s}.withColumn("_row_count", lit(_count))
print(f"[COUNT] {_count} rows")`,o=.95,{code:c,conf:o};if(e.type==="DebugFlow")return c=`# Debug: ${e.name}
df_${s}.show(20, truncate=False)
df_${s}.printSchema()
df_${n} = df_${s}`,o=.95,{code:c,conf:o};if(e.type==="ControlRate"){const r=t["Maximum Rate"]||"1000",l=t["Rate Control Criteria"]||"flowfile count";return c=`# Rate Control: ${e.name}
# ${l}: max ${r}
# In Spark, rate limiting is handled by trigger intervals
df_${n} = df_${s}
# spark.readStream...trigger(processingTime="1 second")
print(f"[RATE] Limited to ${r} per interval")`,o=.92,{code:c,conf:o}}if(e.type==="RetryFlowFile"){const r=t["Maximum Retries"]||t["Retry Count"]||"3",l=t["Penalty Duration"]||"30000 ms";return c=`# Retry: ${e.name}
# Max retries: ${r} | Penalty: ${l}
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError
from pyspark.sql.functions import current_timestamp, lit

@retry(stop=stop_after_attempt(${r}), wait=wait_exponential(multiplier=1, min=2, max=30))
def _process_with_retry_${n}(df):
    return df  # TODO: Apply actual processing logic here

try:
    df_${n} = _process_with_retry_${n}(df_${s})
    print(f"[RETRY] ${e.name.replace(/"/g,"'")} succeeded")
except RetryError as _retry_err:
    print(f"[RETRY] ${e.name.replace(/"/g,"'")} exhausted ${r} retries — writing to DLQ")
    df_${s}.withColumn("_dlq_error", lit(str(_retry_err))).withColumn("_dlq_source", lit("${e.name.replace(/"/g,'\\"')}")).withColumn("_dlq_timestamp", current_timestamp()).write.mode("append").saveAsTable("__dead_letter_queue")
    df_${n} = spark.createDataFrame([], df_${s}.schema)
    print(f"[DLQ] Failed records written to __dead_letter_queue")`,o=.92,{code:c,conf:o}}if(e.type==="EnforceOrder"){const r=t["Order Attribute"]||"sequence";return c=`# Enforce Order: ${e.name}
from pyspark.sql.functions import col
df_${n} = df_${s}.orderBy(col("${r}").asc())
print(f"[ORDER] Sorted by ${r}")`,o=.92,{code:c,conf:o}}if(e.type==="MonitorActivity"){const r=t["Threshold Duration"]||"5 min";return c=`# Monitor Activity: ${e.name}
# Threshold: ${r}
# In Databricks, use Workflow alerts or Delta table monitoring
df_${n} = df_${s}
print(f"[MONITOR] Activity threshold: ${r}")`,o=.92,{code:c,conf:o}}if(e.type==="UpdateCounter")return c=`# Counter: ${e.name}
_counter = spark.sparkContext.accumulator(0)
def _count(row): _counter.add(1)
df_${s}.foreach(_count)
df_${n} = df_${s}`,o=.92,{code:c,conf:o};if(e.type==="UpdateHiveTable")return c=`# Hive DDL: ${e.name}
spark.sql("ALTER TABLE ${t["Table Name"]||"hive_table"} SET TBLPROPERTIES ('updated'='true')")
df_${n} = df_${s}`,o=.92,{code:c,conf:o};if(e.type==="Funnel"||e.type==="InputPort"||e.type==="OutputPort")return c=`# ${e.type}: ${e.name}
# Funnels/Ports are routing constructs — no-op in Spark
df_${n} = df_${s}`,o=.95,{code:c,conf:o};if(e.type==="RemoteProcessGroup"){const r=t.URLs||t["Target URIs"]||"";return c=`# RemoteProcessGroup: ${e.name}
# Remote URL: ${r}
# In Databricks, use Delta Sharing or cross-workspace API calls
df_${n} = df_${s}
print(f"[REMOTE] Site-to-Site -> Delta Sharing")`,o=.9,{code:c,conf:o}}if(e.type==="SendNiFiSiteToSite")return c=`# Site-to-Site: ${e.name}
# Migrate to Delta Sharing or Databricks workspace API
df_${n} = df_${s}
print(f"[S2S] -> Delta Sharing")`,o=.9,{code:c,conf:o};if(e.type==="SpringContextProcessor")return c=`# Spring Context: ${e.name}
df_${n} = df_${s}
print("[SPRING] Migrated Spring bean logic")`,o=.9,{code:c,conf:o};if(e.type==="YandexTranslate")return c=`# Yandex Translate: ${e.name}
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import requests
@udf(StringType())
def translate(text):
    r = requests.post("https://translate.api.cloud.yandex.net/translate/v2/translate", json={"texts": [text], "targetLanguageCode": "${t["Target Language"]||"en"}"})
    return r.json().get("translations", [{}])[0].get("text", text)
df_${n} = df_${s}.withColumn("_translated", translate(col("value")))`,o=.9,{code:c,conf:o};if(e.type==="GetTwitter"){const r=t["Terms to Filter On"]||"databricks";return c=`# Twitter: ${e.name}
import tweepy
_auth = tweepy.OAuth2BearerHandler(dbutils.secrets.get(scope="twitter", key="bearer_token"))
_api = tweepy.API(_auth)
_tweets = [{"text": t.text} for t in _api.search_tweets(q="${r}", count=100)]
df_${n} = spark.createDataFrame(_tweets)`,o=.9,{code:c,conf:o}}if(e.type==="PostSlack"){const r=t["Webhook URL"]||"";return c=`# Slack: ${e.name}
import requests
requests.post("${r}", json={"text": "Pipeline notification"})
df_${n} = df_${s}`,o=.92,{code:c,conf:o}}return e.type==="SendTelegram"?(c=`# Telegram: ${e.name}
import requests
_token = dbutils.secrets.get(scope="telegram", key="bot_token")
requests.post(f"https://api.telegram.org/bot{_token}/sendMessage", json={"chat_id": "${t["Chat ID"]||""}", "text": "Pipeline complete"})
df_${n} = df_${s}`,o=.9,{code:c,conf:o}):null}function Xn(e,t,n,s,a,i){let c=a,o=i;if(/^(List|Fetch|Get|Put|Delete)S3/.test(e.type)){const r=t.Bucket||"s3_bucket",l=t["Object Key"]||t.Prefix||"data/",p=/^(Put|Delete)/.test(e.type);return/^List/.test(e.type)?c=`# S3 List: ${e.name}
# Bucket: ${r} | Prefix: ${l}
df_${n} = spark.createDataFrame(
    [{"path": f.path, "name": f.name, "size": f.size}
     for f in dbutils.fs.ls(f"s3://${r}/${l}")]
)
print(f"[S3] Listed objects from s3://${r}/${l}")`:p&&e.type!=="DeleteS3Object"?c=`# S3 Write: ${e.name}
# Bucket: ${r} | Key: ${l}
(df_${s}.write
  .format("delta")
  .mode("append")
  .save(f"s3://${r}/${l}")
)
print(f"[S3] Wrote to s3://${r}/${l}")`:e.type==="DeleteS3Object"?c=`# S3 Delete: ${e.name}
dbutils.fs.rm(f"s3://${r}/${l}", recurse=True)
print(f"[S3] Deleted s3://${r}/${l}")`:c=`# S3 Read: ${e.name}
# Bucket: ${r} | Key: ${l}
df_${n} = spark.read.format("delta").load(f"s3://${r}/${l}")
print(f"[S3] Read from s3://${r}/${l}")`,o=.93,{code:c,conf:o}}if(e.type==="PutSNS"){const r=t["Amazon Resource Name (ARN)"]||t["Topic ARN"]||"arn:aws:sns:us-east-1:123456789:topic",l=t.Region||"us-east-1";return c=`# SNS: ${e.name}
# Topic: ${r}
import boto3
_sns = boto3.client("sns", region_name="${l}")
for row in df_${s}.limit(10000).collect():
    _sns.publish(TopicArn="${r}", Message=str(row.asDict()))
print(f"[SNS] Published to ${r}")`,o=.9,{code:c,conf:o}}if(e.type==="GetSQS"){const r=t["Queue URL"]||"https://sqs.us-east-1.amazonaws.com/123456789/queue",l=t.Region||"us-east-1";return c=`# SQS Consumer: ${e.name}
# Queue: ${r}
import boto3
_sqs = boto3.client("sqs", region_name="${l}")
_msgs = []
while True:
    _resp = _sqs.receive_message(QueueUrl="${r}", MaxNumberOfMessages=10, WaitTimeSeconds=5)
    _batch = _resp.get("Messages", [])
    if not _batch: break
    for m in _batch:
        _msgs.append({"body": m["Body"], "receipt_handle": m["ReceiptHandle"]})
        _sqs.delete_message(QueueUrl="${r}", ReceiptHandle=m["ReceiptHandle"])
    if len(_msgs) >= 10000: break
df_${n} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")
print(f"[SQS] Consumed {len(_msgs)} messages")`,o=.9,{code:c,conf:o}}if(e.type==="PutDynamoDB"||e.type==="GetDynamoDB"){const r=t["Table Name"]||"dynamodb_table",l=t.Region||"us-east-1";return e.type==="PutDynamoDB"?c=`# DynamoDB Write: ${e.name}
# Table: ${r}
import boto3
_dynamodb = boto3.resource("dynamodb", region_name="${l}")
_table = _dynamodb.Table("${r}")
with _table.batch_writer() as _batch:
    for row in df_${s}.limit(10000).collect():
        _batch.put_item(Item=row.asDict())
df_${n} = df_${s}
print(f"[DYNAMODB] Wrote to ${r}")`:c=`# DynamoDB Read: ${e.name}
# Table: ${r}
import boto3
_dynamodb = boto3.resource("dynamodb", region_name="${l}")
_table = _dynamodb.Table("${r}")
_items = _table.scan().get("Items", [])
df_${n} = spark.createDataFrame(_items) if _items else spark.createDataFrame([], "id STRING")
print(f"[DYNAMODB] Read {len(_items)} items from ${r}")`,o=.9,{code:c,conf:o}}if(e.type==="DeleteDynamoDB")return c=`# DynamoDB Delete: ${e.name}
import boto3
_dynamodb = boto3.resource("dynamodb", region_name="${t.Region||"us-east-1"}")
_table = _dynamodb.Table("${t["Table Name"]||"table"}")
with _table.batch_writer() as _batch:
    for row in df_${s}.limit(10000).collect():
        _batch.delete_item(Key={"id": row["id"]})`,o=.9,{code:c,conf:o};if(e.type==="DeleteSQS")return c=`# SQS Delete: ${e.name}
import boto3
_sqs = boto3.client("sqs", region_name="${t.Region||"us-east-1"}")
df_${n} = df_${s}`,o=.9,{code:c,conf:o};if(e.type==="PutKinesisStream"){const r=t["Kinesis Stream Name"]||t["Amazon Kinesis Stream Name"]||"stream",l=t.Region||"us-east-1";return c=`# Kinesis Write: ${e.name}
# Stream: ${r}
import boto3, json
_kinesis = boto3.client("kinesis", region_name="${l}")
for row in df_${s}.limit(10000).collect():
    _kinesis.put_record(StreamName="${r}", Data=json.dumps(row.asDict()), PartitionKey=str(row[0]))
print(f"[KINESIS] Wrote to ${r}")`,o=.9,{code:c,conf:o}}if(e.type==="PutLambda"){const r=t["Amazon Lambda Name"]||t["Function Name"]||"lambda_function",l=t.Region||"us-east-1";return c=`# Lambda: ${e.name}
# Function: ${r}
import boto3, json
_lambda = boto3.client("lambda", region_name="${l}")
for row in df_${s}.limit(1000).collect():
    _lambda.invoke(FunctionName="${r}", InvocationType="Event", Payload=json.dumps(row.asDict()))
print(f"[LAMBDA] Invoked ${r}")`,o=.9,{code:c,conf:o}}if(e.type==="InvokeAWSGatewayApi"){const r=t["API Gateway URL"]||t.URL||"https://api.execute-api.amazonaws.com";return c=`# AWS API GW: ${e.name}
import requests
_response = requests.post("${r}", json=df_${s}.limit(100).toPandas().to_dict(orient="records"))
df_${n} = spark.createDataFrame([_response.json()] if isinstance(_response.json(), dict) else _response.json())`,o=.9,{code:c,conf:o}}return null}function Yn(e,t,n,s,a,i){let c=a,o=i;if(/^(Put|Fetch|List|Delete)Azure(Blob|DataLake)/.test(e.type)){const r=t["Container Name"]||t["Filesystem Name"]||"mycontainer",l=t["Storage Account Name"]||"mystorageaccount",p=t["Blob Name"]||t.Directory||"data/",d=/^Put/.test(e.type),u=/DataLake/.test(e.type),m=u?"abfss":"wasbs",y=u?"dfs.core.windows.net":"blob.core.windows.net";return d?c=`# Azure Write: ${e.name}
(df_${s}.write
  .format("delta")
  .mode("append")
  .save("${m}://${r}@${l}.${y}/${p}")
)
print(f"[AZURE] Wrote to ${m}://${r}@${l}")`:c=`# Azure Read: ${e.name}
df_${n} = spark.read.format("delta").load("${m}://${r}@${l}.${y}/${p}")
print(f"[AZURE] Read from Azure")`,o=.93,{code:c,conf:o}}if(e.type==="ConsumeAzureEventHub"){const r=t["Event Hub Namespace"]||"my-eventhub-ns",l=t["Event Hub Name"]||"my-hub",p=t["Consumer Group"]||"$Default";return t["Event Hub Connection String"]||t["Connection String"],c=`# Azure Event Hub Consumer: ${e.name}
# Namespace: ${r} | Hub: ${l} | Group: ${p}
_conn_str = dbutils.secrets.get(scope="azure", key="eventhub-conn-str")
_ehConf = {
    "eventhubs.connectionString": sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(_conn_str),
    "eventhubs.consumerGroup": "${p}",
    "eventhubs.startingPosition": '{"offset": "-1", "seqNo": -1, "enqueuedTime": null, "isInclusive": true}'
}
df_${n} = (spark.readStream
  .format("eventhubs")
  .options(**_ehConf)
  .load()
  .selectExpr("CAST(body AS STRING) as value", "enqueuedTime as timestamp", "offset", "sequenceNumber")
)
print(f"[EVENTHUB] Consuming from ${r}/${l}")`,o=.92,{code:c,conf:o}}if(e.type==="PutEventHub"||e.type==="PublishAzureEventHub"){const r=t["Event Hub Namespace"]||"my-eventhub-ns",l=t["Event Hub Name"]||"my-hub";return c=`# Azure Event Hub Producer: ${e.name}
# Namespace: ${r} | Hub: ${l}
_conn_str = dbutils.secrets.get(scope="azure", key="eventhub-conn-str")
_ehConf = {
    "eventhubs.connectionString": sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(_conn_str)
}
(df_${s}
  .selectExpr("CAST(value AS STRING) as body")
  .write
  .format("eventhubs")
  .options(**_ehConf)
  .save()
)
print(f"[EVENTHUB] Published to ${r}/${l}")`,o=.92,{code:c,conf:o}}if(e.type==="PutAzureQueueStorage"||e.type==="GetAzureQueueStorage"){const r=t["Queue Name"]||"my-queue";return t["Storage Account Name"],/^Put/.test(e.type)?c=`# Azure Queue Write: ${e.name}
from azure.storage.queue import QueueClient
_queue = QueueClient.from_connection_string(
    dbutils.secrets.get(scope="azure", key="storage-conn-str"), "${r}")
for row in df_${s}.limit(10000).collect():
    _queue.send_message(str(row.asDict()))
print(f"[AZURE-QUEUE] Published to ${r}")`:c=`# Azure Queue Read: ${e.name}
from azure.storage.queue import QueueClient
_queue = QueueClient.from_connection_string(
    dbutils.secrets.get(scope="azure", key="storage-conn-str"), "${r}")
_msgs = [{"body": m.content} for m in _queue.receive_messages(messages_per_page=32)]
df_${n} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")
print(f"[AZURE-QUEUE] Read {len(_msgs)} messages from ${r}")`,o=.9,{code:c,conf:o}}return null}function Zn(e,t,n,s,a,i){let c=a,o=i;if(/^(Put|List|Fetch|Get|Delete)GCS/.test(e.type)){const r=t.Bucket||t["GCS Bucket"]||"gcs_bucket",l=t.Key||t.Prefix||"data/",p=/^List/.test(e.type),d=/^Put/.test(e.type),u=/^Delete/.test(e.type);return p?c=`# GCS List: ${e.name}
# Bucket: ${r} | Prefix: ${l}
df_${n} = spark.createDataFrame(
    [{"path": f.path, "name": f.name, "size": f.size}
     for f in dbutils.fs.ls(f"gs://${r}/${l}")]
)
print(f"[GCS] Listed objects from gs://${r}/${l}")`:d?c=`# GCS Write: ${e.name}
# Bucket: ${r} | Key: ${l}
(df_${s}.write
  .format("delta")
  .mode("append")
  .save(f"gs://${r}/${l}")
)
print(f"[GCS] Wrote to gs://${r}/${l}")`:u?c=`# GCS Delete: ${e.name}
dbutils.fs.rm(f"gs://${r}/${l}", recurse=True)
print(f"[GCS] Deleted gs://${r}/${l}")`:c=`# GCS Read: ${e.name}
# Bucket: ${r} | Key: ${l}
df_${n} = spark.read.format("delta").load(f"gs://${r}/${l}")
print(f"[GCS] Read from gs://${r}/${l}")`,o=.93,{code:c,conf:o}}if(e.type==="PutBigQueryBatch"||e.type==="PutBigQuery"){const r=t["Project ID"]||t.Project||"my-project",l=t.Dataset||"my_dataset",p=t["Table Name"]||t.Table||"my_table";return c=`# BigQuery Write: ${e.name}
# Project: ${r} | Dataset: ${l} | Table: ${p}
(df_${s}.write
  .format("bigquery")
  .option("project", "${r}")
  .option("dataset", "${l}")
  .option("table", "${p}")
  .option("temporaryGcsBucket", "gs://${t["Temporary Bucket"]||"temp-bucket"}/bq-staging")
  .mode("append")
  .save()
)
print(f"[BQ] Wrote to ${r}.${l}.${p}")`,o=.92,{code:c,conf:o}}if(e.type==="ConsumeGCPubSub"){const r=t.Subscription||t["Subscription Name"]||"projects/my-project/subscriptions/my-sub";return t["Project ID"],c=`# GCP Pub/Sub Consumer: ${e.name}
# Subscription: ${r}
from google.cloud import pubsub_v1
import json
_subscriber = pubsub_v1.SubscriberClient()
_msgs = []
def _callback(message):
    _msgs.append({"data": message.data.decode("utf-8"), "attributes": dict(message.attributes)})
    message.ack()
_future = _subscriber.subscribe("${r}", callback=_callback)
import time; time.sleep(10)
_future.cancel()
df_${n} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "data STRING")
print(f"[PUBSUB] Consumed {len(_msgs)} messages")`,o=.9,{code:c,conf:o}}if(e.type==="PublishGCPubSub"){const r=t["Topic Name"]||t.Topic||"projects/my-project/topics/my-topic";return c=`# GCP Pub/Sub Publisher: ${e.name}
# Topic: ${r}
from google.cloud import pubsub_v1
import json
_publisher = pubsub_v1.PublisherClient()
for row in df_${s}.limit(10000).collect():
    _publisher.publish("${r}", json.dumps(row.asDict()).encode("utf-8"))
print(f"[PUBSUB] Published to ${r}")`,o=.9,{code:c,conf:o}}return null}function es(e,t,n,s,a,i){let c=a,o=i;if(/^(Query|Put|Get)Cassandra/.test(e.type)){const r=t.Keyspace||"my_keyspace",l=t.Table||t["Table Name"]||"my_table",p=t["Contact Points"]||t["Cassandra Contact Points"]||"cassandra_host";return/^Put/.test(e.type)?c=`# Cassandra Write: ${e.name}
# Keyspace: ${r} | Table: ${l}
(df_${s}.write
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "${r}")
  .option("table", "${l}")
  .option("spark.cassandra.connection.host", "${p}")
  .mode("append")
  .save()
)
print(f"[CASSANDRA] Wrote to ${r}.${l}")`:c=`# Cassandra Read: ${e.name}
# Keyspace: ${r} | Table: ${l}
df_${n} = (spark.read
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "${r}")
  .option("table", "${l}")
  .option("spark.cassandra.connection.host", "${p}")
  .load()
)
print(f"[CASSANDRA] Read from ${r}.${l}")`,o=.92,{code:c,conf:o}}if(/^(Get|Put|Scan|Delete)HBase/.test(e.type)){const r=t["Table Name"]||"hbase_table",l=t["HBase Hostname"]||t["HBase Client Service"]||"hbase_host",p=/^Put/.test(e.type);return/^Delete/.test(e.type)?c=`# HBase Delete: ${e.name}
import happybase
_conn = happybase.Connection("${l}")
_table = _conn.table("${r}")
for row in df_${s}.limit(1000).collect():
    _table.delete(row["row_key"].encode())
_conn.close()
print(f"[HBASE] Deleted from ${r}")`:p?c=`# HBase Write: ${e.name}
# Table: ${r}
import happybase, json
_conn = happybase.Connection("${l}")
_table = _conn.table("${r}")
for row in df_${s}.limit(10000).collect():
    _d = row.asDict()
    _rk = str(_d.get("row_key", _d.get("id", "")))
    _table.put(_rk.encode(), {f"cf:{k}".encode(): str(v).encode() for k, v in _d.items()})
_conn.close()
print(f"[HBASE] Wrote to ${r}")`:c=`# HBase Read: ${e.name}
# Table: ${r}
import happybase
_conn = happybase.Connection("${l}")
_table = _conn.table("${r}")
_rows = [{**{"row_key": k.decode()}, **{c.decode(): v.decode() for c, v in d.items()}} for k, d in _table.scan(limit=50000)]
df_${n} = spark.createDataFrame(_rows) if _rows else spark.createDataFrame([], "row_key STRING")
_conn.close()
print(f"[HBASE] Read {len(_rows)} rows from ${r}")`,o=.9,{code:c,conf:o}}if(/^(SelectHiveQL|PutHiveQL|PutHiveStreaming|UpdateHiveTable)$/.test(e.type)){const r=t["HiveQL Select Query"]||t["HiveQL Statement"]||"",l=t["Table Name"]||"hive_table";if(/^(Put|Update)/.test(e.type))c=`# Hive Write: ${e.name}
(df_${s}.write
  .mode("append")
  .saveAsTable("${l}")
)
print(f"[HIVE] Wrote to ${l}")`;else{const d=r||`SELECT * FROM ${l}`;c=`# Hive Query: ${e.name}
df_${n} = spark.sql("""${d.substring(0,300)}""")
print(f"[HIVE] Queried ${l}")`}return o=.92,{code:c,conf:o}}if(/^(Put|Get|Query)Kudu/.test(e.type)){const r=t["Kudu Masters"]||"kudu_master:7051",l=t["Table Name"]||"kudu_table";return/^Put/.test(e.type)?c=`# Kudu Write: ${e.name}
# Master: ${r} | Table: ${l}
(df_${s}.write
  .format("kudu")
  .option("kudu.master", "${r}")
  .option("kudu.table", "${l}")
  .mode("append")
  .save()
)
print(f"[KUDU] Wrote to ${l}")`:c=`# Kudu Read: ${e.name}
df_${n} = (spark.read
  .format("kudu")
  .option("kudu.master", "${r}")
  .option("kudu.table", "${l}")
  .load()
)
print(f"[KUDU] Read from ${l}")`,o=.9,{code:c,conf:o}}if(/^(Query|Put)Phoenix/.test(e.type)){const r=t["Phoenix URL"]||t["ZooKeeper Quorum"]||"zk_host:2181",l=t["Table Name"]||"phoenix_table";return/^Put/.test(e.type)?c=`# Phoenix Write: ${e.name}
(df_${s}.write
  .format("org.apache.phoenix.spark")
  .option("table", "${l}")
  .option("zkUrl", "${r}")
  .mode("overwrite")
  .save()
)
print(f"[PHOENIX] Wrote to ${l}")`:c=`# Phoenix Read: ${e.name}
df_${n} = (spark.read
  .format("org.apache.phoenix.spark")
  .option("table", "${l}")
  .option("zkUrl", "${r}")
  .load()
)
print(f"[PHOENIX] Read from ${l}")`,o=.9,{code:c,conf:o}}if(/^(Query|Put)Oracle/.test(e.type)){const r=t["Table Name"]||"oracle_table";return c=`# Oracle: ${e.name}
df_${n} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="oracle", key="jdbc-url"))
  .option("dbtable", "${r}")
  .option("driver", "oracle.jdbc.driver.OracleDriver")
  .option("user", dbutils.secrets.get(scope="oracle", key="user"))
  .option("password", dbutils.secrets.get(scope="oracle", key="pass"))
  .load()
)
print(f"[ORACLE] Read from ${r}")`,o=.92,{code:c,conf:o}}if(/^(Query|Put)Teradata/.test(e.type)){const r=t["Table Name"]||"teradata_table";return c=`# Teradata: ${e.name}
df_${n} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="teradata", key="jdbc-url"))
  .option("dbtable", "${r}")
  .option("driver", "com.teradata.jdbc.TeraDriver")
  .option("user", dbutils.secrets.get(scope="teradata", key="user"))
  .option("password", dbutils.secrets.get(scope="teradata", key="pass"))
  .load()
)
print(f"[TERADATA] Read from ${r}")`,o=.92,{code:c,conf:o}}if(/^(Get|Put|Query)Snowflake/.test(e.type)){const r=t.Database||"snowflake_db",l=t.Schema||"public",p=t["Table Name"]||"snowflake_table";return/^Put/.test(e.type)?c=`# Snowflake Write: ${e.name}
(df_${s}.write
  .format("snowflake")
  .option("sfUrl", dbutils.secrets.get(scope="snowflake", key="url"))
  .option("sfUser", dbutils.secrets.get(scope="snowflake", key="user"))
  .option("sfPassword", dbutils.secrets.get(scope="snowflake", key="pass"))
  .option("sfDatabase", "${r}")
  .option("sfSchema", "${l}")
  .option("dbtable", "${p}")
  .mode("append")
  .save()
)
print(f"[SNOWFLAKE] Wrote to ${r}.${l}.${p}")`:c=`# Snowflake Read: ${e.name}
df_${n} = (spark.read
  .format("snowflake")
  .option("sfUrl", dbutils.secrets.get(scope="snowflake", key="url"))
  .option("sfUser", dbutils.secrets.get(scope="snowflake", key="user"))
  .option("sfPassword", dbutils.secrets.get(scope="snowflake", key="pass"))
  .option("sfDatabase", "${r}")
  .option("sfSchema", "${l}")
  .option("dbtable", "${p}")
  .load()
)
print(f"[SNOWFLAKE] Read from ${r}.${l}.${p}")`,o=.92,{code:c,conf:o}}return null}function ts(e,t,n,s,a,i){let c=a,o=i;if(/^(Consume|Publish)(JMS|AMQP)$/.test(e.type)){const r=t["Destination Name"]||t.Queue||"default_queue",l=/^Consume/.test(e.type);if(/AMQP/.test(e.type)){const p=t.Hostname||"amqp_host";l?c=`# AMQP Consumer: ${e.name}
# Queue: ${r}
import pika
_conn = pika.BlockingConnection(pika.ConnectionParameters(
    host="${p}",
    credentials=pika.PlainCredentials(
        dbutils.secrets.get(scope="amqp", key="user"),
        dbutils.secrets.get(scope="amqp", key="pass"))))
_ch = _conn.channel()
_msgs = []
def _cb(ch, method, properties, body):
    _msgs.append({"body": body.decode("utf-8")})
    ch.basic_ack(delivery_tag=method.delivery_tag)
    if len(_msgs) >= 1000: ch.stop_consuming()
_ch.basic_consume(queue="${r}", on_message_callback=_cb)
try:
    _ch.start_consuming()
except: pass
df_${n} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")
_conn.close()
print(f"[AMQP] Consumed {len(_msgs)} messages from ${r}")`:c=`# AMQP Publisher: ${e.name}
# Queue: ${r}
import pika, json
_conn = pika.BlockingConnection(pika.ConnectionParameters(
    host="${p}",
    credentials=pika.PlainCredentials(
        dbutils.secrets.get(scope="amqp", key="user"),
        dbutils.secrets.get(scope="amqp", key="pass"))))
_ch = _conn.channel()
_ch.queue_declare(queue="${r}", durable=True)
for row in df_${s}.limit(10000).collect():
    _ch.basic_publish(exchange="", routing_key="${r}",
        body=json.dumps(row.asDict()),
        properties=pika.BasicProperties(delivery_mode=2))
_conn.close()
print(f"[AMQP] Published to ${r}")`}else{const p=t.Hostname||"jms_host",d=t.Port||"61613",u=t["Client ID"]||"nifi-migration-client";l?c=`# JMS Consumer: ${e.name}
# Destination: ${r} | Client: ${u}
import stomp
import time

_msgs = []
class _JMSListener(stomp.ConnectionListener):
    """JMS message listener with connection management."""
    def on_message(self, frame):
        _msgs.append({"body": frame.body, "headers": str(frame.headers)})
    def on_error(self, frame):
        print(f"[JMS ERROR] {frame.body}")
    def on_disconnected(self):
        print("[JMS] Disconnected")

_conn = stomp.Connection([("${p}", ${d})])
_conn.set_listener("jms_listener", _JMSListener())
try:
    _conn.connect(
        dbutils.secrets.get(scope="jms", key="user"),
        dbutils.secrets.get(scope="jms", key="pass"),
        wait=True, headers={"client-id": "${u}"})
    _conn.subscribe(destination="/queue/${r}", id=1, ack="client-individual")
    time.sleep(5)  # Collect messages for 5 seconds
finally:
    _conn.disconnect()

df_${n} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING, headers STRING")
print(f"[JMS] Consumed {len(_msgs)} messages from ${r}")`:c=`# JMS Publisher: ${e.name}
# Destination: ${r} | Client: ${u}
import stomp, json

_conn = stomp.Connection([("${p}", ${d})])
try:
    _conn.connect(
        dbutils.secrets.get(scope="jms", key="user"),
        dbutils.secrets.get(scope="jms", key="pass"),
        wait=True, headers={"client-id": "${u}"})
    _sent = 0
    for row in df_${s}.limit(10000).collect():
        _conn.send(
            destination="/queue/${r}",
            body=json.dumps(row.asDict()),
            content_type="application/json",
            headers={"persistent": "true"})
        _sent += 1
    print(f"[JMS] Published {_sent} messages to ${r}")
finally:
    _conn.disconnect()`}return o=.9,{code:c,conf:o}}if(e.type==="GetJMSQueue"){const r=t["Destination Name"]||t.Queue||"default_queue",l=t.Hostname||"jms_host",p=t.Port||"61613";return c=`# JMS Queue: ${e.name}
import stomp
_msgs = []
class _L(stomp.ConnectionListener):
    def on_message(self, frame): _msgs.append({"body": frame.body})
_conn = stomp.Connection([("${l}", ${p})])
_conn.set_listener("", _L())
_conn.connect(dbutils.secrets.get(scope="jms", key="user"), dbutils.secrets.get(scope="jms", key="pass"), wait=True)
_conn.subscribe(destination="/queue/${r}", id=1, ack="auto")
import time; time.sleep(5)
_conn.disconnect()
df_${n} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")`,o=.9,{code:c,conf:o}}if(/^(Consume|Publish)MQTT$/.test(e.type)){const r=t["Topic Filter"]||t.Topic||"iot/sensors/#",l=t["Broker URI"]||"tcp://mqtt_broker:1883",p=l.replace("tcp://","").split(":")[0]||"mqtt_broker",d=l.split(":").pop()||"1883";return/^Consume/.test(e.type)?c=`# MQTT Consumer: ${e.name}
# Broker: ${l} | Topic: ${r}
import paho.mqtt.client as mqtt
import json, time
_msgs = []
def _on_msg(client, userdata, msg):
    _msgs.append({"topic": msg.topic, "payload": msg.payload.decode("utf-8")})
_client = mqtt.Client()
_client.username_pw_set(dbutils.secrets.get(scope="mqtt", key="user"),
                        dbutils.secrets.get(scope="mqtt", key="pass"))
_client.on_message = _on_msg
_client.connect("${p}", ${d})
_client.subscribe("${r}")
_client.loop_start()
time.sleep(10)
_client.loop_stop()
_client.disconnect()
df_${n} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "topic STRING, payload STRING")
print(f"[MQTT] Consumed {len(_msgs)} messages from ${r}")`:c=`# MQTT Publisher: ${e.name}
# Broker: ${l} | Topic: ${r}
import paho.mqtt.client as mqtt
import json
_client = mqtt.Client()
_client.username_pw_set(dbutils.secrets.get(scope="mqtt", key="user"),
                        dbutils.secrets.get(scope="mqtt", key="pass"))
_client.connect("${p}", ${d})
for row in df_${s}.limit(10000).collect():
    _client.publish("${r}", json.dumps(row.asDict()))
_client.disconnect()
print(f"[MQTT] Published to ${r}")`,o=.9,{code:c,conf:o}}if(e.type==="ListenSMTP")return c=`# SMTP: ${e.name}
# Deploy as Databricks App with aiosmtpd
df_${n} = df_${s}
print("[SMTP] Email receiver configured")`,o=.9,{code:c,conf:o};if(e.type==="ConnectWebSocket"||e.type==="ListenWebSocket"||e.type==="PutWebSocket"){const r=t["WebSocket URI"]||t.URL||"ws://localhost:8080";return c=`# WebSocket: ${e.name}
import websocket, json
_ws = websocket.create_connection("${r}")
df_${n} = df_${s}`,o=.9,{code:c,conf:o}}if(e.type==="ListenGRPC"||e.type==="InvokeGRPC")return c=`# gRPC: ${e.name}
import grpc
df_${n} = df_${s}`,o=.9,{code:c,conf:o};if(e.type==="ListenTCPRecord"||e.type==="ListenUDPRecord")return c=`# ${e.type}: ${e.name}
df_${n} = (spark.readStream
  .format("socket")
  .option("host", "${t["Local Network Interface"]||"localhost"}")
  .option("port", "${t.Port||"9999"}")
  .load())`,o=.9,{code:c,conf:o};if(e.type==="GetSmbFile"||e.type==="PutSmbFile"){const r=t.Hostname||t["SMB Share"]||"smb_server";return c=`# SMB: ${e.name}
import smbclient
smbclient.register_session("${r}", username=dbutils.secrets.get(scope="smb", key="user"), password=dbutils.secrets.get(scope="smb", key="pass"))
df_${n} = df_${s}`,o=.9,{code:c,conf:o}}return null}function ns(e,t,n,s,a,i){let c=a,o=i;if(e.type==="PutSplunkHTTP"||e.type==="GetSplunk"){if(/^Put/.test(e.type)){const l=t["HTTP Event Collector URL"]||"https://splunk:8088/services/collector";c=`# Splunk HEC: ${e.name}
import requests
_token = dbutils.secrets.get(scope="splunk", key="hec_token")
_sent = 0
for row in df_${s}.limit(10000).collect():
    requests.post("${l}",
        json={"event": row.asDict()},
        headers={"Authorization": f"Splunk {_token}"},
        verify=False)
    _sent += 1
print(f"[SPLUNK] Sent {_sent} events to HEC")`}else{const l=t["Splunk URL"]||"https://splunk:8089";c=`# Splunk Search: ${e.name}
import requests
_token = dbutils.secrets.get(scope="splunk", key="api_token")
_resp = requests.post("${l}/services/search/jobs/export",
    data={"search": "${t["Splunk Query"]||"search index=main | head 1000"}",
          "output_mode": "json"},
    headers={"Authorization": f"Bearer {_token}"},
    verify=False)
_events = [e for e in _resp.json().get("results", [])]
df_${n} = spark.createDataFrame(_events) if _events else df_${s}
print(f"[SPLUNK] Retrieved {len(_events)} events")`}return o=.9,{code:c,conf:o}}if(e.type==="ExecuteInfluxDBQuery"||e.type==="PutInfluxDB"){const r=t["InfluxDB Connection URL"]||"http://influxdb:8086";if(/^Put/.test(e.type)){const p=t.Bucket||t["Database Name"]||"default",d=t.Organization||"my-org";c=`# InfluxDB Write: ${e.name}
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS
_client = InfluxDBClient(url="${r}", token=dbutils.secrets.get(scope="influx", key="token"), org="${d}")
_write_api = _client.write_api(write_options=SYNCHRONOUS)
for row in df_${s}.limit(10000).collect():
    _point = Point("measurement").field("value", str(row[0]))
    _write_api.write(bucket="${p}", org="${d}", record=_point)
_client.close()
print(f"[INFLUXDB] Wrote to ${p}")`}else c=`# InfluxDB Query: ${e.name}
from influxdb_client import InfluxDBClient
_client = InfluxDBClient(url="${r}", token=dbutils.secrets.get(scope="influx", key="token"))
_tables = _client.query_api().query("${t["Flux Query"]||'from(bucket: \\"default\\")'}")
_records = [r.values for table in _tables for r in table.records]
df_${n} = spark.createDataFrame(_records) if _records else df_${s}
_client.close()
print(f"[INFLUXDB] Retrieved {len(_records)} records")`;return o=.9,{code:c,conf:o}}if(/^(Put|Send)Datadog/.test(e.type))return c=`# Datadog: ${e.name}
import requests
_api_key = dbutils.secrets.get(scope="datadog", key="api_key")
for row in df_${s}.limit(10000).collect():
    requests.post("https://api.datadoghq.com/api/v2/logs",
        json={"ddsource": "nifi-migration", "message": str(row.asDict())},
        headers={"DD-API-KEY": _api_key, "Content-Type": "application/json"})
df_${n} = df_${s}
print(f"[DATADOG] Sent logs")`,o=.9,{code:c,conf:o};if(/^(Push|Query)Prometheus/.test(e.type)){const r=t["Push Gateway URL"]||t["Prometheus URL"]||"http://prometheus:9091";return c=`# Prometheus: ${e.name}
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
_registry = CollectorRegistry()
_gauge = Gauge("nifi_migration_metric", "Migrated metric", registry=_registry)
_gauge.set(df_${s}.count())
push_to_gateway("${r}", job="nifi_migration", registry=_registry)
df_${n} = df_${s}
print(f"[PROMETHEUS] Pushed metrics to ${r}")`,o=.9,{code:c,conf:o}}if(/^(Send|Push)Grafana/.test(e.type)){const r=t["Grafana URL"]||"http://grafana:3000";return c=`# Grafana: ${e.name}
import requests
requests.post("${r}/api/annotations",
    json={"text": "Pipeline event", "tags": ["nifi-migration"]},
    headers={"Authorization": f"Bearer {dbutils.secrets.get(scope='grafana', key='api_key')}"})
df_${n} = df_${s}
print(f"[GRAFANA] Sent annotation")`,o=.9,{code:c,conf:o}}return e.type==="PutRiemann"?(c=`# Riemann: ${e.name}
df_${n} = df_${s}
print("[RIEMANN] Monitoring event sent")`,o=.9,{code:c,conf:o}):null}function ss(e,t,n,s,a,i){let c=a,o=i;if(/^(Put|Fetch|Get|Query|Scroll|JsonQuery|Delete)Elasticsearch/.test(e.type)){const r=t["Elasticsearch URL"]||t["HTTP Hosts"]||"https://es_host:9200",l=t.Index||"default_index",p=/^Put/.test(e.type),d=/^Delete/.test(e.type);return p?c=`# Elasticsearch Write: ${e.name}
# URL: ${r} | Index: ${l}
from elasticsearch import Elasticsearch, helpers
_es = Elasticsearch("${r}",
    basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")),
    verify_certs=False)

_records = df_${s}.limit(10000).toPandas().to_dict(orient="records")
_actions = [{"_index": "${l}", "_source": r} for r in _records]
helpers.bulk(_es, _actions, chunk_size=500, request_timeout=60)
print(f"[ES] Indexed {len(_records)} documents to ${l}")`:d?c=`# Elasticsearch Delete: ${e.name}
from elasticsearch import Elasticsearch
_es = Elasticsearch("${r}", basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")))
_es.delete_by_query(index="${l}", body={"query": {"match_all": {}}})
df_${n} = df_${s}
print(f"[ES] Deleted from ${l}")`:c=`# Elasticsearch Read: ${e.name}
# URL: ${r} | Index: ${l}
from elasticsearch import Elasticsearch
_es = Elasticsearch("${r}",
    basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")),
    verify_certs=False)

_result = _es.search(index="${l}", body={"query": {"match_all": {}}}, size=10000, scroll="2m")
_hits = [h["_source"] for h in _result["hits"]["hits"]]
df_${n} = spark.createDataFrame(_hits) if _hits else spark.createDataFrame([], "id STRING")
print(f"[ES] Read {len(_hits)} documents from ${l}")`,o=.9,{code:c,conf:o}}if(/^(Get|Put|Delete)Mongo/.test(e.type)){const r=t["Mongo URI"]||"mongodb://mongo_host:27017",l=t["Mongo Database Name"]||"mydb",p=t["Mongo Collection Name"]||"mycollection";return/^(Put|Delete)/.test(e.type)&&e.type!=="DeleteMongo"?c=`# MongoDB Write: ${e.name}
# URI: ${r} | DB: ${l} | Collection: ${p}
from pymongo import MongoClient
_client = MongoClient("${r}")
_db = _client["${l}"]
_coll = _db["${p}"]

_records = df_${s}.limit(10000).toPandas().to_dict(orient="records")
_result = _coll.insert_many(_records)
print(f"[MONGO] Inserted {len(_result.inserted_ids)} documents into ${l}.${p}")
_client.close()`:e.type==="DeleteMongo"?c=`# MongoDB Delete: ${e.name}
from pymongo import MongoClient
_client = MongoClient("${r}")
_coll = _client["${l}"]["${p}"]
_result = _coll.delete_many({})
print(f"[MONGO] Deleted {_result.deleted_count} documents from ${l}.${p}")
_client.close()
df_${n} = df_${s}`:c=`# MongoDB Read: ${e.name}
# URI: ${r} | DB: ${l} | Collection: ${p}
from pymongo import MongoClient
_client = MongoClient("${r}")
_db = _client["${l}"]
_coll = _db["${p}"]

_docs = list(_coll.find({}, {"_id": 0}).limit(50000))
df_${n} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
print(f"[MONGO] Read {len(_docs)} documents from ${l}.${p}")
_client.close()`,o=.9,{code:c,conf:o}}if(e.type==="GetMongoRecord"){const r=t["Mongo URI"]||"mongodb://mongo:27017",l=t["Mongo Database Name"]||"mydb",p=t["Mongo Collection Name"]||"collection";return c=`# Mongo Record: ${e.name}
from pymongo import MongoClient
_client = MongoClient("${r}")
_docs = list(_client["${l}"]["${p}"].find({}, {"_id": 0}).limit(50000))
df_${n} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
_client.close()`,o=.9,{code:c,conf:o}}if(e.type==="RunMongoAggregation"){const r=t["Mongo URI"]||"mongodb://mongo:27017",l=t["Mongo Database Name"]||"mydb",p=t["Mongo Collection Name"]||"collection";return c=`# Mongo Aggregation: ${e.name}
from pymongo import MongoClient
_client = MongoClient("${r}")
_results = list(_client["${l}"]["${p}"].aggregate([]))
df_${n} = spark.createDataFrame(_results) if _results else df_${s}
_client.close()`,o=.9,{code:c,conf:o}}if(/^(Fetch|Put|Delete)GridFS$/.test(e.type)){const r=t["Mongo URI"]||"mongodb://mongo:27017",l=t["Mongo Database Name"]||"files_db";return c=`# GridFS ${e.type}: ${e.name}
from pymongo import MongoClient
import gridfs
_client = MongoClient("${r}")
_fs = gridfs.GridFS(_client["${l}"])
df_${n} = df_${s}
_client.close()`,o=.9,{code:c,conf:o}}if(/^(Put|Get|Fetch|Delete)Redis/.test(e.type)){const r=t["Redis Connection Pool"]||t.Hostname||"redis_host",l=t.Port||"6379";return/^(Put|Delete)/.test(e.type)?c=`# Redis Write: ${e.name}
import redis, json
_r = redis.Redis(host="${r}", port=${l},
    password=dbutils.secrets.get(scope="redis", key="pass"))
for row in df_${s}.limit(10000).collect():
    _d = row.asDict()
    _r.set(str(_d.get("id", "")), json.dumps(_d))
print(f"[REDIS] Wrote to Redis")
df_${n} = df_${s}`:c=`# Redis Read: ${e.name}
import redis, json
_r = redis.Redis(host="${r}", port=${l},
    password=dbutils.secrets.get(scope="redis", key="pass"))
_keys = _r.keys("*")
_data = [json.loads(_r.get(k)) for k in _keys[:50000] if _r.get(k)]
df_${n} = spark.createDataFrame(_data) if _data else spark.createDataFrame([], "id STRING")
print(f"[REDIS] Read {len(_data)} keys")`,o=.9,{code:c,conf:o}}if(/^(Get|Put)Couchbase/.test(e.type)){const r=t["Cluster Hostname"]||"couchbase_host",l=t["Bucket Name"]||"default";return/^Put/.test(e.type)?c=`# Couchbase Write: ${e.name}
from couchbase.cluster import Cluster
from couchbase.auth import PasswordAuthenticator
_auth = PasswordAuthenticator(
    dbutils.secrets.get(scope="couchbase", key="user"),
    dbutils.secrets.get(scope="couchbase", key="pass"))
_cluster = Cluster(f"couchbase://${r}", authenticator=_auth)
_bucket = _cluster.bucket("${l}")
_coll = _bucket.default_collection()
for row in df_${s}.limit(10000).collect():
    _d = row.asDict()
    _coll.upsert(str(_d.get("id", "")), _d)
df_${n} = df_${s}
print(f"[COUCHBASE] Wrote to ${l}")`:c=`# Couchbase Read: ${e.name}
from couchbase.cluster import Cluster
from couchbase.auth import PasswordAuthenticator
_auth = PasswordAuthenticator(
    dbutils.secrets.get(scope="couchbase", key="user"),
    dbutils.secrets.get(scope="couchbase", key="pass"))
_cluster = Cluster(f"couchbase://${r}", authenticator=_auth)
_result = _cluster.query("SELECT * FROM \`${l}\` LIMIT 50000")
_rows = [r for r in _result]
df_${n} = spark.createDataFrame(_rows) if _rows else spark.createDataFrame([], "id STRING")
print(f"[COUCHBASE] Read from ${l}")`,o=.9,{code:c,conf:o}}if(/^(Put|Query|Get)Solr/.test(e.type)){const r=t["Solr Location"]||t["Solr URL"]||"http://solr:8983/solr",l=t.Collection||"default_collection";return/^Put/.test(e.type)?c=`# Solr Write: ${e.name}
import requests, json
for row in df_${s}.limit(10000).collect():
    requests.post(f"${r}/${l}/update/json/docs",
        json=row.asDict(),
        params={"commit": "true"})
df_${n} = df_${s}
print(f"[SOLR] Indexed to ${l}")`:c=`# Solr Read: ${e.name}
import requests
_resp = requests.get(f"${r}/${l}/select", params={"q": "*:*", "rows": 50000, "wt": "json"})
_docs = _resp.json().get("response", {}).get("docs", [])
df_${n} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
print(f"[SOLR] Read {len(_docs)} documents from ${l}")`,o=.9,{code:c,conf:o}}if(/^(Get|Put|Delete)RethinkDB$/.test(e.type)){const r=t.Hostname||"rethinkdb_host",l=t["DB Name"]||"test",p=t["Table Name"]||"data";return c=`# RethinkDB: ${e.name}
from rethinkdb import r
_conn = r.connect(host="${r}", port=28015, db="${l}")
_docs = list(r.table("${p}").limit(50000).run(_conn))
df_${n} = spark.createDataFrame(_docs) if _docs else df_${s}
_conn.close()`,o=.9,{code:c,conf:o}}return null}function os(e,t,n,s,a,i){let c=a,o=i;if(/^(Get|Put|Fetch|List|Move|Delete)HDFS$/.test(e.type)){const r=t.Directory||"/data";return/^(Put|Move|Delete)/.test(e.type)&&e.type==="PutHDFS"?c=`# HDFS Write: ${e.name}
# Directory: ${r}
(df_${s}.write
  .format("delta")
  .mode("append")
  .save("${r}")
)
print(f"[HDFS] Wrote to ${r}")`:e.type==="MoveHDFS"?c=`# HDFS Move: ${e.name}
dbutils.fs.mv("${r}/source", "${r}/dest", recurse=True)
print(f"[HDFS] Moved files")`:e.type==="DeleteHDFS"?c=`# HDFS Delete: ${e.name}
dbutils.fs.rm("${r}", recurse=True)
print(f"[HDFS] Deleted ${r}")`:c=`# HDFS Read: ${e.name}
# Directory: ${r}
df_${n} = spark.read.format("delta").load("${r}")
print(f"[HDFS] Read from ${r}")`,o=.93,{code:c,conf:o}}if(e.type==="GetHDFSEvents")return c=`# HDFS Events: ${e.name}
df_${n} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .load("${t["HDFS Path"]||"/data"}"))
print(f"[HDFS] Streaming events via Auto Loader")`,o=.92,{code:c,conf:o};if(e.type==="GetHDFSFileInfo")return c=`# HDFS Info: ${e.name}
_files = dbutils.fs.ls("${t.Directory||"/data"}")
df_${n} = spark.createDataFrame([{"path": f.path, "name": f.name, "size": f.size} for f in _files])`,o=.93,{code:c,conf:o};if(e.type==="GetHDFSSequenceFile")return c=`# SequenceFile: ${e.name}
df_${n} = spark.sparkContext.sequenceFile("${t.Directory||"/data"}", "org.apache.hadoop.io.Text", "org.apache.hadoop.io.Text").toDF(["key", "value"])`,o=.9,{code:c,conf:o};if(/^(Put|Get|Fetch)ORC$/.test(e.type)||e.type==="ConvertAvroToORC"){const r=t.Directory||t.Path||"/mnt/data";return/^Put/.test(e.type)?c=`# ORC Write: ${e.name}
(df_${s}.write
  .format("orc")
  .mode("append")
  .save("${r}")
)
print(f"[ORC] Wrote to ${r}")`:c=`# ORC Read: ${e.name}
df_${n} = spark.read.format("orc").load("${r}")
print(f"[ORC] Read from ${r}")`,o=.93,{code:c,conf:o}}if(/^(Put|Get|Fetch)Parquet$/.test(e.type)){const r=t.Directory||t.Path||"/mnt/data";return/^Put/.test(e.type)?c=`# Parquet Write: ${e.name}
(df_${s}.write
  .format("parquet")
  .mode("append")
  .save("${r}")
)
print(f"[PARQUET] Wrote to ${r}")`:c=`# Parquet Read: ${e.name}
df_${n} = spark.read.format("parquet").load("${r}")
print(f"[PARQUET] Read from ${r}")`,o=.95,{code:c,conf:o}}if(/^(Put|Get|Listen)Flume/.test(e.type)){const r=t.Hostname||"flume_host",l=t.Port||"41414";return c=`# Flume: ${e.name}
# Flume is deprecated — use Spark Structured Streaming directly
# Host: ${r}:${l}
df_${n} = (spark.readStream
  .format("socket")
  .option("host", "${r}")
  .option("port", "${l}")
  .load()
)
print(f"[FLUME] Streaming from ${r}:${l} — consider migrating to native Spark source")`,o=.85,{code:c,conf:o}}if(/^Put(Hudi|HoodieRecord)$/.test(e.type)){const r=t["Table Name"]||"hudi_table",l=t["Base Path"]||"/mnt/hudi",p=t["Record Key Field"]||"id",d=t["Precombine Key Field"]||"timestamp";return c=`# Hudi Write: ${e.name}
(df_${s}.write
  .format("hudi")
  .option("hoodie.table.name", "${r}")
  .option("hoodie.datasource.write.recordkey.field", "${p}")
  .option("hoodie.datasource.write.precombine.field", "${d}")
  .option("hoodie.datasource.write.operation", "upsert")
  .mode("append")
  .save("${l}/${r}")
)
print(f"[HUDI] Wrote to ${r}")`,o=.92,{code:c,conf:o}}if(/^PutIceberg$/.test(e.type)){const r=t["Table Name"]||"iceberg_table";return c=`# Iceberg Write: ${e.name}
(df_${s}.writeTo("${r}")
  .using("iceberg")
  .append()
)
print(f"[ICEBERG] Wrote to ${r}")`,o=.92,{code:c,conf:o}}return null}const _e={GetFile:{cat:"Auto Loader",tpl:`df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "{format}")
  .option("cloudFiles.schemaLocation", "/mnt/schema/{v}")
  .load("/Volumes/{catalog}/{schema}/{path}"))`,desc:"Auto Loader from Databricks Volumes",notes:"Configure volume mount path",imp:[],conf:.9},GetHTTP:{cat:"Spark HTTP",tpl:`# Ingest from REST API via Spark
_url = "{url}"
_resp_rdd = spark.sparkContext.parallelize([_url])
df_{v} = spark.read.json(spark.sparkContext.wholeTextFiles(_url).values())`,desc:"REST API ingestion via Spark",notes:'For paginated APIs use spark.read.format("json") with custom schema',imp:[],conf:.9},ConsumeKafka:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load())`,desc:"Kafka streaming source",notes:"Configure security protocol if needed",imp:[],conf:.9},ConsumeKafka_2_6:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load())`,desc:"Kafka streaming source",notes:"Same as ConsumeKafka",imp:[],conf:.9},ConsumeKafkaRecord_2_6:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load()
  .select(from_json(col("value").cast("string"), schema).alias("data"))
  .select("data.*"))`,desc:"Kafka record streaming",notes:"Define schema for deserialization",imp:[],conf:.9},QueryDatabaseTable:{cat:"JDBC Source",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "{table}")
  .option("driver", "{driver}")
  .load())`,desc:"JDBC database read",notes:"Store credentials in Databricks secret scope",imp:[],conf:.9},QueryDatabaseTableRecord:{cat:"JDBC Source",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "{table}")
  .load())`,desc:"JDBC database read with record",notes:"Same as QueryDatabaseTable",imp:[],conf:.9},ListenHTTP:{cat:"Model Serving",tpl:`# ListenHTTP -> Databricks Model Serving / Delta Live Tables
# Deploy an MLflow serving endpoint that writes to Delta
df_{v} = spark.readStream.format("delta").table("{v}_incoming")
print(f"[HTTP] Streaming from {v}_incoming")`,desc:"HTTP listener -> Model Serving",notes:"Never run Flask in notebook — use Model Serving",imp:["mlflow"],conf:.92},GetSFTP:{cat:"External Storage",tpl:`# Stage from SFTP to Unity Catalog Volumes
dbutils.fs.cp("sftp://{host}/{path}", "/Volumes/{catalog}/{schema}/landing/")
df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/landing/")`,desc:"SFTP file retrieval",notes:"Requires SFTP mount or external transfer utility",imp:[],conf:.9},GetFTP:{cat:"External Storage",tpl:`# Stage from FTP to Unity Catalog Volumes (use external transfer)
df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/landing/")`,desc:"FTP file retrieval",notes:"No native FTP — use external transfer tool to stage to Volumes",imp:[],conf:.9},GenerateFlowFile:{cat:"Test Data",tpl:`df_{v} = spark.range({count}).toDF("id")
# Add test columns as needed`,desc:"Test data generator",notes:"Replace with actual test data generation",imp:[],conf:.9},ListS3:{cat:"Cloud Storage",tpl:`_files = dbutils.fs.ls("s3://{bucket}/{prefix}")
df_{v} = spark.createDataFrame(_files)`,desc:"List S3 objects",notes:"Use Unity Catalog external locations",imp:[],conf:.9},FetchS3Object:{cat:"Cloud Storage",tpl:'df_{v} = spark.read.format("{format}").load("s3://{bucket}/{key}")',desc:"Read S3 object",notes:"Configure external location in Unity Catalog",imp:[],conf:.9},GetS3Object:{cat:"Cloud Storage",tpl:'df_{v} = spark.read.format("{format}").load("s3://{bucket}/{key}")',desc:"Read S3 object",notes:"Same as FetchS3Object",imp:[],conf:.9},TailFile:{cat:"Auto Loader",tpl:`df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "text")
  .load("/Volumes/{catalog}/{schema}/{path}"))`,desc:"File tail via Auto Loader",notes:"Streaming mode handles new files automatically",imp:[],conf:.9},ListFile:{cat:"Cloud Storage",tpl:`_files = dbutils.fs.ls("/Volumes/{catalog}/{schema}/{path}")
df_{v} = spark.createDataFrame(_files)`,desc:"List files in directory",notes:"Use Volumes or external location",imp:[],conf:.9},GetMongo:{cat:"MongoDB Connector",tpl:`df_{v} = (spark.read
  .format("mongodb")
  .option("connection.uri", dbutils.secrets.get(scope="{scope}", key="mongo-uri"))
  .option("database", "{database}")
  .option("collection", "{collection}")
  .load())`,desc:"MongoDB read",notes:"Install mongodb-spark-connector library",imp:[],conf:.9},GetElasticsearch:{cat:"ES Connector",tpl:`df_{v} = (spark.read
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .option("es.resource", "{index}")
  .load())`,desc:"Elasticsearch read",notes:"Install elasticsearch-spark library",imp:[],conf:.9},FetchFile:{cat:"Volumes Read",tpl:`# Fetch file content by path attribute
_path = "/Volumes/{catalog}/{schema}/landing/{filename}"
df_{v} = spark.read.format("{format}").load(_path)`,desc:"Read file by path from Unity Catalog Volumes",notes:"Map NiFi filename attribute to Volumes path",imp:[],conf:.9},ConvertRecord:{cat:"DataFrame API",tpl:`df_{v} = df_{in}.selectExpr("*")  # Convert record format
# Adjust column types/names as needed`,desc:"Record format conversion",notes:"Spark handles format conversion natively",imp:[],conf:.9},ConvertJSONToSQL:{cat:"Spark SQL",tpl:`df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("SELECT * FROM tmp_{v}")`,desc:"JSON to SQL conversion",notes:"Parse JSON and query with SQL",imp:[],conf:.9},SplitRecord:{cat:"DataFrame API",tpl:`df_{v} = df_{in}.withColumn("item", explode(col("{array_field}")))
  .select("item.*")`,desc:"Split records by array field",notes:"Identify the array field to explode",imp:[],conf:.9},MergeRecord:{cat:"DataFrame API",tpl:"df_{v} = df_{in1}.unionByName(df_{in2}, allowMissingColumns=True)",desc:"Merge/union records",notes:"Ensure compatible schemas",imp:[],conf:.9},MergeContent:{cat:"DataFrame API",tpl:"df_{v} = df_{in1}.unionByName(df_{in2}, allowMissingColumns=True)",desc:"Merge content streams",notes:"Same as MergeRecord for DataFrames",imp:[],conf:.9},ReplaceText:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{col}", regexp_replace(col("{col}"), "{pattern}", "{replacement}"))',desc:"Regex text replacement",notes:"Apply to specific text columns",imp:[],conf:.9},UpdateAttribute:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{attr}", lit("{value}"))',desc:"Set/update attributes as columns",notes:"Map NiFi attributes to DataFrame columns",imp:[],conf:.9},JoltTransformJSON:{cat:"JSON Processing",tpl:`# Define target schema for JSON transform
_schema = "..."
df_{v} = df_{in}.withColumn("parsed", from_json(col("value"), _schema))
  .select("parsed.*")`,desc:"Complex JSON transformation",notes:"Jolt specs must be manually translated to Spark JSON ops",imp:[],conf:.9},EvaluateJsonPath:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{field}", get_json_object(col("value"), "$.{path}"))',desc:"Extract JSON paths",notes:"Map each JsonPath expression to get_json_object",imp:[],conf:.9},ExtractText:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{field}", regexp_extract(col("{col}"), "{pattern}", {group}))',desc:"Regex text extraction",notes:"Translate NiFi regex groups to Spark",imp:[],conf:.9},SplitJson:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("items", explode(from_json(col("value"), ArrayType(StringType()))))',desc:"Split JSON array",notes:"Define element schema",imp:[],conf:.9},SplitText:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("lines", explode(split(col("value"), "\\\\n")))',desc:"Split text by delimiter",notes:"Adjust delimiter as needed",imp:[],conf:.9},SplitContent:{cat:"DataFrame API",tpl:`# Split content by byte boundary or delimiter
df_{v} = df_{in}.withColumn("parts", split(col("value"), "{byte_sequence}"))
df_{v} = df_{v}.withColumn("part", explode(col("parts"))).drop("parts")`,desc:"Split content by delimiter/boundary",notes:"Adjust byte_sequence pattern for content splitting",imp:[],conf:.9},CompressContent:{cat:"Native",tpl:`# Delta Lake handles compression natively (snappy/zstd)
# No explicit compression step needed
df_{v} = df_{in}`,desc:"Compression",notes:"Delta Lake auto-compresses",imp:[],conf:.95},EncryptContent:{cat:"Security",tpl:`# Use Databricks column-level encryption or workspace-level encryption
# See: docs.databricks.com/security/column-level-encryption
df_{v} = df_{in}  # TODO: Apply aes_encrypt() on sensitive columns`,desc:"Encryption",notes:"Use workspace-level encryption or aes_encrypt()",imp:[],conf:.9},HashContent:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{col}_hash", sha2(col("{col}").cast("string"), 256))',desc:"SHA-256 hashing",notes:"Apply to specific columns",imp:[],conf:.9},TransformXml:{cat:"XML Processing",tpl:`# Use spark-xml library
df_{v} = spark.read.format("com.databricks.spark.xml").option("rowTag", "{tag}").load("{path}")`,desc:"XML transformation",notes:"Install spark-xml; define row tag",imp:[],conf:.9},ExecuteScript:{cat:"PySpark Cell",tpl:`# Custom script from NiFi ExecuteScript
# Original engine: {engine}
# TODO: Translate script logic to PySpark DataFrame operations
df_{v} = df_{in}`,desc:"Custom script execution",notes:"Manual translation to PySpark required — review original script",imp:[],conf:.9},ExecuteStreamCommand:{cat:"dbutils Shell",tpl:`# Execute shell command via %sh magic or dbutils
# %sh {command}
# Or use dbutils.notebook.run() to call a helper notebook
dbutils.fs.put("/tmp/{v}_cmd.sh", "{command}", True)`,desc:"External command via dbutils",notes:"Use %sh cell magic or dbutils.notebook.run()",imp:[],conf:.9},ConvertAvroToJSON:{cat:"DataFrame API",tpl:`from pyspark.sql.avro.functions import from_avro
df_{v} = df_{in}.select(from_avro("value", _schema).alias("data")).select("data.*")`,desc:"Avro to JSON",notes:"Spark handles Avro natively",imp:[],conf:.9},AttributesToJSON:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.select(to_json(struct("*")).alias("json_value"))',desc:"Columns to JSON",notes:"Converts all columns to single JSON string",imp:[],conf:.9},RouteOnAttribute:{cat:"DataFrame Filter",tpl:`# Route based on attribute conditions
df_{v}_matched = df_{in}.filter("{condition}")
df_{v}_unmatched = df_{in}.filter("NOT ({condition})")`,desc:"Conditional routing",notes:"Map NiFi routing rules to filter conditions",imp:[],conf:.9},RouteOnContent:{cat:"DataFrame Filter",tpl:'df_{v} = df_{in}.filter(col("value").rlike("{pattern}"))',desc:"Content-based routing",notes:"Translate content match patterns to regex",imp:[],conf:.9},ValidateRecord:{cat:"DLT Expectations",tpl:`# Data quality validation using DLT expectations
# @dlt.expect_or_drop("{rule}", "{expression}")
df_{v} = df_{in}.filter("{expression}")  # Drop invalid rows`,desc:"Record validation",notes:"Best implemented as DLT expectations",imp:[],conf:.9},DistributeLoad:{cat:"Spark Partitioning",tpl:"df_{v} = df_{in}.repartition({partitions})",desc:"Load distribution",notes:"Spark handles distribution automatically; repartition if needed",imp:[],conf:.9},DetectDuplicate:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.dropDuplicates(["{key}"])',desc:"Duplicate detection/removal",notes:"Specify dedup key columns",imp:[],conf:.9},ExecuteSQL:{cat:"Spark SQL",tpl:`df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("""
{sql}
""")`,desc:"SQL execution",notes:"Register input as temp view first",imp:[],conf:.9},ExecuteSQLRecord:{cat:"Spark SQL",tpl:`df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("""
{sql}
""")`,desc:"SQL execution with records",notes:"Same as ExecuteSQL",imp:[],conf:.9},LookupRecord:{cat:"DataFrame Join",tpl:`# Load lookup table from Unity Catalog
df_lookup = spark.table("{catalog}.{schema}.{lookup_table}")
df_{v} = df_{in}.join(df_lookup, on="{key}", how="left")`,desc:"Record lookup via join",notes:"Ensure lookup table exists in Unity Catalog",imp:[],conf:.9},InvokeHTTP:{cat:"Spark UDF",tpl:`# HTTP call via PySpark pandas UDF (Databricks-compatible)
from pyspark.sql.functions import pandas_udf
import pandas as pd
@pandas_udf("string")
def _call_api(urls: pd.Series) -> pd.Series:
  import urllib.request, json
  def _get(u):
    with urllib.request.urlopen(u) as r: return r.read().decode()
  return urls.apply(_get)
df_{v} = df_{in}.withColumn("api_response", _call_api(col("url")))`,desc:"HTTP API call via PySpark UDF",notes:"Uses pandas UDF for distributed execution; add error handling",imp:[],conf:.9},PutDatabaseRecord:{cat:"JDBC Write",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "{table}")
  .mode("append")
  .save())`,desc:"Database record write",notes:"Store JDBC credentials in secret scope",imp:[],conf:.9},HandleHttpRequest:{cat:"Model Serving",tpl:`# HandleHttpRequest -> Databricks Model Serving Endpoint
# Incoming data lands in Delta table for downstream processing
df_{v} = spark.readStream.format("delta").table("{v}_incoming")`,desc:"HTTP server -> Model Serving",notes:"Never run blocking web server in notebook",imp:["mlflow"],conf:.92},HandleHttpResponse:{cat:"Manual",tpl:`# TODO: No direct equivalent for HandleHttpResponse
# Pair with HandleHttpRequest replacement`,desc:"HTTP response handler",notes:"Use with HandleHttpRequest alternative",imp:[],conf:.9},PutFile:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"Write to Delta Lake table",notes:"Uses Unity Catalog managed table",imp:[],conf:.9},PutSQL:{cat:"JDBC Write",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "{table}")
  .mode("append")
  .save())`,desc:"SQL database write",notes:"Store credentials in secret scope",imp:[],conf:.9},PutKafka:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Write to Kafka topic",notes:"Configure security protocol",imp:[],conf:.9},PublishKafka:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Publish to Kafka",notes:"Same as PutKafka",imp:[],conf:.9},PublishKafka_2_6:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Publish to Kafka 2.6",notes:"Same as PutKafka",imp:[],conf:.9},PublishKafkaRecord_2_6:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Publish Kafka records",notes:"Same as PutKafka",imp:[],conf:.9},PutS3Object:{cat:"Cloud Storage Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .save("s3a://{bucket}/{path}"))`,desc:"Write to S3",notes:"Use external location in Unity Catalog",imp:[],conf:.9},PutHDFS:{cat:"Cloud Storage Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .save("{path}"))`,desc:"Write to cloud storage",notes:"Map HDFS path to cloud storage / Volumes",imp:[],conf:.9},PutSFTP:{cat:"External Storage Write",tpl:`# NiFi PutSFTP → paramiko SFTP transfer
import paramiko
_sftp_host = dbutils.secrets.get(scope="{scope}", key="sftp-host") if "{hostname}" == "<hostname>" else "{hostname}"
_sftp_user = dbutils.secrets.get(scope="{scope}", key="sftp-user")
_sftp_key = dbutils.secrets.get(scope="{scope}", key="sftp-private-key")
# Stage data to local temp file
_local_path = "/tmp/{v}_export"
df_{in}.toPandas().to_csv(_local_path, index=False)
# Transfer via SFTP
_pkey = paramiko.RSAKey.from_private_key_file(_sftp_key) if _sftp_key.startswith("/") else paramiko.RSAKey(data=_sftp_key.encode())
_transport = paramiko.Transport((_sftp_host, 22))
_transport.connect(username=_sftp_user, pkey=_pkey)
_sftp = paramiko.SFTPClient.from_transport(_transport)
_sftp.put(_local_path, "{remote_path}/{v}.csv")
_sftp.close()
_transport.close()
print(f"[SFTP] Uploaded {_local_path} → {_sftp_host}:{remote_path}/{v}.csv")`,desc:"SFTP upload via paramiko",notes:"Install paramiko; store credentials in Secret Scopes",imp:["import paramiko"],conf:.9},PutEmail:{cat:"Workflow Notification",tpl:`# Use Databricks workflow email notifications
# Configure in Job settings: email_notifications.on_success / on_failure
# Or use dbutils.notebook.exit() with downstream webhook task
dbutils.notebook.exit("NOTIFY: {subject}")`,desc:"Email via workflow notification",notes:"Configure email notifications in Databricks Job settings",imp:[],conf:.9},PutMongo:{cat:"MongoDB Connector",tpl:`(df_{in}.write
  .format("mongodb")
  .option("connection.uri", dbutils.secrets.get(scope="{scope}", key="mongo-uri"))
  .option("database", "{database}")
  .option("collection", "{collection}")
  .mode("append")
  .save())`,desc:"MongoDB write",notes:"Install mongodb-spark-connector",imp:[],conf:.9},PutElasticsearch:{cat:"ES Connector",tpl:`(df_{in}.write
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .save("{index}"))`,desc:"Elasticsearch write",notes:"Install elasticsearch-spark",imp:[],conf:.9},PutDatabaseRecord:{cat:"JDBC Write",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "{table}")
  .mode("append")
  .save())`,desc:"Database record write",notes:"Store credentials in secret scope",imp:[],conf:.9},LogMessage:{cat:"Spark Logging",tpl:`# Databricks notebook logging
print(f"[INFO] {v}: Processing complete")
spark.sparkContext.setLocalProperty("callSite.short", "{v}")`,desc:"Spark-native logging",notes:"Captured in Spark driver logs and notebook output",imp:[],conf:.9},LogAttribute:{cat:"Spark Display",tpl:`# Inspect schema and sample data
display(df_{in})
df_{in}.printSchema()`,desc:"Display attributes and preview",notes:"display() renders interactive table in Databricks",imp:[],conf:.9},Wait:{cat:"Workflow Dependency",tpl:`# Wait -> Databricks Workflow task dependency or Delta CDF streaming
# DO NOT use while/sleep polling loops
df_{v} = spark.readStream.format("delta").option("readChangeFeed","true").table("workflow_signals").filter("signal_id = '{v}_signal' AND status = 'ready'")`,desc:"Workflow task dependency / Delta CDF wait",notes:"Use Workflow depends_on or Delta CDF — never poll in a loop",imp:[],conf:.92},Notify:{cat:"Workflow Signal",tpl:`# NiFi Notify → Delta table signal write (notify downstream tasks)
_notify_key = "{v}_signal"
spark.sql(f"MERGE INTO __workflow_signals AS t USING (SELECT '{{_notify_key}}' AS signal_key, 'READY' AS status, current_timestamp() AS updated_at) AS s ON t.signal_key = s.signal_key WHEN MATCHED THEN UPDATE SET status = s.status, updated_at = s.updated_at WHEN NOT MATCHED THEN INSERT *")
print(f"[NOTIFY] Signal {{_notify_key}} set to READY")`,desc:"Write signal to Delta table for downstream",notes:"Downstream Wait processors poll for this signal",imp:[],conf:.9},DebugFlow:{cat:"Spark Display",tpl:`display(df_{in})
df_{in}.printSchema()`,desc:"Debug/inspect data",notes:"display() renders interactive table in Databricks",imp:[],conf:.9},CountText:{cat:"DataFrame API",tpl:`_count = df_{in}.count()
displayHTML(f"<h3>Row count: {_count}</h3>")`,desc:"Count rows",notes:"displayHTML renders formatted output in Databricks",imp:[],conf:.95},ControlRate:{cat:"Streaming Trigger",tpl:`# NiFi ControlRate → Python rate limiter
import time as _time
_rate_interval = 10  # seconds between batches
print(f"[RATE] Throttling: {_rate_interval}s between executions")
_time.sleep(_rate_interval)`,desc:"Rate limiting via sleep",notes:"Adjust _rate_interval; for streaming use .trigger(processingTime=...)",imp:[],conf:.9},PutHiveQL:{cat:"Spark SQL",tpl:`spark.sql("""
{sql}
""")`,desc:"HiveQL write",notes:"HiveQL → Spark SQL direct",imp:[],conf:.9},SelectHiveQL:{cat:"Spark SQL",tpl:`df_{v} = spark.sql("""
{sql}
""")`,desc:"HiveQL read",notes:"HiveQL → Spark SQL direct",imp:[],conf:.9},PutHiveStreaming:{cat:"Delta Streaming",tpl:`(df_{in}.writeStream
  .format("delta")
  .outputMode("append")
  .toTable("{catalog}.{schema}.{table}"))`,desc:"Hive streaming → Delta streaming",notes:"Replace Hive ACID streaming with Delta Lake streaming",imp:[],conf:.9},PutORC:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"ORC → Delta Lake",notes:"Delta provides ACID on top of Parquet; better than ORC",imp:[],conf:.9},PutParquet:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"Parquet → Delta Lake",notes:"Delta adds ACID, time travel, Z-order to Parquet",imp:[],conf:.9},PutKudu:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"Kudu → Delta Lake",notes:"Replace Kudu upsert with Delta MERGE",imp:[],conf:.9},PutHBaseCell:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"HBase cell write → Delta",notes:"Replace HBase cells with Delta columns",imp:[],conf:.9},PutHBaseJSON:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"HBase JSON write → Delta",notes:"JSON → Delta with auto-inferred schema",imp:[],conf:.9},PutHBaseRecord:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"HBase record write → Delta",notes:"Record-based HBase → Delta table append",imp:[],conf:.9},GetHBase:{cat:"Delta Lake Read",tpl:'df_{v} = spark.table("{catalog}.{schema}.{table}")',desc:"HBase read → Delta table",notes:"Replace HBase scan with Delta read + filter",imp:[],conf:.9},ScanHBase:{cat:"Delta Lake Read",tpl:'df_{v} = spark.table("{catalog}.{schema}.{table}").filter("{condition}")',desc:"HBase scan → Delta filter",notes:"HBase row key scan → Delta point lookup or range filter",imp:[],conf:.9},FetchHBaseRow:{cat:"Delta Lake Read",tpl:'df_{v} = spark.table("{catalog}.{schema}.{table}").filter(col("rowkey") == "{key}")',desc:"HBase row fetch → Delta point lookup",notes:"Use Z-ORDER BY rowkey for fast lookups",imp:[],conf:.9},GetHDFS:{cat:"Volumes Read",tpl:'df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/{path}")',desc:"HDFS read → Volumes",notes:"Replace HDFS paths with Unity Catalog Volumes",imp:[],conf:.9},ListHDFS:{cat:"Volumes List",tpl:`_files = dbutils.fs.ls("/Volumes/{catalog}/{schema}/{path}")
df_{v} = spark.createDataFrame(_files)`,desc:"HDFS list → Volumes list",notes:"dbutils.fs.ls for file listing",imp:[],conf:.9},FetchHDFS:{cat:"Volumes Read",tpl:'df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/{path}")',desc:"HDFS fetch → Volumes read",notes:"Direct file read from Volumes",imp:[],conf:.9},MoveHDFS:{cat:"dbutils.fs",tpl:'dbutils.fs.mv("/Volumes/{catalog}/{schema}/{source}", "/Volumes/{catalog}/{schema}/{dest}")',desc:"HDFS move → dbutils.fs.mv",notes:"File move between Volumes paths",imp:[],conf:.9},DeleteHDFS:{cat:"dbutils.fs",tpl:'dbutils.fs.rm("/Volumes/{catalog}/{schema}/{path}", recurse=True)',desc:"HDFS delete → dbutils.fs.rm",notes:"File deletion in Volumes",imp:[],conf:.9},CreateHadoopSequenceFile:{cat:"Delta Lake Write",tpl:`(df_{in}.write
  .format("delta")
  .saveAsTable("{catalog}.{schema}.{table}"))`,desc:"Sequence file → Delta",notes:"Replace Hadoop SequenceFile with Delta Lake",imp:[],conf:.9},PutCassandraQL:{cat:"Cassandra Connector",tpl:`(df_{in}.write
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "{keyspace}")
  .option("table", "{table}")
  .mode("append")
  .save())`,desc:"Cassandra write via Spark connector",notes:"Install spark-cassandra-connector library on cluster",imp:[],conf:.9},QueryCassandra:{cat:"Cassandra Connector",tpl:`df_{v} = (spark.read
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "{keyspace}")
  .option("table", "{table}")
  .load())`,desc:"Cassandra read via Spark connector",notes:"Install spark-cassandra-connector library",imp:[],conf:.9},ConsumeJMS:{cat:"Custom Source",tpl:`# JMS → Custom Spark data source or Python JMS client
# Use stomp.py or java JMS via spark._jvm
df_{v} = spark.createDataFrame([])  # TODO: Implement JMS consumer`,desc:"JMS consumer",notes:"No native JMS source; use stomp.py or custom connector",imp:[],conf:.9},PublishJMS:{cat:"Custom Sink",tpl:`# Publish to JMS
# Use stomp.py or java JMS via spark._jvm
for row in df_{in}.collect(): pass  # TODO: Publish rows`,desc:"JMS publisher",notes:"No native JMS sink; use stomp.py or custom connector",imp:[],conf:.9},ConsumeAMQP:{cat:"Custom Source",tpl:`# AMQP/RabbitMQ → pika library
import pika
# connection = pika.BlockingConnection(pika.ConnectionParameters("{host}"))`,desc:"AMQP consumer",notes:"Use pika library for RabbitMQ",imp:[],conf:.9},PublishAMQP:{cat:"Custom Sink",tpl:`# AMQP/RabbitMQ → pika library
import pika
# channel.basic_publish(exchange="", routing_key="{queue}", body=msg)`,desc:"AMQP publisher",notes:"Use pika library for RabbitMQ",imp:[],conf:.9},ConsumeMQTT:{cat:"Custom Source",tpl:`# MQTT → paho-mqtt
import paho.mqtt.client as mqtt
# client.connect("{host}", {port})
# client.subscribe("{topic}")`,desc:"MQTT subscriber",notes:"Use paho-mqtt library",imp:[],conf:.9},PublishMQTT:{cat:"Custom Sink",tpl:`# MQTT → paho-mqtt
import paho.mqtt.client as mqtt
# client.publish("{topic}", payload=msg)`,desc:"MQTT publisher",notes:"Use paho-mqtt library",imp:[],conf:.9},PutSNS:{cat:"AWS boto3",tpl:`import boto3
sns = boto3.client("sns")
sns.publish(TopicArn="{topic_arn}", Message=str(msg))`,desc:"AWS SNS publish",notes:"Use boto3; store AWS credentials in Secret Scopes",imp:[],conf:.9},GetSQS:{cat:"AWS boto3",tpl:`import boto3
sqs = boto3.client("sqs")
msgs = sqs.receive_message(QueueUrl="{queue_url}")`,desc:"AWS SQS receive",notes:"Use boto3; for streaming use Kinesis instead",imp:[],conf:.9},PutSQS:{cat:"AWS boto3",tpl:`import boto3
sqs = boto3.client("sqs")
sqs.send_message(QueueUrl="{queue_url}", MessageBody=str(msg))`,desc:"AWS SQS send",notes:"Use boto3; store credentials in Secret Scopes",imp:[],conf:.9},PutDynamoDB:{cat:"DynamoDB Connector",tpl:`(df_{in}.write
  .format("dynamodb")
  .option("tableName", "{table}")
  .option("region", "{region}")
  .save())`,desc:"DynamoDB write",notes:"Install emr-dynamodb-connector library",imp:[],conf:.9},GetDynamoDB:{cat:"DynamoDB Connector",tpl:`df_{v} = (spark.read
  .format("dynamodb")
  .option("tableName", "{table}")
  .option("region", "{region}")
  .load())`,desc:"DynamoDB read",notes:"Install emr-dynamodb-connector library",imp:[],conf:.9},PutKinesisFirehose:{cat:"Kinesis Connector",tpl:`(df_{in}.writeStream
  .format("kinesis")
  .option("streamName", "{stream}")
  .option("region", "{region}")
  .start())`,desc:"Kinesis Firehose write",notes:"Use kinesis-spark connector",imp:[],conf:.9},PutLambda:{cat:"AWS boto3",tpl:`import boto3
lam = boto3.client("lambda")
lam.invoke(FunctionName="{function}", Payload=json.dumps(payload))`,desc:"Lambda invocation",notes:"Use boto3 for Lambda; or replace with Databricks Job",imp:[],conf:.9},PutAzureBlobStorage:{cat:"Azure Storage",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .save("wasbs://{container}@{account}.blob.core.windows.net/{path}"))`,desc:"Azure Blob write",notes:"Use Unity Catalog external location",imp:[],conf:.9},FetchAzureBlobStorage:{cat:"Azure Storage",tpl:'df_{v} = spark.read.format("{format}").load("wasbs://{container}@{account}.blob.core.windows.net/{path}")',desc:"Azure Blob read",notes:"Unity Catalog external location",imp:[],conf:.9},ListAzureBlobStorage:{cat:"Azure Storage",tpl:`_files = dbutils.fs.ls("wasbs://{container}@{account}.blob.core.windows.net/{path}")
df_{v} = spark.createDataFrame(_files)`,desc:"Azure Blob list",notes:"Use dbutils.fs.ls",imp:[],conf:.9},PutAzureDataLakeStorage:{cat:"Azure ADLS",tpl:`(df_{in}.write
  .format("delta")
  .mode("append")
  .save("abfss://{container}@{account}.dfs.core.windows.net/{path}"))`,desc:"ADLS Gen2 write",notes:"Unity Catalog external location",imp:[],conf:.9},PutAzureEventHub:{cat:"Event Hubs Connector",tpl:`(df_{in}.writeStream
  .format("eventhubs")
  .option("eventhubs.connectionString", dbutils.secrets.get(scope="{scope}", key="eh-conn-string"))
  .start())`,desc:"Event Hubs write",notes:"Install azure-eventhubs-spark library",imp:[],conf:.9},ConsumeAzureEventHub:{cat:"Event Hubs Connector",tpl:`df_{v} = (spark.readStream
  .format("eventhubs")
  .option("eventhubs.connectionString", dbutils.secrets.get(scope="{scope}", key="eh-conn-string"))
  .load())`,desc:"Event Hubs read",notes:"Install azure-eventhubs-spark library",imp:[],conf:.9},ListGCSBucket:{cat:"GCS",tpl:`_files = dbutils.fs.ls("gs://{bucket}/{prefix}")
df_{v} = spark.createDataFrame(_files)`,desc:"GCS list",notes:"Unity Catalog external location for GCS",imp:[],conf:.9},FetchGCSObject:{cat:"GCS",tpl:'df_{v} = spark.read.format("{format}").load("gs://{bucket}/{key}")',desc:"GCS read",notes:"Unity Catalog external location",imp:[],conf:.9},PutGCSObject:{cat:"GCS",tpl:`(df_{in}.write
  .format("delta")
  .save("gs://{bucket}/{key}"))`,desc:"GCS write",notes:"Unity Catalog external location",imp:[],conf:.9},PutBigQueryBatch:{cat:"BigQuery Connector",tpl:`(df_{in}.write
  .format("bigquery")
  .option("table", "{project}.{dataset}.{table}")
  .save())`,desc:"BigQuery write",notes:"Install spark-bigquery-connector",imp:[],conf:.9},PutSlack:{cat:"Webhook",tpl:`import requests
requests.post("{webhook_url}", json={"text": f"Pipeline update: {msg}"})`,desc:"Slack notification",notes:"Use Slack webhook URL; store in Secret Scopes",imp:[],conf:.9},PutSyslog:{cat:"Python Logging",tpl:`import syslog
syslog.syslog(syslog.LOG_INFO, f"Pipeline: {msg}")`,desc:"Syslog send",notes:"Use Python syslog module or Databricks logging",imp:[],conf:.9},ListenSyslog:{cat:"Streaming Socket",tpl:`df_{v} = (spark.readStream
  .format("socket")
  .option("host", "{host}")
  .option("port", "{port}")
  .load())`,desc:"Syslog listener",notes:"Use socket source or external syslog collector",imp:[],conf:.9},ParseSyslog:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("parsed", regexp_extract(col("value"), "{pattern}", 0))',desc:"Syslog parsing",notes:"Use regex to parse syslog format",imp:[],conf:.9},PutSplunk:{cat:"Splunk HEC",tpl:`import requests
requests.post("{hec_url}", headers={"Authorization":"Splunk {token}"}, json={"event":data})`,desc:"Splunk HEC write",notes:"Use Splunk HTTP Event Collector",imp:[],conf:.9},PutInfluxDB:{cat:"InfluxDB Client",tpl:`from influxdb_client import InfluxDBClient
client = InfluxDBClient(url="{url}", token="{token}", org="{org}")
client.write_api().write(bucket="{bucket}", record=data)`,desc:"InfluxDB write",notes:"Install influxdb-client-python",imp:[],conf:.9},PutTCP:{cat:"Python Socket",tpl:`import socket
s = socket.socket()
s.connect(("{host}", {port}))
s.send(data.encode())`,desc:"TCP send",notes:"Use Python socket module",imp:[],conf:.9},ListenTCP:{cat:"Streaming Socket",tpl:`df_{v} = (spark.readStream
  .format("socket")
  .option("host", "0.0.0.0")
  .option("port", "{port}")
  .load())`,desc:"TCP listener",notes:"Use Spark socket source",imp:[],conf:.9},UpdateRecord:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.withColumn("{field}", expr("{expression}"))',desc:"Record field update",notes:"Map UpdateRecord field expressions to withColumn",imp:[],conf:.9},LookupRecord:{cat:"DataFrame Join",tpl:`df_lookup = spark.table("{catalog}.{schema}.{lookup_table}").cache()
df_{v} = df_{in}.join(df_lookup, on="{key}", how="left")`,desc:"Record lookup via cached join",notes:"Cache lookup table for performance",imp:[],conf:.9},ValidateRecord:{cat:"DLT Expectations",tpl:`# Data quality validation
# @dlt.expect_or_drop("{rule}", "{expression}")
df_{v} = df_{in}.filter("{expression}")`,desc:"Record validation via DLT expectations",notes:"Best implemented as DLT expectations",imp:[],conf:.9},PartitionRecord:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.repartition("{partition_field}")',desc:"Record partitioning",notes:"Spark handles partitioning natively",imp:[],conf:.9},QueryRecord:{cat:"Spark SQL",tpl:`df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("{sql}")`,desc:"SQL query on records",notes:"Register DataFrame as temp view then query",imp:[],conf:.9},ConvertAvroToJSON:{cat:"DataFrame API",tpl:`from pyspark.sql.avro.functions import from_avro
df_{v} = df_{in}.select(from_avro("value", _schema).alias("data")).select("data.*")`,desc:"Avro to JSON",notes:"Spark handles Avro natively",imp:[],conf:.9},ConvertJSONToAvro:{cat:"DataFrame API",tpl:`from pyspark.sql.avro.functions import to_avro
df_{v} = df_{in}.select(to_avro(struct("*")).alias("value"))`,desc:"JSON to Avro",notes:"Use Spark Avro functions",imp:[],conf:.9},GenerateTableFetch:{cat:"JDBC Incremental",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "(SELECT * FROM {table} WHERE {column} > ?) subq")
  .load())`,desc:"Incremental JDBC fetch",notes:"Use query pushdown for incremental reads",imp:[],conf:.9},PutDistributedMapCache:{cat:"Delta Lookup",tpl:`# DistributedMapCache -> Delta lookup table (persistent, shared, versioned)
df_{in}.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("{catalog}.{schema}.cache_{v}")
print(f"[CACHE] Persisted to {catalog}.{schema}.cache_{v}")`,desc:"Delta lookup table write",notes:"Replace NiFi cache with Delta table — shared across clusters",imp:[],conf:.93},FetchDistributedMapCache:{cat:"Delta Lookup",tpl:`# FetchDistributedMapCache -> Cached Delta lookup join
df_lookup = spark.table("{catalog}.{schema}.cache_{v}").cache()
df_{v} = df_{in}.join(df_lookup, on="{key}", how="left")
print(f"[CACHE] Joined with cached Delta lookup table")`,desc:"Delta lookup table read + join",notes:"Delta table cached in memory — fast lookups",imp:[],conf:.93},PutCouchbaseKey:{cat:"Couchbase Connector",tpl:`# Install couchbase-spark-connector
(df_{in}.write
  .format("couchbase.kv")
  .option("couchbase.bucket", "{bucket}")
  .save())`,desc:"Couchbase write",notes:"Install couchbase-spark connector",imp:[],conf:.9},GetCouchbaseKey:{cat:"Couchbase Connector",tpl:`df_{v} = (spark.read
  .format("couchbase.kv")
  .option("couchbase.bucket", "{bucket}")
  .load())`,desc:"Couchbase read",notes:"Install couchbase-spark connector",imp:[],conf:.9},PutSolrContentStream:{cat:"Solr Connector",tpl:`# Install solr-spark connector or use pysolr
import pysolr
solr = pysolr.Solr("{url}")
solr.add([row.asDict() for row in df_{in}.collect()])`,desc:"Solr write",notes:"Use pysolr or solr-spark connector",imp:[],conf:.9},QuerySolr:{cat:"Solr Connector",tpl:`# Use pysolr or solr-spark connector
import pysolr
solr = pysolr.Solr("{url}")
results = solr.search("{query}")`,desc:"Solr query",notes:"Use pysolr for queries",imp:[],conf:.9},ExecuteGroovyScript:{cat:"PySpark Cell",tpl:`# NiFi Groovy script → PySpark equivalent
# Original engine: Groovy
# Translate Groovy logic to PySpark DataFrame operations
df_{v} = df_{in}`,desc:"Groovy script execution",notes:"Manual translation from Groovy to PySpark required",imp:[],conf:.9},ExecuteProcess:{cat:"subprocess",tpl:`# Execute external process
import subprocess as _sp
_result = _sp.run(["{command}"], capture_output=True, text=True, timeout=300)
if _result.returncode != 0:
    print(f"[ERROR] Process failed: {_result.stderr[:200]}")
else:
    print(f"[OK] {_result.stdout[:200]}")`,desc:"External process execution via subprocess",notes:"Review command for Databricks compatibility",imp:[],conf:.9},ExecuteFlumeSink:{cat:"Structured Streaming",tpl:`# Flume sink → Structured Streaming write
(df_{in}.writeStream
  .format("delta")
  .outputMode("append")
  .option("checkpointLocation", "/tmp/checkpoint/{v}")
  .toTable("{catalog}.{schema}.{table}"))`,desc:"Flume sink → Delta streaming",notes:"Replace Flume with Structured Streaming to Delta",imp:[],conf:.9},ExecuteFlumeSource:{cat:"Structured Streaming",tpl:`# Flume source → Structured Streaming / Auto Loader
df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "{format}")
  .load("/Volumes/{catalog}/{schema}/{path}"))`,desc:"Flume source → Auto Loader",notes:"Replace Flume agent with Auto Loader",imp:[],conf:.9},ListDatabaseTables:{cat:"JDBC Source",tpl:`# List database tables via JDBC
df_{v} = (spark.read
  .format("jdbc")
  .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
  .option("dbtable", "(SELECT table_name FROM information_schema.tables WHERE table_schema = '{schema}') t")
  .load())`,desc:"List database tables",notes:"Query information_schema via JDBC",imp:[],conf:.9},UnpackContent:{cat:"DataFrame API",tpl:`# Unpack/decompress content — Spark handles gzip/snappy/lz4 natively
df_{v} = spark.read.format("{format}").load("/Volumes/{catalog}/{schema}/{path}")`,desc:"Content decompression",notes:"Spark auto-decompresses gzip, snappy, lz4, zstd",imp:[],conf:.9},PostHTTP:{cat:"Spark UDF",tpl:`# HTTP POST via PySpark pandas UDF
from pyspark.sql.functions import pandas_udf
import pandas as pd
@pandas_udf("string")
def _post_api(payloads: pd.Series) -> pd.Series:
  import urllib.request, json
  def _post(p):
    req = urllib.request.Request("{url}", data=p.encode(), method="POST", headers={"Content-Type":"application/json"})
    with urllib.request.urlopen(req) as r: return r.read().decode()
  return payloads.apply(_post)
df_{v} = df_{in}.withColumn("response", _post_api(col("value")))`,desc:"HTTP POST via UDF",notes:"For high-volume use external API gateway",imp:[],conf:.9},GetJMSTopic:{cat:"Custom Source",tpl:`# JMS Topic → Custom Spark source or stomp.py
df_{v} = spark.createDataFrame([])  # Implement JMS topic consumer via stomp.py or java bridge`,desc:"JMS topic consumer",notes:"Use stomp.py or java JMS bridge via spark._jvm",imp:[],conf:.9},PutJMS:{cat:"Custom Sink",tpl:`# JMS publish — streaming-safe with foreachBatch
import stomp

def _jms_batch_{v}(batch_df, batch_id):
    _conn = stomp.Connection([("{host}", {port})])
    _conn.connect(wait=True)
    for row in batch_df.collect():
        _conn.send(destination="{queue}", body=str(row.asDict()))
    _conn.disconnect()
    print(f"[JMS] Batch {{batch_id}}: {{batch_df.count()}} messages")

# For streaming: df_{in}.writeStream.foreachBatch(_jms_batch_{v}).start()
# For batch:
_jms_batch_{v}(df_{in}, 0)`,desc:"JMS with foreachBatch",notes:"Streaming-safe",imp:["stomp.py"],conf:.92},PutFTP:{cat:"External Storage Write",tpl:`# FTP upload via ftplib
import ftplib
_ftp = ftplib.FTP("{hostname}")
_ftp.login(dbutils.secrets.get(scope="{scope}", key="ftp-user"), dbutils.secrets.get(scope="{scope}", key="ftp-pass"))
# Stage to local then upload
_local = "/tmp/{v}_export"
df_{in}.toPandas().to_csv(_local, index=False)
with open(_local, "rb") as f:
    _ftp.storbinary(f"STOR {remote_path}/{v}.csv", f)
_ftp.quit()`,desc:"FTP upload via ftplib",notes:"Stage to temp then upload; store creds in Secret Scopes",imp:[],conf:.9},ConsumeWindowsEventLog:{cat:"Custom Source",tpl:`# Windows Event Log → not available on Databricks (Linux)
# Use external agent to ship events to Delta table
df_{v} = spark.table("{catalog}.{schema}.windows_events")`,desc:"Windows Event Log",notes:"Run external Windows agent; ingest to Delta via API",imp:[],conf:.9},ConsumeEWS:{cat:"Custom Source",tpl:`# Exchange Web Services → exchangelib
from exchangelib import Credentials, Account
_creds = Credentials(dbutils.secrets.get(scope="{scope}", key="ews-user"), dbutils.secrets.get(scope="{scope}", key="ews-pass"))
_account = Account(_creds.username, credentials=_creds, autodiscover=True)`,desc:"Exchange email consumer",notes:"Install exchangelib; store credentials in Secret Scopes",imp:[],conf:.9},RetryFlowFile:{cat:"Error Handling",tpl:`# Retry logic — implemented via try/except in each processor cell
# Max retries and penalty duration configured in Databricks Workflows
print(f"[RETRY] Processor {v} — retries handled by Workflows max_retries setting")`,desc:"Retry mechanism",notes:"Handled by Workflows retry policy",imp:[],conf:.9},ReplaceTextWithMapping:{cat:"DataFrame API",tpl:`# Replace text using lookup mapping
import json
_mapping = json.loads('{mapping}')
for _old, _new in _mapping.items():
    df_{v} = df_{in}.withColumn("{col}", regexp_replace(col("{col}"), _old, _new))`,desc:"Text replacement with mapping table",notes:"Load mapping from config or Delta table",imp:[],conf:.9},MonitorActivity:{cat:"Spark Logging",tpl:`# Monitor activity — log throughput metrics
_count = df_{in}.count()
print(f"[MONITOR] {v}: {_count} rows processed at {__import__('datetime').datetime.now()}")`,desc:"Activity monitoring",notes:"Use Spark UI metrics or Ganglia for detailed monitoring",imp:[],conf:.9},RouteText:{cat:"DataFrame Filter",tpl:`# Route text content by pattern matching
df_{v}_matched = df_{in}.filter(col("value").rlike("{pattern}"))
df_{v}_unmatched = df_{in}.filter(~col("value").rlike("{pattern}"))`,desc:"Text-based routing",notes:"Translate NiFi text routing rules to Spark regex filters",imp:[],conf:.9},ScanAttribute:{cat:"DataFrame API",tpl:`# Scan attributes for pattern match
df_{v} = df_{in}.filter(col("{attribute}").rlike("{pattern}"))`,desc:"Attribute scanning",notes:"Filter rows where attribute matches regex",imp:[],conf:.9},ScanContent:{cat:"DataFrame API",tpl:`# Scan content for pattern match
df_{v} = df_{in}.filter(col("value").rlike("{pattern}"))`,desc:"Content scanning",notes:"Filter rows where content matches regex",imp:[],conf:.9},ListenRELP:{cat:"Streaming Socket",tpl:`# RELP listener → socket source or external agent
df_{v} = (spark.readStream
  .format("socket")
  .option("host", "0.0.0.0")
  .option("port", "{port}")
  .load())`,desc:"RELP syslog listener",notes:"Use socket source or external syslog collector",imp:[],conf:.9},PutRELP:{cat:"Python Socket",tpl:`# RELP send
import socket
s = socket.socket()
s.connect(("{host}", {port}))
s.send(data.encode())
s.close()`,desc:"RELP syslog send",notes:"Use Python socket for RELP",imp:[],conf:.9},QueryWhois:{cat:"Python Library",tpl:`# WHOIS lookup
import subprocess as _sp
_result = _sp.run(["whois", "{domain}"], capture_output=True, text=True, timeout=30)
df_{v} = spark.createDataFrame([(_result.stdout,)], ["whois_data"])`,desc:"WHOIS query",notes:"Use whois command or python-whois library",imp:[],conf:.9},GeoEnrichIPRecord:{cat:"Python Library",tpl:`# GeoIP enrichment
# Install geoip2 and download MaxMind GeoLite2 database
import geoip2.database
_reader = geoip2.database.Reader("/Volumes/{catalog}/{schema}/geoip/GeoLite2-City.mmdb")
from pyspark.sql.functions import pandas_udf
import pandas as pd
@pandas_udf("string")
def _geo_lookup(ips: pd.Series) -> pd.Series:
    return ips.apply(lambda ip: str(_reader.city(ip).country.name) if ip else None)
df_{v} = df_{in}.withColumn("geo_country", _geo_lookup(col("{ip_field}")))`,desc:"GeoIP enrichment",notes:"Install geoip2; download MaxMind database to Volumes",imp:[],conf:.9},PublishKafka_1_0:{cat:"Kafka Write",tpl:`(df_{in}
  .selectExpr("to_json(struct(*)) AS value")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Publish to Kafka 1.0",notes:"Same as PublishKafka",imp:[],conf:.9},ConsumeKafka_1_0:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load())`,desc:"Consume from Kafka 1.0",notes:"Same as ConsumeKafka",imp:[],conf:.9},DeleteS3Object:{cat:"Cloud Storage",tpl:'dbutils.fs.rm("s3a://{bucket}/{key}", recurse=False)',desc:"Delete S3 object",notes:"Use Unity Catalog external location",imp:[],conf:.9},TagS3Object:{cat:"AWS boto3",tpl:`import boto3
s3 = boto3.client("s3")
s3.put_object_tagging(Bucket="{bucket}", Key="{key}", Tagging={"TagSet":[{"Key":"{tag_key}","Value":"{tag_value}"}]})`,desc:"Tag S3 object",notes:"Use boto3 for S3 tagging",imp:[],conf:.9},PutKinesisStream:{cat:"Kinesis Connector",tpl:`(df_{in}.writeStream
  .format("kinesis")
  .option("streamName", "{stream}")
  .option("region", "{region}")
  .option("endpointUrl", "https://kinesis.{region}.amazonaws.com")
  .start())`,desc:"Kinesis stream write",notes:"Use kinesis-spark connector",imp:[],conf:.9},GetKinesisStream:{cat:"Kinesis Connector",tpl:`df_{v} = (spark.readStream
  .format("kinesis")
  .option("streamName", "{stream}")
  .option("region", "{region}")
  .option("initialPosition", "TRIM_HORIZON")
  .load())`,desc:"Kinesis stream read",notes:"Use kinesis-spark connector",imp:[],conf:.9},DeleteAzureBlobStorage:{cat:"Azure Storage",tpl:'dbutils.fs.rm("wasbs://{container}@{account}.blob.core.windows.net/{path}")',desc:"Delete Azure Blob",notes:"Use dbutils.fs.rm",imp:[],conf:.9},FetchAzureDataLakeStorage:{cat:"Azure ADLS",tpl:'df_{v} = spark.read.format("{format}").load("abfss://{container}@{account}.dfs.core.windows.net/{path}")',desc:"ADLS Gen2 read",notes:"Unity Catalog external location",imp:[],conf:.9},ListAzureDataLakeStorage:{cat:"Azure ADLS",tpl:`_files = dbutils.fs.ls("abfss://{container}@{account}.dfs.core.windows.net/{path}")
df_{v} = spark.createDataFrame(_files)`,desc:"ADLS Gen2 list",notes:"dbutils.fs.ls for ADLS",imp:[],conf:.9},DeleteAzureDataLakeStorage:{cat:"Azure ADLS",tpl:'dbutils.fs.rm("abfss://{container}@{account}.dfs.core.windows.net/{path}")',desc:"Delete ADLS Gen2 object",notes:"dbutils.fs.rm",imp:[],conf:.9},PutAzureCosmosDB:{cat:"Cosmos DB",tpl:`(df_{in}.write
  .format("cosmos.oltp")
  .option("spark.cosmos.accountEndpoint", dbutils.secrets.get(scope="{scope}", key="cosmos-endpoint"))
  .option("spark.cosmos.accountKey", dbutils.secrets.get(scope="{scope}", key="cosmos-key"))
  .option("spark.cosmos.database", "{database}")
  .option("spark.cosmos.container", "{container}")
  .mode("append")
  .save())`,desc:"Cosmos DB write",notes:"Install azure-cosmos-spark library",imp:[],conf:.9},PutAzureCosmosDBRecord:{cat:"Cosmos DB",tpl:`(df_{in}.write
  .format("cosmos.oltp")
  .option("spark.cosmos.accountEndpoint", dbutils.secrets.get(scope="{scope}", key="cosmos-endpoint"))
  .option("spark.cosmos.accountKey", dbutils.secrets.get(scope="{scope}", key="cosmos-key"))
  .option("spark.cosmos.database", "{database}")
  .option("spark.cosmos.container", "{container}")
  .mode("append")
  .save())`,desc:"Cosmos DB record write",notes:"Install azure-cosmos-spark",imp:[],conf:.9},GetAzureEventHub:{cat:"Event Hubs Connector",tpl:`df_{v} = (spark.readStream
  .format("eventhubs")
  .option("eventhubs.connectionString", dbutils.secrets.get(scope="{scope}", key="eh-conn-string"))
  .load())`,desc:"Event Hubs batch read",notes:"Install azure-eventhubs-spark library",imp:[],conf:.9},PutAzureQueueStorage:{cat:"Azure Queue",tpl:`from azure.storage.queue import QueueClient
_queue = QueueClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="azure-storage-conn"), "{queue}")
_queue.send_message(str(data))`,desc:"Azure Queue Storage write",notes:"Install azure-storage-queue",imp:[],conf:.9},GetAzureQueueStorage:{cat:"Azure Queue",tpl:`from azure.storage.queue import QueueClient
_queue = QueueClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="azure-storage-conn"), "{queue}")
_msgs = _queue.receive_messages()`,desc:"Azure Queue Storage read",notes:"Install azure-storage-queue",imp:[],conf:.9},DeleteGCSObject:{cat:"GCS",tpl:'dbutils.fs.rm("gs://{bucket}/{key}")',desc:"Delete GCS object",notes:"dbutils.fs.rm for GCS",imp:[],conf:.9},PutBigQueryStreaming:{cat:"BigQuery Connector",tpl:`(df_{in}.writeStream
  .format("bigquery")
  .option("table", "{project}.{dataset}.{table}")
  .start())`,desc:"BigQuery streaming write",notes:"Install spark-bigquery-connector",imp:[],conf:.9},PutSnowflake:{cat:"Snowflake Connector",tpl:`(df_{in}.write
  .format("snowflake")
  .option("sfUrl", dbutils.secrets.get(scope="{scope}", key="sf-url"))
  .option("sfUser", dbutils.secrets.get(scope="{scope}", key="sf-user"))
  .option("sfPassword", dbutils.secrets.get(scope="{scope}", key="sf-password"))
  .option("sfDatabase", "{database}")
  .option("sfSchema", "{sf_schema}")
  .option("dbtable", "{table}")
  .mode("append")
  .save())`,desc:"Snowflake write",notes:"Install spark-snowflake connector; store credentials in Secret Scopes",imp:[],conf:.9},GetSnowflake:{cat:"Snowflake Connector",tpl:`df_{v} = (spark.read
  .format("snowflake")
  .option("sfUrl", dbutils.secrets.get(scope="{scope}", key="sf-url"))
  .option("sfUser", dbutils.secrets.get(scope="{scope}", key="sf-user"))
  .option("sfPassword", dbutils.secrets.get(scope="{scope}", key="sf-password"))
  .option("sfDatabase", "{database}")
  .option("sfSchema", "{sf_schema}")
  .option("dbtable", "{table}")
  .load())`,desc:"Snowflake read",notes:"Install spark-snowflake connector",imp:[],conf:.9},PutCypher:{cat:"Neo4j Connector",tpl:`# Neo4j write via neo4j-spark-connector
(df_{in}.write
  .format("org.neo4j.spark.DataSource")
  .option("url", dbutils.secrets.get(scope="{scope}", key="neo4j-url"))
  .option("authentication.basic.username", dbutils.secrets.get(scope="{scope}", key="neo4j-user"))
  .option("authentication.basic.password", dbutils.secrets.get(scope="{scope}", key="neo4j-pass"))
  .option("labels", "{label}")
  .mode("append")
  .save())`,desc:"Neo4j graph write",notes:"Install neo4j-spark-connector",imp:[],conf:.9},GetCypher:{cat:"Neo4j Connector",tpl:`df_{v} = (spark.read
  .format("org.neo4j.spark.DataSource")
  .option("url", dbutils.secrets.get(scope="{scope}", key="neo4j-url"))
  .option("authentication.basic.username", dbutils.secrets.get(scope="{scope}", key="neo4j-user"))
  .option("authentication.basic.password", dbutils.secrets.get(scope="{scope}", key="neo4j-pass"))
  .option("query", "{cypher_query}")
  .load())`,desc:"Neo4j graph read",notes:"Install neo4j-spark-connector",imp:[],conf:.9},PutDruidRecord:{cat:"Druid Connector",tpl:`# Apache Druid write via druid-spark connector
(df_{in}.write
  .format("druid")
  .option("druid.datasource", "{datasource}")
  .option("druid.broker", "{broker_host}:{broker_port}")
  .mode("append")
  .save())`,desc:"Druid record write",notes:"Install druid-spark connector or use Druid ingestion API",imp:[],conf:.9},QueryDruid:{cat:"Druid Connector",tpl:`# Apache Druid query via druid-spark connector
df_{v} = (spark.read
  .format("druid")
  .option("druid.datasource", "{datasource}")
  .option("druid.broker", "{broker_host}:{broker_port}")
  .load())`,desc:"Druid query",notes:"Install druid-spark connector",imp:[],conf:.9},PutClickHouse:{cat:"ClickHouse Connector",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:clickhouse://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
  .mode("append")
  .save())`,desc:"ClickHouse write via JDBC",notes:"Install ClickHouse JDBC driver",imp:[],conf:.9},QueryClickHouse:{cat:"ClickHouse Connector",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:clickhouse://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
  .load())`,desc:"ClickHouse read via JDBC",notes:"Install ClickHouse JDBC driver",imp:[],conf:.9},PutIceberg:{cat:"Iceberg",tpl:`(df_{in}.writeTo("{catalog}.{schema}.{table}")
  .using("iceberg")
  .append())`,desc:"Iceberg table write",notes:"Databricks supports Iceberg via UniForm; prefer Delta Lake",imp:[],conf:.9},PutHudi:{cat:"Hudi",tpl:`(df_{in}.write
  .format("hudi")
  .option("hoodie.table.name", "{table}")
  .option("hoodie.datasource.write.recordkey.field", "{record_key}")
  .option("hoodie.datasource.write.precombine.field", "{precombine_field}")
  .mode("append")
  .save("/Volumes/{catalog}/{schema}/{table}"))`,desc:"Hudi table write",notes:"Databricks supports Hudi; prefer Delta Lake for native features",imp:[],conf:.9},PutElasticsearchHttp:{cat:"ES Connector",tpl:`(df_{in}.write
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .save("{index}"))`,desc:"Elasticsearch HTTP write",notes:"Install elasticsearch-spark",imp:[],conf:.9},PutElasticsearchHttpRecord:{cat:"ES Connector",tpl:`(df_{in}.write
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .save("{index}"))`,desc:"Elasticsearch HTTP record write",notes:"Install elasticsearch-spark",imp:[],conf:.9},PutElasticsearchRecord:{cat:"ES Connector",tpl:`(df_{in}.write
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .save("{index}"))`,desc:"Elasticsearch record write",notes:"Install elasticsearch-spark",imp:[],conf:.9},FetchElasticsearchHttp:{cat:"ES Connector",tpl:`df_{v} = (spark.read
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .option("es.resource", "{index}")
  .load())`,desc:"Elasticsearch HTTP fetch",notes:"Install elasticsearch-spark",imp:[],conf:.9},JsonQueryElasticsearch:{cat:"ES Connector",tpl:`df_{v} = (spark.read
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .option("es.query", "{query}")
  .load("{index}"))`,desc:"Elasticsearch JSON query",notes:"Install elasticsearch-spark; pass query DSL",imp:[],conf:.9},ScrollElasticsearchHttp:{cat:"ES Connector",tpl:`df_{v} = (spark.read
  .format("org.elasticsearch.spark.sql")
  .option("es.nodes", "{host}")
  .option("es.scroll.size", "1000")
  .load("{index}"))`,desc:"Elasticsearch scroll read",notes:"Install elasticsearch-spark; Spark handles pagination",imp:[],conf:.9},PutMongoRecord:{cat:"MongoDB Connector",tpl:`(df_{in}.write
  .format("mongodb")
  .option("connection.uri", dbutils.secrets.get(scope="{scope}", key="mongo-uri"))
  .option("database", "{database}")
  .option("collection", "{collection}")
  .mode("append")
  .save())`,desc:"MongoDB record write",notes:"Install mongodb-spark-connector",imp:[],conf:.9},DeleteMongo:{cat:"MongoDB Connector",tpl:`# MongoDB delete via pymongo
from pymongo import MongoClient
_client = MongoClient(dbutils.secrets.get(scope="{scope}", key="mongo-uri"))
_db = _client["{database}"]
_result = _db["{collection}"].delete_many({filter})
print(f"[MONGO] Deleted {_result.deleted_count} documents")`,desc:"MongoDB delete",notes:"Install pymongo; use for targeted deletes",imp:[],conf:.9},PutCassandraRecord:{cat:"Cassandra Connector",tpl:`(df_{in}.write
  .format("org.apache.spark.sql.cassandra")
  .option("keyspace", "{keyspace}")
  .option("table", "{table}")
  .mode("append")
  .save())`,desc:"Cassandra record write",notes:"Install spark-cassandra-connector",imp:[],conf:.9},PutSolrRecord:{cat:"Solr Connector",tpl:`# Solr record write via pysolr or solr-spark connector
import pysolr
solr = pysolr.Solr("{url}")
solr.add([row.asDict() for row in df_{in}.collect()])`,desc:"Solr record write",notes:"Install pysolr or solr-spark connector",imp:[],conf:.9},GetSolr:{cat:"Solr Connector",tpl:`# Solr read via pysolr
import pysolr
solr = pysolr.Solr("{url}")
results = solr.search("*:*", rows=10000)`,desc:"Solr read",notes:"Use pysolr or solr-spark connector",imp:[],conf:.9},ListSFTP:{cat:"External Storage",tpl:`# SFTP directory listing via paramiko
import paramiko
_transport = paramiko.Transport(("{host}", 22))
_transport.connect(username=dbutils.secrets.get(scope="{scope}", key="sftp-user"))
_sftp = paramiko.SFTPClient.from_transport(_transport)
_files = _sftp.listdir("{remote_path}")
_sftp.close(); _transport.close()
df_{v} = spark.createDataFrame([(f,) for f in _files], ["filename"])`,desc:"SFTP directory listing",notes:"Install paramiko; store credentials in Secret Scopes",imp:["import paramiko"],conf:.9},FetchSFTP:{cat:"External Storage",tpl:`# SFTP file fetch via paramiko
import paramiko
_transport = paramiko.Transport(("{host}", 22))
_transport.connect(username=dbutils.secrets.get(scope="{scope}", key="sftp-user"))
_sftp = paramiko.SFTPClient.from_transport(_transport)
_sftp.get("{remote_path}/{filename}", "/tmp/{v}_download")
_sftp.close(); _transport.close()
df_{v} = spark.read.format("{format}").load("/tmp/{v}_download")`,desc:"SFTP file fetch",notes:"Install paramiko; downloads to local then reads",imp:["import paramiko"],conf:.9},ListFTP:{cat:"External Storage",tpl:`# FTP directory listing via ftplib
import ftplib
_ftp = ftplib.FTP("{hostname}")
_ftp.login(dbutils.secrets.get(scope="{scope}", key="ftp-user"), dbutils.secrets.get(scope="{scope}", key="ftp-pass"))
_files = _ftp.nlst("{remote_path}")
_ftp.quit()
df_{v} = spark.createDataFrame([(f,) for f in _files], ["filename"])`,desc:"FTP directory listing",notes:"Use ftplib; store credentials in Secret Scopes",imp:[],conf:.9},FetchFTP:{cat:"External Storage",tpl:`# FTP file fetch via ftplib
import ftplib
_ftp = ftplib.FTP("{hostname}")
_ftp.login(dbutils.secrets.get(scope="{scope}", key="ftp-user"), dbutils.secrets.get(scope="{scope}", key="ftp-pass"))
with open("/tmp/{v}_download", "wb") as f:
    _ftp.retrbinary("RETR {remote_path}/{filename}", f.write)
_ftp.quit()
df_{v} = spark.read.format("{format}").load("/tmp/{v}_download")`,desc:"FTP file fetch",notes:"Use ftplib; downloads to local then reads",imp:[],conf:.9},GetPOP3:{cat:"Email",tpl:`# POP3 email retrieval
import poplib
_pop = poplib.POP3_SSL("{host}")
_pop.user(dbutils.secrets.get(scope="{scope}", key="pop3-user"))
_pop.pass_(dbutils.secrets.get(scope="{scope}", key="pop3-pass"))
_count = len(_pop.list()[1])
print(f"[POP3] {_count} messages available")
_pop.quit()`,desc:"POP3 email retrieval",notes:"Use poplib; store credentials in Secret Scopes",imp:[],conf:.9},GetIMAP:{cat:"Email",tpl:`# IMAP email retrieval
import imaplib
_mail = imaplib.IMAP4_SSL("{host}")
_mail.login(dbutils.secrets.get(scope="{scope}", key="imap-user"), dbutils.secrets.get(scope="{scope}", key="imap-pass"))
_mail.select("INBOX")
_status, _msgs = _mail.search(None, "ALL")
print(f"[IMAP] {len(_msgs[0].split())} messages")
_mail.logout()`,desc:"IMAP email retrieval",notes:"Use imaplib; store credentials in Secret Scopes",imp:[],conf:.9},ListenUDP:{cat:"Python Socket",tpl:`# UDP listener — not ideal for Databricks
import socket
s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
s.bind(("0.0.0.0", {port}))
data, addr = s.recvfrom(4096)`,desc:"UDP listener",notes:"Use external UDP collector; ingest to Delta",imp:[],conf:.9},GetTCP:{cat:"Python Socket",tpl:`# TCP get via Python socket
import socket
s = socket.socket()
s.connect(("{host}", {port}))
data = s.recv(4096)
s.close()`,desc:"TCP receive",notes:"Use Python socket module",imp:[],conf:.9},PutUDP:{cat:"Python Socket",tpl:`import socket
s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
s.sendto(data.encode(), ("{host}", {port}))`,desc:"UDP send",notes:"Use Python socket module",imp:[],conf:.9},GetSNMP:{cat:"SNMP",tpl:`# SNMP get via pysnmp
from pysnmp.hlapi import *
_errorIndication, _errorStatus, _errorIndex, _varBinds = next(
    getCmd(SnmpEngine(), CommunityData("{community}"), UdpTransportTarget(("{host}", 161)), ContextData(), ObjectType(ObjectIdentity("{oid}"))))
if _errorIndication: print(f"[SNMP ERROR] {_errorIndication}")
else: print(f"[SNMP] {_varBinds}")`,desc:"SNMP get request",notes:"Install pysnmp library",imp:[],conf:.9},SetSNMP:{cat:"SNMP",tpl:`# SNMP set via pysnmp
from pysnmp.hlapi import *
next(setCmd(SnmpEngine(), CommunityData("{community}"), UdpTransportTarget(("{host}", 161)), ContextData(), ObjectType(ObjectIdentity("{oid}"), {value})))`,desc:"SNMP set request",notes:"Install pysnmp library",imp:[],conf:.9},GetSplunk:{cat:"Splunk Connector",tpl:`# Splunk search via REST API
import requests
_resp = requests.get("{splunk_url}/services/search/jobs/export", params={"search":"search {query}","output_mode":"json"}, auth=(dbutils.secrets.get(scope="{scope}",key="splunk-user"), dbutils.secrets.get(scope="{scope}",key="splunk-pass")), verify=False)
df_{v} = spark.read.json(spark.sparkContext.parallelize([_resp.text]))`,desc:"Splunk search read",notes:"Use Splunk REST API or splunk-spark connector",imp:[],conf:.9},QuerySplunkIndexingStatus:{cat:"Splunk Connector",tpl:`# Splunk indexing status query
import requests
_resp = requests.get("{splunk_url}/services/data/indexes", auth=(dbutils.secrets.get(scope="{scope}",key="splunk-user"), dbutils.secrets.get(scope="{scope}",key="splunk-pass")), verify=False)
print(f"[SPLUNK] Status: {_resp.status_code}")`,desc:"Splunk indexing status",notes:"Use Splunk REST API",imp:[],conf:.9},QueryInfluxDB:{cat:"InfluxDB Client",tpl:`from influxdb_client import InfluxDBClient
client = InfluxDBClient(url="{url}", token=dbutils.secrets.get(scope="{scope}", key="influxdb-token"), org="{org}")
_query = 'from(bucket:"{bucket}") |> range(start: -1h)'
_tables = client.query_api().query(_query)
df_{v} = spark.createDataFrame([dict(r) for t in _tables for r in t.records])`,desc:"InfluxDB query",notes:"Install influxdb-client-python",imp:[],conf:.9},PutPrometheusRemoteWrite:{cat:"Monitoring",tpl:`# Prometheus remote write — use Databricks monitoring instead
# Databricks provides built-in metrics via Ganglia and Spark UI
print(f"[PROMETHEUS] Use Databricks built-in monitoring or configure Prometheus remote write endpoint")
# For custom metrics: import prometheus_client`,desc:"Prometheus remote write",notes:"Use Databricks built-in monitoring; or prometheus_client",imp:[],conf:.9},SendNiFiSiteToSite:{cat:"Deprecated",tpl:`# NiFi Site-to-Site — NOT needed in Databricks.
# Data flows are handled within the Databricks workspace.
# If cross-workspace transfer is needed, use Unity Catalog sharing.
print("[MIGRATION] NiFi Site-to-Site replaced by Unity Catalog cross-workspace sharing")`,desc:"NiFi Site-to-Site sender",notes:"Not needed — use UC sharing for cross-workspace data flow",imp:[],conf:.9},ConfluentSchemaRegistry:{cat:"Schema Registry",tpl:`# Confluent Schema Registry integration via Spark
from confluent_kafka.schema_registry import SchemaRegistryClient
_sr_client = SchemaRegistryClient({"url": "{schema_registry_url}"})
_schema = _sr_client.get_latest_version("{subject}").schema
print(f"[SCHEMA] Retrieved schema for {subject}: version {_schema.version}")`,desc:"Confluent Schema Registry",notes:"Install confluent-kafka; use for Kafka schema evolution",imp:[],conf:.9},HortonworksSchemaRegistry:{cat:"Schema Registry",tpl:`# Hortonworks/Cloudera Schema Registry → Confluent Schema Registry or Unity Catalog
# Unity Catalog provides schema governance natively.
print("[MIGRATION] Hortonworks Schema Registry → Unity Catalog schema management")`,desc:"Hortonworks Schema Registry",notes:"Migrate to Unity Catalog or Confluent Schema Registry",imp:[],conf:.9},PutRedis:{cat:"Redis",tpl:`# Redis write via redis-py
import redis
_r = redis.Redis(host="{host}", port={port}, password=dbutils.secrets.get(scope="{scope}", key="redis-pass"))
for row in df_{in}.limit(10000).collect():
    _r.set(row["{key_field}"], str(row.asDict()))`,desc:"Redis write",notes:"Install redis library; for large datasets use RDD mapPartitions",imp:[],conf:.9},GetRedis:{cat:"Redis",tpl:`# Redis read via redis-py
import redis
_r = redis.Redis(host="{host}", port={port}, password=dbutils.secrets.get(scope="{scope}", key="redis-pass"))
_keys = _r.keys("*")
_data = [{k.decode(): _r.get(k).decode()} for k in _keys[:10000]]
df_{v} = spark.createDataFrame(_data)`,desc:"Redis read",notes:"Install redis library; for large datasets use scan_iter",imp:[],conf:.9},PutPhoenix:{cat:"Phoenix/JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:phoenix:{zookeeper_quorum}")
  .option("dbtable", "{table}")
  .option("driver", "org.apache.phoenix.jdbc.PhoenixDriver")
  .mode("append")
  .save())`,desc:"Phoenix write via JDBC",notes:"Phoenix → Spark SQL via JDBC; consider migrating to Delta Lake",imp:[],conf:.9},QueryPhoenix:{cat:"Phoenix/JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:phoenix:{zookeeper_quorum}")
  .option("dbtable", "{table}")
  .option("driver", "org.apache.phoenix.jdbc.PhoenixDriver")
  .load())`,desc:"Phoenix read via JDBC",notes:"Phoenix → Spark SQL read; migrate to Delta Lake",imp:[],conf:.9},PutTeradata:{cat:"Teradata JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:teradata://{host}/DATABASE={database}")
  .option("dbtable", "{table}")
  .option("driver", "com.teradata.jdbc.TeraDriver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="td-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="td-pass"))
  .mode("append")
  .save())`,desc:"Teradata write",notes:"Install Teradata JDBC driver; store credentials in Secret Scopes",imp:[],conf:.9},QueryTeradata:{cat:"Teradata JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:teradata://{host}/DATABASE={database}")
  .option("dbtable", "{table}")
  .option("driver", "com.teradata.jdbc.TeraDriver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="td-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="td-pass"))
  .load())`,desc:"Teradata read",notes:"Install Teradata JDBC driver",imp:[],conf:.9},PutOracle:{cat:"Oracle JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:oracle:thin:@{host}:{port}:{sid}")
  .option("dbtable", "{table}")
  .option("driver", "oracle.jdbc.driver.OracleDriver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="ora-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="ora-pass"))
  .mode("append")
  .save())`,desc:"Oracle database write",notes:"Install Oracle JDBC driver (ojdbc8.jar)",imp:[],conf:.9},QueryOracle:{cat:"Oracle JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:oracle:thin:@{host}:{port}:{sid}")
  .option("dbtable", "{table}")
  .option("driver", "oracle.jdbc.driver.OracleDriver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="ora-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="ora-pass"))
  .load())`,desc:"Oracle database read",notes:"Install Oracle JDBC driver",imp:[],conf:.9},PutSAPHANA:{cat:"SAP HANA JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:sap://{host}:{port}")
  .option("dbtable", "{table}")
  .option("driver", "com.sap.db.jdbc.Driver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="sap-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="sap-pass"))
  .mode("append")
  .save())`,desc:"SAP HANA write",notes:"Install SAP HANA JDBC driver (ngdbc.jar)",imp:[],conf:.9},PutVertica:{cat:"Vertica JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:vertica://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "com.vertica.jdbc.Driver")
  .option("user", dbutils.secrets.get(scope="{scope}", key="vertica-user"))
  .option("password", dbutils.secrets.get(scope="{scope}", key="vertica-pass"))
  .mode("append")
  .save())`,desc:"Vertica write",notes:"Install Vertica JDBC driver",imp:[],conf:.9},QueryPresto:{cat:"Presto/Trino JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:presto://{host}:{port}/{catalog}")
  .option("dbtable", "{table}")
  .option("driver", "com.facebook.presto.jdbc.PrestoDriver")
  .load())`,desc:"Presto query via JDBC",notes:"Consider migrating Presto queries to Spark SQL",imp:[],conf:.9},QueryTrino:{cat:"Presto/Trino JDBC",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:trino://{host}:{port}/{catalog}")
  .option("dbtable", "{table}")
  .option("driver", "io.trino.jdbc.TrinoDriver")
  .load())`,desc:"Trino query via JDBC",notes:"Consider migrating Trino queries to Spark SQL",imp:[],conf:.9},PutGreenplum:{cat:"Greenplum JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:postgresql://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "org.postgresql.Driver")
  .mode("append")
  .save())`,desc:"Greenplum write via JDBC",notes:"Greenplum uses PostgreSQL JDBC driver",imp:[],conf:.9},PutCockroachDB:{cat:"CockroachDB JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:postgresql://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "org.postgresql.Driver")
  .mode("append")
  .save())`,desc:"CockroachDB write via JDBC",notes:"CockroachDB uses PostgreSQL wire protocol",imp:[],conf:.9},PutTimescaleDB:{cat:"TimescaleDB JDBC",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:postgresql://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "org.postgresql.Driver")
  .mode("append")
  .save())`,desc:"TimescaleDB write via JDBC",notes:"TimescaleDB uses PostgreSQL JDBC driver",imp:[],conf:.9},PutDatadog:{cat:"Monitoring",tpl:`# Datadog metrics via API
import requests
requests.post("https://api.datadoghq.com/api/v2/series", headers={"DD-API-KEY": dbutils.secrets.get(scope="{scope}", key="dd-api-key")}, json={"series":[{"metric":"{metric_name}","points":[[int(__import__("time").time()), {value}]]}]})`,desc:"Datadog metrics",notes:"Use Datadog API; or configure Databricks Datadog integration",imp:[],conf:.9},PutGrafanaAnnotation:{cat:"Monitoring",tpl:`# Grafana annotation via API
import requests
requests.post("{grafana_url}/api/annotations", headers={"Authorization":f"Bearer {dbutils.secrets.get(scope=\\"{scope}\\", key=\\"grafana-token\\")}"}, json={"text":"{annotation_text}","tags":["{tag}"]})`,desc:"Grafana annotation",notes:"Use Grafana REST API",imp:[],conf:.9},ExecuteFlinkSQL:{cat:"Spark SQL",tpl:`# Flink SQL → Spark SQL (mostly compatible)
df_{v} = spark.sql("""
{sql}
""")`,desc:"Flink SQL → Spark SQL",notes:"Most Flink SQL syntax is compatible with Spark SQL",imp:[],conf:.9},TriggerAirflowDag:{cat:"Workflows",tpl:`# Airflow DAG trigger → Databricks Workflows
# Use Databricks Workflows for orchestration instead of Airflow
print("[MIGRATION] Airflow DAG trigger → Databricks Workflows job trigger")
# To trigger a Databricks Job programmatically:
# from databricks.sdk import WorkspaceClient
# w = WorkspaceClient()
# w.jobs.run_now(job_id=<job_id>)`,desc:"Airflow DAG trigger",notes:"Replace Airflow with Databricks Workflows",imp:[],conf:.9},ExecuteProcessBash:{cat:"subprocess",tpl:`# Execute bash process
import subprocess as _sp
_result = _sp.run(["/bin/bash", "-c", "{command}"], capture_output=True, text=True, timeout=300)
if _result.returncode != 0:
    print(f"[ERROR] {_result.stderr[:200]}")
else:
    print(f"[OK] {_result.stdout[:200]}")`,desc:"Bash process execution",notes:"Review script for Databricks compatibility",imp:[],conf:.9},ConvertCSVToAvro:{cat:"DataFrame API",tpl:`from pyspark.sql.avro.functions import to_avro
df_{v} = df_{in}.select(to_avro(struct("*")).alias("value"))`,desc:"CSV to Avro conversion",notes:"Spark handles format conversion natively via DataFrame API",imp:[],conf:.9},ConvertExcelToCSVProcessor:{cat:"DataFrame API",tpl:`# Excel to CSV conversion
df_{v} = spark.read.format("com.crealytics.spark.excel")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("/Volumes/{catalog}/{schema}/{path}")`,desc:"Excel to DataFrame",notes:"Install spark-excel library (com.crealytics)",imp:[],conf:.9},EnforceOrder:{cat:"DataFrame API",tpl:'df_{v} = df_{in}.orderBy("{order_column}")',desc:"Enforce ordering",notes:"Use orderBy for deterministic ordering",imp:[],conf:.9},GenerateRecord:{cat:"Test Data",tpl:`df_{v} = spark.range({count}).toDF("id")
# Add test columns as needed`,desc:"Generate test records",notes:"Replace with actual test data generation",imp:[],conf:.9},ListenFTP:{cat:"External Storage",tpl:`# FTP listener → poll-based approach
# No native FTP listener in Databricks
# Use Auto Loader on a staged landing zone instead
df_{v} = spark.readStream.format("cloudFiles").option("cloudFiles.format", "{format}").load("/Volumes/{catalog}/{schema}/ftp_landing/")`,desc:"FTP listener → Auto Loader",notes:"Stage FTP files to Volumes; use Auto Loader for pickup",imp:[],conf:.9},ValidateCSV:{cat:"DLT Expectations",tpl:`# CSV validation via DLT expectations
# @dlt.expect_or_drop("valid_csv", "col1 IS NOT NULL")
df_{v} = df_{in}.filter(col("{validation_col}").isNotNull())`,desc:"CSV validation",notes:"Use DLT expectations for data quality",imp:[],conf:.9},ValidateXml:{cat:"DataFrame API",tpl:`# XML validation — check structure
from pyspark.sql.functions import length
df_{v} = df_{in}.filter(length(col("value")) > 0)`,desc:"XML validation",notes:"Use spark-xml for structured parsing",imp:[],conf:.9},DeleteS3Object:{cat:"AWS S3",tpl:`# Delete S3 object
import boto3
_s3 = boto3.client("s3")
_s3.delete_object(Bucket="{bucket}", Key="{key}")
print(f"[S3] Deleted s3://{bucket}/{key}")`,desc:"S3 object deletion",notes:"Use boto3; or dbutils.fs.rm for DBFS-mounted paths",imp:[],conf:.9},TagS3Object:{cat:"AWS S3",tpl:`# Tag S3 object
import boto3
_s3 = boto3.client("s3")
_s3.put_object_tagging(Bucket="{bucket}", Key="{key}", Tagging={"TagSet":[{"Key":"{tag_key}","Value":"{tag_value}"}]})`,desc:"S3 object tagging",notes:"Use boto3 for S3 tagging operations",imp:[],conf:.9},PutKinesisStream:{cat:"AWS Kinesis",tpl:`# Kinesis — streaming-safe with foreachBatch
import boto3, json

def _kinesis_batch_{v}(batch_df, batch_id):
    _kinesis = boto3.client("kinesis", region_name="{region}")
    _records = [{{"Data": json.dumps(row.asDict()), "PartitionKey": str(row["{partition_key}"])}} for row in batch_df.collect()]
    for i in range(0, len(_records), 500):
        _kinesis.put_records(StreamName="{stream}", Records=_records[i:i+500])
    print(f"[KINESIS] Batch {{batch_id}}: {{len(_records)}} records")

# For streaming: df_{in}.writeStream.foreachBatch(_kinesis_batch_{v}).start()
# For batch:
_kinesis_batch_{v}(df_{in}, 0)`,desc:"Kinesis with foreachBatch",notes:"Streaming-safe; batch API",imp:["boto3"],conf:.92},GetKinesisStream:{cat:"AWS Kinesis",tpl:`# Kinesis stream read via Spark Structured Streaming
df_{v} = (spark.readStream
  .format("kinesis")
  .option("streamName", "{stream}")
  .option("region", "{region}")
  .option("initialPosition", "TRIM_HORIZON")
  .load())`,desc:"Kinesis stream read",notes:"Use Spark Kinesis connector; requires kinesis-asl library",imp:[],conf:.9},PutCloudWatchMetric:{cat:"AWS CloudWatch",tpl:`# CloudWatch metrics
import boto3
_cw = boto3.client("cloudwatch", region_name="{region}")
_cw.put_metric_data(Namespace="{namespace}", MetricData=[{"MetricName":"{metric}","Value":{value},"Unit":"{unit}"}])`,desc:"CloudWatch metric publish",notes:"Use boto3 for CloudWatch integration",imp:[],conf:.9},AmazonGlacierUpload:{cat:"AWS Glacier",tpl:`# Glacier upload via boto3
import boto3
_glacier = boto3.client("glacier", region_name="{region}")
# For archival storage, consider using S3 Glacier storage class instead
print("[AWS] Use S3 with Glacier storage class for archival")`,desc:"Glacier upload",notes:"Use S3 Intelligent-Tiering or Glacier storage class",imp:[],conf:.9},PutCloudWatchLogs:{cat:"AWS CloudWatch",tpl:`# CloudWatch Logs
import boto3
_logs = boto3.client("logs", region_name="{region}")
_logs.put_log_events(logGroupName="{log_group}", logStreamName="{log_stream}", logEvents=[{"timestamp":int(__import__("time").time()*1000),"message":"{message}"}])`,desc:"CloudWatch Logs publish",notes:"Use boto3; or configure Databricks log delivery",imp:[],conf:.9},FetchAzureDataLakeStorage:{cat:"Azure ADLS",tpl:'df_{v} = spark.read.format("{format}").load("abfss://{container}@{account}.dfs.core.windows.net/{path}")',desc:"ADLS Gen2 read",notes:"Use ABFSS paths with Unity Catalog external locations",imp:[],conf:.9},ListAzureDataLakeStorage:{cat:"Azure ADLS",tpl:`_files = dbutils.fs.ls("abfss://{container}@{account}.dfs.core.windows.net/{path}")
for f in _files:
    print(f.name, f.size)`,desc:"ADLS Gen2 list",notes:"Use dbutils.fs.ls or Auto Loader for continuous listing",imp:[],conf:.9},DeleteAzureDataLakeStorage:{cat:"Azure ADLS",tpl:'dbutils.fs.rm("abfss://{container}@{account}.dfs.core.windows.net/{path}", recurse=True)',desc:"ADLS Gen2 delete",notes:"Use dbutils.fs.rm for ADLS operations",imp:[],conf:.9},DeleteAzureBlobStorage:{cat:"Azure Blob",tpl:'dbutils.fs.rm("wasbs://{container}@{account}.blob.core.windows.net/{path}")',desc:"Azure Blob delete",notes:"Use dbutils.fs.rm",imp:[],conf:.9},PutAzureCosmosDB:{cat:"Azure Cosmos",tpl:`(df_{in}.write
  .format("cosmos.oltp")
  .option("spark.cosmos.accountEndpoint", "{endpoint}")
  .option("spark.cosmos.accountKey", dbutils.secrets.get(scope="{scope}", key="cosmos-key"))
  .option("spark.cosmos.database", "{database}")
  .option("spark.cosmos.container", "{container}")
  .mode("append")
  .save())`,desc:"Cosmos DB write",notes:"Use Azure Cosmos DB Spark connector (pre-installed on DBR)",imp:[],conf:.9},PutAzureCosmosDBRecord:{cat:"Azure Cosmos",tpl:`(df_{in}.write
  .format("cosmos.oltp")
  .option("spark.cosmos.accountEndpoint", "{endpoint}")
  .option("spark.cosmos.accountKey", dbutils.secrets.get(scope="{scope}", key="cosmos-key"))
  .option("spark.cosmos.database", "{database}")
  .option("spark.cosmos.container", "{container}")
  .option("spark.cosmos.write.strategy", "ItemOverwrite")
  .mode("append")
  .save())`,desc:"Cosmos DB record write",notes:"Use Cosmos DB Spark connector with record-level writes",imp:[],conf:.9},GetAzureEventHub:{cat:"Azure Event Hubs",tpl:`df_{v} = (spark.readStream
  .format("eventhubs")
  .options(**{"eventhubs.connectionString": dbutils.secrets.get(scope="{scope}", key="eh-connstr")})
  .load())`,desc:"Azure Event Hubs read",notes:"Use Azure Event Hubs Spark connector",imp:[],conf:.9},PutAzureQueueStorage:{cat:"Azure Queue",tpl:`# Azure Queue Storage write
from azure.storage.queue import QueueClient
_q = QueueClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="azure-storage-conn"), "{queue_name}")
for row in df_{in}.limit(1000).collect():
    _q.send_message(str(row.asDict()))`,desc:"Azure Queue write",notes:"Use azure-storage-queue SDK",imp:[],conf:.9},GetAzureQueueStorage:{cat:"Azure Queue",tpl:`# Azure Queue Storage read
from azure.storage.queue import QueueClient
_q = QueueClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="azure-storage-conn"), "{queue_name}")
_msgs = [m.content for m in _q.receive_messages(max_messages=32)]
df_{v} = spark.createDataFrame([{"message": m} for m in _msgs])`,desc:"Azure Queue read",notes:"Use azure-storage-queue SDK",imp:[],conf:.9},PutAzureServiceBus:{cat:"Azure Service Bus",tpl:`# Azure Service Bus write
from azure.servicebus import ServiceBusClient, ServiceBusMessage
_sb = ServiceBusClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="sb-connstr"))
with _sb.get_topic_sender("{topic}") as sender:
    for row in df_{in}.limit(1000).collect():
        sender.send_messages(ServiceBusMessage(str(row.asDict())))`,desc:"Azure Service Bus write",notes:"Use azure-servicebus SDK",imp:[],conf:.9},ConsumeAzureServiceBus:{cat:"Azure Service Bus",tpl:`# Azure Service Bus read
from azure.servicebus import ServiceBusClient
_sb = ServiceBusClient.from_connection_string(dbutils.secrets.get(scope="{scope}", key="sb-connstr"))
with _sb.get_subscription_receiver("{topic}", "{subscription}") as receiver:
    _msgs = [{"body": str(m)} for m in receiver.receive_messages(max_message_count=100)]
df_{v} = spark.createDataFrame(_msgs)`,desc:"Azure Service Bus read",notes:"Use azure-servicebus SDK",imp:[],conf:.9},DeleteGCSObject:{cat:"GCP GCS",tpl:'dbutils.fs.rm("gs://{bucket}/{key}")',desc:"GCS object delete",notes:"Use dbutils.fs.rm for GCS paths",imp:[],conf:.9},PutBigQueryStreaming:{cat:"GCP BigQuery",tpl:`(df_{in}.write
  .format("bigquery")
  .option("table", "{project}.{dataset}.{table}")
  .option("temporaryGcsBucket", "{temp_bucket}")
  .option("writeMethod", "direct")
  .mode("append")
  .save())`,desc:"BigQuery streaming write",notes:"Use Spark BigQuery connector with direct write method",imp:[],conf:.9},PublishGCPubSub:{cat:"GCP Pub/Sub",tpl:`# GCP Pub/Sub publish
from google.cloud import pubsub_v1
_publisher = pubsub_v1.PublisherClient()
_topic = _publisher.topic_path("{project}", "{topic}")
for row in df_{in}.limit(1000).collect():
    _publisher.publish(_topic, str(row.asDict()).encode("utf-8"))`,desc:"GCP Pub/Sub publish",notes:"Use google-cloud-pubsub SDK",imp:[],conf:.9},ConsumeGCPubSub:{cat:"GCP Pub/Sub",tpl:`# GCP Pub/Sub consume
from google.cloud import pubsub_v1
_subscriber = pubsub_v1.SubscriberClient()
_sub = _subscriber.subscription_path("{project}", "{subscription}")
_response = _subscriber.pull(subscription=_sub, max_messages=100)
_msgs = [{"data": m.message.data.decode("utf-8")} for m in _response.received_messages]
df_{v} = spark.createDataFrame(_msgs)`,desc:"GCP Pub/Sub consume",notes:"Use google-cloud-pubsub SDK; or Spark Pub/Sub connector",imp:[],conf:.9},PutGCPDataflow:{cat:"GCP Dataflow",tpl:`# GCP Dataflow trigger
# Migrate Dataflow pipelines to Databricks Structured Streaming
print("[MIGRATION] GCP Dataflow pipeline → Databricks Structured Streaming")`,desc:"GCP Dataflow trigger",notes:"Replace with Databricks Structured Streaming",imp:[],conf:.9},PutSnowflake:{cat:"Snowflake",tpl:`(df_{in}.write
  .format("snowflake")
  .option("sfUrl", "{account}.snowflakecomputing.com")
  .option("sfUser", dbutils.secrets.get(scope="{scope}", key="sf-user"))
  .option("sfPassword", dbutils.secrets.get(scope="{scope}", key="sf-pass"))
  .option("sfDatabase", "{database}")
  .option("sfSchema", "{schema}")
  .option("dbtable", "{table}")
  .mode("append")
  .save())`,desc:"Snowflake write",notes:"Use Databricks Snowflake connector",imp:[],conf:.9},GetSnowflake:{cat:"Snowflake",tpl:`df_{v} = (spark.read
  .format("snowflake")
  .option("sfUrl", "{account}.snowflakecomputing.com")
  .option("sfUser", dbutils.secrets.get(scope="{scope}", key="sf-user"))
  .option("sfPassword", dbutils.secrets.get(scope="{scope}", key="sf-pass"))
  .option("sfDatabase", "{database}")
  .option("sfSchema", "{schema}")
  .option("dbtable", "{table}")
  .load())`,desc:"Snowflake read",notes:"Use Databricks Snowflake connector",imp:[],conf:.9},PutCypher:{cat:"Neo4j",tpl:`# Neo4j write via neo4j-driver
from neo4j import GraphDatabase
_driver = GraphDatabase.driver("{uri}", auth=(dbutils.secrets.get(scope="{scope}", key="neo4j-user"), dbutils.secrets.get(scope="{scope}", key="neo4j-pass")))
with _driver.session() as session:
    for row in df_{in}.limit(1000).collect():
        session.run("{cypher_query}", **row.asDict())`,desc:"Neo4j Cypher write",notes:"Use neo4j-driver; or Neo4j Spark connector for large datasets",imp:[],conf:.9},GetCypher:{cat:"Neo4j",tpl:`# Neo4j read via Spark connector
df_{v} = (spark.read
  .format("org.neo4j.spark.DataSource")
  .option("url", "{uri}")
  .option("authentication.basic.username", dbutils.secrets.get(scope="{scope}", key="neo4j-user"))
  .option("authentication.basic.password", dbutils.secrets.get(scope="{scope}", key="neo4j-pass"))
  .option("query", "{cypher_query}")
  .load())`,desc:"Neo4j Cypher read",notes:"Use Neo4j Spark Connector for distributed reads",imp:[],conf:.9},PutDruidRecord:{cat:"Druid",tpl:`# Apache Druid ingestion
import requests
_payload = {"type":"index_parallel","spec":{"dataSchema":{"dataSource":"{datasource}"}}}
requests.post("{druid_url}/druid/indexer/v1/task", json=_payload)`,desc:"Druid record ingestion",notes:"Use Druid indexer API; consider migrating to Delta Lake + Photon",imp:[],conf:.9},QueryDruid:{cat:"Druid",tpl:`# Druid query via SQL
import requests
_r = requests.post("{druid_url}/druid/v2/sql", json={"query":"{sql}"})
df_{v} = spark.createDataFrame(_r.json())`,desc:"Druid SQL query",notes:"Migrate Druid queries to Spark SQL on Delta Lake",imp:[],conf:.9},PutClickHouse:{cat:"ClickHouse",tpl:`(df_{in}.write
  .format("jdbc")
  .option("url", "jdbc:clickhouse://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
  .mode("append")
  .save())`,desc:"ClickHouse write",notes:"Use ClickHouse JDBC driver",imp:[],conf:.9},QueryClickHouse:{cat:"ClickHouse",tpl:`df_{v} = (spark.read
  .format("jdbc")
  .option("url", "jdbc:clickhouse://{host}:{port}/{database}")
  .option("dbtable", "{table}")
  .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
  .load())`,desc:"ClickHouse read",notes:"Use ClickHouse JDBC driver; consider migrating to Delta Lake + Photon",imp:[],conf:.9},PutIceberg:{cat:"Iceberg",tpl:`(df_{in}.writeTo("spark_catalog.{database}.{table}")
  .using("iceberg")
  .tableProperty("format-version", "2")
  .createOrReplace())`,desc:"Iceberg table write",notes:"Databricks supports Iceberg natively with UniForm; consider Delta Lake",imp:[],conf:.9},PutHudi:{cat:"Hudi",tpl:`(df_{in}.write
  .format("hudi")
  .option("hoodie.table.name", "{table}")
  .option("hoodie.datasource.write.operation", "upsert")
  .option("hoodie.datasource.write.recordkey.field", "{record_key}")
  .mode("append")
  .save("{path}"))`,desc:"Hudi table write",notes:"Consider migrating to Delta Lake for native Databricks support",imp:[],conf:.9},GetSplunk:{cat:"Splunk",tpl:`# Splunk search via SDK
import splunklib.client as client
import splunklib.results as results
_svc = client.connect(host="{host}", port={port}, username=dbutils.secrets.get(scope="{scope}", key="splunk-user"), password=dbutils.secrets.get(scope="{scope}", key="splunk-pass"))
_job = _svc.jobs.create("{search_query}", **{"earliest_time":"-1h"})
while not _job.is_done(): __import__("time").sleep(1)
_reader = results.JSONResultsReader(_job.results(output_mode="json"))
df_{v} = spark.createDataFrame([dict(r) for r in _reader if isinstance(r, dict)])`,desc:"Splunk search read",notes:"Use splunklib SDK; or Splunk Spark connector for large datasets",imp:[],conf:.9},QuerySplunkIndexingStatus:{cat:"Splunk",tpl:`# Splunk indexing status query
import splunklib.client as client
_svc = client.connect(host="{host}", port={port})
print(f"[SPLUNK] Indexing status: {_svc.indexes.list()}")`,desc:"Splunk indexing status",notes:"Use splunklib SDK for status monitoring",imp:[],conf:.9},QueryInfluxDB:{cat:"InfluxDB",tpl:`# InfluxDB query via client
from influxdb_client import InfluxDBClient
_client = InfluxDBClient(url="{url}", token=dbutils.secrets.get(scope="{scope}", key="influx-token"), org="{org}")
_query_api = _client.query_api()
_tables = _query_api.query("{flux_query}")
_records = []
for table in _tables:
    for record in table.records:
        _records.append(record.values)
df_{v} = spark.createDataFrame(_records)`,desc:"InfluxDB query",notes:"Use influxdb-client SDK; consider migrating time-series to Delta Lake",imp:[],conf:.9},ConvertAvroToParquet:{cat:"DataFrame API",tpl:`df_{v} = spark.read.format("avro").load("{input_path}")
df_{v}.write.format("parquet").save("{output_path}")`,desc:"Avro to Parquet conversion",notes:"Spark handles format conversion natively",imp:[],conf:.9},ConvertParquetToAvro:{cat:"DataFrame API",tpl:`df_{v} = spark.read.format("parquet").load("{input_path}")
df_{v}.write.format("avro").save("{output_path}")`,desc:"Parquet to Avro conversion",notes:"Spark handles format conversion natively",imp:[],conf:.9},SplitAvro:{cat:"DataFrame API",tpl:`# Avro split — Spark reads Avro files as DataFrames natively
df_{v} = spark.read.format("avro").load("{input_path}")
# Process each record individually if needed
for row in df_{v}.collect():
    pass  # Process row`,desc:"Avro split",notes:"Not needed in Spark — read entire Avro dataset as DataFrame",imp:[],conf:.9},ConvertJSONToAvro:{cat:"DataFrame API",tpl:`from pyspark.sql.avro.functions import to_avro
from pyspark.sql.functions import struct
df_{v} = df_{in}.select(to_avro(struct("*")).alias("value"))`,desc:"JSON to Avro conversion",notes:"Use Spark built-in avro functions",imp:[],conf:.9},FlattenJson:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import col, explode_outer
# Flatten nested JSON structure
df_{v} = df_{in}
for field in [f for f in df_{in}.schema.fields if str(f.dataType).startswith("Struct")]:
    for nested in field.dataType.fields:
        df_{v} = df_{v}.withColumn(f"{field.name}_{nested.name}", col(f"{field.name}.{nested.name}"))
    df_{v} = df_{v}.drop(field.name)`,desc:"JSON flattening",notes:"Use PySpark struct navigation; or explode for arrays",imp:[],conf:.9},Base64EncodeContent:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import base64, unbase64
df_{v} = df_{in}.withColumn("{column}", base64(col("{column}")))`,desc:"Base64 encode/decode",notes:"Use PySpark base64/unbase64 functions",imp:[],conf:.9},ConvertCharacterSet:{cat:"DataFrame API",tpl:`# Character set conversion — Spark handles encoding via options
df_{v} = spark.read.option("encoding", "{target_encoding}").format("{format}").load("{path}")`,desc:"Character set conversion",notes:"Use Spark read options for encoding; or Python encode/decode",imp:[],conf:.9},ExtractEmailHeaders:{cat:"Email",tpl:`# Extract email headers
import email
from pyspark.sql.functions import udf
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def extract_headers(raw):
    msg = email.message_from_string(raw)
    return dict(msg.items())
df_{v} = df_{in}.withColumn("headers", extract_headers(col("{content_column}")))`,desc:"Email header extraction",notes:"Use Python email module as UDF",imp:[],conf:.9},ExtractEmailAttachments:{cat:"Email",tpl:`# Extract email attachments
import email, base64
def extract_attachments(raw):
    msg = email.message_from_string(raw)
    attachments = []
    for part in msg.walk():
        if part.get_content_disposition() == "attachment":
            attachments.append({"filename": part.get_filename(), "size": len(part.get_payload())})
    return attachments
# Apply as UDF on email content column`,desc:"Email attachment extraction",notes:"Use Python email module; store attachments in Volumes",imp:[],conf:.9},ConsumeIMAP:{cat:"Email",tpl:`# IMAP email consumption
import imaplib, email
_mail = imaplib.IMAP4_SSL("{host}")
_mail.login(dbutils.secrets.get(scope="{scope}", key="email-user"), dbutils.secrets.get(scope="{scope}", key="email-pass"))
_mail.select("INBOX")
_, _nums = _mail.search(None, "UNSEEN")
_emails = []
for num in _nums[0].split():
    _, data = _mail.fetch(num, "(RFC822)")
    _emails.append({"raw": data[0][1].decode("utf-8", errors="replace")})
df_{v} = spark.createDataFrame(_emails)`,desc:"IMAP email consumer",notes:"Use imaplib; schedule as periodic job",imp:[],conf:.9},ConsumePOP3:{cat:"Email",tpl:`# POP3 email consumption
import poplib, email
_pop = poplib.POP3_SSL("{host}")
_pop.user(dbutils.secrets.get(scope="{scope}", key="email-user"))
_pop.pass_(dbutils.secrets.get(scope="{scope}", key="email-pass"))
_count = len(_pop.list()[1])
_emails = []
for i in range(1, min(_count+1, 100)):
    _, lines, _ = _pop.retr(i)
    _emails.append({"raw": b"\\n".join(lines).decode("utf-8", errors="replace")})
df_{v} = spark.createDataFrame(_emails)
_pop.quit()`,desc:"POP3 email consumer",notes:"Use poplib; schedule as periodic job",imp:[],conf:.9},SetSNMP:{cat:"SNMP",tpl:`# SNMP set operation
from pysnmp.hlapi import setCmd, SnmpEngine, CommunityData, UdpTransportTarget, ContextData, ObjectType, ObjectIdentity
_errorIndication, _, _, _ = next(setCmd(
    SnmpEngine(), CommunityData("{community}"),
    UdpTransportTarget(("{host}", {port})), ContextData(),
    ObjectType(ObjectIdentity("{oid}"), "{value}")
))
if _errorIndication: print(f"[SNMP ERROR] {_errorIndication}")`,desc:"SNMP set operation",notes:"Use pysnmp library; install via pip",imp:[],conf:.9},ListenSNMP:{cat:"SNMP",tpl:`# SNMP trap listener — use scheduled polling instead
from pysnmp.hlapi import getCmd, SnmpEngine, CommunityData, UdpTransportTarget, ContextData, ObjectType, ObjectIdentity
print("[MIGRATION] SNMP trap listener → scheduled SNMP polling job")`,desc:"SNMP trap listener",notes:"Replace with scheduled SNMP polling job",imp:[],conf:.9},PutSyslog:{cat:"Syslog",tpl:`# Syslog send
import socket
_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
_sock.sendto("{message}".encode(), ("{host}", {port}))
_sock.close()`,desc:"Syslog sender",notes:"Use socket for UDP syslog; or configure log forwarding",imp:[],conf:.9},PutUDP:{cat:"Network",tpl:`# UDP send
import socket
_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
for row in df_{in}.limit(1000).collect():
    _sock.sendto(str(row.asDict()).encode(), ("{host}", {port}))
_sock.close()`,desc:"UDP sender",notes:"Use Python socket for UDP operations",imp:[],conf:.9},GetTCP:{cat:"Network",tpl:`# TCP read — use structured streaming or scheduled batch instead
print("[MIGRATION] TCP listener → Databricks Structured Streaming or scheduled batch job")
# For TCP sources, stage data to cloud storage and use Auto Loader`,desc:"TCP reader",notes:"Replace with Auto Loader on staged data",imp:[],conf:.9},PutRELP:{cat:"Network",tpl:`# RELP output — use standard syslog or HTTP endpoint
import requests
for row in df_{in}.limit(1000).collect():
    requests.post("{endpoint}", json=row.asDict())`,desc:"RELP output",notes:"Replace with HTTP endpoint or syslog",imp:[],conf:.9},ListenRELP:{cat:"Network",tpl:`# RELP listener → use HTTP endpoint or cloud-based log collection
print("[MIGRATION] RELP listener → HTTP endpoint or cloud logging service")`,desc:"RELP listener",notes:"Replace with cloud-native log collection",imp:[],conf:.9},ExecuteFlumeSink:{cat:"Spark Streaming",tpl:`# Flume sink → Spark Structured Streaming
# Apache Flume is deprecated — migrate to Spark Streaming
df_{v} = (spark.readStream
  .format("{format}")
  .load("{path}"))
print("[MIGRATION] Flume sink replaced by Spark Structured Streaming")`,desc:"Flume sink → Structured Streaming",notes:"Flume is deprecated; migrate to Structured Streaming",imp:[],conf:.9},ExecuteFlumeSource:{cat:"Spark Streaming",tpl:`# Flume source → Spark Structured Streaming
# Apache Flume is deprecated — migrate to Spark Streaming
df_{v} = (spark.readStream
  .format("{format}")
  .load("{path}"))
print("[MIGRATION] Flume source replaced by Spark Structured Streaming")`,desc:"Flume source → Structured Streaming",notes:"Flume is deprecated; migrate to Structured Streaming",imp:[],conf:.9},ConsumeWindowsEventLog:{cat:"Windows",tpl:`# Windows Event Log → schedule a collector script
# In Databricks, collect Windows events via:
# 1. Forward events to a log aggregator (Splunk, ELK)
# 2. Write to cloud storage
# 3. Read with Auto Loader
print("[MIGRATION] Windows Event Log → forward to cloud storage + Auto Loader")`,desc:"Windows Event Log consumer",notes:"Forward events to cloud storage; use Auto Loader",imp:[],conf:.9},ConsumeEWS:{cat:"Email/Exchange",tpl:`# Exchange Web Services consumer
# Use Microsoft Graph API instead of EWS
import requests
_token = dbutils.secrets.get(scope="{scope}", key="graph-token")
_r = requests.get("https://graph.microsoft.com/v1.0/me/messages", headers={"Authorization":f"Bearer {_token}"})
df_{v} = spark.createDataFrame(_r.json().get("value", []))`,desc:"Exchange Web Services consumer",notes:"Migrate to Microsoft Graph API",imp:[],conf:.9},RetryFlowFile:{cat:"Utility",tpl:`# Retry logic — use try/except with backoff
import time
for _attempt in range(3):
    try:
        # <retry logic here>
        break
    except Exception as e:
        if _attempt < 2: time.sleep(2 ** _attempt)
        else: raise`,desc:"Retry logic",notes:"Use Python try/except with exponential backoff",imp:[],conf:.9},ReplaceTextWithMapping:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import regexp_replace, col
df_{v} = df_{in}
# Apply text replacements
_mappings = {"{pattern}": "{replacement}"}
for pat, rep in _mappings.items():
    df_{v} = df_{v}.withColumn("{column}", regexp_replace(col("{column}"), pat, rep))`,desc:"Text replacement with mapping",notes:"Use PySpark regexp_replace for pattern-based replacements",imp:[],conf:.9},MonitorActivity:{cat:"Utility",tpl:`# Monitor activity — track flow metrics
from datetime import datetime
_now = datetime.now()
print(f"[MONITOR] Flow activity check at {_now}")
# In Databricks, use Workflows monitoring and Spark UI`,desc:"Activity monitor",notes:"Use Databricks Workflows monitoring and alerting",imp:[],conf:.9},ScanAttribute:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import col
# Scan/filter based on attribute patterns
df_{v} = df_{in}.filter(col("{attribute}").rlike("{pattern}"))`,desc:"Attribute scanning",notes:"Use PySpark rlike for regex-based attribute scanning",imp:[],conf:.9},ScanContent:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import col
# Scan content for pattern matches
df_{v} = df_{in}.filter(col("{content_column}").rlike("{pattern}"))`,desc:"Content scanning",notes:"Use PySpark rlike for content pattern matching",imp:[],conf:.9},QueryWhois:{cat:"Network",tpl:`# WHOIS query
import subprocess
_result = subprocess.run(["whois", "{domain}"], capture_output=True, text=True, timeout=30)
print(_result.stdout[:500])`,desc:"WHOIS lookup",notes:"Use whois command or python-whois library",imp:[],conf:.9},GeoEnrichIPRecord:{cat:"DataFrame API",tpl:`# IP geolocation enrichment
# Install: pip install geoip2
import geoip2.database
_reader = geoip2.database.Reader("{geoip_db_path}")
def enrich_ip(ip):
    try:
        r = _reader.city(ip)
        return {"country": r.country.name, "city": r.city.name, "lat": r.location.latitude, "lon": r.location.longitude}
    except: return None
# Register as UDF and apply to IP column`,desc:"IP geolocation enrichment",notes:"Use geoip2 library with MaxMind database",imp:[],conf:.9},PublishKafka_1_0:{cat:"Kafka",tpl:`(df_{in}.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "{bootstrap_servers}")
  .option("topic", "{topic}")
  .save())`,desc:"Kafka 1.0 producer",notes:"Use Spark Kafka connector (version-agnostic)",imp:[],conf:.9},ConsumeKafka_1_0:{cat:"Kafka",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{bootstrap_servers}")
  .option("subscribe", "{topic}")
  .option("startingOffsets", "earliest")
  .load())`,desc:"Kafka 1.0 consumer",notes:"Use Spark Kafka connector (version-agnostic)",imp:[],conf:.9},ConsumeKafkaRecord:{cat:"Kafka",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{bootstrap_servers}")
  .option("subscribe", "{topic}")
  .option("startingOffsets", "earliest")
  .load()
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)"))`,desc:"Kafka record consumer",notes:"Use Spark Kafka connector with schema-aware deserialization",imp:[],conf:.9},ListDatabaseTables:{cat:"JDBC",tpl:`# List database tables
df_{v} = spark.sql("SHOW TABLES IN {database}")
df_{v}.show()`,desc:"List database tables",notes:"Use Spark SQL SHOW TABLES; or JDBC metadata query",imp:[],conf:.9},PutSQL:{cat:"JDBC",tpl:`# Execute SQL statement
(df_{in}.write
  .format("jdbc")
  .option("url", "{jdbc_url}")
  .option("dbtable", "{table}")
  .option("driver", "{driver}")
  .mode("append")
  .save())`,desc:"SQL write via JDBC",notes:"Use Spark JDBC writer",imp:[],conf:.9},AvroSchemaRegistry:{cat:"Schema Registry",tpl:`# Avro Schema Registry integration
from confluent_kafka.schema_registry import SchemaRegistryClient
_sr = SchemaRegistryClient({"url": "{schema_registry_url}"})
_schema = _sr.get_latest_version("{subject}").schema
print(f"[SCHEMA] Latest schema version retrieved")`,desc:"Avro Schema Registry",notes:"Use Confluent Schema Registry client",imp:[],conf:.9},RemoteProcessGroup:{cat:"Deprecated",tpl:`# NiFi Remote Process Group — NOT needed in Databricks
# Cross-workspace data sharing handled by Unity Catalog
print("[MIGRATION] NiFi Remote Process Group → Unity Catalog cross-workspace sharing")`,desc:"NiFi RPG",notes:"Not needed — use Unity Catalog for data sharing",imp:[],conf:.9},InputPort:{cat:"Deprecated",tpl:`# NiFi Input Port — NOT needed in Databricks
# Data ingress handled by Auto Loader or readStream
print("[MIGRATION] NiFi Input Port → Auto Loader / readStream")`,desc:"NiFi Input Port",notes:"Not needed — use Auto Loader for ingestion",imp:[],conf:.9},OutputPort:{cat:"Deprecated",tpl:`# NiFi Output Port — NOT needed in Databricks
# Data egress handled by writeStream or Delta sharing
print("[MIGRATION] NiFi Output Port → writeStream / Delta Sharing")`,desc:"NiFi Output Port",notes:"Not needed — use writeStream or Delta Sharing",imp:[],conf:.9},Funnel:{cat:"Deprecated",tpl:`# NiFi Funnel — NOT needed in Databricks
# DataFrame operations naturally merge data flows via union()
print("[MIGRATION] NiFi Funnel → DataFrame union()")`,desc:"NiFi Funnel",notes:"Not needed — use DataFrame union()",imp:[],conf:.9},PutSlackMessage:{cat:"Slack",tpl:`# Slack message via webhook
import requests
requests.post("{webhook_url}", json={"text": "{message}"})`,desc:"Slack message",notes:"Use Slack webhook URL; store in secrets",imp:[],conf:.9},SendTelegram:{cat:"Notification",tpl:`# Telegram notification
import requests
requests.post(f"https://api.telegram.org/bot{dbutils.secrets.get(scope=\\"{scope}\\", key=\\"telegram-token\\")}/sendMessage", json={"chat_id":"{chat_id}","text":"{message}"})`,desc:"Telegram notification",notes:"Use Telegram Bot API",imp:[],conf:.9},PutPagerDuty:{cat:"Notification",tpl:`# PagerDuty alert
import requests
requests.post("https://events.pagerduty.com/v2/enqueue", json={"routing_key":dbutils.secrets.get(scope="{scope}",key="pd-key"),"event_action":"trigger","payload":{"summary":"{summary}","severity":"{severity}","source":"{source}"}})`,desc:"PagerDuty alert",notes:"Use PagerDuty Events API v2",imp:[],conf:.9},PutOpsGenie:{cat:"Notification",tpl:`# OpsGenie alert
import requests
requests.post("https://api.opsgenie.com/v2/alerts", headers={"Authorization":"GenieKey "+dbutils.secrets.get(scope="{scope}",key="opsgenie-key")}, json={"message":"{message}","priority":"{priority}"})`,desc:"OpsGenie alert",notes:"Use OpsGenie REST API",imp:[],conf:.9},PostHTTP:{cat:"HTTP",tpl:`# HTTP POST — streaming-safe with foreachBatch
import requests

def _post_batch_{v}(batch_df, batch_id):
    _records = batch_df.toPandas().to_dict(orient="records")
    _response = requests.post("{url}", json=_records, headers={"{header_key}":"{header_value}"}, timeout=30)
    print(f"[HTTP] Batch {batch_id}: {len(_records)} records -> {_response.status_code}")

# For streaming: df_{in}.writeStream.foreachBatch(_post_batch_{v}).start()
# For batch:
_post_batch_{v}(df_{in}, 0)`,desc:"HTTP POST with foreachBatch",notes:"Streaming-safe",imp:["requests"],conf:.92},GetHTTP:{cat:"HTTP",tpl:`import requests
_response = requests.get("{url}", headers={"{header_key}":"{header_value}"}, timeout=30)
df_{v} = spark.createDataFrame([_response.json()] if isinstance(_response.json(), dict) else _response.json())`,desc:"HTTP GET",notes:"Use requests library; for large responses use streaming",imp:[],conf:.9},CryptographicHashContent:{cat:"DataFrame API",tpl:`from pyspark.sql.functions import sha2, col
df_{v} = df_{in}.withColumn("{hash_column}", sha2(col("{content_column}"), 256))`,desc:"Cryptographic hash",notes:"Use PySpark sha2, md5, or sha1 functions",imp:[],conf:.9},SignContent:{cat:"Security",tpl:`# Digital signature — use Python cryptography library
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import padding
print("[CRYPTO] Use cryptography library for digital signatures")`,desc:"Digital signature",notes:"Use Python cryptography library",imp:[],conf:.9},VerifyContentMAC:{cat:"Security",tpl:`# MAC verification — use Python hmac module
import hmac, hashlib
_mac = hmac.new(key=b"{key}", msg=b"{message}", digestmod=hashlib.sha256)
print(f"[CRYPTO] MAC: {_mac.hexdigest()}")`,desc:"MAC verification",notes:"Use Python hmac module",imp:[],conf:.9},ValidateJSON:{cat:"DLT Expectations",tpl:`from pyspark.sql.functions import col, from_json, schema_of_json
# Validate JSON structure
_sample = df_{in}.select("{json_column}").first()[0]
_schema = schema_of_json(_sample)
df_{v} = df_{in}.withColumn("_parsed", from_json(col("{json_column}"), _schema)).filter(col("_parsed").isNotNull())`,desc:"JSON validation",notes:"Use DLT expectations for production data quality",imp:[],conf:.9},SchemaValidation:{cat:"DLT Expectations",tpl:`# Schema validation via DLT expectations
# @dlt.expect_all_or_drop({{"valid_schema": "col1 IS NOT NULL AND col2 IS NOT NULL"}})
df_{v} = df_{in}.filter(col("{required_col}").isNotNull())`,desc:"Schema validation",notes:"Use DLT expectations for declarative data quality",imp:[],conf:.9},ConsumeKafka_2_0:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .option("startingOffsets", "earliest")
  .load()
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)", "topic", "partition", "offset", "timestamp"))`,desc:"Kafka 2.0 consumer via Structured Streaming",notes:"Update broker config",imp:["pyspark.sql.functions"],conf:.95},ConsumeKafkaRecord_1_0:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load()
  .selectExpr("CAST(value AS STRING) as json_str"))`,desc:"Kafka Record 1.0 consumer",notes:"Schema handled by Spark",imp:["pyspark.sql.functions"],conf:.95},ConsumeKafkaRecord_2_0:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{topic}")
  .load()
  .selectExpr("CAST(value AS STRING) as json_str"))`,desc:"Kafka Record 2.0 consumer",notes:"Schema handled by Spark",imp:["pyspark.sql.functions"],conf:.95},PublishKafka_2_0:{cat:"Structured Streaming",tpl:`(df_{v}
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .write.format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Kafka 2.0 producer",notes:"Update broker config",imp:[],conf:.95},PublishKafkaRecord_1_0:{cat:"Structured Streaming",tpl:`(df_{v}
  .selectExpr("CAST(key AS STRING)", "to_json(struct(*)) AS value")
  .write.format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Kafka Record 1.0 producer",notes:"Update broker config",imp:["pyspark.sql.functions"],conf:.95},PublishKafkaRecord_2_0:{cat:"Structured Streaming",tpl:`(df_{v}
  .selectExpr("CAST(key AS STRING)", "to_json(struct(*)) AS value")
  .write.format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("topic", "{topic}")
  .save())`,desc:"Kafka Record 2.0 producer",notes:"Update broker config",imp:["pyspark.sql.functions"],conf:.95},ConsumeKinesisStream:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kinesis")
  .option("streamName", "{stream}")
  .option("region", "{region}")
  .option("initialPosition", "TRIM_HORIZON")
  .load())`,desc:"Kinesis consumer via Structured Streaming",notes:"Use Databricks Kinesis connector",imp:[],conf:.92},ConvertAvroToORC:{cat:"Spark DataFrame",tpl:`df_{v} = spark.read.format("avro").load("{path}")
df_{v}.write.format("orc").save("{output_path}")`,desc:"Avro to ORC via Spark",notes:"Format-agnostic DataFrames",imp:[],conf:.95},FetchParquet:{cat:"Spark DataFrame",tpl:'df_{v} = spark.read.format("parquet").load("{path}")',desc:"Read Parquet natively",notes:"Parquet is Spark native",imp:[],conf:.95},JoltTransformRecord:{cat:"Spark DataFrame",tpl:`df_{v} = df_{input}
for src, dst in _jolt_mappings.items():
    df_{v} = df_{v}.withColumnRenamed(src, dst)`,desc:"Jolt record transform via DataFrame ops",notes:"Translate Jolt spec to PySpark",imp:["pyspark.sql.functions"],conf:.9},EvaluateXPath:{cat:"Spark XML",tpl:`from pyspark.sql.functions import xpath_string, col
df_{v} = df_{input}.withColumn("_xpath_result", xpath_string(col("xml_content"), "{xpath_expr}"))`,desc:"XPath evaluation via spark-xml",notes:"Use spark-xml library",imp:["pyspark.sql.functions"],conf:.9},EvaluateXQuery:{cat:"Spark XML",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import lxml.etree as ET
@udf(StringType())
def eval_xquery(xml_str):
    doc = ET.fromstring(xml_str.encode())
    return str(doc.xpath("{xquery_expr}"))
df_{v} = df_{input}.withColumn("_xq", eval_xquery(col("value")))`,desc:"XQuery via lxml UDF",notes:"Install lxml on cluster",imp:["lxml"],conf:.9},SplitXml:{cat:"Spark XML",tpl:'df_{v} = spark.read.format("xml").option("rowTag", "{tag}").load("{path}")',desc:"Split XML via spark-xml rowTag",notes:"Install spark-xml",imp:[],conf:.92},ExtractGrok:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import regexp_extract, col
df_{v} = df_{input}.withColumn("_extracted", regexp_extract(col("value"), r"{regex_pattern}", 1))`,desc:"Grok extraction via regexp_extract",notes:"Translate Grok to regex",imp:["pyspark.sql.functions"],conf:.9},ExtractAvroMetadata:{cat:"Spark DataFrame",tpl:`df_{v} = spark.read.format("avro").load("{path}")
print(f"Schema: {df_{v}.schema.simpleString()}")`,desc:"Extract Avro schema metadata",notes:"Schema auto-detected by Spark",imp:[],conf:.93},ExtractHL7Attributes:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_hl7(msg):
    segs = msg.split("\\r")
    return {s.split("|")[0]: "|".join(s.split("|")[1:4]) for s in segs if "|" in s}
df_{v} = df_{input}.withColumn("hl7_attrs", parse_hl7(col("value")))`,desc:"HL7 message parsing via UDF",notes:"Use hl7apy for production",imp:["pyspark.sql.functions"],conf:.9},ExtractCCDAAttributes:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_ccda(xml):
    import lxml.etree as ET
    doc = ET.fromstring(xml.encode())
    return {"patient": doc.findtext(".//{urn:hl7-org:v3}patient/{urn:hl7-org:v3}name", default="")}
df_{v} = df_{input}.withColumn("ccda_attrs", parse_ccda(col("value")))`,desc:"CCDA clinical document parsing",notes:"Install lxml",imp:["lxml"],conf:.9},RouteHL7:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import col
df_{v}_adt = df_{input}.filter(col("value").contains("ADT"))
df_{v}_orm = df_{input}.filter(col("value").contains("ORM"))
df_{v} = df_{input}`,desc:"HL7 message routing by type",notes:"Filter by MSH segment",imp:["pyspark.sql.functions"],conf:.9},ExtractTNEFAttachments:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import ArrayType, StringType
@udf(ArrayType(StringType()))
def extract_tnef(data):
    return ["attachment_extracted"]
df_{v} = df_{input}.withColumn("tnef_attachments", extract_tnef(col("content")))`,desc:"TNEF attachment extraction",notes:"Install tnefparse",imp:["tnefparse"],conf:.9},ParseCEF:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import regexp_extract, col
df_{v} = df_{input}.withColumn("cef_vendor", regexp_extract(col("value"), r"CEF:\\d+\\|([^|]+)", 1)).withColumn("cef_severity", regexp_extract(col("value"), r"CEF:\\d+(?:\\|[^|]*){6}\\|([^|]+)", 1))`,desc:"CEF security log parsing",notes:"Regex-based",imp:["pyspark.sql.functions"],conf:.9},ParseEvtx:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import MapType, StringType
@udf(MapType(StringType(), StringType()))
def parse_evtx(xml):
    import lxml.etree as ET
    doc = ET.fromstring(xml.encode())
    return {"EventID": doc.findtext(".//{*}EventID", default="")}
df_{v} = df_{input}.withColumn("event_data", parse_evtx(col("value")))`,desc:"Windows Event Log XML parsing",notes:"Parse EVTX XML",imp:["lxml"],conf:.9},ParseNetflowv5:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import col
df_{v} = df_{input}.selectExpr("*", "substring(value,1,4) as src_ip", "substring(value,5,4) as dst_ip")`,desc:"NetFlow v5 packet parsing",notes:"Binary format needs UDF",imp:["pyspark.sql.functions"],conf:.9},ParseSyslog5424:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import regexp_extract, col
df_{v} = df_{input}.withColumn("priority", regexp_extract(col("value"), r"<(\\d+)>", 1)).withColumn("hostname", regexp_extract(col("value"), r"<\\d+>\\d+ [\\S]+ ([\\S]+)", 1))`,desc:"RFC 5424 Syslog parsing",notes:"Regex extraction",imp:["pyspark.sql.functions"],conf:.92},ListenGRPC:{cat:"Databricks Serving",tpl:`import grpc
from concurrent import futures
print(f"[gRPC] Server endpoint configured")`,desc:"gRPC listener via Databricks Serving",notes:"Deploy as Databricks App",imp:["grpcio"],conf:.9},InvokeGRPC:{cat:"PySpark UDF",tpl:`import grpc
_channel = grpc.insecure_channel("{host}:{port}")
df_{v} = df_{input}`,desc:"gRPC client invocation",notes:"Generate stubs from .proto",imp:["grpcio"],conf:.9},ConnectWebSocket:{cat:"Structured Streaming",tpl:`import websocket, json
_ws = websocket.create_connection("{ws_url}")
_msgs = [{"data": _ws.recv()} for _ in range(100)]
_ws.close()
df_{v} = spark.createDataFrame(_msgs)`,desc:"WebSocket client",notes:"Use websocket-client",imp:["websocket-client"],conf:.9},ListenWebSocket:{cat:"Structured Streaming",tpl:`import asyncio, websockets
print("[WS] WebSocket listener configured")`,desc:"WebSocket server via Databricks App",notes:"Deploy as Databricks App",imp:["websockets"],conf:.9},PutWebSocket:{cat:"PySpark UDF",tpl:`import websocket, json
_ws = websocket.create_connection("{ws_url}")
for row in df_{input}.limit(1000).collect():
    _ws.send(json.dumps(row.asDict()))
_ws.close()`,desc:"WebSocket message sender",notes:"Use websocket-client",imp:["websocket-client"],conf:.9},ListenSMTP:{cat:"Databricks App",tpl:`from aiosmtpd.controller import Controller
print("[SMTP] Email receiver configured")`,desc:"SMTP receiver via Databricks App",notes:"Deploy as Databricks App",imp:["aiosmtpd"],conf:.9},ForkRecord:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import explode, col
df_{v} = df_{input}.select(explode(col("{array_field}")).alias("record"), "*")`,desc:"Fork/explode nested records",notes:"Use explode",imp:["pyspark.sql.functions"],conf:.93},SampleRecord:{cat:"Spark DataFrame",tpl:"df_{v} = df_{input}.sample(fraction={sample_rate}, seed=42)",desc:"Sample records",notes:"Adjust fraction",imp:[],conf:.95},ScriptedTransformRecord:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col, struct
from pyspark.sql.types import StringType
import json
@udf(StringType())
def transform_record(row_json):
    data = json.loads(row_json)
    data["_processed"] = True
    return json.dumps(data)
df_{v} = df_{input}.withColumn("_transformed", transform_record(col("value")))`,desc:"Scripted record transform via UDF",notes:"Migrate NiFi script",imp:["pyspark.sql.functions"],conf:.9},PutRecord:{cat:"Delta Lake",tpl:'df_{input}.write.format("delta").mode("append").saveAsTable("{table_name}")',desc:"Generic record writer via Delta",notes:"Use Delta for persistence",imp:[],conf:.93},InvokeScriptedProcessor:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
@udf(StringType())
def scripted_logic(value):
    return value
df_{v} = df_{input}.withColumn("_result", scripted_logic(col("value")))`,desc:"Scripted processor via UDF",notes:"Translate NiFi script to Python",imp:["pyspark.sql.functions"],conf:.9},CryptographicHashAttribute:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import sha2, col
df_{v} = df_{input}.withColumn("_hash", sha2(col("{attr_name}"), 256))`,desc:"Hash attribute with SHA-256",notes:"Built-in sha2",imp:["pyspark.sql.functions"],conf:.95},HashAttribute:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import sha2, col
df_{v} = df_{input}.withColumn("_hash", sha2(col("{attr_name}"), 256))`,desc:"Hash attribute value",notes:"Built-in sha2/md5",imp:["pyspark.sql.functions"],conf:.95},EncryptContentPGP:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import BinaryType
import gnupg
_gpg = gnupg.GPG()
@udf(BinaryType())
def pgp_encrypt(data):
    return bytes(str(_gpg.encrypt(data, "{recipient_key}")), "utf-8")
df_{v} = df_{input}.withColumn("_encrypted", pgp_encrypt(col("value")))`,desc:"PGP encryption",notes:"Install python-gnupg",imp:["gnupg"],conf:.9},DecryptContentPGP:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import gnupg
_gpg = gnupg.GPG()
@udf(StringType())
def pgp_decrypt(data):
    return str(_gpg.decrypt(data, passphrase=dbutils.secrets.get(scope="pgp", key="passphrase")))
df_{v} = df_{input}.withColumn("_decrypted", pgp_decrypt(col("value")))`,desc:"PGP decryption",notes:"Install python-gnupg",imp:["gnupg"],conf:.9},AttributeRollingWindow:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import col, avg, window
df_{v} = df_{input}.groupBy(window("timestamp", "{window_duration}")).agg(avg("{metric_col}").alias("rolling_avg"))`,desc:"Rolling window aggregation",notes:"Use Spark window functions",imp:["pyspark.sql.functions"],conf:.92},AttributesToCSV:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import concat_ws, col
df_{v} = df_{input}.withColumn("_csv", concat_ws(",", *[col(c) for c in df_{input}.columns]))`,desc:"Attributes to CSV string",notes:"Use concat_ws",imp:["pyspark.sql.functions"],conf:.93},LookupAttribute:{cat:"Spark DataFrame",tpl:`_lookup_df = spark.table("{lookup_table}")
df_{v} = df_{input}.join(_lookup_df, df_{input}["{key_col}"] == _lookup_df["{lookup_key}"], "left")`,desc:"Attribute lookup via join",notes:"Broadcast join for small lookups",imp:["pyspark.sql.functions"],conf:.92},CaptureChangeMySQL:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("delta")
  .option("readChangeFeed", "true")
  .table("{source_table}"))`,desc:"MySQL CDC via DLT Change Data Feed",notes:"Or use Debezium + Kafka",imp:[],conf:.92},CompareFuzzyHash:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import FloatType
@udf(FloatType())
def fuzzy_sim(a, b):
    if not a or not b: return 0.0
    sa, sb = set(a), set(b)
    return float(len(sa & sb)) / float(len(sa | sb))
df_{v} = df_{input}.withColumn("_similarity", fuzzy_sim(col("hash1"), col("hash2")))`,desc:"Fuzzy hash comparison",notes:"Use ssdeep for production",imp:[],conf:.9},FuzzyHashContent:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import hashlib
@udf(StringType())
def fuzzy_hash(content):
    return hashlib.sha256(content.encode()).hexdigest()[:16]
df_{v} = df_{input}.withColumn("_fuzzy_hash", fuzzy_hash(col("value")))`,desc:"Fuzzy hash generation",notes:"Use ssdeep for true fuzzy hashing",imp:[],conf:.9},GeoEnrichIP:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StructType, StructField, StringType, FloatType
import geoip2.database
_reader = geoip2.database.Reader("/dbfs/geo/GeoLite2-City.mmdb")
@udf(StructType([StructField("city",StringType()),StructField("country",StringType())]))
def geo_lookup(ip):
    try:
        r = _reader.city(ip)
        return (r.city.name, r.country.name)
    except: return (None, None)
df_{v} = df_{input}.withColumn("_geo", geo_lookup(col("ip_address")))`,desc:"IP geolocation enrichment",notes:"Download GeoLite2 DB",imp:["geoip2"],conf:.9},ISPEnrichIP:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import geoip2.database
_reader = geoip2.database.Reader("/dbfs/geo/GeoLite2-ASN.mmdb")
@udf(StringType())
def isp_lookup(ip):
    try: return _reader.asn(ip).autonomous_system_organization
    except: return None
df_{v} = df_{input}.withColumn("_isp", isp_lookup(col("ip_address")))`,desc:"ISP/ASN enrichment",notes:"Download GeoLite2-ASN DB",imp:["geoip2"],conf:.9},IdentifyMimeType:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import mimetypes
@udf(StringType())
def detect_mime(fname):
    mime, _ = mimetypes.guess_type(fname or "")
    return mime or "application/octet-stream"
df_{v} = df_{input}.withColumn("mime_type", detect_mime(col("filename")))`,desc:"MIME type detection",notes:"Use python-magic for binary",imp:["mimetypes"],conf:.93},ModifyBytes:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import substring, col
df_{v} = df_{input}.withColumn("_modified", substring(col("content"), 1, 100))`,desc:"Byte-level content modification",notes:"Use substring",imp:["pyspark.sql.functions"],conf:.9},SegmentContent:{cat:"Spark DataFrame",tpl:`from pyspark.sql.functions import explode, split, col
df_{v} = df_{input}.withColumn("_segment", explode(split(col("value"), "{delimiter}")))`,desc:"Segment content by delimiter",notes:"Use split + explode",imp:["pyspark.sql.functions"],conf:.92},DuplicateFlowFile:{cat:"Spark DataFrame",tpl:`from functools import reduce
from pyspark.sql import DataFrame
df_{v} = reduce(DataFrame.union, [df_{input}] * {num_copies})`,desc:"Duplicate records N times",notes:"Use union",imp:[],conf:.93},GetHTMLElement:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
@udf(StringType())
def extract_html(html):
    soup = BeautifulSoup(html, "html.parser")
    el = soup.select_one("{css_selector}")
    return el.get_text() if el else None
df_{v} = df_{input}.withColumn("_extracted", extract_html(col("html")))`,desc:"HTML element extraction",notes:"Install beautifulsoup4",imp:["beautifulsoup4"],conf:.9},ModifyHTMLElement:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
@udf(StringType())
def modify_html(html):
    soup = BeautifulSoup(html, "html.parser")
    el = soup.select_one("{css_selector}")
    if el: el.string = "{new_value}"
    return str(soup)
df_{v} = df_{input}.withColumn("_modified_html", modify_html(col("html")))`,desc:"HTML element modification",notes:"Install beautifulsoup4",imp:["beautifulsoup4"],conf:.9},PutHTMLElement:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
@udf(StringType())
def insert_html(html):
    soup = BeautifulSoup(html, "html.parser")
    tag = soup.new_tag("{tag_name}")
    tag.string = "{content}"
    soup.body.append(tag)
    return str(soup)
df_{v} = df_{input}.withColumn("_html", insert_html(col("html")))`,desc:"HTML element insertion",notes:"Install beautifulsoup4",imp:["beautifulsoup4"],conf:.9},GetSmbFile:{cat:"PySpark UDF",tpl:`import smbclient
smbclient.register_session("{server}", username=dbutils.secrets.get(scope="smb", key="user"), password=dbutils.secrets.get(scope="smb", key="pass"))
_data = smbclient.open_file(r"\\\\{server}\\{share}\\{path}", mode="rb").read()
df_{v} = spark.createDataFrame([{"content": _data.decode("utf-8", errors="replace")}])`,desc:"Read from SMB/Windows share",notes:"Install smbprotocol",imp:["smbprotocol"],conf:.9},PutSmbFile:{cat:"PySpark UDF",tpl:`import smbclient
smbclient.register_session("{server}", username=dbutils.secrets.get(scope="smb", key="user"), password=dbutils.secrets.get(scope="smb", key="pass"))
df_{input}.toPandas().to_csv(r"\\\\{server}\\{share}\\output.csv", index=False)`,desc:"Write to SMB/Windows share",notes:"Install smbprotocol",imp:["smbprotocol"],conf:.9},GetTwitter:{cat:"PySpark UDF",tpl:`import tweepy
_auth = tweepy.OAuth2BearerHandler(dbutils.secrets.get(scope="twitter", key="bearer_token"))
_api = tweepy.API(_auth)
_tweets = [{"text": t.text, "created_at": str(t.created_at)} for t in _api.search_tweets(q="{query}", count=100)]
df_{v} = spark.createDataFrame(_tweets)`,desc:"Twitter/X data ingestion",notes:"Use tweepy",imp:["tweepy"],conf:.9},PostSlack:{cat:"PySpark UDF",tpl:`import requests
requests.post("{webhook_url}", json={"text": "Pipeline notification"})
df_{v} = df_{input}`,desc:"Slack via webhook",notes:"Use Slack webhook URL",imp:["requests"],conf:.92},GetRethinkDB:{cat:"PySpark UDF",tpl:`from rethinkdb import r
_conn = r.connect(host="{host}", port=28015, db="{database}")
_docs = list(r.table("{table}").limit(50000).run(_conn))
df_{v} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
_conn.close()`,desc:"RethinkDB reader",notes:"Install rethinkdb",imp:["rethinkdb"],conf:.9},PutRethinkDB:{cat:"PySpark UDF",tpl:`from rethinkdb import r
_conn = r.connect(host="{host}", port=28015, db="{database}")
r.table("{table}").insert(df_{input}.limit(10000).toPandas().to_dict(orient="records")).run(_conn)
_conn.close()`,desc:"RethinkDB writer",notes:"Install rethinkdb",imp:["rethinkdb"],conf:.9},DeleteRethinkDB:{cat:"PySpark UDF",tpl:`from rethinkdb import r
_conn = r.connect(host="{host}", port=28015, db="{database}")
r.table("{table}").delete().run(_conn)
_conn.close()`,desc:"RethinkDB delete",notes:"Install rethinkdb",imp:["rethinkdb"],conf:.9},FetchGridFS:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
import gridfs
_client = MongoClient("{mongo_uri}")
_fs = gridfs.GridFS(_client["{database}"])
_files = [{"filename": f.filename, "length": f.length} for f in _fs.find().limit(1000)]
df_{v} = spark.createDataFrame(_files) if _files else spark.createDataFrame([], "filename STRING, length LONG")
_client.close()`,desc:"GridFS file listing",notes:"Use pymongo gridfs",imp:["pymongo"],conf:.9},PutGridFS:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
import gridfs
_client = MongoClient("{mongo_uri}")
_fs = gridfs.GridFS(_client["{database}"])
for row in df_{input}.limit(1000).collect():
    _fs.put(str(row.asDict()).encode(), filename=f"record")
_client.close()`,desc:"GridFS file upload",notes:"Use pymongo gridfs",imp:["pymongo"],conf:.9},DeleteGridFS:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
import gridfs
_client = MongoClient("{mongo_uri}")
_fs = gridfs.GridFS(_client["{database}"])
for f in _fs.find({"filename": "{pattern}"}):
    _fs.delete(f._id)
_client.close()`,desc:"GridFS file deletion",notes:"Use pymongo gridfs",imp:["pymongo"],conf:.9},FetchElasticsearch:{cat:"PySpark UDF",tpl:`from elasticsearch import Elasticsearch
_es = Elasticsearch("{es_url}", basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")))
_doc = _es.get(index="{index}", id="{doc_id}")
df_{v} = spark.createDataFrame([_doc["_source"]])`,desc:"Fetch single ES document",notes:"Use elasticsearch-py",imp:["elasticsearch"],conf:.9},DeleteByQueryElasticsearch:{cat:"PySpark UDF",tpl:`from elasticsearch import Elasticsearch
_es = Elasticsearch("{es_url}", basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")))
_es.delete_by_query(index="{index}", body={"query": {"match_all": {}}})
df_{v} = df_{input}`,desc:"ES delete by query",notes:"Use elasticsearch-py",imp:["elasticsearch"],conf:.9},QueryElasticsearchHttp:{cat:"PySpark UDF",tpl:`from elasticsearch import Elasticsearch
_es = Elasticsearch("{es_url}", basic_auth=(dbutils.secrets.get(scope="es", key="user"), dbutils.secrets.get(scope="es", key="pass")))
_result = _es.search(index="{index}", size=10000)
_hits = [h["_source"] for h in _result["hits"]["hits"]]
df_{v} = spark.createDataFrame(_hits) if _hits else spark.createDataFrame([], "id STRING")`,desc:"ES HTTP query",notes:"Use elasticsearch-py",imp:["elasticsearch"],conf:.9},DeleteHBaseCells:{cat:"PySpark UDF",tpl:`import happybase
_conn = happybase.Connection("{hbase_host}")
_table = _conn.table("{table}")
for row in df_{input}.limit(1000).collect():
    _table.delete(row["row_key"].encode())
_conn.close()`,desc:"HBase cell deletion",notes:"Use happybase",imp:["happybase"],conf:.9},DeleteHBaseRow:{cat:"PySpark UDF",tpl:`import happybase
_conn = happybase.Connection("{hbase_host}")
_table = _conn.table("{table}")
for row in df_{input}.limit(1000).collect():
    _table.delete(row["row_key"].encode())
_conn.close()`,desc:"HBase row deletion",notes:"Use happybase",imp:["happybase"],conf:.9},GetHDFSEvents:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", "/mnt/schema/hdfs_events")
  .load("{hdfs_path}"))`,desc:"HDFS events via Auto Loader",notes:"Use cloudFiles",imp:[],conf:.92},GetHDFSFileInfo:{cat:"Spark DataFrame",tpl:`_files = dbutils.fs.ls("{hdfs_path}")
df_{v} = spark.createDataFrame([{"path": f.path, "name": f.name, "size": f.size} for f in _files])`,desc:"HDFS file info listing",notes:"Use dbutils.fs.ls",imp:[],conf:.93},GetHDFSSequenceFile:{cat:"Spark DataFrame",tpl:'df_{v} = spark.sparkContext.sequenceFile("{hdfs_path}", "org.apache.hadoop.io.Text", "org.apache.hadoop.io.Text").toDF(["key", "value"])',desc:"Hadoop SequenceFile reader",notes:"Use sparkContext.sequenceFile",imp:[],conf:.9},DeleteDynamoDB:{cat:"PySpark UDF",tpl:`import boto3
_dynamodb = boto3.resource("dynamodb", region_name="{region}")
_table = _dynamodb.Table("{table}")
with _table.batch_writer() as _batch:
    for row in df_{input}.limit(10000).collect():
        _batch.delete_item(Key={"id": row["id"]})`,desc:"DynamoDB batch delete",notes:"Configure AWS creds",imp:["boto3"],conf:.9},DeleteSQS:{cat:"PySpark UDF",tpl:`import boto3
_sqs = boto3.client("sqs", region_name="{region}")
_msgs = _sqs.receive_message(QueueUrl="{queue_url}", MaxNumberOfMessages=10)
for msg in _msgs.get("Messages", []):
    _sqs.delete_message(QueueUrl="{queue_url}", ReceiptHandle=msg["ReceiptHandle"])`,desc:"SQS message deletion",notes:"Configure AWS creds",imp:["boto3"],conf:.9},InvokeAWSGatewayApi:{cat:"PySpark UDF",tpl:`import requests
_response = requests.post("{api_gateway_url}", json=df_{input}.limit(100).toPandas().to_dict(orient="records"))
df_{v} = spark.createDataFrame([_response.json()] if isinstance(_response.json(), dict) else _response.json())`,desc:"AWS API Gateway invocation",notes:"Configure AWS creds",imp:["boto3","requests"],conf:.9},PutSplunkHTTP:{cat:"PySpark UDF",tpl:`import requests
_token = dbutils.secrets.get(scope="splunk", key="hec_token")
for row in df_{input}.limit(10000).collect():
    requests.post("{splunk_hec_url}", json={"event": row.asDict()}, headers={"Authorization": f"Splunk {_token}"}, verify=False)`,desc:"Splunk HEC sender",notes:"Configure HEC token",imp:["requests"],conf:.9},PutRiemann:{cat:"PySpark UDF",tpl:`import socket
_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
_sock.connect(("{riemann_host}", 5555))
print("[RIEMANN] Sent monitoring events")
df_{v} = df_{input}`,desc:"Riemann monitoring events",notes:"Use riemann-client",imp:[],conf:.9},ListenTCPRecord:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("socket")
  .option("host", "{host}")
  .option("port", "{port}")
  .load())`,desc:"TCP record stream listener",notes:"Use socket source",imp:[],conf:.9},ListenUDPRecord:{cat:"Structured Streaming",tpl:`df_{v} = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "{brokers}")
  .option("subscribe", "{udp_topic}")
  .load())`,desc:"UDP record stream via Kafka",notes:"Pipe UDP through Kafka",imp:[],conf:.9},UpdateCounter:{cat:"Spark DataFrame",tpl:`_counter = spark.sparkContext.accumulator(0)
def _count(row): _counter.add(1)
df_{input}.foreach(_count)
print(f"[COUNTER] Count: {_counter.value}")
df_{v} = df_{input}`,desc:"Counter via accumulator",notes:"Use accumulators or Delta",imp:[],conf:.92},UpdateHiveTable:{cat:"Delta Lake",tpl:`spark.sql("ALTER TABLE {table_name} SET TBLPROPERTIES ('updated'='true')")
df_{v} = df_{input}`,desc:"Hive table DDL update",notes:"Use Unity Catalog",imp:[],conf:.92},ValidateCsv:{cat:"Spark DataFrame",tpl:`df_{v} = spark.read.option("header", "true").option("mode", "PERMISSIVE").csv("{path}")
_corrupt = df_{v}.filter(col("_corrupt_record").isNotNull())`,desc:"CSV validation",notes:"Use permissive mode",imp:["pyspark.sql.functions"],conf:.93},GetMongoRecord:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
_client = MongoClient("{mongo_uri}")
_docs = list(_client["{database}"]["{collection}"].find({}, {"_id": 0}).limit(50000))
df_{v} = spark.createDataFrame(_docs) if _docs else spark.createDataFrame([], "id STRING")
_client.close()`,desc:"MongoDB record reader",notes:"Use pymongo",imp:["pymongo"],conf:.9},RunMongoAggregation:{cat:"PySpark UDF",tpl:`from pymongo import MongoClient
_client = MongoClient("{mongo_uri}")
_results = list(_client["{database}"]["{collection}"].aggregate([{pipeline}]))
df_{v} = spark.createDataFrame(_results) if _results else spark.createDataFrame([], "id STRING")
_client.close()`,desc:"MongoDB aggregation pipeline",notes:"Translate to PySpark",imp:["pymongo"],conf:.9},ExecuteInfluxDBQuery:{cat:"PySpark UDF",tpl:`from influxdb_client import InfluxDBClient
_client = InfluxDBClient(url="{influx_url}", token=dbutils.secrets.get(scope="influx", key="token"), org="{org}")
_tables = _client.query_api().query("{flux_query}")
_records = [r.values for table in _tables for r in table.records]
df_{v} = spark.createDataFrame(_records) if _records else spark.createDataFrame([], "time STRING, value DOUBLE")`,desc:"InfluxDB Flux query",notes:"Use influxdb-client",imp:["influxdb-client"],conf:.9},QueryDNS:{cat:"PySpark UDF",tpl:`from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import socket
@udf(StringType())
def dns_lookup(hostname):
    try: return socket.gethostbyname(hostname)
    except: return None
df_{v} = df_{input}.withColumn("_ip", dns_lookup(col("hostname")))`,desc:"DNS lookup via UDF",notes:"Use socket.gethostbyname",imp:[],conf:.93},GetJMSQueue:{cat:"PySpark UDF",tpl:`import stomp
_msgs = []
class _L(stomp.ConnectionListener):
    def on_message(self, frame): _msgs.append({"body": frame.body})
_conn = stomp.Connection([("{jms_host}", {jms_port})])
_conn.set_listener("", _L())
_conn.connect(dbutils.secrets.get(scope="jms", key="user"), dbutils.secrets.get(scope="jms", key="pass"), wait=True)
_conn.subscribe(destination="/queue/{queue}", id=1, ack="auto")
import time; time.sleep(5)
_conn.disconnect()
df_{v} = spark.createDataFrame(_msgs) if _msgs else spark.createDataFrame([], "body STRING")`,desc:"JMS Queue consumer",notes:"Use stomp.py",imp:["stomp.py"],conf:.9},SpringContextProcessor:{cat:"PySpark UDF",tpl:`# Spring Context Processor — migrate Spring bean logic to Python
df_{v} = df_{input}`,desc:"Spring context migration",notes:"Manual migration",imp:[],conf:.9},YandexTranslate:{cat:"PySpark UDF",tpl:`import requests
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
@udf(StringType())
def translate(text):
    r = requests.post("https://translate.api.cloud.yandex.net/translate/v2/translate", json={"texts": [text], "targetLanguageCode": "{lang}"})
    return r.json()["translations"][0]["text"]
df_{v} = df_{input}.withColumn("_translated", translate(col("value")))`,desc:"Yandex translation",notes:"Configure API key",imp:["requests"],conf:.9}},rs={source:{tpl:`# Source processor: {type}
# Auto Loader ingestion from landing zone
df_{v} = (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", "/Volumes/{catalog}/{schema}/_schema/{v}")
  .option("cloudFiles.inferColumnTypes", "true")
  .load("/Volumes/{catalog}/{schema}/landing/{v}"))
print(f"[SOURCE] {v}: streaming from landing zone")`,desc:"Source - Auto Loader ingestion",conf:.7},sink:{tpl:`# Sink processor: {type}
# Delta Lake write with merge semantics
(df_{in}.write
  .format("delta")
  .mode("append")
  .option("mergeSchema", "true")
  .saveAsTable("{catalog}.{schema}.{v}"))
print(f"[SINK] {v}: written to Delta table")`,desc:"Sink - Delta Lake write",conf:.7},transform:{tpl:`# Transform processor: {type}
# DataFrame transformation pipeline
from pyspark.sql.functions import col, lit, current_timestamp
df_{v} = (df_{in}
  .withColumn("_processed_at", current_timestamp())
  .withColumn("_source_processor", lit("{type}")))
print(f"[TRANSFORM] {v}: applied transformation")`,desc:"Transform - DataFrame transformation",conf:.7},route:{tpl:`# Route processor: {type}
# Conditional DataFrame routing via filter
from pyspark.sql.functions import col
_condition = "1=1"  # TODO: translate NiFi routing rules
df_{v}_matched = df_{in}.filter(_condition)
df_{v}_unmatched = df_{in}.filter(f"NOT ({_condition})")
df_{v} = df_{v}_matched
print(f"[ROUTE] {v}: matched={{df_{v}_matched.count()}}, unmatched={{df_{v}_unmatched.count()}}")`,desc:"Route - DataFrame filter routing",conf:.7},process:{tpl:`# Process processor: {type}
# Custom processing step
from pyspark.sql.functions import col, expr, current_timestamp
df_{in}.createOrReplaceTempView("tmp_{v}")
df_{v} = spark.sql("SELECT *, current_timestamp() AS _processed_at FROM tmp_{v}")
print(f"[PROCESS] {v}: processed {{df_{v}.count()}} rows")`,desc:"Process - Custom processing via Spark SQL",conf:.7},utility:{tpl:`# Utility processor: {type}
# Logging and inspection
print(f"[UTILITY] {v}: {{df_{in}.count()}} rows")
df_{in}.printSchema()
display(df_{in}.limit(5))
df_{v} = df_{in}`,desc:"Utility - logging and inspection",conf:.8}};function as(e,t,n,s,a,i){const c=i||3;if(!/^(InvokeHTTP|PutElasticsearch|PutMongo|PutS3|PutDynamoDB|ExecuteSQL|PutSQL|PutDatabaseRecord|ConsumeKafka|PublishKafka|Fetch|Get(HTTP|SQS|DynamoDB))/.test(t))return n;const r=t.toLowerCase().replace(/[^a-z0-9]/g,"_"),l=e.replace(/[^a-zA-Z0-9]/g,"_"),p=n.split(`
`).map(d=>"    "+d).join(`
`);return`# ${e} — with retry logic
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import logging
_logger = logging.getLogger("nifi_migration")

@retry(
    stop=stop_after_attempt(${c}),
    wait=wait_exponential(multiplier=1, min=2, max=30),
    retry=retry_if_exception_type((ConnectionError, TimeoutError, IOError)),
    before_sleep=lambda rs: _logger.warning(f"Retry {rs.attempt_number}/${c} for ${e}")
)
def _exec_${r}():
${p}
    return df_${l}

try:
    _exec_${r}()
except Exception as e:
    _logger.error(f"[FAILED] ${e}: {e}")
    raise`}function is(e,t,n,s,a){if(!e.includes("subprocess.run")&&!e.includes("_sp.run"))return e;const i=(a||{})["Input Source"]||"";if(i.toLowerCase().includes("flowfile")||i.toLowerCase().includes("content")){const c=e.match(/(?:subprocess\.run|_sp\.run)\(\s*(?:"([^"]+)"|'([^']+)'|\[([^\]]+)\])/),o=c?c[1]||c[2]||c[3]:"echo";return`# ${t} — distributed via Pandas UDF (per-row execution)
from pyspark.sql.functions import pandas_udf, col
import pandas as pd
import subprocess

@pandas_udf("string")
def _exec_cmd_${n}(rows: pd.Series) -> pd.Series:
    """Execute command per row — runs on workers, not driver"""
    results = []
    for row_val in rows:
        try:
            _r = subprocess.run(${JSON.stringify(o)}.split(), input=str(row_val), capture_output=True, text=True, timeout=30)
            results.append(_r.stdout.strip() if _r.returncode == 0 else f"ERROR: {_r.stderr[:100]}")
        except Exception as e:
            results.append(f"ERROR: {str(e)[:100]}")
    return pd.Series(results)

df_${n} = df_${s}.withColumn("_cmd_result", _exec_cmd_${n}(col("value")))
print(f"[CMD] Distributed execution via Pandas UDF")`}return e}const cs=[{pattern:/\b(ssn|social.?security|ss_num)\b/i,field:"SSN"},{pattern:/\b(mrn|medical.?record|patient.?id|patient_number)\b/i,field:"MRN"},{pattern:/\b(dob|date.?of.?birth|birth.?date|birthdate)\b/i,field:"DOB"},{pattern:/\b(patient|patient.?name|first.?name|last.?name|full.?name)\b/i,field:"Patient Name"},{pattern:/\b(diagnosis|icd.?\d{1,2}|cpt.?code|procedure.?code|drg)\b/i,field:"Diagnosis/Code"},{pattern:/\b(phone|fax|email|address|zip.?code|street)\b/i,field:"Contact Info"},{pattern:/\b(insurance|policy.?number|member.?id|subscriber)\b/i,field:"Insurance"},{pattern:/\b(prescription|medication|rx_|drug.?name|ndc)\b/i,field:"Medication"},{pattern:/\b(lab.?result|test.?result|vital|blood.?pressure|heart.?rate)\b/i,field:"Clinical Data"},{pattern:/\b(account.?number|billing|charge|payment)\b/i,field:"Financial/Billing"}];function Mt(e){const t=[],n=JSON.stringify(e||{}).toLowerCase();cs.forEach(a=>{a.pattern.test(n)&&(Object.keys(e||{}).forEach(i=>{a.pattern.test(i)&&t.push({key:i,category:a.field})}),Object.values(e||{}).forEach(i=>{typeof i=="string"&&a.pattern.test(i)&&t.push({key:i.substring(0,50),category:a.field})}))});const s=new Set;return t.filter(a=>s.has(a.key)?!1:(s.add(a.key),!0))}function ls(e,t){const{NIFI_DATABRICKS_MAP:n,ROLE_FALLBACK_TEMPLATES:s,parseVariableRegistry:a,resolveVariables:i,translateNELtoPySpark:c,generateRetryWrapper:o,wrapSubprocessAsPandasUDF:r,detectPHIFields:l}=t;window._lastParsedNiFi=e;const p=e.controllerServices||[],d=a(e),u=Object.keys(d).length>0,m=e.processors||[],y=e.connections||[],_={};y.forEach(k=>{_[k.destinationName]||(_[k.destinationName]={inputs:[],outputs:[]}),_[k.destinationName].inputs.push(k.sourceName),_[k.sourceName]||(_[k.sourceName]={inputs:[],outputs:[]}),_[k.sourceName].outputs.push(k.destinationName)});const b=Bn(m,y),S={};function P(k){if(S[k])return S[k];const D=Gn(k,p);return D&&(S[k]=D),D}return m.map(k=>{const D=X(k.type),C=n[k.type],f=B(k.name),g=_[k.name]&&_[k.name].inputs||[],h=g.length?B(g[0]):"input";if(C){const E=k.properties||{};if(u)for(const[I,L]of Object.entries(E))typeof L=="string"&&(E[I]=i(L,d));let{code:R,conf:A}=On(C,f,h,g,E,c,(I,L)=>Hn(I,L,c),k.name);R=R.replace(/\{(\w+)\}/g,"<$1>");const T=[k,E,f,h,R,A],N=zn(...T)||Jn(...T)||Qn(k,E,f,h,R,A,c)||Wn(k,E,f,h,R,A,c)||Kn(...T)||Vn(...T)||Xn(...T)||Yn(...T)||Zn(...T)||es(...T)||ts(...T)||ns(...T)||ss(...T)||os(...T);if(N&&(R=N.code,A=N.conf),/^(ExecuteSQL|QueryDatabase|GenerateTableFetch|PutDatabaseRecord|PutSQL|SelectHiveQL)/.test(k.type)){const I=E["Database Connection Pooling Service"]||E["JDBC Connection Pool"]||"",L=P(I);if(L&&L.jdbcUrl){const G=L.jdbcUrl,J=L.driver||"",Y=L.user||"",W=E["SQL select query"]||E["SQL Statement"]||"",te=E["Table Name"]||"";if(/ExecuteSQL|QueryDatabase|GenerateTableFetch|SelectHiveQL/.test(k.type)){const ie=W?`"(${W.replace(/"/g,'\\"').substring(0,300)}) AS subq"`:`"${te}"`;R=`# SQL: ${k.name} [CS: ${L.name}]
# JDBC: ${G}
# Driver: ${J}
df_${f} = (spark.read
  .format("jdbc")
  .option("url", "${G}")
  .option("dbtable", ${ie})
  .option("driver", "${J}")
  .option("user", dbutils.secrets.get(scope="db", key="${Y||"user"}"))
  .option("password", dbutils.secrets.get(scope="db", key="pass"))
  .load()
)
print(f"[SQL] Read from ${te||"query"} via ${L.name}")`,A=.95}else{const ie=E["Schema Name"]?`${E["Schema Name"]}.${te}`:te;R=`# DB Write: ${k.name} [CS: ${L.name}]
# JDBC: ${G}
(df_${h}.write
  .format("jdbc")
  .option("url", "${G}")
  .option("dbtable", "${ie}")
  .option("driver", "${J}")
  .option("user", dbutils.secrets.get(scope="db", key="${Y||"user"}"))
  .option("password", dbutils.secrets.get(scope="db", key="pass"))
  .option("batchsize", 1000)
  .mode("append")
  .save()
)
print(f"[DB] Wrote to ${ie} via ${L.name}")`,A=.95}}}if(/Kafka/.test(k.type)){const I=E["Kafka Client Service"]||"",L=P(I);if(L&&L.props){const G=L.props["bootstrap.servers"]||L.props["Kafka Brokers"]||"";G&&!R.includes(G)&&(R=R.replace(/kafka[_.]?[a-z]*:9092/gi,G))}}if(/ConvertRecord|ValidateRecord|LookupRecord|PutRecord|QueryRecord|PartitionRecord|SplitRecord|ForkRecord|SampleRecord|UpdateRecord/.test(k.type)){const I=E["Record Reader"]||"",L=E["Record Writer"]||"",G=P(I),J=P(L),Y=G?G.format||"json":/CSV/i.test(I)?"csv":/Avro/i.test(I)?"avro":"json",W=J?J.format||"json":/CSV/i.test(L)?"csv":/Avro/i.test(L)?"avro":"json";(!R||R.includes("{"))&&(R=`# ${k.type}: ${k.name}
# Reader: ${I} (${Y}) | Writer: ${L} (${W})
df_${f} = df_${h}
# Input format: ${Y}, Output format: ${W}
# Write: df_${f}.write.format("${W}").save("/path/output")
print(f"[RECORD] ${k.type} — ${Y} -> ${W}")`,A=.93)}const U=b.has(k.name);R=Un(R,k.name,f,h,U),U&&!R.includes("readStream")&&!R.includes("writeStream")&&!R.includes("foreachBatch")&&(R=`# ⚠ STREAMING CONTEXT: upstream is a streaming source
# Ensure all operations are streaming-compatible
`+R);const z=E["Penalty Duration"]||"30 sec",Q=E["Yield Duration"]||"1 sec",w=E["Max Retries"]||E["Retry Count"]||"3";R=o(k.name,k.type,R,z,Q,parseInt(w)||3),R=r(R,k.name,f,h,E);const F=l(E);return F.length>0&&(R=`# ⚠ PHI/HIPAA WARNING: Protected health information detected
# Fields: `+F.map(I=>I.key+" ("+I.category+")").join(", ")+`
# All PHI fields are hashed (SHA-256) in DLQ writes
`+R),{name:k.name,type:k.type,group:k.group,role:D,mapped:!0,confidence:A,category:C.cat,code:R,desc:C.desc,notes:C.notes,imports:C.imp||[],state:k.state}}const v=s[D]||s.process,$=`${v.tpl}
# Original: ${k.name} (${k.type}) in ${k.group||"root"}`;return{name:k.name,type:k.type,group:k.group,role:D,mapped:!1,confidence:v.conf,category:"Manual Migration",code:$,desc:v.desc,notes:"Role-based template. Manual implementation required.",imports:[],state:k.state,gapReason:`No direct mapping - ${D}-based template provided`,fallbackUsed:!0}})}const ps={NIFI_DATABRICKS_MAP:_e,ROLE_FALLBACK_TEMPLATES:rs,parseVariableRegistry:In,resolveVariables:Fn,translateNELtoPySpark:Ln,generateRetryWrapper:as,wrapSubprocessAsPandasUDF:is,detectPHIFields:Mt};function Je(e){return ls(e,ps)}function ds(e,t){const n=t.connections||[],s={},a={};(t.processors||[]).forEach(o=>{a[o.id]=o});const i={},c={};return n.forEach(o=>{i[o.sourceName]||(i[o.sourceName]=[]),i[o.sourceName].push({dest:o.destinationName,rel:o.relationship||"success"}),c[o.destinationName]||(c[o.destinationName]=[]),c[o.destinationName].push({src:o.sourceName,rel:o.relationship||"success"})}),e.forEach(o=>{const r=B(o.name),l=(c[o.name]||[]).map(d=>({varName:"df_"+B(d.src),procName:d.src,relationship:d.rel})),p=(i[o.name]||[]).map(d=>({varName:"df_"+B(d.dest),procName:d.dest,relationship:d.rel}));s[o.name]={outputVar:"df_"+r,inputVars:l,outputTargets:p,role:o.role,type:o.type}}),s}function us(e,t){const n={pyspark:new Set(["from pyspark.sql.functions import *","from pyspark.sql.types import *"]),python:new Set(["from datetime import datetime, timedelta","import json","import logging"]),databricks:new Set,thirdParty:new Set};e.forEach(o=>{(o.imports||[]).forEach(r=>{r.includes("pyspark")?n.pyspark.add(r):r.includes("dbutils")||r.includes("databricks")?n.databricks.add(r):n.python.add(r)}),o.code&&(o.code.includes("requests.")&&n.thirdParty.add("import requests"),o.code.includes("subprocess")&&n.python.add("import subprocess"),o.code.includes("re.")&&n.python.add("import re"),o.code.includes("os.")&&n.python.add("import os"),o.code.includes("hashlib")&&n.python.add("import hashlib"),o.code.includes("base64")&&n.python.add("import base64"),o.code.includes("xml.etree")&&n.python.add("import xml.etree.ElementTree as ET"),(o.code.includes("readStream")||o.code.includes("writeStream"))&&n.pyspark.add("from pyspark.sql.streaming import StreamingQuery"),o.code.includes("Window")&&n.pyspark.add("from pyspark.sql.window import Window"))});let s=`# ═══════════════════════════════════════════
# IMPORTS — Auto-collected from all processors
# ═══════════════════════════════════════════

`;s+=`# PySpark
`+[...n.pyspark].filter(o=>!o.startsWith("#")).sort().join(`
`);const a=[...n.python].filter(o=>!o.startsWith("#")).sort();a.length&&(s+=`

# Python Standard Library
`+a.join(`
`));const i=[...n.databricks].filter(o=>!o.startsWith("#")).sort();i.length&&(s+=`

# Databricks
`+i.join(`
`));const c=[...n.thirdParty].filter(o=>!o.startsWith("#")).sort();return c.length&&(s+=`

# Third-party
`+c.join(`
`)),{code:s,all:n}}function ms(e,t){const n=t.connections||[],s={};e.forEach((p,d)=>{s[p.name]=d});const a={},i={};e.forEach(p=>{a[p.name]=[],i[p.name]=0}),n.forEach(p=>{s[p.sourceName]!==void 0&&s[p.destinationName]!==void 0&&(a[p.sourceName].push(p.destinationName),i[p.destinationName]=(i[p.destinationName]||0)+1)});const c=[];e.forEach(p=>{(i[p.name]||0)===0&&c.push(p.name)});const o=[],r=new Set,l={source:0,route:1,transform:2,process:3,sink:4,utility:5};for(;c.length;){c.sort((d,u)=>{const m=e[s[d]],y=e[s[u]];return(l[m.role]||3)-(l[y.role]||3)});const p=c.shift();r.has(p)||(r.add(p),o.push(e[s[p]]),(a[p]||[]).forEach(d=>{i[d]--,i[d]<=0&&!r.has(d)&&c.push(d)}))}return e.forEach(p=>{r.has(p.name)||o.push(p)}),o}function fs(e,t){const n=(t.processors||[]).find(o=>o.name===e.name);if(!n)return{used:{},unused:{},all:{}};const s=n.properties||{},a=new Set,i=["Input Directory","File Filter","Output Directory","Directory","Database Connection Pooling Service","SQL select query","Table Name","Record Reader","Record Writer","Kafka Brokers","Topic Name","Group ID","Routing Strategy","JDBC Connection URL","Conflict Resolution Strategy","Log Level","Log Message","Command","Command Arguments"];Object.entries(s).forEach(([o])=>{i.some(r=>o.includes(r))&&a.add(o)});const c={};return Object.entries(s).forEach(([o,r])=>{a.has(o)||(c[o]=r)}),{used:Object.fromEntries([...a].map(o=>[o,s[o]])),unused:c,all:s}}function gs({flowName:e,totalProcessors:t,mappedCount:n,coveragePct:s,qualifiedSchema:a}){return{type:"md",label:"Header",source:`# NiFi Migration: ${e}
Generated by SEG Demo | ${new Date().toISOString().split("T")[0]}

**Processors:** ${t} | **Mapped:** ${n} | **Coverage:** ${s}%

**Target:** ${a}

**Enhancements:** DataFrame Lineage | Smart Imports | Topo-Sort | Adaptive Code | Error Framework | Auto Recovery | Full Properties | Relationship Routing | Exec Report | E2E Validation`,role:"config"}}function ys({smartImportsCode:e,catalogName:t,schemaName:n,secretScope:s}){let a=e+`

# Databricks notebook configuration
spark.conf.set("spark.sql.adaptive.enabled", "true")`;t&&(a+=`
spark.sql("USE CATALOG ${t}")`),a+=`
spark.sql("USE SCHEMA ${n}")`,s&&(a+=`

SECRET_SCOPE = "${s}"`);const i=t?`/Volumes/${t}/${n}/checkpoints`:ee.workspacePath+"/checkpoints";return a+=`

# Config-driven paths (no hardcoded /tmp/ or /mnt/)`,a+=`
CHECKPOINT_BASE = "${i}"`,a+=`
VOLUMES_BASE = "${t?`/Volumes/${t}/${n}`:"/Volumes/main/default"}"`,a+=`
print(f"Notebook initialized — Spark version: {spark.version}")`,{type:"code",label:"Imports & Config",source:a,role:"config"}}function hs({catalogName:e,schemaName:t,qualifiedSchema:n,tables:s}){if(!s||s.length===0)return null;let a="";return e&&(a+=`CREATE CATALOG IF NOT EXISTS ${e};
USE CATALOG ${e};
`),a+=`CREATE SCHEMA IF NOT EXISTS ${t};
USE SCHEMA ${t};
`,s.forEach(i=>{a+=`
CREATE TABLE IF NOT EXISTS ${n}.${i.name} (
`,a+=i.columns.map(c=>`  ${c.name} ${(c.data_type||c.type||"STRING").toUpperCase()}`).join(`,
`),a+=`
) USING DELTA;`}),{type:"sql",label:"Unity Catalog Setup",source:a,role:"config"}}function _s(e,t,n){const s=B(e.name);return e.role==="source"&&e.mapped&&e.code?"# Adaptive format detection for "+e.name+`
def _detect_format_`+s+`(path):
    import os
    ext = os.path.splitext(path)[1].lower() if path else ""
    fmt_map = {".csv":"csv",".tsv":"csv",".json":"json",".jsonl":"json",
               ".parquet":"parquet",".avro":"avro",".orc":"orc",".xml":"xml"}
    if ext in fmt_map: return fmt_map[ext]
    try:
        head = dbutils.fs.head(path, 4)
        if head.startswith("PAR1"): return "parquet"
        if head.startswith("{"): return "json"
    except: pass
    return "csv"

`+e.code:e.code}function bs(e,t,n,s){if(!e.mapped||!e.code||e.code.startsWith("# TODO"))return e.code;const a=B(e.name),i=s[e.name]||{},c=i.outputVar||"df_"+a,o=e.code.split(`
`).map(l=>"        "+l).join(`
`),r=e.name.replace(/"/g,'\\"').replace(/'/g,"''");return"# ["+e.role.toUpperCase()+"] "+e.name+`
# `+e.desc+(e.notes?"  |  "+e.notes:"")+`
_cell_start_`+a+` = datetime.now()
_cell_status_`+a+` = "SUCCESS"
_cell_error_`+a+` = ""
_cell_rows_`+a+` = 0
try:
`+o+`
        try: _cell_rows_`+a+" = "+c+`.count()
        except: pass
        print(f"[OK] `+r+" ({_cell_rows_"+a+`} rows)")
except Exception as _e:
        _cell_status_`+a+` = "FAILED"
        _cell_error_`+a+` = str(_e)
        print(f"[ERROR] `+r+`: {_e}")
        spark.sql(f"""INSERT INTO `+t+`.__execution_log VALUES (
            '`+r+"', '"+e.type+"', '"+e.role+`',
            current_timestamp(), '{_cell_status_`+a+`}',
            '{_cell_error_`+a+"}', {_cell_rows_"+a+`},
            '{str(datetime.now() - _cell_start_`+a+`)}',
            `+Math.round(e.confidence*100)+`,
            '`+(i.inputVars||[]).map(l=>l.procName).join(",")+`'
        )""")`}function ks(e,t,n){if(!e.mapped||!e.code||e.code.startsWith("# TODO"))return"";const s=B(e.name),a=n[e.name]||{},i=a.inputVars&&a.inputVars.length?a.inputVars[0].varName:"df_input",c=a.outputVar||"df_"+s,o=e.name.replace(/"/g,'\\"');return e.role==="source"?`
        # RECOVERY: Try relaxed schema
        try:
            `+c+` = spark.read.option("mode","PERMISSIVE").option("inferSchema","true").option("header","true").csv("/Volumes/fallback")
            _cell_status_`+s+` = "RECOVERED"
            print(f"[RECOVERED] `+o+`")
        except Exception as _e2:
            `+c+` = spark.createDataFrame([], "col1 STRING")
            print(f"[FALLBACK] `+o+' — empty df: {_e2}")':e.role==="transform"||e.role==="process"?`
        # RECOVERY: Pass through input
        try:
            `+c+" = "+i+`
            _cell_status_`+s+` = "PASSTHROUGH"
            print(f"[PASSTHROUGH] `+o+`")
        except: print(f"[SKIP] `+o+'")':e.role==="sink"?`
        # RECOVERY: Write to DLQ
        try:
            `+i+'.limit(1000).write.mode("append").saveAsTable("'+t+`.__dead_letter_queue")
            _cell_status_`+s+` = "DLQ"
            print(f"[DLQ] `+o+`")
        except: print(f"[LOST] `+o+'")':""}function Ss(e,t,n){const a=(t.connections||[]).filter(r=>r.sourceName===e.name);if(a.length<=1)return"";const i=B(e.name),c={};if(a.forEach(r=>{const l=r.relationship||"success";c[l]||(c[l]=[]),c[l].push(r.destinationName)}),Object.keys(c).length<=1)return"";let o=`
# ── Relationship Routing for `+e.name+` ──
`;return Object.entries(c).forEach(([r,l])=>{const p=l.map(d=>"df_"+B(d));r==="success"||r==="matched"||r==="valid"?(o+='# Route "'+r+'" -> '+l.join(", ")+`
`,p.forEach(d=>{o+=d+" = df_"+i+`  # success path
`})):r==="failure"||r==="unmatched"||r==="invalid"?o+='# Route "'+r+'" -> '+l.join(", ")+` (error path)
`:(o+='# Route "'+r+'" -> '+l.join(", ")+`
`,p.forEach(d=>{o+=d+" = df_"+i+"_"+B(r)+`  # conditional
`}))}),o}function vs(e,{lineage:t,qualifiedSchema:n,nifi:s,fullProps:a,cellIndex:i}){const c=`[${e.role.toUpperCase()}] ${e.name} → ${e.category}`,o=t[e.name]||{},r=(o.inputVars||[]).map(y=>`${y.varName} (${y.relationship})`).join(", ")||"none";let l=_s(e);const p=a||{unused:{}},d=Object.keys(p.unused).length>0?`
# Unused NiFi properties:
`+Object.entries(p.unused).slice(0,8).map(([y,_])=>"#   "+y+": "+String(_).substring(0,80)).join(`
`):"",u=Ss(e,s);let m;return e.mapped&&e.code&&!e.code.startsWith("# TODO")?(m=bs(e,n,i,t),m+=ks(e,n,t)):m=`# ${c}
# ${e.desc}${e.notes?"  |  "+e.notes:""}
# Input: ${r}
${l}`,m=`# Input lineage: ${r}
# Output: ${o.outputVar||"df_"+B(e.name)}${d}
${m}${u}`,{type:"code",label:c,source:m,role:e.role,processor:e.name,procType:e.type,confidence:e.confidence,mapped:e.mapped}}function Cs(e,t){return`# ═══════════════════════════════════════════════════════════
# EXECUTION REPORT GENERATOR
# ═══════════════════════════════════════════════════════════
import json
from datetime import datetime

_exec_report = {
    "pipeline_name": "`+t+`",
    "generated_at": datetime.now().isoformat(),
    "total_processors": `+e.length+`,
    "mapped_processors": `+e.filter(n=>n.mapped).length+`,
    "coverage_pct": `+Math.round(e.filter(n=>n.mapped).length/Math.max(e.length,1)*100)+`,
    "processors": []
}

try:
    _exec_rows = spark.sql("""
        SELECT processor_name, processor_type, role, status, error_message,
               rows_processed, duration, confidence, upstream_procs
        FROM `+t+`.__execution_log ORDER BY timestamp
    """).collect()
    for _r in _exec_rows:
        _exec_report["processors"].append({
            "name": _r.processor_name, "type": _r.processor_type,
            "role": _r.role, "status": _r.status,
            "error": _r.error_message or "", "rows": _r.rows_processed or 0,
            "duration": _r.duration or "", "confidence": _r.confidence or 0
        })
except Exception as _e:
    print(f"[WARN] Could not read execution log: {_e}")

_successes = len([p for p in _exec_report["processors"] if p.get("status") == "SUCCESS"])
_failures = len([p for p in _exec_report["processors"] if p.get("status") == "FAILED"])
_recovered = len([p for p in _exec_report["processors"] if p.get("status") in ("RECOVERED","PASSTHROUGH","DLQ")])
_exec_report["summary"] = {"successes":_successes,"failures":_failures,"recovered":_recovered,
    "success_rate":round(_successes/max(len(_exec_report["processors"]),1)*100,1)}

try:
    _report_df = spark.createDataFrame([{"report_json":json.dumps(_exec_report),
        "generated_at":datetime.now().isoformat(),
        "success_rate":_exec_report["summary"]["success_rate"],
        "total_procs":_successes+_failures+_recovered}])
    _report_df.write.mode("append").saveAsTable("`+t+`.__execution_reports")
except: pass

print("=" * 60)
print("EXECUTION REPORT")
print("=" * 60)
print(f"Successes: {_successes} | Failures: {_failures} | Recovered: {_recovered}")
print(f"Success Rate: {_exec_report[\\"summary\\"][\\"success_rate\\"]}%")
print("=" * 60)`}function ws(e,t,n){const s=e.filter(o=>o.role==="source"),a=e.filter(o=>o.role==="sink");let i=s.slice(0,20).map(o=>{const r="df_"+B(o.name);return'try: _source_vars["'+o.name+'"] = '+r+`.count()
except: _source_vars["`+o.name+'"] = -1'}).join(`
`),c=a.slice(0,20).map(o=>{const r="df_"+B(o.name);return'try: _sink_vars["'+o.name+'"] = '+r+`.count()
except: _sink_vars["`+o.name+'"] = -1'}).join(`
`);return`# ═══════════════════════════════════════════════════════════
# END-TO-END VALIDATION
# ═══════════════════════════════════════════════════════════
import json
_source_vars = {}
_sink_vars = {}
`+i+`
`+c+`

_validation_report = {
    "pipeline": "`+t+`",
    "timestamp": datetime.now().isoformat(),
    "source_row_counts": _source_vars,
    "sink_row_counts": _sink_vars,
    "total_source_rows": sum(v for v in _source_vars.values() if v > 0),
    "total_sink_rows": sum(v for v in _sink_vars.values() if v > 0)
}
_src_total = _validation_report["total_source_rows"]
_snk_total = _validation_report["total_sink_rows"]
if _src_total > 0 and _snk_total > 0:
    _retention = round(_snk_total / _src_total * 100, 1)
    _validation_report["data_retention_pct"] = _retention
    print(f"Data retention: {_retention}%")

try:
    _val_df = spark.createDataFrame([{"report_json":json.dumps(_validation_report),
        "validated_at":datetime.now().isoformat(),
        "source_rows":_src_total,"sink_rows":_snk_total}])
    _val_df.write.mode("append").saveAsTable("`+t+`.__validation_reports")
except: pass

print("=" * 60)
print("END-TO-END VALIDATION COMPLETE")
print("=" * 60)
print(json.dumps(_validation_report, indent=2))`}function Ps(e,t,n){if(!e||e.length<2)return null;const s=e.map(r=>t.find(l=>l.name===r)).filter(Boolean);if(s.length===0)return null;const a=s.some(r=>/InvokeHTTP|PostHTTP|GetHTTP/i.test(r.type)),i=B(e[0]),c=$s(s,n);if(a){const r=c||"    result = df  # passthrough (no mapped code)";return`# Cycle detected: ${e.join(" → ")} → (loop back)
# Pattern: API retry loop — converted to tenacity retry
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError

@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=60))
def _retry_loop_${i}(df):
    """Retry loop for: ${e.join(" → ")}"""
    result = df
${r}
    return result

try:
    df_${i} = _retry_loop_${i}(df_${i})
except RetryError:
    print(f"[LOOP] Retry cycle exhausted for ${e[0]}")`}const o=c||"    _loop_result = _loop_df_"+i+"  # passthrough (no mapped code)";return`# Cycle detected: ${e.join(" → ")} → (loop back)
# Pattern: Conditional loop — converted to while iteration
_max_iterations = 100
_iteration = 0
_loop_df_${i} = df_${i}
while _iteration < _max_iterations:
    _iteration += 1
${o}
    if _loop_result.count() == 0:
        break  # No more records to process
    _loop_df_${i} = _loop_result
df_${i} = _loop_df_${i}
print(f"[LOOP] Completed {_iteration} iterations for ${e[0]}")`}function $s(e,t){const n=[];for(const s of e){if(!s.mapped||!s.code||s.code.startsWith("# TODO"))continue;const i=(t[s.name]||{}).outputVar||"df_"+B(s.name);n.push(`    # --- ${s.name} (${s.type}) ---`);const c=s.code.split(`
`).map(o=>"    "+o).join(`
`);n.push(c),n.push(`    result = ${i}`)}return n.length>0?n.join(`
`):null}function Es(e){const t=[];return e&&e.forEach((n,s)=>{const a=typeof n=="string"?n:n.code||n.textContent||"",i=[],c=a.match(/\{[a-z_]+\}/g)||[];c.length>3&&i.push(`${c.length} unresolved placeholders: ${c.slice(0,3).join(", ")}...`);const o=(a.match(/\(/g)||[]).length,r=(a.match(/\)/g)||[]).length;Math.abs(o-r)>2&&i.push(`Unbalanced parentheses: ${o} open, ${r} close`);const l=a.match(/\$\{[^}]+\}/g)||[];l.length>0&&i.push(`${l.length} unresolved NiFi EL expressions: ${l.slice(0,2).join(", ")}...`);const p=new Set;a.includes("requests.")&&p.add("requests"),a.includes("boto3.")&&p.add("boto3"),a.includes("paramiko.")&&p.add("paramiko"),a.includes("pika.")&&p.add("pika"),a.includes("stomp.")&&p.add("stomp"),a.includes("pymongo")&&p.add("pymongo"),a.includes("elasticsearch")&&p.add("elasticsearch");const d=new Set;(a.match(/^import\s+(\w+)/gm)||[]).forEach(m=>d.add(m.replace("import ",""))),(a.match(/^from\s+(\w+)/gm)||[]).forEach(m=>d.add(m.replace("from ","")));const u=[...p].filter(m=>!d.has(m));u.length>0&&i.push(`Missing imports: ${u.join(", ")}`),t.push({cellIndex:s,valid:i.length===0,issues:i})}),t}function qt(e,t,n,s){s=s||{};const a=s.catalog||"",i=s.schema||"nifi_migration",c=a?`${a}.${i}`:i,o=[],r=ds(e,t),l=us(e),p=ms(e,t),d={};p.forEach(f=>{d[f.name]=fs(f,t)});const u=t.processGroups&&t.processGroups[0]?t.processGroups[0].name:"NiFi Flow",m=p.filter(f=>f.mapped).length,y=Math.round(m/p.length*100);o.push(gs({flowName:u,totalProcessors:p.length,mappedCount:m,coveragePct:y,qualifiedSchema:c})),o.push(ys({smartImportsCode:l.code,catalogName:a,schemaName:i,secretScope:s.secretScope})),o.push({type:"code",label:"Execution Framework Setup",source:`# Execution Tracking Framework
from datetime import datetime

spark.sql(f"""
CREATE TABLE IF NOT EXISTS ${c}.__execution_log (
  processor_name STRING, processor_type STRING, role STRING,
  timestamp TIMESTAMP DEFAULT current_timestamp(), status STRING,
  error_message STRING, rows_processed LONG, duration STRING,
  confidence INT, upstream_procs STRING
) USING DELTA TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')
""")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS ${c}.__dead_letter_queue (
  source_processor STRING, error STRING, record_data STRING,
  timestamp STRING, _ingested_at TIMESTAMP DEFAULT current_timestamp()
) USING DELTA TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')
""")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS ${c}.__execution_reports (
  report_json STRING, generated_at STRING, success_rate DOUBLE, total_procs INT
) USING DELTA
""")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS ${c}.__validation_reports (
  report_json STRING, validated_at STRING, source_rows LONG, sink_rows LONG
) USING DELTA
""")

print(f"[FRAMEWORK] Execution tracking ready: ${c}")`,role:"utility",processor:"Framework Setup",procType:"Internal",confidence:1,mapped:!0});const _=n&&n.tables||[],b=hs({catalogName:a,schemaName:i,qualifiedSchema:c,tables:_});b&&o.push(b);const S=Object.entries(r).slice(0,30).map(([f,g])=>{const h=(g.inputVars||[]).map(v=>v.procName).join(", ")||"(source)";return`# ${g.outputVar} <- ${h}`}).join(`
`);o.push({type:"code",label:"DataFrame Lineage Map",source:`# DataFrame Lineage Map
${S}
${Object.keys(r).length>30?"# ... and "+(Object.keys(r).length-30)+" more":""}
print("[LINEAGE] DataFrame lineage map loaded — ${Object.keys(r).length} variables tracked")`,role:"config"});const P={};p.forEach(f=>{P[f.group]||(P[f.group]=[]),P[f.group].push(f)});let k=o.length;Object.entries(P).forEach(([f,g])=>{const h=g.filter(v=>v.mapped).length;o.push({type:"md",label:f,source:`## Process Group: ${f}
**${g.length} processors** | ${h} mapped | ${g.length-h} manual

*Cells ordered by connection topology*`,role:"config"}),g.forEach(v=>{k++,o.push(vs(v,{lineage:r,qualifiedSchema:c,nifi:t,fullProps:d[v.name],cellIndex:k}))})}),o.push({type:"code",label:"Execution Report",source:Cs(p,c),role:"utility",processor:"ExecutionReport",procType:"Internal",confidence:1,mapped:!0}),o.push({type:"code",label:"End-to-End Validation",source:ws(p,c),role:"utility",processor:"E2EValidation",procType:"Internal",confidence:1,mapped:!0}),o.push({type:"sql",label:"Migration Error Table",source:`CREATE TABLE IF NOT EXISTS ${c}.__migration_errors (
  processor_name STRING, processor_type STRING,
  error_time TIMESTAMP, error_message STRING
) USING DELTA;`,role:"config"}),o.push({type:"code",label:"Pipeline Complete",source:`# Final status
import json
try:
    _exec_log = spark.sql("SELECT status, count(*) as cnt FROM ${c}.__execution_log GROUP BY status").collect()
    _counts = {r.status: r.cnt for r in _exec_log}
    _ok = _counts.get("SUCCESS", 0)
    _fail = _counts.get("FAILED", 0)
    _recov = sum(v for k,v in _counts.items() if k in ("RECOVERED","PASSTHROUGH","DLQ"))
    print("=" * 60)
    print(f"PIPELINE COMPLETE: {_ok} success, {_fail} failed, {_recov} recovered")
    print(f"Success rate: {round(_ok/max(_ok+_fail+_recov,1)*100,1)}%")
    print("=" * 60)
    if _fail > 0:
        display(spark.sql("SELECT * FROM ${c}.__execution_log WHERE status='FAILED'"))
    dbutils.notebook.exit(json.dumps({"status":"COMPLETE","success":_ok,"failed":_fail,"recovered":_recov}))
except Exception as _e:
    print(f"[WARN] Status check failed: {_e}")
    dbutils.notebook.exit("COMPLETE")`,role:"utility"});try{let f;try{f=window.analyzeFlowGraph||null}catch{f=null}if(f){const g=f(t.processors||[],t.connections||[]);g.circularRefs&&g.circularRefs.length>0&&g.circularRefs.forEach(h=>{const v=Ps(h.cycle,p,r);v&&o.push({type:"code",label:"Loop: "+h.cycle[0],source:v,role:"transform",processor:h.cycle[0],procType:"CycleLoop",confidence:.75,mapped:!0})})}}catch(f){console.warn("Cycle-to-loop generation:",f)}s.catalog&&o.forEach(f=>{f.source=Zt(f.source,s)});const C=Es(o.map(f=>f.source||"")).filter(f=>!f.valid);return C.length>0&&o.push({type:"code",label:"Code Validation Report",source:`# Code Validation Report
# `+C.length+` cells with potential issues:
`+C.map(f=>"# Cell "+f.cellIndex+": "+f.issues.join("; ")).join(`
`),role:"utility",processor:"CodeValidator",procType:"Internal",confidence:1,mapped:!0}),typeof window<"u"&&(window._lastNotebookCells=o,window._lastLineage=r),{cells:o,flowName:u,lineage:r,metadata:{processorCount:p.length,mappedCount:m,generatedAt:new Date().toISOString(),config:{catalog:a,schema:i},improvements:["lineage","smartImports","topoSort","adaptiveCode","errorFramework","autoRecovery","fullProperties","relationshipRouting","executionReport","e2eValidation","nelParser","phiDetection","sharedClusters","cycleDetection","streamingGuard"]}}}function jt(e,t,n){n=n||{};const s=n.workspacePath||"/Workspace/Migrations/NiFi",a=n.sparkVersion||"14.3.x-scala2.12",i=n.nodeType||"Standard_DS3_v2",c=n.numWorkers||2,o=t.connections||[],r={};(t.processors||[]).forEach(y=>{r[y.name]=y.group||"(root)"});const l=[...new Set(Object.values(r))],p={};l.forEach(y=>{p[y]=new Set}),o.forEach(y=>{const _=r[y.sourceName],b=r[y.destinationName];_&&b&&_!==b&&p[b].add(_)});const d="nifi_migration_cluster",u=[{job_cluster_key:d,new_cluster:{spark_version:a,node_type_id:i,num_workers:c,spark_conf:{"spark.databricks.delta.optimizeWrite.enabled":"true","spark.databricks.delta.autoCompact.enabled":"true","spark.sql.adaptive.enabled":"true","spark.sql.shuffle.partitions":"auto"},custom_tags:{source:"nifi_migration"}}}],m=l.map(y=>({task_key:B(y),description:`Process group: ${y}`,notebook_task:{notebook_path:`${s}/${B(y)}_notebook`,source:"WORKSPACE"},depends_on:[...p[y]].map(_=>({task_key:B(_)})),job_cluster_key:d}));return{name:`NiFi_Migration_${B(l[0]||"flow")}`,job_clusters:u,tasks:m,format:"MULTI_TASK",tags:{source:"nifi_migration",generated_by:"seg_demo"}}}const Ds=["source","route","transform","process","sink","utility"],ke={source:"#3B82F6",route:"#EAB308",transform:"#A855F7",process:"#6366F1",sink:"#21C354",utility:"#808495"};function Bt(e,t){const n=e.length,s=e.filter(p=>p.mapped).length,a={};Ds.forEach(p=>{a[p]={total:0,mapped:0,unmapped:0,procs:[]}}),e.forEach(p=>{const d=a[p.role]||a.process;d.total++,p.mapped?d.mapped++:d.unmapped++,d.procs.push(p)});const i={};e.forEach(p=>{i[p.group]||(i[p.group]={total:0,mapped:0,unmapped:0,procs:[]}),i[p.group].total++,p.mapped?i[p.group].mapped++:i[p.group].unmapped++,i[p.group].procs.push(p)});const c=e.filter(p=>!p.mapped||p.confidence<.3).map(p=>({name:p.name,type:p.type,group:p.group,role:p.role,reason:p.gapReason||`Low confidence mapping (${Math.round(p.confidence*100)}%)`,recommendation:p.type.match(/^(Listen|Handle)/)?"Consider Databricks Model Serving or external API gateway":p.type.match(/^Execute(Script|Stream)/)?"Manual translation required — review original script logic":p.type.match(/^(Put|Send)(Email|TCP|Syslog)/)?"Use webhook notification service or Databricks workflow alerts":"Review processor documentation and implement custom PySpark logic"})),o=[];c.length>n*.2&&o.push("High gap rate — consider custom UDFs for unsupported processor types"),e.some(p=>p.type.match(/Listen|Handle/))&&o.push("HTTP endpoints detected — evaluate Databricks Model Serving for REST API replacement"),e.some(p=>p.type.match(/Consume.*Kafka|Subscribe/))&&o.push("Streaming sources present — use Structured Streaming with Auto Loader trigger intervals"),(t.controllerServices||[]).length&&o.push(`${t.controllerServices.length} controller service(s) detected — map credentials to Databricks secret scopes`),s>n*.8&&o.push("High coverage — prioritize testing the mapped processors before addressing gaps"),o.push("Run the generated notebook in a Databricks workspace to validate each cell");const r=n?Math.round(s/n*100):0,l=r>=85?"Low":r>=60?"Medium":"High";return{summary:{totalProcessors:n,mappedProcessors:s,unmappedProcessors:n-s,coveragePercent:r,totalProcessGroups:Object.keys(i).length,totalConnections:(t.connections||[]).length,controllerServices:(t.controllerServices||[]).length},byRole:a,byGroup:i,gaps:c,recommendations:o,effort:l}}const Ts={kafka:{pip:["confluent-kafka"],dbx:"Pre-installed on DBR",desc:"Kafka"},oracle:{pip:["oracledb"],dbx:"JDBC driver via cluster library",desc:"Oracle Database"},mysql:{pip:["mysql-connector-python"],dbx:"JDBC driver via cluster library",desc:"MySQL"},postgresql:{pip:["psycopg2-binary"],dbx:"JDBC driver via cluster library",desc:"PostgreSQL"},sqlserver:{pip:["pymssql"],dbx:"JDBC driver via cluster library",desc:"SQL Server"},mongodb:{pip:["pymongo"],dbx:"MongoDB Spark Connector",desc:"MongoDB"},elasticsearch:{pip:["elasticsearch"],dbx:"Elasticsearch Spark library",desc:"Elasticsearch"},cassandra:{pip:["cassandra-driver"],dbx:"Spark Cassandra Connector",desc:"Cassandra"},redis:{pip:["redis"],dbx:"Custom library install",desc:"Redis"},hbase:{pip:[],dbx:"Delta Lake migration",desc:"HBase"},kudu:{pip:[],dbx:"Delta Lake (direct replacement)",desc:"Kudu"},s3:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS S3"},azure_blob:{pip:["azure-storage-blob"],dbx:"Pre-installed on DBR",desc:"Azure Blob"},azure_adls:{pip:["azure-storage-file-datalake"],dbx:"Pre-installed on DBR",desc:"Azure Data Lake"},gcs:{pip:["google-cloud-storage"],dbx:"GCS Spark connector",desc:"GCS"},hdfs:{pip:[],dbx:"dbutils.fs (pre-installed)",desc:"HDFS"},sftp:{pip:["paramiko"],dbx:"Volumes-based staging",desc:"SFTP/FTP"},http:{pip:["requests"],dbx:"Pre-installed on DBR",desc:"HTTP/REST"},email:{pip:["sendgrid"],dbx:"Webhook notification",desc:"Email"},mqtt:{pip:["paho-mqtt"],dbx:"Custom library",desc:"MQTT"},jms:{pip:["stomp.py"],dbx:"Custom library",desc:"JMS/AMQP"},snowflake:{pip:["snowflake-connector-python"],dbx:"Snowflake Spark Connector",desc:"Snowflake"},neo4j:{pip:["neo4j"],dbx:"Neo4j Spark Connector",desc:"Neo4j"},splunk:{pip:["splunklib"],dbx:"Splunk Spark Add-on",desc:"Splunk"},influxdb:{pip:["influxdb-client"],dbx:"Custom library",desc:"InfluxDB"},solr:{pip:["pysolr"],dbx:"Solr Spark library",desc:"Solr"},hive:{pip:[],dbx:"Pre-installed (Spark SQL)",desc:"Hive"},iceberg:{pip:[],dbx:"Pre-installed on DBR 13+",desc:"Iceberg"},teradata:{pip:["teradatasql"],dbx:"JDBC driver",desc:"Teradata"},slack:{pip:["slack-sdk"],dbx:"Webhook integration",desc:"Slack"},kerberos:{pip:[],dbx:"Unity Catalog identity federation",desc:"Kerberos"},azure_eventhub:{pip:["azure-eventhub"],dbx:"Pre-installed on DBR",desc:"Azure Event Hubs"},azure_servicebus:{pip:["azure-servicebus"],dbx:"Custom library install",desc:"Azure Service Bus"},azure_cosmos:{pip:[],dbx:"Pre-installed (Cosmos Spark connector)",desc:"Azure Cosmos DB"},azure_queue:{pip:["azure-storage-queue"],dbx:"Custom library install",desc:"Azure Queue Storage"},gcp_pubsub:{pip:["google-cloud-pubsub"],dbx:"Custom library install",desc:"GCP Pub/Sub"},gcp_bigquery:{pip:["google-cloud-bigquery"],dbx:"BigQuery Spark connector",desc:"GCP BigQuery"},clickhouse:{pip:["clickhouse-driver"],dbx:"ClickHouse JDBC driver",desc:"ClickHouse"},druid:{pip:[],dbx:"Druid JDBC driver",desc:"Apache Druid"},hudi:{pip:[],dbx:"Pre-installed on DBR 13+",desc:"Apache Hudi"},kinesis:{pip:["boto3"],dbx:"Kinesis Spark connector",desc:"AWS Kinesis"},cloudwatch:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS CloudWatch"},sqs:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS SQS"},sns:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS SNS"},dynamodb:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS DynamoDB"},lambda_aws:{pip:["boto3"],dbx:"Pre-installed on DBR",desc:"AWS Lambda"},pagerduty:{pip:["pdpyras"],dbx:"Custom library install",desc:"PagerDuty"},opsgenie:{pip:["opsgenie-sdk"],dbx:"Custom library install",desc:"OpsGenie"},telegram:{pip:[],dbx:"HTTP API (no SDK needed)",desc:"Telegram"},geoip:{pip:["geoip2"],dbx:"Custom library install",desc:"GeoIP"},exchange:{pip:["exchangelib"],dbx:"Custom library install",desc:"Microsoft Exchange"},whois:{pip:["python-whois"],dbx:"Custom library install",desc:"WHOIS"},snmp:{pip:["pysnmp"],dbx:"Custom library install",desc:"SNMP"},datadog:{pip:["datadog-api-client"],dbx:"Datadog integration",desc:"Datadog"},prometheus:{pip:["prometheus-client"],dbx:"Custom library install",desc:"Prometheus"},grafana:{pip:[],dbx:"Grafana REST API",desc:"Grafana"},phoenix:{pip:[],dbx:"Phoenix JDBC driver",desc:"Apache Phoenix"},cockroachdb:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"CockroachDB"},timescaledb:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"TimescaleDB"},greenplum:{pip:["psycopg2-binary"],dbx:"PostgreSQL JDBC driver",desc:"Greenplum"},vertica:{pip:["vertica-python"],dbx:"Vertica JDBC driver",desc:"Vertica"},saphana:{pip:["hdbcli"],dbx:"SAP HANA JDBC driver",desc:"SAP HANA"},presto:{pip:[],dbx:"Presto JDBC driver",desc:"Presto"},trino:{pip:["trino"],dbx:"Trino JDBC driver",desc:"Trino"}},Rs={ClickHouse:"clickhouse",Druid:"druid",Hudi:"hudi",Kinesis:"kinesis",CloudWatch:"cloudwatch",SQS:"sqs",SNS:"sns",DynamoDB:"dynamodb",Lambda:"lambda_aws",PagerDuty:"pagerduty",OpsGenie:"opsgenie",GeoIP:"geoip",Exchange:"exchange",SNMP:"snmp",Datadog:"datadog",Prometheus:"prometheus",Grafana:"grafana",Phoenix:"phoenix",CockroachDB:"cockroachdb",TimescaleDB:"timescaledb",Greenplum:"greenplum",Vertica:"vertica",SAPHANA:"saphana",Presto:"presto",Trino:"trino",CosmosDB:"azure_cosmos",ServiceBus:"azure_servicebus",EventHub:"azure_eventhub",PubSub:"gcp_pubsub",BigQuery:"gcp_bigquery",Kafka:"kafka",Oracle:"oracle",MySQL:"mysql",Postgres:"postgresql",Mongo:"mongodb",Elastic:"elasticsearch",Cassandra:"cassandra",Redis:"redis",HBase:"hbase",Kudu:"kudu",S3:"s3",AzureBlob:"azure_blob",ADLS:"azure_adls",GCS:"gcs",HDFS:"hdfs",SFTP:"sftp",FTP:"sftp",HTTP:"http",InvokeHTTP:"http",Email:"email",MQTT:"mqtt",JMS:"jms",AMQP:"jms",Snowflake:"snowflake",Neo4j:"neo4j",Splunk:"splunk",InfluxDB:"influxdb",Solr:"solr",Hive:"hive",Iceberg:"iceberg",Teradata:"teradata",Syslog:"syslog",Slack:"slack"};function Ke(e){const t=new Set;for(const[n,s]of Object.entries(Rs))e.includes(n)&&t.add(s);return[...t].map(n=>Ts[n]).filter(Boolean)}function me(e){return'<div class="metrics">'+e.map(t=>{const n=Array.isArray(t)?t[0]:t.label,s=Array.isArray(t)?t[1]:t.value,a=Array.isArray(t)?t[2]:t.delta,i=Array.isArray(t)?"":t.color||"";return`<div class="metric"><div class="label">${n}</div><div class="value"${i?' style="color:'+i+'"':""}>${s}</div>${a?`<div class="delta">${a}</div>`:""}</div>`}).join("")+"</div>"}function be(e,t){return`<div class="table-scroll"><table><thead><tr>${e.map(n=>`<th>${n}</th>`).join("")}</tr></thead><tbody>${t.map(n=>`<tr>${n.map(s=>`<td>${s??""}</td>`).join("")}</tr>`).join("")}</tbody></table></div>`}function Oe(e,t,n=!1){return`<div class="expander ${n?"open":""}"><div class="expander-header" data-expander-toggle><span>${e}</span><span class="expander-arrow">▶</span></div><div class="expander-body">${t}</div></div>`}typeof document<"u"&&document.addEventListener("click",e=>{const t=e.target.closest("[data-expander-toggle]");t&&t.parentElement.classList.toggle("open")});async function ye(){const e=vt(),t=document.getElementById("pasteInput"),n=e||(t?t.value:"");if(!n.trim()){const k=document.getElementById("parseResults");k&&(k.innerHTML='<div class="alert alert-error">Please upload or paste a NiFi flow XML.</div>');return}const s=document.getElementById("parseBtn");s&&(s.disabled=!0),H("load","processing");const a=document.getElementById("parseProgress");a&&(a.style.display="flex");const i=document.getElementById("parsePBar"),c=document.getElementById("parsePPct"),o=document.getElementById("parsePStatus"),r=(k,D)=>{i&&(i.style.width=k+"%"),c&&(c.textContent=k+"%"),o&&(o.textContent=D)};r(10,"Cleaning & parsing input..."),await new Promise(k=>setTimeout(k,50));let l;const p=Ct();try{l=Nt(n,p||"NiFi Flow")}catch(k){const D=document.getElementById("parseResults");D&&(D.innerHTML='<div class="alert alert-error">Failed to parse: '+x(k.message)+"</div>"),s&&(s.disabled=!1),H("load","ready"),a&&(a.style.display="none");return}if(r(20,"Validating flow..."),await new Promise(k=>setTimeout(k,50)),!l||!l._nifi||l._nifi.processors.length===0){const k=document.getElementById("parseResults");k&&(k.innerHTML='<div class="alert alert-error">No NiFi processors found. Please provide a valid NiFi flow XML.</div>'),s&&(s.disabled=!1),H("load","ready"),a&&(a.style.display="none");return}if(r(50,"Processing flow..."),l._deferredProcessorWork){const k=l._deferredProcessorWork,D=50;for(let C=0;C<k.processors.length;C+=D)k.batchFn(k.processors.slice(C,C+D)),r(50+Math.round(C/k.processors.length*30),"Analyzing processor "+(C+1)+"/"+k.processors.length+"..."),await new Promise(f=>setTimeout(f,0));k.finalize()}r(85,"Building resource manifest..."),await new Promise(k=>setTimeout(k,50)),K({parsed:l,manifest:Ot(l._nifi)}),r(95,"Rendering results...");const d=l._nifi,u=X;let m='<div class="alert alert-success" style="margin-top:16px">Successfully parsed NiFi flow: <strong>'+x(l.source_name)+"</strong></div>";m+=me([{label:"Processors",value:d.processors.length},{label:"Connections",value:d.connections.length},{label:"Process Groups",value:d.processGroups.length},{label:"Controller Services",value:d.controllerServices.length},{label:"External Systems",value:d.clouderaTools.length}]);const y={};d.processors.forEach(k=>{y[k.type]=(y[k.type]||0)+1});const _=Object.entries(y).sort((k,D)=>D[1]-k[1]).map(([k,D])=>{const C=u(k),f=ke[C]||"#808495";return['<span style="color:'+f+';font-weight:600">'+x(k)+"</span>",D,'<span class="badge" style="background:'+f+"22;color:"+f+'">'+C+"</span>"]});m+=Oe("Processor Types ("+Object.keys(y).length+" unique)",be(["Type","Count","Role"],_)),l.parse_warnings.length&&(m+=l.parse_warnings.map(k=>'<div class="alert alert-warn" style="margin:4px 0;font-size:0.85rem">'+x(k)+"</div>").join(""));const b=document.getElementById("parseResults");b&&(b.innerHTML=m),r(100,"Done!"),s&&(s.disabled=!1),H("load","done"),ce("analyze");const S=document.getElementById("analyzeNotReady"),P=document.getElementById("analyzeReady");S&&S.classList.add("hidden"),P&&P.classList.remove("hidden"),setTimeout(()=>{a&&(a.style.display="none")},500),await new Promise(k=>setTimeout(k,200)),V("analyze"),Me(),await new Promise(k=>setTimeout(k,150)),V("assess"),qe(),await new Promise(k=>setTimeout(k,150)),V("convert"),je(),await new Promise(k=>setTimeout(k,150)),V("report"),Be(),await new Promise(k=>setTimeout(k,150)),typeof window.generateFinalReport=="function"&&(V("reportFinal"),await window.generateFinalReport()),await new Promise(k=>setTimeout(k,150)),typeof window.runValidation=="function"&&(V("validate"),await window.runValidation()),await new Promise(k=>setTimeout(k,150)),typeof window.runValueAnalysis=="function"&&(V("value"),window.runValueAnalysis())}function Me(){const e=O();if(!e.parsed||!e.parsed._nifi)return;H("analyze","processing");const t=e.parsed._nifi;e.manifest;const n=Qe(t),s=De(t);let a="";const i=Object.keys(t.deepPropertyInventory.nifiEL||{}).length,c=t.sqlTables?t.sqlTables.length:0,o=Object.keys(t.deepPropertyInventory.credentialRefs||{}).length,r=Object.keys(s).length;a+=me([{label:"Processors",value:t.processors.length},{label:"Connections",value:t.connections.length},{label:"Process Groups",value:t.processGroups.length},{label:"Controller Services",value:t.controllerServices.length},{label:"External Systems",value:r},{label:"EL Expressions",value:i},{label:"SQL Tables",value:c},{label:"Credentials",value:o}]);const l=We(e.parsed),p=fn(l,e.parsed);a+='<hr class="divider"><h3>Flow Visualization</h3>',a+='<div id="analysisTierContainer" style="position:relative"></div><div id="analysisTierDetail"></div><div id="analysisTierLegend"></div>';const d=Object.keys(s);if(d.length&&(a+='<hr class="divider"><h3>External Systems &amp; Dependencies ('+d.length+")</h3>",a+='<p style="color:var(--text2);font-size:0.82rem;margin-bottom:8px">Detected from processor types, JDBC URLs, and properties.</p>',d.forEach(S=>{const P=s[S];let D='<div class="sys-detail-row"><span class="sys-label">Processors:</span><span class="sys-value">'+P.processors.map(C=>{const f=_e[C.type]?_e[C.type].conf:0;return'<span class="conf-dot '+(f>=.7?"high":f>=.3?"med":"low")+'"></span>'+x(C.name)+' <span style="color:var(--text2)">('+C.direction+")</span>"}).join("<br>")+"</span></div>";D+='<div class="sys-detail-row"><span class="sys-label">Databricks:</span><span class="sys-value">'+x(P.dbxApproach)+"</span></div>",P.jdbcUrls.length&&(D+='<div class="sys-detail-row"><span class="sys-label">JDBC:</span><span class="sys-value"><code style="font-size:0.75rem">'+P.jdbcUrls.map(C=>x(C)).join("<br>")+"</code></span></div>"),P.credentials.length&&(D+='<div class="sys-detail-row"><span class="sys-label">Credentials:</span><span class="sys-value" style="color:var(--amber)">'+P.credentials.map(C=>x(C)).join(", ")+"</span></div>"),P.packages.length&&(D+='<div class="sys-detail-row"><span class="sys-label">Packages:</span><span class="sys-value"><code>'+P.packages.join(", ")+"</code></span></div>"),a+=Oe(x(P.name)+' <span style="color:var(--text2);font-size:0.8rem">('+P.processors.length+" processor"+(P.processors.length!==1?"s":"")+")</span>",D,!1)})),a+='<hr class="divider"><h3>Processor Inventory ('+t.processors.length+")</h3>",a+='<p style="color:var(--text2);font-size:0.82rem;margin-bottom:8px">Click to expand for properties, scheduling, and dependencies.</p>',t.processors.forEach(S=>{const P=X(S.type),k=ke[P]||"#808495",D=_e[S.type],C=D?D.conf:0,f=C>=.7?"high":C>=.3?"med":"low",g=n.upstream[S.name]||[],h=n.downstream[S.name]||[],v=n.fullUpstream[S.name]||[],$=n.fullDownstream[S.name]||[];let E='<div style="display:grid;grid-template-columns:1fr 1fr;gap:8px;margin-bottom:8px">';E+='<div><strong style="font-size:0.78rem;color:var(--text2)">Type:</strong> '+x(S.type)+"</div>",E+='<div><strong style="font-size:0.78rem;color:var(--text2)">Role:</strong> <span style="color:'+k+'">'+P+"</span></div>",E+='<div><strong style="font-size:0.78rem;color:var(--text2)">Group:</strong> '+x(S.group||"(root)")+"</div>",E+='<div><strong style="font-size:0.78rem;color:var(--text2)">State:</strong> '+(S.state||"UNKNOWN")+"</div>",E+='<div><strong style="font-size:0.78rem;color:var(--text2)">Scheduling:</strong> '+(S.schedulingStrategy||"-")+" / "+(S.schedulingPeriod||"-")+"</div>",E+='<div><strong style="font-size:0.78rem;color:var(--text2)">Confidence:</strong> <span class="conf-dot '+f+'"></span>'+Math.round(C*100)+"%</div></div>",(g.length||h.length)&&(E+='<div style="display:flex;gap:16px;margin-bottom:8px;font-size:0.8rem">',g.length&&(E+='<div><strong style="color:#3B82F6">Upstream ('+v.length+"):</strong> "+g.map(T=>x(T)).join(", ")+"</div>"),h.length&&(E+='<div><strong style="color:#21C354">Downstream ('+$.length+"):</strong> "+h.map(T=>x(T)).join(", ")+"</div>"),E+="</div>");const R=Object.entries(S.properties||{});R.length&&(E+='<table style="width:100%;font-size:0.78rem;border-collapse:collapse">',R.forEach(([T,N])=>{const U=Dt(T)?"********":N,Q=N&&N.includes("${")?String(U).replace(/\$\{([^}]+)\}/g,'<span class="el-highlight">${$1}</span>'):x(String(U||""));E+='<tr><td style="color:var(--text2);padding:2px 6px;border-bottom:1px solid var(--border);white-space:nowrap">'+x(T)+'</td><td style="padding:2px 6px;border-bottom:1px solid var(--border);word-break:break-all">'+Q+"</td></tr>"}),E+="</table>"),D&&(E+='<div style="margin-top:8px;padding:8px;background:var(--bg);border-radius:4px;font-size:0.75rem"><strong style="color:var(--green)">Databricks: </strong>'+x(D.desc)+'<br><span style="color:var(--text2)">'+x(D.notes)+"</span></div>");const A='<span class="conf-dot '+f+'"></span><span style="color:'+k+'">['+P.toUpperCase()+"]</span> "+x(S.name)+' <span style="color:var(--text2);font-size:0.8rem">'+x(S.type)+"</span>";a+=Oe(A,E,!1)}),a+='<hr class="divider"><h3>Connection Map ('+t.connections.length+")</h3>",t.connections.length){const S=t.connections.map(P=>[x(P.sourceName||P.sourceId),x(P.destinationName||P.destinationId),(P.relationships||[]).map(k=>'<span class="badge" style="background:var(--surface2);font-size:0.7rem">'+x(k)+"</span>").join(" "),P.backPressure?x(P.backPressure):"-"]);a+=be(["Source","Destination","Relationships","Back Pressure"],S)}t.controllerServices.length&&(a+='<hr class="divider"><h3>Controller Services ('+t.controllerServices.length+")</h3>",a+=be(["Name","Type","State","Properties"],t.controllerServices.map(S=>[x(S.name),x(S.type),S.state||"-",Object.keys(S.properties||{}).length+" props"])));const u=Object.entries(t.deepPropertyInventory.nifiEL||{});u.length&&(a+='<hr class="divider"><h3>NiFi Expression Language ('+u.length+")</h3>",a+=be(["Expression","Used By"],u.slice(0,50).map(([S,P])=>['<code class="el-highlight">'+x(S.substring(0,80))+"</code>",(Array.isArray(P)?P:[P]).map(k=>x(String(k))).join(", ")])),u.length>50&&(a+='<p style="color:var(--text2);font-size:0.82rem">... and '+(u.length-50)+" more</p>"));const m={TIMER_DRIVEN:0,CRON_DRIVEN:0,EVENT_DRIVEN:0,OTHER:0};t.processors.forEach(S=>{const P=(S.schedulingStrategy||"").toUpperCase();P in m?m[P]++:m.OTHER++}),a+='<hr class="divider"><h3>Scheduling Summary</h3>',a+=me([{label:"Timer Driven",value:m.TIMER_DRIVEN},{label:"Cron Driven",value:m.CRON_DRIVEN},{label:"Event Driven",value:m.EVENT_DRIVEN}]);const y=document.getElementById("analyzeResults");y&&(y.innerHTML=a),K({analysis:{blueprint:l,tierData:p,depGraph:n,systems:s}}),setTimeout(()=>{En(p,"analysisTierContainer","analysisTierDetail","analysisTierLegend")},50),H("analyze","done"),ce("assess");const _=document.getElementById("assessNotReady"),b=document.getElementById("assessReady");_&&_.classList.add("hidden"),b&&b.classList.remove("hidden")}function qe(){const e=O();if(!e.parsed||!e.parsed._nifi)return;H("assess","processing");const t=e.parsed._nifi,n=Je(t),s=Qe(t),a=De(t),i=n.length,c=n.filter(T=>T.mapped&&T.confidence>=.7),o=n.filter(T=>T.mapped&&T.confidence>0&&T.confidence<.7),r=n.filter(T=>!T.mapped||T.confidence===0),l=n.filter(T=>T.mapped),p=i?c.length/i:0,d=l.length?l.reduce((T,N)=>T+N.confidence,0)/l.length:0,u=i?l.length/i:0,m=Object.keys(a).length,y=Math.max(0,1-m/20),_=Math.round(p*50+d*20+u*20+y*10),b=c.length*.5+o.length*2+r.length*5,S=Math.ceil(b/5),P=_>=75?"green":_>=40?"amber":"red",k=_>=75?"&#x1F7E2;":_>=40?"&#x1F7E1;":"&#x1F534;",D=_>=75?"HIGH READINESS":_>=40?"MODERATE READINESS":"LOW READINESS";let C='<hr class="divider">';C+='<div class="score-big" style="color:var(--'+P+')">'+k+" "+D+" &mdash; "+_+"%</div>",C+=me([{label:"Auto-Convert",value:c.length,color:"var(--green)"},{label:"Manual",value:o.length,color:"var(--amber)"},{label:"Unsupported",value:r.length,color:"var(--red)"},{label:"Effort Est.",value:b.toFixed(0)+" days (~"+S+" wks)"}]),C+=me([{label:"Auto-Convert % (50w)",value:Math.round(p*100)+"%"},{label:"Avg Confidence (20w)",value:Math.round(d*100)+"%"},{label:"Coverage (20w)",value:Math.round(u*100)+"%"},{label:"Simplicity (10w)",value:Math.round(y*100)+"%"}]);const f=i?c.length/i*100:0,g=i?o.length/i*100:0,h=i?r.length/i*100:0;C+='<div class="effort-bar">',f>0&&(C+='<div class="effort-seg" style="width:'+f+'%;background:var(--green)">'+c.length+" auto</div>"),g>0&&(C+='<div class="effort-seg" style="width:'+g+'%;background:var(--amber)">'+o.length+" manual</div>"),h>0&&(C+='<div class="effort-seg" style="width:'+h+'%;background:var(--red)">'+r.length+" unsupported</div>"),C+="</div>",C+='<hr class="divider"><h3>Per-Processor Confidence</h3>';const v=n.map(T=>{const N=T.confidence>=.7?"high":T.confidence>=.3?"med":"low",U=s.fullUpstream[T.name]||[],z=s.fullDownstream[T.name]||[];return[x(T.name),'<span style="color:'+(ke[T.role]||"#808495")+'">'+T.role+"</span>",x(T.group),'<span class="conf-dot '+N+'"></span>'+Math.round(T.confidence*100)+"%",T.mapped?x((T.desc||"").substring(0,50)):'<em style="color:var(--red)">'+(T.gapReason||"Unmapped").substring(0,50)+"</em>",T.confidence>=.7?"0.5d":T.confidence>=.3?"2d":"5d",U.length+" up / "+z.length+" down"]});C+=be(["Processor","Role","Group","Confidence","Approach","Effort","Deps"],v);const $=new Set;n.forEach(T=>{Ke(T.type).forEach(N=>N.pip.forEach(U=>$.add(U)))}),$.size&&(C+='<hr class="divider"><h3>Required Packages</h3>',C+=`<pre style="background:var(--bg);padding:12px;border-radius:6px;font-size:0.8rem"># requirements.txt
`,[...$].sort().forEach(T=>{C+=T+`
`}),C+="</pre>");const E=document.getElementById("assessResults");E&&(E.innerHTML=C),K({assessment:{mappings:n,readinessScore:_,autoCount:c.length,manualCount:o.length,unsupportedCount:r.length,effortDays:b,systems:a,depGraph:s}}),H("assess","done"),ce("convert");const R=document.getElementById("convertNotReady"),A=document.getElementById("convertReady");R&&R.classList.add("hidden"),A&&A.classList.remove("hidden")}function je(){const e=O();if(!e.parsed||!e.parsed._nifi)return;H("convert","processing");const t=e.parsed._nifi,n=St(),s=e.assessment?e.assessment.mappings:Je(t),i=qt(s,t,e.analysis?e.analysis.blueprint:We(e.parsed),n).cells,c=new Set;s.forEach(m=>{Ke(m.type).forEach(y=>y.pip.forEach(_=>c.add(_)))}),c.size&&i.unshift({type:"code",role:"config",label:"Package Requirements",source:`# Install required packages
`+[...c].sort().map(m=>"%pip install "+m).join(`
`)+`
dbutils.library.restartPython()`});const o=jt(s,t,n);K({notebook:{mappings:s,cells:i,workflow:o,config:n}});let r='<hr class="divider">';r+="<h3>Processor Mapping</h3>";const l=s.map(m=>[x(m.name),`<span style="color:${ke[m.role]||"#808495"}">${x(m.role)}</span>`,x(m.group||"—"),m.mapped?x(m.desc):'<em style="color:var(--text2)">No equivalent</em>',m.mapped?`<span class="conf-badge ${m.confidence>=.8?"conf-high":m.confidence>=.5?"conf-med":"conf-low"}">${Math.round(m.confidence*100)}%</span>`:'<span class="conf-badge conf-none">—</span>']);r+=`<div class="table-scroll"><table class="mapping-table"><thead><tr><th>NiFi Processor</th><th>Role</th><th>Group</th><th>Databricks Equivalent</th><th>Confidence</th></tr></thead><tbody>${l.map(m=>`<tr>${m.map(y=>`<td>${y}</td>`).join("")}</tr>`).join("")}</tbody></table></div>`,r+='<hr class="divider"><h3>Generated Notebook</h3>',r+='<div class="notebook-preview">',i.forEach((m,y)=>{const _=m.label||(m.type==="md"?"markdown":"code"),b=m.role?"lb-"+m.role:"lb-config",S=m.type==="md"?' <span style="opacity:0.5">[md]</span>':m.type==="sql"?' <span style="opacity:0.5">[sql]</span>':"",P=m.source.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;");r+=`<div class="notebook-cell"><div class="cell-label ${b}">[${y+1}] ${_}${S}</div><pre>${P}</pre></div>`}),r+="</div>",r+='<hr class="divider"><div style="display:flex;gap:8px;flex-wrap:wrap">',r+='<button class="btn" onclick="downloadNotebook()">Download .py Notebook</button>',r+='<button class="btn" onclick="downloadWorkflow()">Download Workflow JSON</button>',r+="</div>";const p=document.getElementById("notebookResults");p&&(p.innerHTML=r),H("convert","done");const d=document.getElementById("reportNotReady"),u=document.getElementById("reportReady");d&&d.classList.add("hidden"),u&&u.classList.remove("hidden"),ce("report")}function Be(){const e=O();if(!e.notebook||!e.parsed||!e.parsed._nifi)return;H("report","processing");const t=e.parsed._nifi,n=Bt(e.notebook.mappings,t);K({migrationReport:n});const s=n.summary;let a='<hr class="divider">';const i=s.coveragePercent,c=i>=85?"HIGH COVERAGE":i>=60?"PARTIAL COVERAGE":"LOW COVERAGE";a+=`<div class="score-big">${c} — ${i}%</div>`,a+=me([["Total Processors",s.totalProcessors],["Mapped",s.mappedProcessors],["Unmapped",s.unmappedProcessors],["Process Groups",s.totalProcessGroups],["Connections",s.totalConnections],["Effort",`<span class="badge badge-${n.effort==="Low"?"green":n.effort==="Medium"?"amber":"red"}">${n.effort}</span>`]]),a+='<hr class="divider"><h3>Coverage by Role</h3>',["source","route","transform","process","sink","utility"].forEach(d=>{const u=n.byRole[d];if(!u)return;const m=u.total?Math.round(u.mapped/u.total*100):0,y=m>=85?"green":m>=60?"amber":"red",_=ke[d]||"#808495";a+='<div style="margin:8px 0">',a+='<div style="display:flex;justify-content:space-between;align-items:center;margin-bottom:2px">',a+=`<span style="font-weight:600;color:${_};text-transform:uppercase;font-size:0.85rem">${d}</span>`,a+=`<span style="font-size:0.85rem;color:var(--text2)">${u.mapped}/${u.total} (${m}%)</span>`,a+="</div>",a+=`<div class="progress-bar"><div class="progress-fill ${y}" style="width:${m}%"></div></div>`,a+="</div>"}),n.gaps&&n.gaps.length&&(a+='<hr class="divider"><h3>Gap Analysis — Unmapped Processors</h3>',n.gaps.forEach(d=>{a+='<div class="gap-card">',a+=`<div class="gap-title">${x(d.name)} <span class="gap-meta">${x(d.type)} &middot; ${x(d.group||"ungrouped")}</span></div>`,a+=`<div class="gap-rec">${x(d.recommendation||"Manual implementation required")}</div>`,a+="</div>"})),n.recommendations&&n.recommendations.length&&(a+='<hr class="divider"><h3>Recommendations</h3>',a+='<ul style="margin:0;padding-left:20px">',n.recommendations.forEach(d=>{a+=`<li style="margin:4px 0">${x(d)}</li>`}),a+="</ul>"),a+='<hr class="divider"><div style="display:flex;gap:8px;flex-wrap:wrap;align-items:center">',a+='<button class="btn" onclick="downloadReport()">Download Report (Markdown)</button>',a+="</div>";const r=document.getElementById("reportResults");r&&(r.innerHTML=a),H("report","done");const l=document.getElementById("reportFinalNotReady"),p=document.getElementById("reportFinalReady");l&&l.classList.add("hidden"),p&&p.classList.remove("hidden"),ce("reportFinal")}function xs(e,t){const n={deadEnds:[],orphans:[],circularRefs:[],disconnected:[],backpressure:[]};if(!e||!t)return n;const s={},a={};e.forEach(u=>{s[u.id]=u,a[u.name]=u});const i={},c={};e.forEach(u=>{i[u.id]=[],c[u.id]=[]}),t.forEach(u=>{const m=u.sourceId||u.source&&u.source.id,y=u.destId||u.destination&&u.destination.id;m&&y&&i[m]&&c[y]&&(i[m].push(y),c[y].push(m))});const o=/^(Put|Log|Publish|Send|Notify|PutEmail|PutSlack)/;e.forEach(u=>{i[u.id]&&i[u.id].length===0&&!o.test(u.type)&&n.deadEnds.push({name:u.name,type:u.type,id:u.id})});const r=/^(Get|List|Listen|Consume|Generate|TailFile|HandleHttp)/;e.forEach(u=>{c[u.id]&&c[u.id].length===0&&!r.test(u.type)&&n.orphans.push({name:u.name,type:u.type,id:u.id})});const l=new Set,p=new Set;function d(u,m){if(p.has(u))return n.circularRefs.push({cycle:m.concat(u).map(y=>s[y]?s[y].name:y)}),!0;if(l.has(u))return!1;l.add(u),p.add(u);for(const y of i[u]||[])d(y,m.concat(u));return p.delete(u),!1}return e.forEach(u=>{l.has(u.id)||d(u.id,[])}),e.forEach(u=>{i[u.id]&&i[u.id].length===0&&c[u.id]&&c[u.id].length===0&&n.disconnected.push({name:u.name,type:u.type,id:u.id})}),n}const As=[{name:"SQL Injection",regex:/(\bDROP\s+TABLE\b|\bUNION\s+SELECT\b|\bxp_cmdshell\b|\bEXEC\s+sp_|\bDELETE\s+FROM\b.*;\s*--|;\s*DROP\b|'\s*OR\s+'[^']*'\s*=\s*')/i,severity:"CRITICAL"},{name:"Command Injection",regex:/(rm\s+-rf\s+\/|curl\s+evil|wget\s+.*\|\s*bash|\/bin\/sh\s+-[ic]|exec\s*\(|system\s*\(|subprocess)/i,severity:"CRITICAL"},{name:"SSRF",regex:/(169\.254\.169\.254|metadata\.google\.internal|localhost:\d{4}\/admin)/i,severity:"HIGH"},{name:"Hardcoded Secret",regex:/(password\s*=\s*['"][^'"]{3,}|secret\s*=\s*['"][^'"]{3,}|api_key\s*=\s*['"][^'"]{3,})/i,severity:"HIGH"},{name:"Reverse Shell",regex:/(\/dev\/tcp\/|nc\s+-[el]|ncat\s|socket\.SOCK_STREAM.*connect|bash\s+-i\s*>&)/i,severity:"CRITICAL"},{name:"Path Traversal",regex:/(\.\.\/\.\.\/|\/etc\/passwd|\/etc\/shadow|\/root\/)/i,severity:"MEDIUM"},{name:"Dangerous File Access",regex:/(\.pem|\.key|\.crt|credentials\.json|\.env\b)/i,severity:"MEDIUM"},{name:"Perl Injection",regex:/(use\s+IO::Socket|eval\s*\(|open\s*\(\s*F\s*,\s*'\|)/i,severity:"CRITICAL"},{name:"Java Exploitation",regex:/(Runtime\.getRuntime\(\)|ProcessBuilder|jndi:ldap|Class\.forName)/i,severity:"CRITICAL"},{name:"Data Exfiltration",regex:/(LOAD_FILE|INTO\s+OUTFILE|COPY\s+.*\s+TO\s+'\/|UTL_HTTP\.REQUEST)/i,severity:"HIGH"}];function Is(e){const t=[];return e.forEach(n=>{const s=Object.values(n.properties||{}).join(" ");As.forEach(a=>{const i=a.regex.exec(s);i&&t.push({processor:n.name,type:n.type,finding:a.name,severity:a.severity,snippet:i[0].substring(0,100)})})}),t}const Fs=["minute","hour","day-of-month","month","day-of-week"],Ls=["second","minute","hour","day-of-month","month","day-of-week"],Se=[null,"January","February","March","April","May","June","July","August","September","October","November","December"],ve=["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"];function Ns(e,t){if(e==="*")return`every ${t}`;if(e==="?")return`any ${t}`;if(e.includes("/")){const[n,s]=e.split("/");return n==="*"?`every ${s} ${t}s`:`every ${s} ${t}s starting at ${n}`}if(e.includes("-")&&!e.includes(",")){const[n,s]=e.split("-");return t==="day-of-week"?`${ve[n]||n} through ${ve[s]||s}`:t==="month"?`${Se[n]||n} through ${Se[s]||s}`:`${t}s ${n} through ${s}`}if(e.includes(",")){const n=e.split(",");return t==="day-of-week"?n.map(s=>ve[s]||s).join(", "):t==="month"?n.map(s=>Se[s]||s).join(", "):`${t}s ${n.join(", ")}`}return t==="day-of-week"?ve[e]||`day ${e}`:t==="month"?Se[e]||`month ${e}`:`at ${t} ${e}`}function Os(e){if(typeof e!="string")return{valid:!1,description:"Invalid CRON expression",fields:{}};const t=e.trim().split(/\s+/);if(t.length<5||t.length>7)return{valid:!1,description:`Invalid CRON expression (${t.length} fields, expected 5-6)`,fields:{}};const s=t.length>=6?Ls:Fs,a={},i=[];for(let o=0;o<s.length&&o<t.length;o++)a[s[o]]=t[o],t[o]!=="*"&&t[o]!=="?"&&i.push(Ns(t[o],s[o]));return{valid:!0,description:i.length>0?i.join(", "):"every minute (all wildcards)",fields:a}}function Ms(e){const t=[],n=[],s=[];return(e||[]).forEach(a=>{const i=a.schedulingStrategy||"TIMER_DRIVEN",c=a.schedulingPeriod||"0 sec";if(i==="CRON_DRIVEN"){const o=Os(c);n.push({processor:a.name,type:a.type,group:a.group||"(root)",expression:c,...o})}else i==="EVENT_DRIVEN"?s.push({processor:a.name,type:a.type,group:a.group||"(root)"}):t.push({processor:a.name,type:a.type,group:a.group||"(root)",period:c,isZero:c==="0 sec"||c==="0 ms"||c==="0"})}),{timers:t,cronSchedules:n,eventDriven:s,summary:{totalProcessors:(e||[]).length,timerDriven:t.length,cronDriven:n.length,eventDrivenCount:s.length,zeroPeriodCount:t.filter(a=>a.isZero).length}}}function qs(e){const t={filePaths:{},urls:{},jdbcUrls:{},nifiEL:{},cronExprs:{},credentialRefs:{},hostPorts:{},dataFormats:new Set,encodings:new Set};return(e||[]).forEach(n=>{const s=n.properties||{},a=Object.values(s),i=Object.keys(s);for(let c=0;c<a.length;c++){const o=a[c];if(!o||typeof o!="string")continue;const r=o.length;if(r>3&&o.includes("/")){const l=o.match(ne.filePath);l&&l.forEach(p=>{p.length>3&&!/^\/\//.test(p)&&(t.filePaths[p]||(t.filePaths[p]=[]),t.filePaths[p].push({processor:n.name,group:n.group,property:i[c]}))})}if(r>8&&/https?:/.test(o)){const l=o.match(ne.urlPattern);l&&l.forEach(p=>{t.urls[p]||(t.urls[p]=[]),t.urls[p].push({processor:n.name,group:n.group,property:i[c]})})}if(r>10&&o.startsWith("jdbc:")){const l=o.match(ne.jdbcUrl);l&&l.forEach(p=>{t.jdbcUrls[p]||(t.jdbcUrls[p]=[]),t.jdbcUrls[p].push({processor:n.name,group:n.group,property:i[c]})})}if(r>3&&o.includes("${")){const l=o.match(ne.nifiEL);l&&l.forEach(p=>{const d=p.slice(2,-1);t.nifiEL[d]||(t.nifiEL[d]=[]),t.nifiEL[d].push({processor:n.name,group:n.group,property:i[c]})})}if(r>8&&ne.cronExpr.test(o)&&(t.cronExprs[o.trim()]||(t.cronExprs[o.trim()]=[]),t.cronExprs[o.trim()].push({processor:n.name,group:n.group,property:i[c]})),ne.credentialKey.test(i[c])){const l=o.length>3?o.substring(0,2)+"***"+o.substring(o.length-1):"***";t.credentialRefs[i[c]]||(t.credentialRefs[i[c]]=[]),t.credentialRefs[i[c]].push({processor:n.name,group:n.group,value:l})}if(r>5){ne.hostPort.lastIndex=0;let l;for(;(l=ne.hostPort.exec(o))!==null;){const p=l[2]?l[1]+":"+l[2]:l[1];!/\.(css|js|html|png|jpg|gif|svg|ico|woff|ttf|eot)$/i.test(p)&&!/^(www\.|http|example\.com|localhost)/i.test(l[1])&&(t.hostPorts[p]||(t.hostPorts[p]=[]),t.hostPorts[p].push({processor:n.name,group:n.group,property:i[c]}))}}if(ne.dataFormat.test(o)){const l=o.match(ne.dataFormat);l&&l.forEach(p=>t.dataFormats.add(p.toLowerCase()))}}n.schedulingStrategy==="CRON_DRIVEN"&&n.schedulingPeriod&&(t.cronExprs[n.schedulingPeriod]||(t.cronExprs[n.schedulingPeriod]=[]),t.cronExprs[n.schedulingPeriod].push({processor:n.name,group:n.group,property:"schedulingPeriod"}))}),t}function js(e){e.deepPropertyInventory||(e.deepPropertyInventory=qs(e.processors));const t=Qe(e),n=De(e),s=Tt(t.downstream),a=Ot(e),i=xs(e.processors,e.connections),c=Is(e.processors||[]),o=[];(e.processors||[]).forEach(p=>{const d=Mt(p.properties);d.length>0&&o.push({processor:p.name,type:p.type,group:p.group,fields:d})});const r=Ms(e.processors);let l=null;return e.tables&&e.tables.length>0&&(l=We(e)),{dependencyGraph:t,externalSystems:n,cycles:s,manifest:a,flowGraph:i,securityFindings:c,phiResults:o,scheduling:r,blueprint:l,deepPropertyInventory:e.deepPropertyInventory}}function Bs(e){const{mappings:t,nifi:n,blueprint:s,config:a}=e,i=qt(t,n,s,a),c=jt(t,n,a);return{notebook:i,workflow:c}}async function Us({processors:e,mappings:t,mappingByName:n,allCellTextLower:s,batchSize:a=100,onProgress:i}){const c=[];e.forEach(u=>{const m=X(u.type);let y="";const _=u.properties||{};if(m==="source"){const S=Object.values(_).filter(P=>P&&typeof P=="string").join(" ").match(/\b(s3|hdfs|kafka|jdbc|file|ftp|sftp|http)/i);y="INGEST data"+(S?" from "+S[0].toUpperCase():"")}else if(m==="sink")y="WRITE/OUTPUT data"+(_.Directory?" to "+_.Directory:"")+(_["Topic Name"]?" to Kafka:"+_["Topic Name"]:"");else if(m==="transform")y="TRANSFORM data"+(u.type.includes("JSON")?" (JSON)":u.type.includes("SQL")?" (SQL)":u.type.includes("Attribute")?" (attributes)":"");else if(m==="route"){const b=Object.keys(_).filter(S=>S!=="Routing Strategy").length;y="ROUTE/BRANCH"+(b>0?" ("+b+" conditions)":"")}else m==="process"?y="PROCESS data ("+u.type.replace(/^org\.apache\.nifi\.processors?\.\w+\./,"")+")":y="UTILITY ("+u.type.replace(/^org\.apache\.nifi\.processors?\.\w+\./,"")+")";c.push({name:u.name,type:u.type,role:m,intent:y,props:_})});let o=0,r=0,l=0;const p=[];for(let u=0;u<c.length;u+=a)c.slice(u,u+a).forEach(y=>{const _=n[y.name];if(!_){l++,p.push({proc:y.name,type:y.type,intent:y.intent,issue:"No mapping found — processor entirely missing from notebook"});return}if(!_.mapped){l++,p.push({proc:y.name,type:y.type,intent:y.intent,issue:"Unmapped — no Databricks equivalent generated"});return}const b=_.name.replace(/[^a-zA-Z0-9]/g,"_").toLowerCase();if(!(s.includes(b)||s.includes(y.name.toLowerCase()))){r++,p.push({proc:y.name,type:y.type,intent:y.intent,issue:"Mapped but no dedicated notebook cell references this processor"});return}_.confidence>=.7?o++:(r++,p.push({proc:y.name,type:y.type,intent:y.intent,issue:"Low confidence ("+Math.round(_.confidence*100)+"%) — intent may not be fully preserved"}))}),i&&i(8+Math.round(u/c.length*20),"Intent analysis: "+Math.min(u+a,c.length)+"/"+c.length+" processors..."),await new Promise(y=>setTimeout(y,0));const d=c.length?Math.round(o/c.length*100):0;return{nifiIntents:c,intentMatched:o,intentPartial:r,intentMissing:l,intentGaps:p,intentScore:d}}async function Gs({mappings:e,procByName:t,cellTextsLower:n,findCellsWithVar:s,batchSize:a=100,onProgress:i}){const c=[];let o=0,r=0;for(let p=0;p<e.length;p+=a)e.slice(p,p+a).forEach(u=>{const m=u.name.replace(/[^a-zA-Z0-9]/g,"_").toLowerCase(),y=s(m);if(y.length===0){const f=u.name.toLowerCase(),g=s(f);g.length>0&&y.push(...g)}const _=t[u.name],b=_?_.properties||{}:{},S=Object.entries(b).filter(([f,g])=>g&&!/password|secret|token/i.test(f));let P=0,k=[];if(y.length>0){const f=y.map(g=>n[g]).join(`
`);S.forEach(([g,h])=>{const v=g.toLowerCase().replace(/[\s-]/g,""),$=String(h).toLowerCase().substring(0,50);f.includes(v)||f.includes($)||f.includes(g.toLowerCase())?P++:k.push(g)})}const D=S.length>0?Math.round(P/S.length*100):100,C=u.mapped?y.length===0?"no-cell":D>=70?"good":D>=30?"partial":"weak":"missing";C==="good"?o++:r++,c.push({name:u.name,type:u.type,role:u.role,mapped:u.mapped,cellCount:y.length,cellIndices:y,propTotal:S.length,propsInCode:P,propCoverage:D,propsMissing:k,status:C,confidence:u.confidence,desc:u.desc})}),i&&i(30+Math.round(p/e.length*25),"Line validation: "+Math.min(p+a,e.length)+"/"+e.length+" mappings..."),await new Promise(u=>setTimeout(u,0));const l=e.length?Math.round(o/e.length*100):0;return{lineResults:c,lineMatched:o,lineGaps:r,lineScore:l}}async function Hs({nifi:e,systems:t,allCellTextLower:n,onProgress:s}){let a=0;const i=[],c=e.connections.length,o=e.processors.length;let r=0;e.connections.forEach(g=>{const h=(g.sourceName||"").replace(/[^a-zA-Z0-9]/g,"_").toLowerCase(),v=(g.destinationName||"").replace(/[^a-zA-Z0-9]/g,"_").toLowerCase();h&&v&&n.includes(h)&&n.includes(v)&&r++});const l=c?Math.round(r/c*100):100;i.push({label:"Flow Topology Preserved",score:l,detail:r+"/"+c+" connections reflected in variable chaining"}),s&&s(65,"Checking processor type identifiability..."),await new Promise(g=>setTimeout(g,0));let p=0;e.processors.forEach(g=>{const h=g.type.split(".").pop().toLowerCase();(n.includes(h)||n.includes("nifi: "+h)||n.includes(g.type.toLowerCase()))&&p++});const d=o?Math.round(p/o*100):100;i.push({label:"Processor Types Identifiable",score:d,detail:p+"/"+o+" NiFi types referenced in notebook comments/code"});let u=0;e.processors.forEach(g=>{g.schedulingPeriod&&g.schedulingPeriod!=="0 sec"?(n.includes(g.schedulingPeriod.toLowerCase())||n.includes("schedule")||n.includes("trigger"))&&u++:u++});const m=o?Math.round(u/o*100):100;i.push({label:"Scheduling Parameters Preserved",score:m,detail:u+"/"+o+" scheduling configs captured"}),s&&s(72,"Checking controller services and external systems..."),await new Promise(g=>setTimeout(g,0));const y=e.controllerServices.length;let _=0;e.controllerServices.forEach(g=>{const h=g.name.replace(/[^a-zA-Z0-9]/g,"_").toLowerCase();(n.includes(h)||n.includes(g.name.toLowerCase()))&&_++});const b=y?Math.round(_/y*100):100;i.push({label:"Controller Services Referenced",score:b,detail:_+"/"+y+" controller services found in notebook"});const S=Object.keys(t);let P=0;S.forEach(g=>{n.includes(g.toLowerCase())&&P++});const k=S.length?Math.round(P/S.length*100):100;i.push({label:"External Systems Connected",score:k,detail:P+"/"+S.length+" external systems referenced in code"});const D=[];e.processors.forEach(g=>{g.autoTerminatedRelationships&&g.autoTerminatedRelationships.length>0&&D.push({name:g.name,rels:g.autoTerminatedRelationships})});const C=n.includes("try:")||n.includes("except")||n.includes('.option("failonerror"'),f=C?70:D.length>0?30:50;return i.push({label:"Error Handling Coverage",score:f,detail:C?"Try/except or error options found in notebook":"No explicit error handling — "+D.length+" processors have auto-terminated failure relationships"}),a=Math.round(i.reduce((g,h)=>g+h.score,0)/i.length),{reScore:a,reChecks:i}}async function zs({mappings:e,procByName:t,batchSize:n=100,onProgress:s}){const a=[];let i=0,c=0,o=0;for(let l=0;l<e.length;l+=n)e.slice(l,l+n).forEach(d=>{const u=t[d.name];if(!u)return;const m=X(d.type),y=u.properties||{},_=[];m==="source"&&_.push("Data ingestion"),m==="sink"&&_.push("Data output/write"),m==="transform"&&_.push("Data transformation"),m==="route"&&_.push("Conditional routing"),m==="process"&&_.push("Data processing"),m==="utility"&&_.push("Utility"),(d.type.includes("SQL")||d.type.includes("Sql"))&&_.push("SQL execution"),(d.type.includes("JSON")||d.type.includes("Json"))&&_.push("JSON processing"),d.type.includes("Avro")&&_.push("Avro serialization"),(d.type.includes("CSV")||d.type.includes("Csv"))&&_.push("CSV processing"),d.type.includes("Kafka")&&_.push("Kafka messaging"),(d.type.includes("HDFS")||d.type.includes("Hdfs"))&&_.push("HDFS file operations"),(d.type.includes("S3")||d.type.includes("AWS"))&&_.push("S3/AWS operations"),(d.type.includes("Encrypt")||d.type.includes("Hash"))&&_.push("Encryption/hashing"),(d.type.includes("HTTP")||d.type.includes("Http"))&&_.push("HTTP communication"),(d.type.includes("Merge")||d.type.includes("Split"))&&_.push("Data merge/split"),d.type.includes("Attribute")&&_.push("Attribute manipulation"),(d.type.includes("Wait")||d.type.includes("Notify"))&&_.push("Flow coordination"),d.type.includes("Kudu")&&_.push("Kudu table operations");const b=Object.entries(y).filter(([D,C])=>C&&String(C).includes("${"));b.length&&_.push("NiFi Expression Language ("+b.length+" expressions)");let S=[],P="missing";d.mapped&&d.confidence>=.7?(P="mapped",i++,d.desc&&S.push(d.desc)):d.mapped?(P="partial",c++,d.desc&&S.push(d.desc+" (low confidence)")):o++;const k=Ke(d.type);a.push({name:d.name,type:d.type,role:m,nifiFunctions:_,dbxFunctions:S,status:P,confidence:d.confidence,packages:k,elCount:b.length})}),s&&s(78+Math.round(l/e.length*15),"Function mapping: "+Math.min(l+n,e.length)+"/"+e.length+"..."),await new Promise(d=>setTimeout(d,0));const r=e.length?Math.round((i+c*.5)/e.length*100):0;return{funcResults:a,funcMapped:i,funcPartial:c,funcMissing:o,funcScore:r}}function Ws({intentGaps:e,lineGapItems:t,nifiDatabricksMap:n}){const s=[...e];t.forEach(c=>{s.some(o=>o.proc===c.name)||s.push({proc:c.name,type:c.type,intent:"",issue:c.status==="missing"?"No mapping":c.status==="no-cell"?"No cell":"Low prop coverage"})});const a={};s.forEach(c=>{const o=c.type.split(".").pop();a[o]||(a[o]=[]),a[o].push(c)});const i={};return Object.entries(a).sort((c,o)=>o[1].length-c[1].length).forEach(([c,o])=>{const r=o.some(p=>p.issue.includes("Missing")||p.issue.includes("Unmapped")||p.issue.includes("No mapping"))?"HIGH":"MEDIUM",l=n?n[c]:null;i[c]={gaps:o,severity:r,mapEntry:l}}),{allGaps:s,gapsByType:i}}async function dt({nifi:e,mappings:t,cells:n,systems:s,nifiDatabricksMap:a,onProgress:i}){const c=i||(()=>{});c(2,"Building lookup indexes..."),await new Promise(D=>setTimeout(D,0));const o={};e.processors.forEach(D=>{o[D.name]=D});const r={};t.forEach(D=>{r[D.name]=D});const l=n.map(D=>(D.source||"").toLowerCase()),p=l.join(`
`);function d(D){const C=[];for(let f=0;f<l.length;f++)l[f].includes(D)&&C.push(f);return C}c(5,"Building connection graph..."),await new Promise(D=>setTimeout(D,0));const u={};e.connections.forEach(D=>{u[D.sourceName]||(u[D.sourceName]=[]),u[D.sourceName].push(D)}),c(8,"Running intent analysis ("+e.processors.length+" processors)...");const m=await Us({processors:e.processors,mappings:t,mappingByName:r,allCellTextLower:p,onProgress:c});c(30,"Running line validation ("+t.length+" mappings)...");const y=await Gs({mappings:t,procByName:o,cellTextsLower:l,findCellsWithVar:d,onProgress:c});c(58,"Running reverse engineering readiness checks...");const _=await Hs({nifi:e,systems:s,allCellTextLower:p,onProgress:c});c(78,"Running function mapping analysis...");const b=await zs({mappings:t,procByName:o,onProgress:c});c(95,"Computing overall score..."),await new Promise(D=>setTimeout(D,0));const S=Math.round((m.intentScore+y.lineScore+_.reScore+b.funcScore)/4),P=y.lineResults.filter(D=>D.status!=="good"),k=Ws({intentGaps:m.intentGaps,lineGapItems:P,nifiDatabricksMap:a});return c(100,"Validation complete!"),{overallScore:S,intentScore:m.intentScore,lineScore:y.lineScore,reScore:_.reScore,funcScore:b.funcScore,intentGaps:m.intentGaps,nifiIntents:m.nifiIntents,intentMatched:m.intentMatched,intentPartial:m.intentPartial,intentMissing:m.intentMissing,lineResults:y.lineResults,lineMatched:y.lineMatched,lineGaps:y.lineGaps,reChecks:_.reChecks,funcResults:b.funcResults,funcMapped:b.funcMapped,funcPartial:b.funcPartial,funcMissing:b.funcMissing,allGaps:k.allGaps,gapsByType:k.gapsByType,connMap:u,timestamp:new Date().toISOString()}}function Qs(e,t){const n=e.length,s=e.filter(p=>p.mapped&&p.confidence>=.8).length,a=e.filter(p=>p.mapped).length,i=t.connections||[],c=new Set(e.filter(p=>p.mapped).map(p=>p.name)),o=i.length,r=i.filter(p=>c.has(p.sourceName)&&c.has(p.destinationName)).length,l=e.map((p,d)=>{let u;return p.mapped?p.confidence>=.8?u="exact":u="functional":u="gap",{idx:d+1,name:p.name,type:p.type,group:p.group||"—",role:p.role,equiv:p.mapped?p.desc:"—",category:p.mapped?p.category:"—",matchType:u,confidence:p.confidence,code:p.code}});return{exact:{count:s,total:n,pct:n?Math.round(s/n*100):0},functional:{count:a,total:n,pct:n?Math.round(a/n*100):0},actions:{count:r,total:o,pct:o?Math.round(r/o*100):0},rows:l}}function Ut(e){const t=e.parsed?e.parsed._nifi:null;return{meta:{generated:new Date().toISOString(),tool:"NiFi Flow Analyzer",version:"1.0"},flow_summary:{source_name:e.parsed?e.parsed.source_name:"Unknown",processor_count:t?t.processors.length:0,connection_count:t?t.connections.length:0,process_group_count:t?t.processGroups.length:0,controller_service_count:t?t.controllerServices.length:0,external_system_count:t?t.clouderaTools.length:0},processors:t?t.processors.map(s=>({name:s.name,type:s.type,group:s.group,state:s.state,role:X(s.type),scheduling:{strategy:s.schedulingStrategy,period:s.schedulingPeriod},properties:Object.fromEntries(Object.entries(s.properties).map(([a,i])=>[a,/password|secret|token/i.test(a)?"***":i]))})):[],connections:t?t.connections.map(s=>({source:s.sourceName,destination:s.destinationName,relationships:s.relationships,backPressure:s.backPressure})):[],controller_services:t?t.controllerServices.map(s=>({name:s.name,type:s.type,properties:Object.fromEntries(Object.entries(s.properties).map(([a,i])=>[a,/password|secret|token/i.test(a)?"***":i]))})):[],assessment:e.assessment?{readiness_score:e.assessment.readinessScore,auto_convertible:e.assessment.autoCount,manual_conversion:e.assessment.manualCount,unsupported:e.assessment.unsupportedCount,mappings:e.assessment.mappings?e.assessment.mappings.map(s=>({name:s.name,nifi_type:s.nifiType||s.type,role:s.role,mapped:s.mapped,databricks:s.desc,confidence:s.confidence})):[]}:null,notebook:e.notebook?{cell_count:e.notebook.cells.length,config:e.notebook.config}:null,migration_report:e.migrationReport||null,manifest:e.manifest?{directories:Object.keys(e.manifest.directories).length,sql_tables:Object.keys(e.manifest.sqlTables).length,http_endpoints:e.manifest.httpEndpoints.length,kafka_topics:e.manifest.kafkaTopics.length,scripts:e.manifest.scripts.length,db_connections:e.manifest.dbConnections.length,external_systems:(e.manifest.clouderaTools||[]).length}:null,deep_property_inventory:t?{file_paths:Object.keys(t.deepPropertyInventory.filePaths||{}).length,urls:Object.keys(t.deepPropertyInventory.urls||{}).length,jdbc_urls:Object.keys(t.deepPropertyInventory.jdbcUrls||{}).length,nifi_el_expressions:Object.keys(t.deepPropertyInventory.nifiEL||{}).length,cron_expressions:Object.keys(t.deepPropertyInventory.cronExprs||{}).length,credential_references:Object.keys(t.deepPropertyInventory.credentialRefs||{}).length}:null}}function Js(e,t,n){const s=Ut(e),a=s;let i='<hr class="divider">';i+="<h3>Report Summary</h3>",i+=t([{label:"Processors",value:a.flow_summary.processor_count},{label:"Connections",value:a.flow_summary.connection_count},{label:"External Systems",value:a.flow_summary.external_system_count},{label:"Readiness",value:a.assessment?a.assessment.readiness_score+"%":"N/A"}]),a.assessment&&(i+="<h3>Assessment Overview</h3>",i+=t([{label:"Auto-Convert",value:a.assessment.auto_convertible,color:"var(--green)"},{label:"Manual",value:a.assessment.manual_conversion,color:"var(--amber)"},{label:"Unsupported",value:a.assessment.unsupported,color:"var(--red)"}]));const c=JSON.stringify(a,null,2).substring(0,5e3);return i+='<hr class="divider"><h3>Report Preview</h3>',i+='<pre style="max-height:400px;overflow:auto;font-size:0.75rem">'+n(c)+(JSON.stringify(a).length>5e3?`
... (truncated)`:"")+"</pre>",i+='<hr class="divider"><div style="display:flex;gap:8px">',i+='<button class="btn btn-primary" onclick="downloadFinalReport()">Download Full Report (JSON)</button>',i+="</div>",{html:i,report:s}}function Ee(e){if(typeof e=="string")return e.replace(/[\x00-\x1f]/g,"");if(Array.isArray(e))return e.map(Ee);if(e&&typeof e=="object"){const t={};for(const[n,s]of Object.entries(e))t[n]=Ee(s);return t}return e}const Ks={MergeContent:{reason:"Spark handles partitioned reads natively — no need to merge small files manually",savings:"medium",risk:"low"},MergeRecord:{reason:"Spark handles partitioned reads natively — no need to merge records manually",savings:"medium",risk:"low"},CompressContent:{reason:"Delta Lake handles compression (zstd/snappy) automatically",savings:"low",risk:"none"},UnpackContent:{reason:"Delta Lake handles decompression automatically on read",savings:"low",risk:"none"},SplitText:{reason:"Spark reads entire datasets at once — no need to split text into individual records",savings:"medium",risk:"low"},SplitJson:{reason:"Spark reads JSON files as DataFrames natively — no splitting needed",savings:"medium",risk:"low"},SplitXml:{reason:"spark-xml reads entire XML documents — no splitting needed",savings:"medium",risk:"low"},SplitContent:{reason:"Spark operates on entire partitions — no content splitting needed",savings:"medium",risk:"low"},SplitAvro:{reason:"Spark reads Avro files as DataFrames natively",savings:"medium",risk:"low"},SplitRecord:{reason:"Spark operates on entire DataFrames — individual record splitting unnecessary",savings:"medium",risk:"low"},UpdateAttribute:{reason:"Use .withColumn() to add/modify columns — no separate attribute update step",savings:"low",risk:"none"},RouteOnAttribute:{reason:"Use .filter() or .when() for simple attribute-based routing",savings:"low",risk:"low"},LogAttribute:{reason:"Use Spark logging or display() — no dedicated log processor needed",savings:"low",risk:"none"},LogMessage:{reason:"Use print() or logging module — no dedicated log processor needed",savings:"low",risk:"none"},Wait:{reason:"Use Databricks Workflows task dependencies instead of in-flow waits",savings:"medium",risk:"low"},Notify:{reason:"Use Databricks Workflows task dependencies instead of notifications",savings:"medium",risk:"low"},DetectDuplicate:{reason:"Use dropDuplicates() — built into Spark DataFrame API",savings:"medium",risk:"low"},ControlRate:{reason:"Spark handles backpressure natively via Structured Streaming",savings:"low",risk:"none"},DistributeLoad:{reason:"Spark handles data distribution across executors automatically",savings:"medium",risk:"none"},ValidateRecord:{reason:"Use DLT expectations for declarative data quality rules",savings:"low",risk:"low"},RetryFlowFile:{reason:"Use Spark retry mechanisms or Workflows retry policies",savings:"low",risk:"low"},MonitorActivity:{reason:"Use Databricks Workflows monitoring and Spark UI",savings:"low",risk:"none"},DebugFlow:{reason:"Use Spark UI, display(), or notebook debugging — no dedicated debug processor",savings:"low",risk:"none"},CountText:{reason:"Use df.count() — built into Spark DataFrame API",savings:"low",risk:"none"},AttributesToJSON:{reason:"Use to_json() — built into PySpark",savings:"low",risk:"none"},GenerateFlowFile:{reason:"Use spark.range() or createDataFrame() for test data generation",savings:"low",risk:"none"},EnforceOrder:{reason:"Use orderBy() — built into Spark DataFrame API",savings:"low",risk:"none"},Funnel:{reason:"Use DataFrame union() — NiFi funnels have no Databricks equivalent needed",savings:"low",risk:"none"},InputPort:{reason:"NiFi ports not needed — data flows handled by notebooks/jobs",savings:"low",risk:"none"},OutputPort:{reason:"NiFi ports not needed — data flows handled by notebooks/jobs",savings:"low",risk:"none"}},se={file_polling:{nifi:"GetFile/ListFile polling with scheduling",dbx:"Auto Loader (cloudFiles) with file notification mode",benefit:"10-100x faster file discovery, exactly-once guarantees, schema evolution"},batch_loop:{nifi:"Repeated batch processing via CRON-scheduled processors",dbx:"Structured Streaming with trigger(availableNow=True)",benefit:"Incremental processing, checkpoint-based exactly-once, auto-scaling"},schema_mgmt:{nifi:"Schema Registry + Avro/JSON schema enforcement per processor",dbx:"Unity Catalog schema governance with automatic schema evolution",benefit:"Centralized governance, lineage tracking, access controls"},data_quality:{nifi:"ValidateRecord + RouteOnAttribute for quality checks",dbx:"DLT Expectations (expect, expect_or_drop, expect_or_fail)",benefit:"Declarative quality rules, automatic quarantine, quality dashboards"},dedup:{nifi:"DetectDuplicate processor with distributed cache",dbx:"dropDuplicates() or MERGE INTO with Delta Lake",benefit:"No external cache needed, ACID guarantees, time travel"},merge_small:{nifi:"MergeContent to combine small files",dbx:"Delta Lake Auto Optimize + Auto Compaction",benefit:"Automatic small file compaction, no manual merge logic"},scheduling:{nifi:"CRON-driven processor scheduling with backpressure",dbx:"Databricks Workflows with task dependencies and triggers",benefit:"DAG-based orchestration, conditional logic, cost-optimized clusters"},caching:{nifi:"DistributedMapCache for lookup enrichment",dbx:"Broadcast variables or Delta Lake lookups",benefit:"No external cache infrastructure, automatic distribution"},security:{nifi:"Per-processor credentials and NiFi Registry policies",dbx:"Unity Catalog + Secret Scopes + identity federation",benefit:"Centralized IAM, fine-grained ACLs, audit logging"},monitoring:{nifi:"NiFi bulletins, provenance, and flow status",dbx:"Spark UI, Ganglia, custom Databricks dashboards",benefit:"Deep execution insights, cost tracking, ML-integrated monitoring"}};function Gt({nifi:e,notebook:t,escapeHTML:n}){const s=e.processors||[],a=e.connections||[];let i="";i+='<h3 style="margin:0 0 12px;color:var(--primary)">1. What This Workflow Does</h3>';const c=s.filter(w=>X(w.type)==="source"),o=s.filter(w=>X(w.type)==="sink"),r=s.filter(w=>X(w.type)==="transform"),l=s.filter(w=>X(w.type)==="route"),p=s.filter(w=>X(w.type)==="process"),d=[...new Set(c.map(w=>w.type))],u=[...new Set(o.map(w=>w.type))],m=De(e),y=Object.values(m).map(w=>w.name);let _='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px;margin-bottom:16px">';_+='<p style="font-size:0.95rem;line-height:1.6;margin:0">',_+=`This NiFi flow consists of <strong>${s.length} processors</strong> organized into <strong>${e.processGroups.length} process group(s)</strong>. `,_+=`It ingests data from <strong>${c.length} source(s)</strong> (${d.join(", ")||"none identified"}), `,_+=`applies <strong>${r.length} transformation(s)</strong> and <strong>${l.length} routing decision(s)</strong>, `,_+=`then delivers to <strong>${o.length} destination(s)</strong> (${u.join(", ")||"none identified"}).`,y.length&&(_+=` External systems involved: <strong>${y.join(", ")}</strong>.`),_+="</p></div>",_+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:12px;margin-bottom:16px">',[{role:"Sources",count:c.length,color:"#3B82F6",icon:"&#9654;"},{role:"Transforms",count:r.length,color:"#A855F7",icon:"&#9881;"},{role:"Routing",count:l.length,color:"#EAB308",icon:"&#8644;"},{role:"Processing",count:p.length,color:"#6366F1",icon:"&#9881;"},{role:"Sinks",count:o.length,color:"#21C354",icon:"&#9632;"}].forEach(w=>{_+=`<div style="background:${w.color}11;border:1px solid ${w.color}44;border-radius:8px;padding:12px;text-align:center">`,_+=`<div style="font-size:1.5rem">${w.icon}</div>`,_+=`<div style="font-size:1.8rem;font-weight:700;color:${w.color}">${w.count}</div>`,_+=`<div style="font-size:0.8rem;color:var(--text2)">${w.role}</div></div>`}),_+="</div>",Object.keys(m).length&&(_+='<h4 style="margin:12px 0 6px">Integration Points</h4>',_+='<div style="display:flex;gap:8px;flex-wrap:wrap">',Object.values(m).forEach(w=>{const F=w.processors.map(I=>I.direction).includes("WRITE")&&w.processors.map(I=>I.direction).includes("READ")?"&#8644;":w.processors.map(I=>I.direction).includes("WRITE")?"&#8594;":"&#8592;";_+=`<span style="display:inline-flex;align-items:center;gap:4px;padding:4px 10px;background:var(--surface);border:1px solid var(--border);border-radius:6px;font-size:0.82rem">${F} <strong>${n(w.name)}</strong> <span style="color:var(--text2)">(${w.processors.length})</span></span>`}),_+="</div>");const S=new Set;s.forEach(w=>{/JSON/i.test(w.type)&&S.add("JSON"),/XML/i.test(w.type)&&S.add("XML"),/Avro/i.test(w.type)&&S.add("Avro"),/CSV|Delimited/i.test(w.type)&&S.add("CSV"),/Parquet/i.test(w.type)&&S.add("Parquet"),/ORC/i.test(w.type)&&S.add("ORC");const F=Object.values(w.properties||{}).join(" ");/json/i.test(F)&&S.add("JSON"),/xml/i.test(F)&&S.add("XML"),/csv|delimited/i.test(F)&&S.add("CSV"),/avro/i.test(F)&&S.add("Avro"),/parquet/i.test(F)&&S.add("Parquet")}),S.size&&(_+=`<p style="margin:12px 0 0;font-size:0.85rem;color:var(--text2)">Data formats detected: <strong>${[...S].join(", ")}</strong></p>`),i+=_,i+='<h3 style="margin:24px 0 12px;color:var(--primary)">2. How to Build It Better in Databricks</h3>';const P=[];s.some(w=>/^(GetFile|ListFile|TailFile|FetchFile)$/i.test(w.type))&&P.push({...se.file_polling,category:"Ingestion",priority:1}),s.some(w=>w.schedulingStrategy==="CRON_DRIVEN"||/TIMER_DRIVEN/i.test(w.schedulingStrategy))&&P.push({...se.batch_loop,category:"Processing",priority:2}),s.some(w=>/Schema|Avro|Record/i.test(w.type))&&P.push({...se.schema_mgmt,category:"Governance",priority:2}),s.some(w=>/Validate|RouteOn/i.test(w.type))&&P.push({...se.data_quality,category:"Quality",priority:1}),s.some(w=>/DetectDuplicate/i.test(w.type))&&P.push({...se.dedup,category:"Deduplication",priority:2}),s.some(w=>/MergeContent|MergeRecord/i.test(w.type))&&P.push({...se.merge_small,category:"Optimization",priority:2}),s.length>5&&P.push({...se.scheduling,category:"Orchestration",priority:3}),s.some(w=>/Cache|Lookup/i.test(w.type))&&P.push({...se.caching,category:"Enrichment",priority:3}),e.controllerServices.some(w=>/SSL|Kerberos|LDAP|Credential/i.test(w.type))&&P.push({...se.security,category:"Security",priority:1}),P.push({...se.monitoring,category:"Monitoring",priority:3}),P.sort((w,F)=>w.priority-F.priority),i+='<div style="display:grid;gap:12px">',P.forEach(w=>{const F=w.priority===1?"var(--green)":w.priority===2?"var(--primary)":"var(--text2)",I=w.priority===1?"HIGH":w.priority===2?"MEDIUM":"LOW";i+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:14px">',i+='<div style="display:flex;justify-content:space-between;align-items:center;margin-bottom:8px">',i+=`<strong style="font-size:0.95rem">${n(w.category)}</strong>`,i+=`<span style="font-size:0.72rem;padding:2px 8px;border-radius:4px;background:${F}22;color:${F};font-weight:700">${I} PRIORITY</span>`,i+="</div>",i+='<div style="display:grid;grid-template-columns:1fr 1fr;gap:12px;font-size:0.85rem">',i+=`<div><div style="color:var(--text2);font-size:0.75rem;margin-bottom:4px">CURRENT (NiFi)</div>${n(w.nifi)}</div>`,i+=`<div><div style="color:var(--green);font-size:0.75rem;margin-bottom:4px">RECOMMENDED (Databricks)</div><strong>${n(w.dbx)}</strong></div>`,i+="</div>",i+=`<div style="margin-top:8px;font-size:0.82rem;color:var(--text2)">&#9889; ${n(w.benefit)}</div>`,i+="</div>"}),i+="</div>",i+=`<h3 style="margin:24px 0 12px;color:var(--primary)">3. Steps That Aren't Needed in Databricks</h3>`;const k=[];if(s.forEach(w=>{const F=Ks[w.type];F&&k.push({name:w.name,type:w.type,group:w.group,...F})}),k.length===0)i+='<div class="val-ok">All processors in this flow serve essential functions in the Databricks migration.</div>';else{const w={high:3,medium:2,low:1},F=k.sort((I,L)=>(w[L.savings]||0)-(w[I.savings]||0));i+=`<div class="alert alert-success" style="margin-bottom:12px"><strong>${k.length}</strong> of ${s.length} processors (${Math.round(k.length/s.length*100)}%) can be eliminated in Databricks</div>`,i+='<div class="table-scroll"><table style="font-size:0.82rem"><thead><tr><th>Processor</th><th>Type</th><th>Reason to Drop</th><th>Savings</th><th>Risk</th></tr></thead><tbody>',F.forEach(I=>{const L=I.savings==="high"?"var(--green)":I.savings==="medium"?"var(--primary)":"var(--text2)",G=I.risk==="high"?"var(--red)":I.risk==="medium"?"var(--amber)":I.risk==="low"?"var(--text2)":"var(--green)";i+=`<tr><td><strong>${n(I.name)}</strong></td>`,i+=`<td><code style="font-size:0.75rem">${n(I.type)}</code></td>`,i+=`<td style="font-size:0.8rem">${n(I.reason)}</td>`,i+=`<td><span style="color:${L};font-weight:600">${I.savings.toUpperCase()}</span></td>`,i+=`<td><span style="color:${G};font-weight:600">${(I.risk||"none").toUpperCase()}</span></td></tr>`}),i+="</tbody></table></div>"}i+='<h3 style="margin:24px 0 12px;color:var(--primary)">4. Quantified Complexity Reduction</h3>';const D=s.length,C=k.length,f=D-C,g=D>0?Math.round(C/D*100):0,h=t?(t.cells||[]).length:f,v=Math.max(3,h-C);if(i+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(180px,1fr));gap:12px;margin-bottom:16px">',[{label:"NiFi Processors",value:D,color:"var(--text2)",sub:"current flow"},{label:"Can Be Dropped",value:C,color:"var(--amber)",sub:`${g}% reduction`},{label:"Essential Steps",value:f,color:"var(--green)",sub:"remain in Databricks"},{label:"Notebook Cells",value:v,color:"var(--primary)",sub:"projected output"}].forEach(w=>{i+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:14px;text-align:center">',i+=`<div style="font-size:2rem;font-weight:800;color:${w.color}">${w.value}</div>`,i+=`<div style="font-size:0.85rem;font-weight:600">${w.label}</div>`,i+=`<div style="font-size:0.75rem;color:var(--text2)">${w.sub}</div></div>`}),i+="</div>",k.length){const w={};k.forEach(F=>{const I=F.type.match(/Merge|Split/)?"Merge/Split":F.type.match(/Log|Debug|Count|Monitor/)?"Logging/Monitoring":F.type.match(/Route|Distribute|Control|Detect/)?"Routing/Control":F.type.match(/Update|Attribute|Enforce/)?"Attribute Management":F.type.match(/Wait|Notify|Retry/)?"Flow Control":F.type.match(/Validate/)?"Validation":F.type.match(/Compress|Unpack/)?"Compression":F.type.match(/Generate|Funnel|Port/)?"NiFi Internal":"Other";w[I]||(w[I]=[]),w[I].push(F)}),i+='<h4 style="margin:12px 0 6px">Dropped Processors by Category</h4>',i+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:8px">',Object.entries(w).sort((F,I)=>I[1].length-F[1].length).forEach(([F,I])=>{i+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:6px;padding:10px">',i+=`<strong style="font-size:0.85rem">${n(F)}</strong> <span style="color:var(--text2);font-size:0.8rem">(${I.length})</span>`,i+=`<div style="font-size:0.78rem;color:var(--text2);margin-top:4px">${I.map(L=>n(L.type)).join(", ")}</div></div>`}),i+="</div>"}i+='<h3 style="margin:24px 0 12px;color:var(--primary)">5. Migration ROI Summary</h3>';const E=s.length*2+a.length+(e.controllerServices||[]).length*3+Object.keys(m).length*5,R=f*2+Object.keys(m).length*3,A=E>0?Math.round((1-R/E)*100):0,T=[{name:"ACID Transactions",desc:"Delta Lake provides full ACID guarantees on all data operations",icon:"&#128274;"},{name:"Time Travel",desc:"Query and restore previous versions of data with VERSION AS OF",icon:"&#9200;"},{name:"Unity Catalog Governance",desc:"Centralized access control, lineage, and audit logging",icon:"&#128737;"},{name:"ML Integration",desc:"Seamless integration with MLflow, Feature Store, and model serving",icon:"&#129302;"},{name:"Auto Scaling",desc:"Automatic cluster scaling based on workload demand",icon:"&#128200;"},{name:"Photon Engine",desc:"Vectorized query engine for 2-8x performance improvement",icon:"&#9889;"},{name:"Delta Live Tables",desc:"Declarative data pipelines with built-in quality management",icon:"&#128736;"},{name:"Liquid Clustering",desc:"Automatic data layout optimization replacing manual Z-ordering",icon:"&#128204;"}];i+='<div style="display:grid;grid-template-columns:1fr 1fr;gap:16px;margin-bottom:16px">',i+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px">',i+='<h4 style="margin:0 0 12px;font-size:0.95rem">Complexity Comparison</h4>';const N=100,U=E>0?Math.round(R/E*100):50;i+=`<div style="margin-bottom:12px"><div style="display:flex;justify-content:space-between;font-size:0.82rem;margin-bottom:4px"><span>NiFi (current)</span><strong>${E} pts</strong></div>`,i+=`<div style="height:20px;background:var(--red)33;border-radius:4px;overflow:hidden"><div style="height:100%;width:${N}%;background:var(--red);border-radius:4px"></div></div></div>`,i+=`<div><div style="display:flex;justify-content:space-between;font-size:0.82rem;margin-bottom:4px"><span>Databricks (projected)</span><strong>${R} pts</strong></div>`,i+=`<div style="height:20px;background:var(--green)33;border-radius:4px;overflow:hidden"><div style="height:100%;width:${U}%;background:var(--green);border-radius:4px"></div></div></div>`,i+=`<div style="text-align:center;margin-top:12px;font-size:1.1rem;font-weight:700;color:var(--green)">${A}% complexity reduction</div>`,i+="</div>",i+='<div style="background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px">',i+='<h4 style="margin:0 0 12px;font-size:0.95rem">Migration Summary</h4>',[{label:"Processors eliminated",value:`${C} of ${D} (${g}%)`},{label:"External systems",value:`${Object.keys(m).length} integration(s)`},{label:"Controller services",value:`${(e.controllerServices||[]).length} to migrate`},{label:"Process groups",value:`${e.processGroups.length} → Databricks jobs`},{label:"Notebook cells",value:`${v} (vs ${D} NiFi processors)`},{label:"New capabilities",value:`${T.length} Databricks features gained`}].forEach(w=>{i+=`<div style="display:flex;justify-content:space-between;padding:4px 0;border-bottom:1px solid var(--border);font-size:0.85rem"><span style="color:var(--text2)">${w.label}</span><strong>${w.value}</strong></div>`}),i+="</div></div>",i+='<h4 style="margin:16px 0 8px">New Capabilities Gained with Databricks</h4>',i+='<div style="display:grid;grid-template-columns:repeat(auto-fit,minmax(220px,1fr));gap:8px">',T.forEach(w=>{i+='<div style="background:var(--green)08;border:1px solid var(--green)33;border-radius:8px;padding:10px">',i+=`<div style="font-size:1.1rem;margin-bottom:4px">${w.icon}</div>`,i+=`<div style="font-weight:600;font-size:0.88rem">${n(w.name)}</div>`,i+=`<div style="font-size:0.78rem;color:var(--text2);margin-top:2px">${n(w.desc)}</div></div>`}),i+="</div>",i+='<div style="margin-top:24px;text-align:center">',i+='<button class="btn btn-primary" onclick="downloadValueAnalysis()">Download Value Analysis (JSON)</button>',i+="</div>";const Q={summary:{totalProcessors:D,droppable:C,essential:f,reductionPct:g,nifiComplexity:E,dbxComplexity:R,complexityReduction:A},droppableProcessors:k,recommendations:P.map(w=>({category:w.category,nifi:w.nifi,dbx:w.dbx,benefit:w.benefit,priority:w.priority})),externalSystems:Object.keys(m),newCapabilities:T.map(w=>w.name),timestamp:new Date().toISOString()};return{html:i,valueAnalysis:Q}}function fe(e,t){const n=document.createElement("a");n.href=URL.createObjectURL(e),n.download=t,n.click(),URL.revokeObjectURL(n.href)}function Ie(e){if(!e.notebook)return;const n=`# Databricks notebook source

`+e.notebook.cells.map(s=>s.type==="md"?`# MAGIC %md
`+s.source.split(`
`).map(a=>"# MAGIC "+a).join(`
`):s.type==="sql"?`# MAGIC %sql
`+s.source.split(`
`).map(a=>"# MAGIC "+a).join(`
`):s.source).join(`

# COMMAND ----------

`);fe(new Blob([n],{type:"text/plain"}),"nifi_migration_notebook.py")}function Fe(e){e.notebook&&fe(new Blob([JSON.stringify(e.notebook.workflow,null,2)],{type:"application/json"}),"databricks_workflow.json")}function Le(e){if(!e.migrationReport)return;const t=e.migrationReport,n=t.summary;let s=`# NiFi → Databricks Migration Report

`;s+=`## Summary
| Metric | Value |
|--------|-------|
`,s+=`| Total Processors | ${n.totalProcessors} |
| Mapped | ${n.mappedProcessors} |
| Unmapped | ${n.unmappedProcessors} |
`,s+=`| Coverage | ${n.coveragePercent}% |
| Process Groups | ${n.totalProcessGroups} |
| Effort | ${t.effort} |

`,s+=`## By Role
| Role | Mapped | Total | % |
|------|--------|-------|---|
`,Object.entries(t.byRole).forEach(([a,i])=>{s+=`| ${a} | ${i.mapped} | ${i.total} | ${i.total?Math.round(i.mapped/i.total*100):0}% |
`}),s+=`
## By Group
| Group | Mapped | Total | % |
|-------|--------|-------|---|
`,Object.entries(t.byGroup).forEach(([a,i])=>{s+=`| ${a} | ${i.mapped} | ${i.total} | ${i.total?Math.round(i.mapped/i.total*100):0}% |
`}),t.gaps.length&&(s+=`
## Gaps
| Processor | Type | Group | Recommendation |
|-----------|------|-------|----------------|
`,t.gaps.forEach(a=>{s+=`| ${a.processor} | ${a.type} | ${a.group||"—"} | ${a.recommendation||"Manual"} |
`})),t.recommendations.length&&(s+=`
## Recommendations
`,t.recommendations.forEach(a=>{s+=`- ${a}
`})),fe(new Blob([s],{type:"text/markdown"}),"migration_report.md")}function ut(e){if(!e.finalReport)return;const t=JSON.stringify(Ee(e.finalReport),null,2);fe(new Blob([t],{type:"application/json"}),"nifi_analysis_report.json")}function mt(e){if(!e.validation)return;const t=JSON.stringify(Ee(e.validation),null,2);fe(new Blob([t],{type:"application/json"}),"nifi_validation_report.json")}function ft(e){if(!e.valueAnalysis)return;const t=JSON.stringify(e.valueAnalysis,null,2);fe(new Blob([t],{type:"application/json"}),"nifi_value_analysis.json")}function gt(e){if(!e||e.length===0){alert("No notebook cells generated yet. Run the conversion pipeline first.");return}let t=`# Databricks notebook source
`;t+=`# Generated by NiFi Flow Analyzer — Automated Migration
`,t+="# Date: "+new Date().toISOString().split("T")[0]+`

`,e.forEach((i,c)=>{c>0&&(t+=`
# COMMAND ----------

`);const o=i.source||i.code||"";i.type==="markdown"||i.role==="markdown"?(t+=`# MAGIC %md
`,o.split(`
`).forEach(r=>{t+="# MAGIC "+r+`
`})):t+=o+`
`});const n=new Blob([t],{type:"text/plain"}),s=URL.createObjectURL(n),a=document.createElement("a");a.href=s,a.download="nifi_migration_notebook.py",a.click(),URL.revokeObjectURL(s)}function yt(e){if(!e||e.length===0){alert("No notebook cells generated yet. Run the conversion pipeline first.");return}const t={nbformat:4,nbformat_minor:5,metadata:{kernelspec:{display_name:"Python 3",language:"python",name:"python3"},language_info:{name:"python",version:"3.10.0"}},cells:e.map(i=>({cell_type:i.type==="markdown"||i.role==="markdown"?"markdown":"code",metadata:{},source:(i.source||i.code||"").split(`
`).map((c,o,r)=>o<r.length-1?c+`
`:c),outputs:[],execution_count:null}))},n=new Blob([JSON.stringify(t,null,2)],{type:"application/json"}),s=URL.createObjectURL(n),a=document.createElement("a");a.href=s,a.download="nifi_migration_notebook.ipynb",a.click(),URL.revokeObjectURL(s)}function Vs(e,t){if(!e){alert("No NiFi flow loaded yet.");return}const n=e.processGroups||[],s=e.connections||[],a=t(n,s);let i=`# Databricks Workflow — Generated from NiFi Flow
`;i+="# Date: "+new Date().toISOString().split("T")[0]+`

`,i+=`name: nifi_migration_workflow
`,i+=`tasks:
`,a.tasks.forEach(l=>{i+="  - task_key: "+l.task_key+`
`,i+=`    notebook_task:
`,i+="      notebook_path: "+l.notebook_task.notebook_path+`
`,i+=`      source: WORKSPACE
`,l.depends_on&&l.depends_on.length>0&&(i+=`    depends_on:
`,l.depends_on.forEach(p=>{i+="      - task_key: "+p.task_key+`
`})),i+=`
`});const c=new Blob([i],{type:"text/yaml"}),o=URL.createObjectURL(c),r=document.createElement("a");r.href=o,r.download="nifi_migration_workflow.yml",r.click(),URL.revokeObjectURL(o)}function Xs(e,{escapeHTML:t,metricsHTML:n}={}){const s={};return e.notebook&&e.parsed&&e.parsed._nifi&&(s.migrationReport=Bt(e.notebook.mappings||e.assessment?.mappings||[],e.parsed._nifi)),e.assessment&&e.parsed&&e.parsed._nifi&&(s.comparison=Qs(e.assessment.mappings,e.parsed._nifi)),e.parsed&&(s.finalReport=Ut(e)),e.parsed&&e.parsed._nifi&&e.notebook&&t&&(s.valueAnalysis=Gt({nifi:e.parsed._nifi,notebook:e.notebook,escapeHTML:t})),s}const pe={flowData:null,processors:[],connections:[],controllerServices:[],variables:{},blueprint:null,mappings:[],notebook:null,workflow:null,validationResults:null,reportData:null,currentStep:0,isProcessing:!1,errors:[]};document.addEventListener("DOMContentLoaded",()=>{Qt(pe);const e=tt(),t=new en;window.addEventListener("unhandledrejection",f=>{$e(new ue(f.reason?.message||"Unhandled promise rejection",{code:"UNHANDLED_REJECTION",severity:"high",cause:f.reason}))}),window.addEventListener("error",f=>{$e(new ue(f.message||"Uncaught error",{code:"UNCAUGHT_ERROR",severity:"high",context:{filename:f.filename,lineno:f.lineno}}))}),tn(),Pt();const n=document.getElementById("fileInput");n&&n.addEventListener("change",async()=>{de(),K(pe);const{handleFile:f}=await ht(async()=>{const{handleFile:g}=await Promise.resolve().then(()=>nn);return{handleFile:g}},void 0,import.meta.url);await f(),ye()});const s=document.getElementById("parseBtn");s&&s.addEventListener("click",()=>{de(),K(pe),ye()}),document.querySelectorAll("[data-sample-flow]").forEach(f=>{f.addEventListener("click",()=>{const g=f.dataset.sampleFlow;de(),K(pe),on(g,ye)})}),document.querySelectorAll("[data-sample-file]").forEach(f=>{f.addEventListener("click",()=>{const g=f.dataset.sampleFile,h=f.dataset.sampleName||g.split("/").pop();de(),K(pe),rn(g,h,ye)})});const c=document.getElementById("analyzeBtn");c&&c.addEventListener("click",()=>{Me()});const o=document.getElementById("assessBtn");o&&o.addEventListener("click",()=>{qe()});const r=document.getElementById("convertBtn");r&&r.addEventListener("click",()=>{je()});const l=document.getElementById("reportBtn");l&&l.addEventListener("click",()=>{Be()});const p=document.getElementById("downloadNotebookBtn");p&&p.addEventListener("click",()=>{Ie(O())});const d=document.getElementById("downloadWorkflowBtn");d&&d.addEventListener("click",()=>{Fe(O())});const u=document.getElementById("downloadReportBtn");u&&u.addEventListener("click",()=>{Le(O())});const m=document.getElementById("downloadFinalReportBtn");m&&m.addEventListener("click",()=>{ut(O())});const y=document.getElementById("downloadValidationBtn");y&&y.addEventListener("click",()=>{mt(O())});const _=document.getElementById("downloadValueBtn");_&&_.addEventListener("click",()=>{ft(O())});const b=document.getElementById("exportDatabricksBtn");b&&b.addEventListener("click",()=>{const g=O().notebook?.cells||[];gt(g)});const S=document.getElementById("exportJupyterBtn");S&&S.addEventListener("click",()=>{const g=O().notebook?.cells||[];yt(g)});const P=document.getElementById("exportWorkflowYamlBtn");P&&P.addEventListener("click",()=>{const f=O();f.parsed?._nifi&&Vs(f.parsed._nifi,(g,h)=>({tasks:(g||[]).map((v,$)=>({task_key:v.name?.replace(/\s+/g,"_").toLowerCase()||`task_${$}`,notebook_task:{notebook_path:`/Workspace/Migrations/NiFi/${v.name||"task_"+$}`},depends_on:$>0?[{task_key:(g[$-1].name||`task_${$-1}`).replace(/\s+/g,"_").toLowerCase()}]:[]}))}))});const k=document.getElementById("cfgSaveBtn");k&&k.addEventListener("click",()=>{const f=St();Yt(f),q.emit("config:saved",f);const g=k.textContent;k.textContent="Saved!",k.disabled=!0,setTimeout(()=>{k.textContent=g,k.disabled=!1},1500)});const D=document.getElementById("cfgResetBtn");D&&D.addEventListener("click",()=>{const f=tt(),g={cfgCatalog:f.catalog,cfgSchema:f.schema,cfgScope:f.secretScope,cfgCloud:f.cloudProvider,cfgSparkVersion:f.sparkVersion,cfgNodeType:f.nodeType,cfgWorkers:f.numWorkers,cfgWorkspacePath:f.workspacePath};Object.entries(g).forEach(([h,v])=>{const $=document.getElementById(h);$&&($.value=v??"")}),q.emit("config:reset",f)}),t.register([{name:"parse",tab:"load",run:re(async()=>{q.emit("step:parse:start"),Te(0,"Starting parse..."),await ye(),q.emit("step:parse:done",O())},{phase:"parse",code:"PARSE_FAILED"})},{name:"analyze",tab:"analyze",run:re(async()=>{q.emit("step:analyze:start"),V("analyze"),Me(),q.emit("step:analyze:done",O())},{phase:"analyze",code:"ANALYZE_FAILED"})},{name:"assess",tab:"assess",run:re(async()=>{q.emit("step:assess:start"),V("assess"),qe(),q.emit("step:assess:done",O())},{phase:"assess",code:"ASSESS_FAILED"})},{name:"generate",tab:"convert",run:re(async()=>{q.emit("step:generate:start"),V("convert"),je(),q.emit("step:generate:done",O())},{phase:"generate",code:"GENERATE_FAILED"})},{name:"report",tab:"report",run:re(async()=>{q.emit("step:report:start"),V("report"),Be(),q.emit("step:report:done",O())},{phase:"report",code:"REPORT_FAILED"})},{name:"reportFinal",tab:"reportFinal",run:re(async()=>{q.emit("step:reportFinal:start"),V("reportFinal"),typeof window.generateFinalReport=="function"&&await window.generateFinalReport(),q.emit("step:reportFinal:done",O())},{phase:"reportFinal",code:"FINAL_REPORT_FAILED"})},{name:"validate",tab:"validate",run:re(async()=>{q.emit("step:validate:start"),V("validate"),typeof window.runValidation=="function"&&await window.runValidation(),q.emit("step:validate:done",O())},{phase:"validate",code:"VALIDATE_FAILED"})},{name:"value",tab:"value",run:re(async()=>{q.emit("step:value:start"),V("value"),typeof window.runValueAnalysis=="function"&&window.runValueAnalysis(),q.emit("step:value:done",O())},{phase:"value",code:"VALUE_ANALYSIS_FAILED"})}]),q.on("flow:loaded",()=>{de(),K(pe),t.execute()}),q.on("pipeline:step:start",({step:f})=>{const g={parse:"Parsing flow...",analyze:"Analyzing flow...",assess:"Assessing migration readiness...",generate:"Generating notebook...",report:"Generating reports...",reportFinal:"Building final report...",validate:"Running validation...",value:"Running value analysis..."},h=["parse","analyze","assess","generate","report","reportFinal","validate","value"].indexOf(f),v=h>=0?Math.round((h+1)/8*100):0;Te(v,g[f]||f)}),q.on("pipeline:done",()=>{Te(100,"All steps complete!"),setTimeout(an,1500)}),q.on("pipeline:step:error",({step:f,error:g})=>{console.error(`[main] Pipeline step "${f}" failed:`,g)}),document.body.addEventListener("click",f=>{const g=f.target.closest("button");if(!g)return;const h=g.textContent.trim().toLowerCase();h.includes("download")&&h.includes("notebook")?(f.preventDefault(),Ie(O())):h.includes("download")&&h.includes("workflow")?(f.preventDefault(),Fe(O())):h.includes("download")&&h.includes("report")&&h.includes("markdown")&&(f.preventDefault(),Le(O()))}),window.downloadNotebook=()=>Ie(O()),window.downloadWorkflow=()=>Fe(O()),window.downloadReport=()=>Le(O()),window.downloadFinalReport=()=>ut(O()),window.downloadValidationReport=()=>mt(O()),window.downloadValueAnalysis=()=>ft(O()),window.exportAsDatabricksNotebook=()=>{const f=O();gt(f.notebook?.cells||[])},window.exportAsJupyterNotebook=()=>{const f=O();yt(f.notebook?.cells||[])},window.parseFlow=Nt,window.runAnalysisEngine=js,window.mapNiFiToDatabricks=Je,window.generateNotebookAndWorkflow=Bs,window.runValidationEngine=dt,window.generateReportSuite=Xs;const C=f=>'<div class="metrics">'+f.map(g=>{const h=Array.isArray(g)?g[0]:g.label,v=Array.isArray(g)?g[1]:g.value,$=Array.isArray(g)?g[2]:g.delta,E=Array.isArray(g)?"":g.color||"";return`<div class="metric"><div class="label">${h}</div><div class="value"${E?' style="color:'+E+'"':""}>${v}</div>${$?`<div class="delta">${$}</div>`:""}</div>`}).join("")+"</div>";window.generateFinalReport=async()=>{const f=O();if(!f.parsed)return;H("reportFinal","processing");const{html:g,report:h}=Js(f,C,x);K({finalReport:h});const v=document.getElementById("reportFinalResults");v&&(v.innerHTML=g),H("reportFinal","done"),ce("validate");const $=document.getElementById("validateNotReady"),E=document.getElementById("validateReady");$&&$.classList.add("hidden"),E&&E.classList.remove("hidden")},window.runValidation=async()=>{const f=O();if(!f.parsed||!f.parsed._nifi||!f.notebook)return;H("validate","processing");const g=document.getElementById("validateResults"),h=await dt({nifi:f.parsed._nifi,mappings:f.notebook.mappings||f.assessment?.mappings||[],cells:f.notebook.cells||[],systems:f.assessment?.systems||{},nifiDatabricksMap:_e,onProgress:(E,R)=>{g&&(g.innerHTML=`<div style="color:var(--text2);padding:16px">${R} (${E}%)</div>`)}});if(K({validation:h}),g){const E=h.overallScore||0;let A=`<hr class="divider"><div class="score-big" style="color:var(--${E>=80?"green":E>=50?"amber":"red"})">Validation Score: ${Math.round(E)}%</div>`;A+=C([{label:"Intent Match",value:Math.round(h.intentScore||0)+"%"},{label:"Line Coverage",value:Math.round(h.lineScore||0)+"%"},{label:"Reverse Eng.",value:Math.round(h.reScore||0)+"%"},{label:"Function Map",value:Math.round(h.funcScore||0)+"%"}]),h.allGaps&&h.allGaps.length&&(A+='<hr class="divider"><h3>Gaps ('+h.allGaps.length+")</h3>",A+='<ul style="margin:0;padding-left:20px;font-size:0.85rem">',h.allGaps.slice(0,30).forEach(T=>{A+='<li style="margin:4px 0">'+x(T.processor||T.name||"")+": "+x(T.gap||T.reason||T.message||"")+"</li>"}),h.allGaps.length>30&&(A+="<li>... and "+(h.allGaps.length-30)+" more</li>"),A+="</ul>"),A+='<hr class="divider"><button class="btn" onclick="downloadValidationReport()">Download Validation Report</button>',g.innerHTML=A}H("validate","done"),ce("value");const v=document.getElementById("valueNotReady"),$=document.getElementById("valueReady");v&&v.classList.add("hidden"),$&&$.classList.remove("hidden")},window.runValueAnalysis=()=>{const f=O();if(!f.parsed||!f.parsed._nifi||!f.notebook)return;H("value","processing");const g=Gt({nifi:f.parsed._nifi,notebook:f.notebook,escapeHTML:x});K({valueAnalysis:g});const h=document.getElementById("valueResults");h&&(h.innerHTML=typeof g=="string"?g:g?.html||""),H("value","done")},console.info("[main] NiFi Flow Analyzer initialized"),q.emit("app:ready",{config:e})});</script>
  <style rel="stylesheet" crossorigin>:root{--bg: #0e1117;--surface: #1a1d27;--surface2: #262730;--border: #363842;--text: #fafafa;--text2: #808495;--primary: #ff4b4b;--primary-hover: #ff6b6b;--green: #21c354;--amber: #faca15;--red: #ff4b4b;--blue: #1d4ed8;--font: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;--mono: "SF Mono", "Fira Code", monospace}*{box-sizing:border-box;margin:0;padding:0}body{background:var(--bg);color:var(--text);font-family:var(--font);line-height:1.6}h1{font-size:2rem;margin-bottom:4px}h2{font-size:1.4rem;margin:24px 0 12px;border-bottom:1px solid var(--border);padding-bottom:8px}h3{font-size:1.1rem;margin:16px 0 8px}.caption{color:var(--text2);font-size:.9rem;margin-bottom:20px}code{background:var(--surface2);padding:2px 6px;border-radius:4px;font-family:var(--mono);font-size:.85rem}pre{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px;overflow-x:auto;font-family:var(--mono);font-size:.85rem;margin:12px 0}.hidden{display:none}.divider{border:none;border-top:1px solid var(--border);margin:24px 0}.container{max-width:1200px;margin:0 auto;padding:20px}.tabs{display:grid;grid-template-columns:repeat(6,1fr);gap:0;margin-bottom:24px}.tab{padding:10px 8px;cursor:pointer;color:var(--text2);border-bottom:2px solid var(--border);white-space:nowrap;font-size:.88rem;transition:all .2s;text-align:center}.tab:hover{color:var(--text)}.tab.active{color:var(--primary);border-bottom-color:var(--primary);font-weight:600}.tab{position:relative}.tab .check{display:none;margin-left:6px;color:var(--green);font-size:.85rem}.tab.done .check{display:inline-block;animation:pop .4s ease}.tab.locked{opacity:.4;pointer-events:none}.tab .spinner-sm{display:none;width:12px;height:12px;border:2px solid var(--border);border-top:2px solid var(--primary);border-radius:50%;animation:spin .6s linear infinite;margin-left:6px;vertical-align:middle}.tab.processing .spinner-sm{display:inline-block}.panel{display:none}.panel.active{display:block}.row{display:flex;gap:20px;flex-wrap:wrap}.col{flex:1;min-width:280px}.col-3{flex:1;min-width:200px}@media(max-width:768px){.row{flex-direction:column}.col,.col-3{min-width:100%}}.btn{padding:10px 24px;border:none;border-radius:6px;cursor:pointer;font-size:.95rem;font-weight:600;transition:all .2s;display:inline-flex;align-items:center;gap:8px}.btn-primary{background:var(--primary);color:#fff}.btn-primary:hover{background:var(--primary-hover)}.btn-secondary{background:var(--surface2);color:var(--text);border:1px solid var(--border)}.btn-secondary:hover{background:var(--border)}.btn:disabled{opacity:.5;cursor:not-allowed}.alert{padding:12px 16px;border-radius:6px;margin:12px 0;font-size:.9rem}.alert-info{background:#1e3a5f;border:1px solid #2563eb;color:#93c5fd}.alert-success{background:#14532d;border:1px solid var(--green);color:#86efac}.alert-warn{background:#713f12;border:1px solid var(--amber);color:#fde68a}.alert-error{background:#450a0a;border:1px solid var(--red);color:#fca5a5}.badge{display:inline-block;padding:2px 10px;border-radius:12px;font-size:.8rem;font-weight:600}.badge-green{background:#14532d;color:#86efac}.badge-amber{background:#713f12;color:#fde68a}.badge-red{background:#450a0a;color:#fca5a5}.conf-badge{padding:1px 6px;border-radius:3px;font-size:.65rem;font-weight:700}.conf-high{background:#21c35433;color:#86efac}.conf-med{background:#eab30833;color:#fde68a}.conf-low{background:#ef444433;color:#fca5a5}.conf-none{background:#80849526;color:#9ca3af}.conf-dot{display:inline-block;width:8px;height:8px;border-radius:50%;margin-right:4px;vertical-align:middle}.conf-dot.high{background:var(--green)}.conf-dot.med{background:var(--amber)}.conf-dot.low{background:var(--red)}.conf-dot.none{background:var(--text2)}.progress-bar{height:8px;background:var(--surface2);border-radius:4px;overflow:hidden;margin:6px 0}.progress-fill{height:100%;border-radius:4px;transition:width .3s}.progress-fill.green{background:var(--green)}.progress-fill.amber{background:var(--amber)}.progress-fill.red{background:var(--red)}.file-upload{border:2px dashed var(--border);border-radius:8px;padding:40px 20px;text-align:center;cursor:pointer;transition:border-color .2s}.file-upload:hover{border-color:var(--primary)}.file-upload input{display:none}.sources-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(140px,1fr));gap:8px;margin:16px 0}.source-badge{background:var(--surface);border:1px solid var(--border);border-radius:6px;padding:8px 12px;text-align:center;border-top:3px solid var(--border);transition:border-color .2s}.source-badge:hover{border-color:var(--primary)}.source-badge .src-name{font-weight:600;font-size:.85rem}.source-badge .src-type{color:var(--text2);font-size:.7rem;text-transform:uppercase;letter-spacing:.3px}.detected-source{display:inline-flex;align-items:center;gap:8px;padding:6px 14px;background:var(--surface);border:1px solid var(--green);border-radius:6px;margin:8px 0;font-size:.9rem}.detected-source .dot{width:8px;height:8px;border-radius:50%;background:var(--green)}.gap-card{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:10px 14px;margin:6px 0;border-left:3px solid var(--red)}.gap-card .gap-title{font-weight:600;font-size:.85rem;margin-bottom:2px}.gap-card .gap-meta{color:var(--text2);font-size:.75rem}.gap-card .gap-rec{color:var(--amber);font-size:.8rem;margin-top:4px}.sys-card{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:12px;margin:6px 0}.sys-card-header{display:flex;align-items:center;gap:8px;margin-bottom:8px;flex-wrap:wrap}.sys-card-header .sys-name{font-weight:700;font-size:.95rem}.sys-card-header .sys-badge{font-size:.7rem;padding:2px 8px;border-radius:10px}.sys-detail-row{display:flex;gap:16px;flex-wrap:wrap;font-size:.82rem;padding:3px 0}.sys-detail-row .sys-label{color:var(--text2);min-width:90px}.sys-detail-row .sys-value{color:var(--text);word-break:break-all}.el-highlight{background:#3b82f626;color:#60a5fa;padding:1px 3px;border-radius:2px;font-family:var(--mono);font-size:.75rem}.effort-bar{display:flex;height:28px;border-radius:6px;overflow:hidden;margin:8px 0}.effort-bar .effort-seg{display:flex;align-items:center;justify-content:center;font-size:.7rem;font-weight:700;color:#fff;transition:width .4s ease}.node-upstream{box-shadow:0 0 0 3px #3b82f699!important}.node-downstream{box-shadow:0 0 0 3px #21c35499!important}.node-selected{box-shadow:0 0 0 3px #ff4b4bcc!important}.conform-check{padding:4px 0;font-size:.82rem}.conform-check .check-icon{margin-right:6px}.resource-dot{display:inline-block;width:8px;height:8px;border-radius:50%;margin-right:4px;vertical-align:middle}.resource-dot.used{background:#21c354}.resource-dot.orphaned{background:#faca15}.resource-dot.missing{background:#ff4b4b}.match-badge{display:inline-block;padding:1px 8px;border-radius:3px;font-size:.7rem;font-weight:700}.match-exact{background:#21c35433;color:#86efac}.match-functional{background:#eab30833;color:#fde68a}.match-gap{background:#ef444433;color:#fca5a5}.sample-card{display:flex;gap:10px;align-items:center;padding:10px 14px;border:1px solid var(--border);border-radius:8px;cursor:pointer;transition:all .15s;background:var(--bg1)}.sample-card:hover{border-color:var(--accent);background:#ff6b350d;transform:translateY(-1px);box-shadow:0 2px 8px #0000001a}.sample-icon{font-size:1.5rem;flex-shrink:0;width:32px;text-align:center}.sample-info{display:flex;flex-direction:column;gap:1px;min-width:0}.sample-info strong{font-size:.85rem;color:var(--text1)}.sample-info span{font-size:.75rem;color:var(--text2)}.sample-tags{font-size:.7rem!important;color:var(--accent)!important;opacity:.8}.manifest-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(120px,1fr));gap:8px;margin:8px 0}.manifest-stat{text-align:center;padding:8px;border-radius:8px;background:var(--surface2)}.manifest-stat .num{font-size:1.3rem;font-weight:700;color:var(--accent)}.manifest-stat .lbl{font-size:.72rem;color:var(--text2)}.venv-tree{font-family:var(--mono);font-size:.82rem;line-height:1.7;padding:8px 0}.venv-tree .dir{color:var(--text2);font-weight:600}.venv-tree .file{color:var(--accent)}.venv-tree .venv-badge{display:inline-block;background:var(--surface2);border-radius:4px;padding:1px 6px;font-size:.72rem;margin-left:6px}.venv-summary{display:flex;gap:16px;flex-wrap:wrap;margin:12px 0}.venv-stat{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:12px 16px;text-align:center;flex:1;min-width:100px}.venv-stat .venv-stat-num{font-size:1.5rem;font-weight:700;color:var(--primary)}.venv-stat .venv-stat-label{font-size:.75rem;color:var(--text2);margin-top:4px}.metrics{display:flex;gap:16px;flex-wrap:wrap;margin:16px 0}.metric{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px 20px;min-width:140px;flex:1}.metric .label{font-size:.8rem;color:var(--text2);text-transform:uppercase;letter-spacing:.5px}.metric .value{font-size:1.8rem;font-weight:700;margin-top:4px}.metric .delta{font-size:.85rem;color:var(--red)}.score-big{font-size:3rem;font-weight:800;text-align:center;padding:20px}table{width:100%;border-collapse:collapse;margin:12px 0;font-size:.85rem}th{text-align:left;padding:8px 12px;background:var(--surface2);color:var(--text2);font-weight:600;text-transform:uppercase;font-size:.75rem;letter-spacing:.5px;border-bottom:2px solid var(--border)}td{padding:8px 12px;border-bottom:1px solid var(--border)}tr:hover td{background:var(--surface)}.table-scroll{overflow-x:auto}.mapping-table{width:100%;border-collapse:collapse;font-size:.78rem;margin:12px 0}.mapping-table th{text-align:left;padding:6px 10px;border-bottom:2px solid var(--border);font-size:.7rem;text-transform:uppercase;color:var(--text2)}.mapping-table td{padding:5px 10px;border-bottom:1px solid var(--border)}.mapping-table tr.unmapped{opacity:.5}.comparison-detail{margin-top:16px}.comparison-detail summary{cursor:pointer;font-weight:600;font-size:.95rem;padding:10px 0;color:var(--text)}.comparison-detail summary:hover{color:var(--primary)}.comparison-detail table{width:100%;font-size:.8rem}.comparison-detail td,.comparison-detail th{padding:6px 8px}.comparison-detail tr:hover{background:var(--surface2)}.ops-log{max-height:400px;overflow-y:auto;font-family:var(--mono);font-size:.78rem}.ops-log-row{display:grid;grid-template-columns:50px 180px 100px 1fr 80px;gap:4px;padding:3px 0;border-bottom:1px solid var(--border);align-items:center}.ops-log-row:hover{background:#ff6b350d}.ops-log-row.header{font-weight:700;color:var(--text2);font-size:.72rem;text-transform:uppercase;border-bottom:2px solid var(--border)}.ops-action{font-weight:600;font-size:.75rem}.ops-action.file{color:#4285f4}.ops-action.sql{color:#21c354}.ops-action.token{color:#faca15}.ops-action.signal{color:#ff6d70}.ops-action.counter{color:#8b5cf6}.ops-action.queue{color:#29b5e8}.action-log{max-height:300px;overflow-y:auto;border:1px solid var(--border);border-radius:6px;margin:8px 0}.action-log-entry{display:grid;grid-template-columns:100px 1fr 1fr;gap:8px;padding:6px 10px;border-bottom:1px solid var(--border);font-size:.78rem;align-items:start}.action-log-entry:last-child{border-bottom:none}.action-log-entry .action-type{font-weight:600;font-size:.75rem;text-transform:uppercase}.action-log-entry .action-type.file-op{color:#21c354}.action-log-entry .action-type.sql-op{color:#6366f1}.action-log-entry .action-type.token-op{color:#eab308}.action-log-entry .action-type.signal-op{color:#f97316}.action-log-entry .action-type.queue-op{color:#06b6d4}.expander{border:1px solid var(--border);border-radius:8px;margin:12px 0;overflow:hidden}.expander-header{padding:12px 16px;cursor:pointer;display:flex;justify-content:space-between;align-items:center;background:var(--surface);font-weight:500}.expander-header:hover{background:var(--surface2)}.expander-body{padding:16px;display:none;border-top:1px solid var(--border)}.expander.open .expander-body{display:block}.expander-arrow{transition:transform .2s}.expander.open .expander-arrow{transform:rotate(90deg)}.tier-diagram{position:relative;overflow:auto;border:1px solid var(--border);border-radius:8px;background:var(--bg);min-height:200px;margin:16px 0}.tier-diagram svg.tier-svg{position:absolute;top:0;left:0;width:100%;height:100%;pointer-events:none;z-index:1}.tier-band{padding:16px 20px;border-bottom:1px solid var(--border);position:relative;z-index:2}.tier-band:last-child{border-bottom:none}.tier-band-label{font-size:.7rem;text-transform:uppercase;letter-spacing:1px;font-weight:700;margin-bottom:10px;opacity:.8}.tier-nodes{display:flex;flex-wrap:wrap;gap:12px;justify-content:center}.tier-node{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:10px 16px;min-width:120px;max-width:200px;text-align:center;cursor:pointer;transition:all .2s;position:relative;z-index:3}.tier-node:hover{border-color:var(--primary);transform:translateY(-2px);box-shadow:0 4px 12px #0000004d}.tier-node.selected{border-color:var(--primary);box-shadow:0 0 0 2px #ff4b4b4d}.tier-node.highlighted{border-color:var(--green)}.tier-node.dimmed{opacity:.3}.tier-node .node-name{font-weight:600;font-size:.85rem;word-break:break-all}.tier-node .node-meta{font-size:.7rem;color:var(--text2);margin-top:4px}.tier-node .node-badge{position:absolute;top:-6px;right:-6px;background:var(--primary);color:#fff;border-radius:10px;padding:1px 6px;font-size:.65rem;font-weight:700}.tier-node .node-badge.green{background:var(--green)}.tier-node .node-badge.amber{background:var(--amber);color:#000}.tier-node .node-badge.red{background:var(--red)}.tier-node .node-seq{display:inline-flex;align-items:center;justify-content:center;width:22px;height:22px;border-radius:50%;background:var(--primary);color:#fff;font-size:.7rem;font-weight:700;margin-bottom:4px}.tier-node .node-stats{display:flex;gap:4px;justify-content:center;margin-top:6px;flex-wrap:wrap}.tier-node .node-stats .ns{padding:1px 5px;border-radius:3px;font-size:.6rem;font-weight:700}.ns-tx{background:#3b82f6;color:#fff}.ns-ext{background:#21c354;color:#fff}.ns-lkp{background:#eab308;color:#000}.tier-node.table-output{background:var(--surface2);border-style:dashed;min-width:100px;max-width:180px;padding:8px 12px}.tier-node.table-output .node-name{font-size:.75rem}.tier-node.table-output .node-class{font-size:.6rem;text-transform:uppercase;letter-spacing:.5px;margin-top:2px}.tier-node.conflict-gate{border-color:var(--red);background:#1a0a0a;min-width:140px}.tier-node.conflict-gate .node-name{color:var(--red)}.tier-diagram-wrapper{display:flex;gap:0}.tier-diagram-main{flex:1;min-width:0}.tier-density-sidebar{width:220px;flex-shrink:0;border-left:1px solid var(--border);background:var(--surface);padding:12px;overflow-y:auto;max-height:800px;font-size:.7rem}.tier-density-sidebar h4{font-size:.75rem;margin:0 0 8px;color:var(--text2);text-transform:uppercase;letter-spacing:.5px}.density-row{display:flex;align-items:center;gap:6px;margin:3px 0}.density-bar{height:6px;border-radius:3px;flex-shrink:0;min-width:2px}.density-label{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;color:var(--text2);max-width:120px}@media(max-width:900px){.tier-density-sidebar{display:none}.tier-diagram-wrapper{flex-direction:column}}.node-detail{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px;margin-top:12px;font-size:.85rem}.node-detail h4{margin:0 0 8px}.diagram-legend{display:flex;gap:16px;flex-wrap:wrap;margin:8px 0;font-size:.75rem;color:var(--text2)}.diagram-legend span{display:flex;align-items:center;gap:4px}.diagram-legend .leg-line{width:20px;height:2px;border-radius:1px}.tier-node.in-cycle{border-color:#ef4444;animation:cyclePulse 2s ease-in-out infinite}.cycle-badge{position:absolute;top:-6px;left:-6px;background:#ef4444;color:#fff;border-radius:50%;width:18px;height:18px;font-size:.6rem;display:flex;align-items:center;justify-content:center;z-index:4}.tier-node .expand-indicator{font-size:.6rem;color:var(--text2);margin-top:4px}.tier-node.expanded{border-color:var(--primary);box-shadow:0 0 0 2px #ff4b4b33}.tier-sub-band{padding:8px 20px 12px 40px;border-bottom:1px solid var(--border);background:#ffffff05;position:relative;z-index:2}.tier-sub-band .tier-band-label{font-size:.6rem;opacity:.6;margin-bottom:6px}.tier-sub-band .tier-nodes{gap:8px}.tier-sub-band .tier-node{min-width:100px;max-width:160px;padding:6px 10px}.tier-sub-band .tier-node .node-name{font-size:.75rem}.tier-node.path-selected{border-color:#faca15;box-shadow:0 0 0 3px #faca1566;z-index:10}.tier-node.path-selected:before{content:"✓";position:absolute;top:-8px;left:-8px;width:18px;height:18px;border-radius:50%;background:#faca15;color:#000;font-size:.65rem;font-weight:700;display:flex;align-items:center;justify-content:center;z-index:5}.tier-node.path-member{border-color:#faca15;background:#faca1514}.tier-node.path-dimmed{opacity:.12}.path-trace-toast{position:fixed;bottom:24px;left:50%;transform:translate(-50%);background:var(--surface2);border:1px solid var(--border);border-radius:8px;padding:10px 20px;font-size:.85rem;color:var(--text);z-index:1000;display:none;align-items:center;gap:12px;box-shadow:0 4px 24px #00000080}.path-trace-toast .toast-hint{color:var(--text2);font-size:.75rem}.path-trace-toast .toast-clear{cursor:pointer;color:var(--primary);font-weight:600;font-size:.8rem}.notebook-preview{background:var(--surface);border:1px solid var(--border);border-radius:8px;max-height:600px;overflow-y:auto;margin:12px 0}.notebook-cell{padding:10px 16px;border-bottom:1px solid var(--border);font-family:SF Mono,Monaco,Consolas,monospace;font-size:.78rem;line-height:1.5;white-space:pre-wrap;word-break:break-word}.notebook-cell:last-child{border-bottom:none}.notebook-cell.cell-md{background:#3b82f60f;color:#93c5fd}.notebook-cell.cell-sql{background:#a855f70f}.notebook-cell.cell-code{background:var(--surface)}.notebook-cell .cell-label{display:inline-block;padding:1px 8px;border-radius:3px;font-size:.65rem;font-weight:700;margin-bottom:4px;text-transform:uppercase;letter-spacing:.5px}.cell-label.lb-source{background:#1d3557;color:#93c5fd}.cell-label.lb-transform{background:#2d1b4e;color:#c4b5fd}.cell-label.lb-sink{background:#14532d;color:#86efac}.cell-label.lb-route{background:#422006;color:#fde68a}.cell-label.lb-process{background:#1e1b4b;color:#a5b4fc}.cell-label.lb-utility{background:#1f2937;color:#9ca3af}.cell-label.lb-config{background:#0c4a6e;color:#7dd3fc}.cell-label.lb-manual{background:#450a0a;color:#fca5a5}textarea,input[type=text],input[type=number],select{width:100%;padding:10px 14px;background:var(--surface);border:1px solid var(--border);border-radius:6px;color:var(--text);font-family:var(--mono);font-size:.9rem;resize:vertical}textarea:focus,input:focus{outline:none;border-color:var(--primary)}textarea{min-height:200px}label{display:block;font-size:.85rem;color:var(--text2);margin-bottom:6px;font-weight:500}.coverage-ring{width:120px;height:120px;border-radius:50%;position:relative;display:inline-flex;align-items:center;justify-content:center}.coverage-ring .ring-text{font-size:1.6rem;font-weight:800;z-index:1}.comparison-donuts{display:flex;gap:32px;justify-content:center;flex-wrap:wrap;margin:24px 0}.donut-chart{text-align:center;flex:0 0 160px}.donut-chart svg{filter:drop-shadow(0 2px 8px rgba(0,0,0,.3))}.donut-label{font-size:.85rem;font-weight:600;margin-top:8px;color:var(--text2);text-transform:uppercase;letter-spacing:.5px}.donut-sub{font-size:.75rem;color:var(--text2);margin-top:2px}.val-section{margin:16px 0;padding:16px;background:var(--surface2,#1e2030);border-radius:8px;border-left:4px solid var(--primary)}.val-section h4{margin:0 0 10px;color:var(--primary)}.val-score-ring{display:inline-flex;align-items:center;gap:8px;margin:4px 8px 4px 0}.val-score-ring svg{vertical-align:middle}.val-gap{background:var(--red)11;border:1px solid var(--red)44;border-radius:6px;padding:8px 12px;margin:4px 0;font-size:.85rem}.val-gap .gap-label{color:var(--red);font-weight:600}.val-ok{background:var(--green)11;border:1px solid var(--green)44;border-radius:6px;padding:8px 12px;margin:4px 0;font-size:.85rem}.val-warn{background:var(--amber)11;border:1px solid var(--amber)44;border-radius:6px;padding:8px 12px;margin:4px 0;font-size:.85rem}.val-matrix{display:grid;grid-template-columns:1fr 1fr;gap:12px}@media(max-width:768px){.val-matrix{grid-template-columns:1fr}}.val-item{display:flex;align-items:flex-start;gap:8px;padding:6px 0;border-bottom:1px solid var(--border);font-size:.85rem}.val-item:last-child{border-bottom:none}.val-dot{width:10px;height:10px;border-radius:50%;flex-shrink:0;margin-top:4px}.val-accel-card{background:var(--primary)11;border:1px solid var(--primary)44;border-radius:8px;padding:12px;margin:6px 0}.val-accel-card h5{margin:0 0 6px;color:var(--primary);font-size:.9rem}.val-accel-card pre{font-size:.78rem;margin:6px 0 0;padding:8px;background:var(--bg);border-radius:4px;overflow-x:auto}.sim-progress{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px 20px;margin:16px 0}.sim-progress-bar{height:10px;background:var(--surface2);border-radius:5px;overflow:hidden;margin:8px 0}.sim-progress-fill{height:100%;border-radius:5px;background:linear-gradient(90deg,var(--primary),#ff8a4b);transition:width .4s ease}.sim-status{font-size:.85rem;color:var(--text2);display:flex;justify-content:space-between;align-items:center}.sim-status .engine-label{font-weight:600}.sim-donuts{display:flex;gap:32px;justify-content:center;flex-wrap:wrap;margin:24px 0}.sim-split{display:grid;grid-template-columns:1fr 1fr;gap:0;margin:8px 0;border:1px solid var(--border);border-radius:6px;overflow:hidden}.sim-split-header{background:var(--surface2);padding:8px 12px;font-weight:600;font-size:.8rem;text-transform:uppercase;letter-spacing:.5px;text-align:center}.sim-split-header.nifi-side{border-right:1px solid var(--border);color:#728e9b}.sim-split-header.dbx-side{color:var(--primary)}.sim-split-cell{padding:8px 12px;font-size:.78rem;font-family:var(--mono);background:var(--surface);border-top:1px solid var(--border);overflow-x:auto;max-height:200px;overflow-y:auto}.sim-split-cell.nifi-side{border-right:1px solid var(--border)}.state-diff{display:grid;grid-template-columns:1fr 1fr;gap:16px;margin:12px 0}.state-diff-panel{background:var(--bg2);border-radius:8px;padding:12px}.state-diff-panel h4{margin:0 0 8px;font-size:.85rem;color:var(--text2)}.state-diff-panel pre{font-size:.72rem;max-height:200px;overflow-y:auto}.filter-toolbar{display:flex;gap:8px;flex-wrap:wrap;padding:12px;background:var(--surface);border:1px solid var(--border);border-radius:8px;margin-bottom:12px;align-items:center}.filter-toolbar .filter-group{display:flex;gap:4px;align-items:center}.filter-toolbar label{font-size:.75rem;color:var(--text2);margin-right:4px;white-space:nowrap}.filter-btn{padding:4px 10px;font-size:.75rem;border:1px solid var(--border);border-radius:4px;background:var(--surface2);color:var(--text2);cursor:pointer;transition:all .15s}.filter-btn:hover{border-color:var(--primary);color:var(--text)}.filter-btn.active{background:var(--primary);color:#fff;border-color:var(--primary)}.filter-search{padding:4px 8px;font-size:.75rem;border:1px solid var(--border);border-radius:4px;background:var(--bg);color:var(--text);width:160px}.filter-search::placeholder{color:var(--text2)}.density-row{cursor:pointer;padding:2px 4px;border-radius:4px;transition:background .15s}.density-row:hover{background:#ffffff0f}.density-row.filter-active{background:#faca1526;border:1px solid rgba(250,202,21,.4)}.density-row.filter-dimmed{opacity:.3}.sidebar-filter-hint{font-size:.6rem;color:var(--text2);margin-bottom:6px;font-style:italic}.sidebar-clear-btn{font-size:.65rem;color:var(--primary);cursor:pointer;margin-top:6px;display:none;text-align:center;padding:4px;border-radius:4px}.sidebar-clear-btn:hover{background:#ff4b4b1a}@keyframes pop{0%{transform:scale(0)}60%{transform:scale(1.3)}to{transform:scale(1)}}@keyframes spin{to{transform:rotate(360deg)}}@keyframes cyclePulse{0%,to{box-shadow:0 0 #ef444466}50%{box-shadow:0 0 0 6px #ef444400}}@keyframes simPulse{0%,to{opacity:1}50%{opacity:.5}}.sim-running{animation:simPulse 1s ease-in-out infinite}</style>
</head>
<body>
<div class="container">
  <h1>NiFi Flow Analyzer</h1>
  <p class="caption">Upload a NiFi flow XML &mdash; analyze processors, assess migration readiness, convert to Databricks, and generate migration reports</p>

  <div class="tabs" id="tabs">
    <div class="tab active" data-tab="load">1. Load Flow<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="analyze">2. Analyze<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="assess">3. Assess<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="convert">4. Convert<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="report">5. Report<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="reportFinal">6. Final Report<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="validate">7. Validate<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
    <div class="tab locked" data-tab="value">8. Value Analysis<span class="check">&#10003;</span><span class="spinner-sm"></span></div>
  </div>

  <!-- STEP 1: LOAD NIFI FLOW -->
  <div class="panel active" id="panel-load">
    <h2>Step 1: Load NiFi Flow</h2>
    <div class="row">
      <div class="col">
        <h3>Upload NiFi XML</h3>
        <div class="file-upload" id="fileDropZone">
          <p>Drop a NiFi flow XML here or click to browse</p>
          <p style="color:var(--text2);font-size:0.85rem">Supports NiFi templates, flow definitions, and registry exports</p>
          <input type="file" id="fileInput" accept=".xml,.json">
        </div>
        <div id="fileName" class="alert alert-info hidden" style="margin-top:12px"></div>
      </div>
      <div class="col">
        <h3>Paste NiFi XML</h3>
        <textarea id="pasteInput" placeholder="&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;template&gt;
  &lt;snippet&gt;
    &lt;processors&gt;...&lt;/processors&gt;
  &lt;/snippet&gt;
&lt;/template&gt;"></textarea>
      </div>
    </div>
    <div style="margin-top:16px;display:flex;gap:8px;flex-wrap:wrap;align-items:center">
      <button class="btn btn-primary" id="parseBtn">Analyze Flow</button>
      <div id="parseProgress" style="display:none;flex:1;min-width:300px;align-items:center;gap:10px">
        <div style="flex:0 0 200px;height:8px;background:var(--surface2);border-radius:4px;overflow:hidden"><div id="parsePBar" style="height:100%;width:0%;background:var(--primary);border-radius:4px;transition:width 0.15s"></div></div>
        <span id="parsePPct" style="font-size:0.8rem;font-weight:700;color:var(--primary);min-width:36px">0%</span>
        <span id="parsePStatus" style="font-size:0.78rem;color:var(--text2);white-space:nowrap;overflow:hidden;text-overflow:ellipsis;max-width:500px"></span>
      </div>
    </div>
    <div style="margin-top:20px;border:1px solid var(--border);border-radius:12px;padding:16px;background:var(--bg2)">
      <h3 style="margin:0 0 4px 0;font-size:0.95rem">Sample NiFi Flows &mdash; Click to Load &amp; Analyze</h3>
      <p style="color:var(--text2);font-size:0.8rem;margin:0 0 12px 0">Pre-built NiFi flow samples that demonstrate the full analysis pipeline.</p>
      <div style="display:grid;grid-template-columns:repeat(auto-fill,minmax(220px,1fr));gap:8px">
        <div class="sample-card" data-sample="etl">
          <div class="sample-icon">&#128640;</div>
          <div class="sample-info"><strong>ETL Pipeline</strong><span>NiFi XML &bull; 9 processors</span><span class="sample-tags">GetFile, SQL, Route, PutFile</span></div>
        </div>
        <div class="sample-card" data-sample="streaming">
          <div class="sample-icon">&#9889;</div>
          <div class="sample-info"><strong>Streaming IoT</strong><span>NiFi XML &bull; 10 processors</span><span class="sample-tags">Kafka, JSON, Merge, HDFS</span></div>
        </div>
        <div class="sample-card" data-sample="full">
          <div class="sample-icon">&#127981;</div>
          <div class="sample-info"><strong>Manufacturing Migration</strong><span>NiFi XML &bull; 17 processors</span><span class="sample-tags">ListFile, SFTP, Wait/Notify, SQL</span></div>
        </div>
      </div>
    </div>
    <div id="parseResults"></div>
  </div>

  <!-- STEP 2: ANALYZE -->
  <div class="panel" id="panel-analyze">
    <h2>Step 2: Flow Analysis</h2>
    <div id="analyzeNotReady" class="alert alert-info">Load a NiFi flow first (Step 1).</div>
    <div id="analyzeReady" class="hidden">
      <button class="btn btn-primary" id="analyzeBtn">Run Deep Analysis</button>
      <div id="analyzeResults"></div>
    </div>
  </div>

  <!-- STEP 3: ASSESS -->
  <div class="panel" id="panel-assess">
    <h2>Step 3: Migration Assessment</h2>
    <div id="assessNotReady" class="alert alert-info">Complete the flow analysis first (Step 2).</div>
    <div id="assessReady" class="hidden">
      <button class="btn btn-primary" id="assessBtn">Run Assessment</button>
      <div id="assessResults"></div>
    </div>
  </div>

  <!-- STEP 4: CONVERT -->
  <div class="panel" id="panel-convert">
    <h2>Step 4: NiFi &rarr; Databricks Notebook</h2>
    <div id="convertNotReady" class="alert alert-info">Complete the assessment first (Step 3).</div>
    <div id="convertReady" class="hidden">
      <p style="color:var(--text2);font-size:0.9rem;margin-bottom:12px">
        Reverse-engineer your NiFi flow into a Databricks Python notebook with PySpark equivalents,
        Unity Catalog definitions, and a Databricks workflow.
      </p>
      <div class="expander" id="dbxConfigExpander"><div class="expander-header" id="dbxConfigExpanderHeader"><span>Databricks Configuration</span><span class="expander-arrow">&#9654;</span></div><div class="expander-body">
        <p style="font-size:0.82rem;color:var(--text2);margin-bottom:12px">Configure these to generate a runnable notebook with resolved placeholders. Leave blank for generic templates.</p>
        <div class="row">
          <div class="col"><label>Unity Catalog</label><input type="text" id="cfgCatalog" placeholder="e.g. main"></div>
          <div class="col"><label>Schema</label><input type="text" id="cfgSchema" placeholder="e.g. nifi_migration"></div>
          <div class="col"><label>Secret Scope</label><input type="text" id="cfgScope" placeholder="e.g. migration_secrets"></div>
        </div>
        <div class="row" style="margin-top:8px">
          <div class="col"><label>Cloud Provider</label><select id="cfgCloud"><option value="azure">Azure</option><option value="aws">AWS</option><option value="gcp">GCP</option></select></div>
          <div class="col"><label>Spark Version</label><input type="text" id="cfgSparkVersion" value="14.3.x-scala2.12"></div>
          <div class="col"><label>Node Type</label><input type="text" id="cfgNodeType" value="Standard_DS3_v2"></div>
        </div>
        <div class="row" style="margin-top:8px">
          <div class="col"><label>Workers</label><input type="number" id="cfgWorkers" value="2" min="1" max="100"></div>
          <div class="col" style="flex:2"><label>Workspace Path</label><input type="text" id="cfgWorkspacePath" value="/Workspace/Migrations/NiFi"></div>
        </div>
        <div style="margin-top:8px"><button class="btn" id="saveDbxConfigBtn">Save Configuration</button></div>
      </div></div>
      <div style="margin-top:12px"><button class="btn btn-primary" id="generateNotebookBtn">Generate Notebook</button></div>
      <div id="notebookResults"></div>
    </div>
  </div>

  <!-- STEP 5: MIGRATION REPORT -->
  <div class="panel" id="panel-report">
    <h2>Step 5: Migration Report</h2>
    <div id="reportNotReady" class="alert alert-info">Generate the notebook first (Step 4).</div>
    <div id="reportReady" class="hidden">
      <button class="btn btn-primary" id="reportBtn">Generate Report</button>
      <div id="reportResults"></div>
    </div>
  </div>

  <!-- STEP 6: FINAL REPORT -->
  <div class="panel" id="panel-reportFinal">
    <h2>Step 6: Final Report</h2>
    <div id="reportFinalNotReady" class="alert alert-info">Generate the migration report first (Step 5).</div>
    <div id="reportFinalReady" class="hidden">
      <p style="color:var(--text2);font-size:0.9rem;margin-bottom:12px">
        Comprehensive end-to-end analysis &mdash; every processor, every gap, every recommendation. Download as JSON.
      </p>
      <button class="btn btn-primary" id="finalReportBtn">Generate Final Report</button>
      <div id="reportFinalResults"></div>
    </div>
  </div>

  <!-- STEP 7: VALIDATE -->
  <div class="panel" id="panel-validate">
    <h2>Step 7: Notebook &harr; Flow Validation</h2>
    <div id="validateNotReady" class="alert alert-info">Generate the final report first (Step 6).</div>
    <div id="validateReady" class="hidden">
      <p style="color:var(--text2);font-size:0.9rem;margin-bottom:12px">
        Four-angle comparison between the generated PySpark notebook and the original NiFi flow.
        Identifies gaps, manual items, and feeds corrections back into the accelerator.
      </p>
      <button class="btn btn-primary" id="validateBtn">Run Validation</button>
      <div id="validateResults"></div>
    </div>
  </div>

  <!-- STEP 8: VALUE ANALYSIS -->
  <div class="panel" id="panel-value">
    <h2>Step 8: Workflow Value Analysis</h2>
    <div id="valueNotReady" class="alert alert-info">Complete validation first (Step 7).</div>
    <div id="valueReady" class="hidden">
      <p style="color:var(--text2);font-size:0.9rem;margin-bottom:12px">
        Holistic migration value assessment &mdash; what the flow does, how to build it better in Databricks,
        what steps can be dropped, and quantified migration ROI.
      </p>
      <button class="btn btn-primary" id="valueBtn">Run Value Analysis</button>
      <div id="valueResults"></div>
    </div>
  </div>
</div>
<div class="path-trace-toast" id="pathTraceToast">
  <span id="pathTraceText"></span>
  <span class="toast-hint">Click another node to extend path</span>
  <span class="toast-clear" id="pathTraceClear">Clear</span>
</div>

</body>
</html>
