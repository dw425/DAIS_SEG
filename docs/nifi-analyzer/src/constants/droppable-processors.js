// Processors that can be dropped during migration — Spark handles these natively
export const DROPPABLE_PROCESSORS = {
  MergeContent: { reason: 'Spark handles partitioned reads natively — no need to merge small files manually', savings: 'medium', risk: 'low' },
  MergeRecord: { reason: 'Spark handles partitioned reads natively — no need to merge records manually', savings: 'medium', risk: 'low' },
  CompressContent: { reason: 'Delta Lake handles compression (zstd/snappy) automatically', savings: 'low', risk: 'none' },
  UnpackContent: { reason: 'Delta Lake handles decompression automatically on read', savings: 'low', risk: 'none' },
  SplitText: { reason: 'Spark reads entire datasets at once — no need to split text into individual records', savings: 'medium', risk: 'low' },
  SplitJson: { reason: 'Spark reads JSON files as DataFrames natively — no splitting needed', savings: 'medium', risk: 'low' },
  SplitXml: { reason: 'spark-xml reads entire XML documents — no splitting needed', savings: 'medium', risk: 'low' },
  SplitContent: { reason: 'Spark operates on entire partitions — no content splitting needed', savings: 'medium', risk: 'low' },
  SplitAvro: { reason: 'Spark reads Avro files as DataFrames natively', savings: 'medium', risk: 'low' },
  SplitRecord: { reason: 'Spark operates on entire DataFrames — individual record splitting unnecessary', savings: 'medium', risk: 'low' },
  UpdateAttribute: { reason: 'Use .withColumn() to add/modify columns — no separate attribute update step', savings: 'low', risk: 'none' },
  RouteOnAttribute: { reason: 'Use .filter() or .when() for simple attribute-based routing', savings: 'low', risk: 'low' },
  LogAttribute: { reason: 'Use Spark logging or display() — no dedicated log processor needed', savings: 'low', risk: 'none' },
  LogMessage: { reason: 'Use print() or logging module — no dedicated log processor needed', savings: 'low', risk: 'none' },
  Wait: { reason: 'Use Databricks Workflows task dependencies instead of in-flow waits', savings: 'medium', risk: 'low' },
  Notify: { reason: 'Use Databricks Workflows task dependencies instead of notifications', savings: 'medium', risk: 'low' },
  DetectDuplicate: { reason: 'Use dropDuplicates() — built into Spark DataFrame API', savings: 'medium', risk: 'low' },
  ControlRate: { reason: 'Spark handles backpressure natively via Structured Streaming', savings: 'low', risk: 'none' },
  DistributeLoad: { reason: 'Spark handles data distribution across executors automatically', savings: 'medium', risk: 'none' },
  ValidateRecord: { reason: 'Use DLT expectations for declarative data quality rules', savings: 'low', risk: 'low' },
  RetryFlowFile: { reason: 'Use Spark retry mechanisms or Workflows retry policies', savings: 'low', risk: 'low' },
  MonitorActivity: { reason: 'Use Databricks Workflows monitoring and Spark UI', savings: 'low', risk: 'none' },
  DebugFlow: { reason: 'Use Spark UI, display(), or notebook debugging — no dedicated debug processor', savings: 'low', risk: 'none' },
  CountText: { reason: 'Use df.count() — built into Spark DataFrame API', savings: 'low', risk: 'none' },
  AttributesToJSON: { reason: 'Use to_json() — built into PySpark', savings: 'low', risk: 'none' },
  GenerateFlowFile: { reason: 'Use spark.range() or createDataFrame() for test data generation', savings: 'low', risk: 'none' },
  EnforceOrder: { reason: 'Use orderBy() — built into Spark DataFrame API', savings: 'low', risk: 'none' },
  Funnel: { reason: 'Use DataFrame union() — NiFi funnels have no Databricks equivalent needed', savings: 'low', risk: 'none' },
  InputPort: { reason: 'NiFi ports not needed — data flows handled by notebooks/jobs', savings: 'low', risk: 'none' },
  OutputPort: { reason: 'NiFi ports not needed — data flows handled by notebooks/jobs', savings: 'low', risk: 'none' },
};
