# Pentaho Data Integration (Kettle/PDI) -> Databricks PySpark Mapping
# Maps Pentaho transformation steps to PySpark equivalents

mappings:
  - type: "Table Input"
    category: "JDBC Source"
    template: |
      df_{name} = (spark.read
          .format("jdbc")
          .option("url", dbutils.secrets.get(scope="{scope}", key="jdbc-url"))
          .option("dbtable", "({sql}) subq")
          .option("driver", "{driver}")
          .option("user", dbutils.secrets.get(scope="{scope}", key="db-user"))
          .option("password", dbutils.secrets.get(scope="{scope}", key="db-pass"))
          .load())
    description: "Table Input step via JDBC query"
    imports: []
    confidence: 0.92
    role: "source"

  - type: "CSV File Input"
    category: "File Read"
    template: |
      df_{name} = (spark.read
          .format("csv")
          .option("header", "true")
          .option("delimiter", "{delimiter}")
          .option("inferSchema", "true")
          .option("encoding", "{encoding}")
          .load("/Volumes/{catalog}/{schema}/landing/{filename}"))
    description: "CSV file input from Volumes"
    imports: []
    confidence: 0.92
    role: "source"

  - type: "Text File Input"
    category: "File Read"
    template: |
      df_{name} = (spark.read
          .format("csv")
          .option("header", "{header}")
          .option("delimiter", "{separator}")
          .option("inferSchema", "true")
          .load("/Volumes/{catalog}/{schema}/landing/{filename}"))
    description: "Text file input (fixed-width or delimited)"
    imports: []
    confidence: 0.90
    role: "source"

  - type: "JSON Input"
    category: "JSON Read"
    template: |
      df_{name} = spark.read.json("/Volumes/{catalog}/{schema}/landing/{filename}")
    description: "JSON file input"
    imports: []
    confidence: 0.92
    role: "source"

  - type: "XML Input"
    category: "XML Read"
    template: |
      df_{name} = (spark.read
          .format("com.databricks.spark.xml")
          .option("rowTag", "{row_tag}")
          .load("/Volumes/{catalog}/{schema}/landing/{filename}"))
    description: "XML file input via spark-xml"
    imports: []
    confidence: 0.88
    role: "source"

  - type: "Sort Rows"
    category: "DataFrame API"
    template: |
      df_{name} = df_{input}.orderBy(
          col("{sort_field_1}").asc(),
          col("{sort_field_2}").desc())
    description: "Sort rows by multiple columns"
    imports: ["from pyspark.sql.functions import col"]
    confidence: 0.95
    role: "transform"

  - type: "Select Values"
    category: "DataFrame Select"
    template: |
      df_{name} = (df_{input}
          .select(
              col("{field_1}").alias("{rename_1}"),
              col("{field_2}").cast("{type_2}").alias("{rename_2}"))
          .drop("{remove_field}"))
    description: "Select, rename, retype, and remove fields"
    imports: ["from pyspark.sql.functions import col"]
    confidence: 0.92
    role: "transform"

  - type: "Filter Rows"
    category: "DataFrame Filter"
    template: |
      df_{name} = df_{input}.filter("{condition}")
    description: "Filter rows by condition"
    imports: []
    confidence: 0.95
    role: "route"

  - type: "Calculator"
    category: "DataFrame API"
    template: |
      df_{name} = (df_{input}
          .withColumn("{result_field}", expr("{calculation}")))
    description: "Calculator step via withColumn + expr"
    imports: ["from pyspark.sql.functions import expr"]
    confidence: 0.90
    role: "transform"

  - type: "String Operations"
    category: "DataFrame API"
    template: |
      df_{name} = (df_{input}
          .withColumn("{field}", trim(col("{field}")))
          .withColumn("{field}", upper(col("{field}")))
          .withColumn("{field}", regexp_replace(col("{field}"), "{pattern}", "{replacement}")))
    description: "String operations via PySpark string functions"
    imports: ["from pyspark.sql.functions import trim, upper, lower, regexp_replace, col"]
    confidence: 0.92
    role: "transform"

  - type: "Join Rows"
    category: "DataFrame Join"
    template: |
      df_{name} = df_{input_1}.join(
          df_{input_2},
          on=df_{input_1}["{key_1}"] == df_{input_2}["{key_2}"],
          how="{join_type}")
    description: "Join rows from two streams"
    imports: []
    confidence: 0.92
    role: "transform"

  - type: "Merge Join"
    category: "DataFrame Join"
    template: |
      df_{name} = df_{input_1}.join(
          df_{input_2},
          on="{join_key}",
          how="{join_type}")
    description: "Sorted merge join"
    imports: []
    confidence: 0.92
    role: "transform"

  - type: "Group By"
    category: "DataFrame API"
    template: |
      df_{name} = (df_{input}
          .groupBy("{group_field}")
          .agg(
              count("*").alias("count"),
              sum("{agg_field}").alias("sum_{agg_field}"),
              avg("{agg_field}").alias("avg_{agg_field}")))
    description: "Group by with aggregate functions"
    imports: ["from pyspark.sql.functions import count, sum, avg"]
    confidence: 0.92
    role: "transform"

  - type: "Unique Rows"
    category: "DataFrame API"
    template: |
      df_{name} = df_{input}.dropDuplicates(["{key_field}"])
    description: "Remove duplicate rows"
    imports: []
    confidence: 0.95
    role: "transform"

  - type: "Table Output"
    category: "Delta Write"
    template: |
      (df_{input}.write
          .format("delta")
          .mode("{truncate_table}")
          .saveAsTable("{catalog}.{schema}.{target_table}"))
    description: "Table output to Delta Lake"
    imports: []
    confidence: 0.92
    role: "sink"

  - type: "Insert / Update"
    category: "Delta MERGE"
    template: |
      from delta.tables import DeltaTable
      _target = DeltaTable.forName(spark, "{catalog}.{schema}.{table}")
      _target.alias("t").merge(
          df_{input}.alias("s"),
          "t.{key_field} = s.{key_field}"
      ).whenMatchedUpdateAll(
      ).whenNotMatchedInsertAll(
      ).execute()
    description: "Insert/Update via Delta MERGE upsert"
    imports: ["from delta.tables import DeltaTable"]
    confidence: 0.92
    role: "sink"

  - type: "Text File Output"
    category: "File Write"
    template: |
      (df_{input}.write
          .format("csv")
          .option("header", "{header}")
          .option("delimiter", "{separator}")
          .mode("overwrite")
          .save("/Volumes/{catalog}/{schema}/output/{filename}"))
    description: "Text file output to Volumes"
    imports: []
    confidence: 0.92
    role: "sink"

  - type: "Switch / Case"
    category: "DataFrame Filter"
    template: |
      df_{name}_case1 = df_{input}.filter("{condition_1}")
      df_{name}_case2 = df_{input}.filter("{condition_2}")
      df_{name}_default = df_{input}.filter(
          "NOT ({condition_1}) AND NOT ({condition_2})")
    description: "Switch/Case via multiple DataFrame filters"
    imports: []
    confidence: 0.90
    role: "route"

  - type: "Add Sequence"
    category: "DataFrame API"
    template: |
      from pyspark.sql.functions import monotonically_increasing_id
      df_{name} = df_{input}.withColumn("{valuename}",
          monotonically_increasing_id() + {start_at})
    description: "Add sequence number via monotonically_increasing_id"
    imports: ["from pyspark.sql.functions import monotonically_increasing_id"]
    confidence: 0.90
    role: "transform"

  - type: "Row Denormaliser"
    category: "DataFrame Pivot"
    template: |
      df_{name} = (df_{input}
          .groupBy("{group_field}")
          .pivot("{key_field}")
          .agg(first("{value_field}")))
    description: "Row denormalizer via pivot"
    imports: ["from pyspark.sql.functions import first"]
    confidence: 0.88
    role: "transform"
