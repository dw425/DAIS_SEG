mappings:
- type: Source (Database)
  category: JDBC Source
  template: "df_{name} = (spark.read\n    .format(\"jdbc\")\n    .option(\"url\", dbutils.secrets.get(scope=\"{scope}\", key=\"\
    jdbc-url\"))\n    .option(\"dbtable\", \"{table}\")\n    .option(\"driver\", \"{driver}\")\n    .option(\"user\", dbutils.secrets.get(scope=\"\
    {scope}\", key=\"db-user\"))\n    .option(\"password\", dbutils.secrets.get(scope=\"{scope}\", key=\"db-pass\"))\n   \
    \ .load())\n"
  description: Airbyte DB source replaced by JDBC read
  imports: []
  confidence: 0.92
  role: source
- type: Source (API)
  category: REST API
  template: 'import requests

    _token = dbutils.secrets.get(scope="{scope}", key="api-token")

    _response = requests.get("{api_url}", headers={"Authorization": f"Bearer {{_token}}"}, timeout=60)

    _response.raise_for_status()

    df_{name} = spark.createDataFrame(_response.json()["{data_key}"])

    '
  description: Airbyte API source replaced by requests + createDataFrame
  imports:
  - import requests
  confidence: 0.85
  role: source
- type: Source (File)
  category: Auto Loader
  template: "df_{name} = (spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"{format}\")\n\
    \    .load(\"{source_path}\"))\n"
  description: Airbyte file source replaced by Auto Loader
  imports: []
  confidence: 0.92
  role: source
- type: Destination (Database)
  category: Delta Write
  template: "(df_{input}.write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"{catalog}.{schema}.{table}\"\
    ))\n"
  description: Airbyte destination replaced by Delta write
  imports: []
  confidence: 0.92
  role: sink
- type: Sync (Full Refresh)
  category: Delta Overwrite
  template: "(df_{input}.write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"overwriteSchema\", \"true\"\
    )\n    .saveAsTable(\"{catalog}.{schema}.{table}\"))\n"
  description: Full refresh sync via Delta overwrite
  imports: []
  confidence: 0.92
  role: sink
- type: Sync (Incremental Append)
  category: Delta Append
  template: "(df_{input}.write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"{catalog}.{schema}.{table}\"\
    ))\n"
  description: Incremental append sync via Delta append
  imports: []
  confidence: 0.92
  role: sink
- type: Sync (Incremental Deduped)
  category: Delta MERGE
  template: "from delta.tables import DeltaTable\n_target = DeltaTable.forName(spark, \"{catalog}.{schema}.{table}\")\n_target.alias(\"\
    t\").merge(\n    df_{input}.alias(\"s\"),\n    \"t.{cursor_field} = s.{cursor_field} AND t.{primary_key} = s.{primary_key}\"\
    \n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n"
  description: Incremental deduped sync via Delta MERGE
  imports:
  - from delta.tables import DeltaTable
  confidence: 0.9
  role: sink
- type: Normalization
  category: DataFrame Flatten
  template: "# Airbyte normalization -> flatten nested JSON into typed columns\ndf_{name} = (df_{input}\n    .select(\"*\"\
    , col(\"_airbyte_data.*\"))\n    .drop(\"_airbyte_data\", \"_airbyte_ab_id\", \"_airbyte_emitted_at\"))\n"
  description: Airbyte normalization replaced by struct flattening
  imports:
  - from pyspark.sql.functions import col
  confidence: 0.85
  role: transform
- type: source_postgres
  category: JDBC Source
  template: "df_{name} = (spark.read\n    .format('jdbc')\n    .option('url', 'jdbc:postgresql://{host}:{port}/{database}')\n\
    \    .option('dbtable', '{table}')\n    .option('user', dbutils.secrets.get(scope='{scope}', key='pg-user'))\n    .option('password',\
    \ dbutils.secrets.get(scope='{scope}', key='pg-pass'))\n    .load())\n"
  description: Airbyte source_postgres as JDBC read
  imports: []
  confidence: 0.92
  role: source
- type: source_mysql
  category: JDBC Source
  template: "df_{name} = (spark.read\n    .format('jdbc')\n    .option('url', 'jdbc:mysql://{host}:{port}/{database}')\n \
    \   .option('dbtable', '{table}')\n    .option('user', dbutils.secrets.get(scope='{scope}', key='mysql-user'))\n    .option('password',\
    \ dbutils.secrets.get(scope='{scope}', key='mysql-pass'))\n    .load())\n"
  description: Airbyte source_mysql as JDBC read
  imports: []
  confidence: 0.92
  role: source
- type: source_mssql
  category: JDBC Source
  template: "df_{name} = (spark.read\n    .format('jdbc')\n    .option('url', 'jdbc:sqlserver://{host}:{port};databaseName={database}')\n\
    \    .option('dbtable', '{table}')\n    .option('user', dbutils.secrets.get(scope='{scope}', key='mssql-user'))\n    .option('password',\
    \ dbutils.secrets.get(scope='{scope}', key='mssql-pass'))\n    .load())\n"
  description: Airbyte source_mssql as JDBC read
  imports: []
  confidence: 0.92
  role: source
- type: source_oracle
  category: JDBC Source
  template: "df_{name} = (spark.read\n    .format('jdbc')\n    .option('url', 'jdbc:oracle:thin:@{host}:{port}:{sid}')\n \
    \   .option('dbtable', '{table}')\n    .option('driver', 'oracle.jdbc.OracleDriver')\n    .option('user', dbutils.secrets.get(scope='{scope}',\
    \ key='oracle-user'))\n    .option('password', dbutils.secrets.get(scope='{scope}', key='oracle-pass'))\n    .load())\n"
  description: Airbyte source_oracle as JDBC read
  imports: []
  confidence: 0.92
  role: source
- type: source_mongodb
  category: MongoDB Source
  template: "df_{name} = (spark.read\n    .format('mongo')\n    .option('uri', dbutils.secrets.get(scope='{scope}', key='mongo-uri'))\n\
    \    .option('database', '{database}')\n    .option('collection', '{collection}')\n    .load())\n"
  description: Airbyte source_mongodb as spark-mongo read
  imports: []
  confidence: 0.85
  role: source
- type: source_salesforce
  category: API Source
  template: "%pip install simple_salesforce\nfrom simple_salesforce import Salesforce\nsf = Salesforce(\n    username=dbutils.secrets.get(scope='{scope}',\
    \ key='sf-user'),\n    password=dbutils.secrets.get(scope='{scope}', key='sf-pass'),\n    security_token=dbutils.secrets.get(scope='{scope}',\
    \ key='sf-token'))\n_records = sf.query_all(\"{soql}\")\ndf_{name} = spark.createDataFrame(_records['records'])\n"
  description: Airbyte source_salesforce via simple_salesforce
  imports:
  - from simple_salesforce import Salesforce
  confidence: 0.82
  role: source
- type: source_hubspot
  category: API Source
  template: "import requests\n_token = dbutils.secrets.get(scope='{scope}', key='hubspot-token')\n_response = requests.get(\n\
    \    'https://api.hubapi.com/crm/v3/objects/{object_type}',\n    headers={{'Authorization': f'Bearer {{_token}}'}}, timeout=60)\n\
    _response.raise_for_status()\ndf_{name} = spark.createDataFrame(_response.json()['results'])\n"
  description: Airbyte source_hubspot via HubSpot API
  imports:
  - import requests
  confidence: 0.8
  role: source
- type: source_stripe
  category: API Source
  template: "import requests\n_key = dbutils.secrets.get(scope='{scope}', key='stripe-key')\n_response = requests.get(\n \
    \   'https://api.stripe.com/v1/{resource}',\n    auth=(_key, ''), timeout=60)\n_response.raise_for_status()\ndf_{name}\
    \ = spark.createDataFrame(_response.json()['data'])\n"
  description: Airbyte source_stripe via Stripe API
  imports:
  - import requests
  confidence: 0.8
  role: source
- type: source_shopify
  category: API Source
  template: "import requests\n_token = dbutils.secrets.get(scope='{scope}', key='shopify-token')\n_response = requests.get(\n\
    \    'https://{shop}.myshopify.com/admin/api/2024-01/{resource}.json',\n    headers={{'X-Shopify-Access-Token': _token}},\
    \ timeout=60)\n_response.raise_for_status()\ndf_{name} = spark.createDataFrame(_response.json()['{resource}'])\n"
  description: Airbyte source_shopify via Shopify API
  imports:
  - import requests
  confidence: 0.8
  role: source
- type: source_zendesk
  category: API Source
  template: "import requests\n_auth = (dbutils.secrets.get(scope='{scope}', key='zendesk-email') + '/token',\n         dbutils.secrets.get(scope='{scope}',\
    \ key='zendesk-token'))\n_response = requests.get(\n    'https://{subdomain}.zendesk.com/api/v2/{resource}.json',\n  \
    \  auth=_auth, timeout=60)\n_response.raise_for_status()\ndf_{name} = spark.createDataFrame(_response.json()['{resource}'])\n"
  description: Airbyte source_zendesk via Zendesk API
  imports:
  - import requests
  confidence: 0.8
  role: source
- type: source_jira
  category: API Source
  template: "import requests\n_auth = (dbutils.secrets.get(scope='{scope}', key='jira-email'),\n         dbutils.secrets.get(scope='{scope}',\
    \ key='jira-token'))\n_response = requests.get(\n    'https://{domain}.atlassian.net/rest/api/3/search?jql={jql}',\n \
    \   auth=_auth, timeout=60)\n_response.raise_for_status()\ndf_{name} = spark.createDataFrame(_response.json()['issues'])\n"
  description: Airbyte source_jira via Jira REST API
  imports:
  - import requests
  confidence: 0.8
  role: source
- type: destination_bigquery
  category: BigQuery Sink
  template: "(df_{input}.write\n    .format('bigquery')\n    .option('table', '{project}.{dataset}.{table}')\n    .option('temporaryGcsBucket',\
    \ '{temp_bucket}')\n    .mode('{mode}')\n    .save())\n"
  description: Airbyte destination_bigquery as spark-bigquery write
  imports: []
  confidence: 0.85
  role: sink
- type: destination_snowflake
  category: JDBC Sink
  template: "(df_{input}.write\n    .format('jdbc')\n    .option('url', 'jdbc:snowflake://{account}.snowflakecomputing.com')\n\
    \    .option('dbtable', '{table}')\n    .option('sfDatabase', '{database}')\n    .option('sfSchema', '{schema}')\n   \
    \ .option('user', dbutils.secrets.get(scope='{scope}', key='sf-user'))\n    .option('password', dbutils.secrets.get(scope='{scope}',\
    \ key='sf-pass'))\n    .mode('{mode}')\n    .save())\n"
  description: Airbyte destination_snowflake as JDBC write
  imports: []
  confidence: 0.85
  role: sink
- type: destination_redshift
  category: JDBC Sink
  template: "(df_{input}.write\n    .format('jdbc')\n    .option('url', 'jdbc:redshift://{host}:{port}/{database}')\n    .option('dbtable',\
    \ '{table}')\n    .option('user', dbutils.secrets.get(scope='{scope}', key='rs-user'))\n    .option('password', dbutils.secrets.get(scope='{scope}',\
    \ key='rs-pass'))\n    .mode('{mode}')\n    .save())\n"
  description: Airbyte destination_redshift as JDBC write
  imports: []
  confidence: 0.85
  role: sink
- type: destination_s3
  category: S3 Sink
  template: 'df_{input}.write.format(''{format}'').mode(''{mode}'').save(''s3a://{bucket}/{prefix}'')

    print(''[S3] Written'')

    '
  description: Airbyte destination_s3 as Spark S3 write
  imports: []
  confidence: 0.92
  role: sink
- type: destination_gcs
  category: GCS Sink
  template: 'df_{input}.write.format(''{format}'').mode(''{mode}'').save(''gs://{bucket}/{prefix}'')

    print(''[GCS] Written'')

    '
  description: Airbyte destination_gcs as Spark GCS write
  imports: []
  confidence: 0.92
  role: sink
- type: destination_local_json
  category: JSON Sink
  template: 'df_{input}.write.format(''json'').mode(''overwrite'').save(''/Volumes/{catalog}/{schema}/output/{path}'')

    print(''[JSON] Written'')

    '
  description: Airbyte destination_local_json as Spark JSON write
  imports: []
  confidence: 0.9
  role: sink
