mappings:
- type: Table Input
  category: Spark Table Read
  template: 'df_{name} = spark.table("{catalog}.{schema}.{table}")

    '
  description: Matillion Table Input via spark.table
  imports: []
  confidence: 0.95
  role: source
- type: Database Query
  category: JDBC Source
  template: "df_{name} = (spark.read\n    .format(\"jdbc\")\n    .option(\"url\", dbutils.secrets.get(scope=\"{scope}\", key=\"\
    jdbc-url\"))\n    .option(\"dbtable\", \"({sql}) subq\")\n    .option(\"driver\", \"{driver}\")\n    .load())\n"
  description: Database Query via JDBC
  imports: []
  confidence: 0.92
  role: source
- type: S3 Load
  category: Cloud Storage Read
  template: "df_{name} = (spark.read\n    .format(\"{format}\")\n    .option(\"header\", \"true\")\n    .load(\"s3://{bucket}/{key}\"\
    ))\n"
  description: S3 Load via Spark read
  imports: []
  confidence: 0.92
  role: source
- type: Table Output
  category: Delta Write
  template: "(df_{input}.write\n    .format(\"delta\")\n    .mode(\"{write_mode}\")\n    .saveAsTable(\"{catalog}.{schema}.{target_table}\"\
    ))\n"
  description: Table Output to Delta Lake
  imports: []
  confidence: 0.92
  role: sink
- type: S3 Unload
  category: Cloud Storage Write
  template: "(df_{input}.write\n    .format(\"{format}\")\n    .option(\"header\", \"true\")\n    .mode(\"overwrite\")\n \
    \   .save(\"s3://{bucket}/{prefix}\"))\n"
  description: S3 Unload via Spark write
  imports: []
  confidence: 0.92
  role: sink
- type: Calculator
  category: DataFrame API
  template: "df_{name} = df_{input}.withColumn(\"{output_column}\",\n    expr(\"{calculation_expression}\"))\n"
  description: Calculator via withColumn + expr
  imports:
  - from pyspark.sql.functions import expr
  confidence: 0.9
  role: transform
- type: Filter
  category: DataFrame Filter
  template: 'df_{name} = df_{input}.filter("{filter_expression}")

    '
  description: Row filter
  imports: []
  confidence: 0.95
  role: route
- type: Join
  category: DataFrame Join
  template: "df_{name} = df_{input_left}.join(\n    df_{input_right},\n    on=\"{join_condition}\",\n    how=\"{join_type}\"\
    )\n"
  description: Join via DataFrame join
  imports: []
  confidence: 0.92
  role: transform
- type: Python Script
  category: Python Execution
  template: '# Matillion Python Script -> Databricks Python

    {script_body}

    '
  description: Python Script runs natively in Databricks
  imports: []
  confidence: 0.92
  role: process
- type: SQL Script
  category: Spark SQL
  template: 'spark.sql("""

    {sql_script}

    """)

    '
  description: SQL Script via spark.sql
  imports: []
  confidence: 0.95
  role: process
- type: Create Table
  category: Delta DDL
  template: "spark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table} (\n        {column_definitions}\n\
    \    ) USING delta\n\"\"\")\n"
  description: Create Table as Delta Lake DDL
  imports: []
  confidence: 0.95
  role: utility
- type: Detect Changes
  category: Delta CDF
  template: "spark.sql(\"ALTER TABLE {catalog}.{schema}.{table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n\
    df_{name} = (spark.readStream\n    .format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"{catalog}.{schema}.{table}\"\
    ))\n"
  description: Change detection via Delta Change Data Feed
  imports: []
  confidence: 0.9
  role: source
- type: Aggregate
  category: DataFrame API
  template: "df_{name} = (df_{input}\n    .groupBy(\"{group_by}\")\n    .agg(\n        count(\"*\").alias(\"row_count\"),\n\
    \        sum(\"{measure}\").alias(\"total\")))\n"
  description: Aggregation via groupBy + agg
  imports:
  - from pyspark.sql.functions import count, sum
  confidence: 0.92
  role: transform
- type: Rank
  category: Window Function
  template: 'from pyspark.sql.window import Window

    _w = Window.partitionBy("{partition_by}").orderBy(col("{order_by}").desc())

    df_{name} = df_{input}.withColumn("rank", row_number().over(_w))

    '
  description: Rank via window function
  imports:
  - from pyspark.sql.functions import row_number, col
  - from pyspark.sql.window import Window
  confidence: 0.92
  role: transform
- type: Rewrite Table
  category: Delta Overwrite
  template: 'df_{input}.write.format(''delta'').mode(''overwrite'').option(''overwriteSchema'', ''true'').saveAsTable(''{catalog}.{schema}.{table}'')

    print(''[REWRITE] Table rewritten'')

    '
  description: Matillion Rewrite Table as Delta overwrite with schema
  imports: []
  confidence: 0.92
  role: sink
- type: Pivot
  category: Pivot
  template: "from pyspark.sql.functions import first\ndf_{name} = (df_{input}\n    .groupBy({group_cols})\n    .pivot('{pivot_column}')\n\
    \    .agg(first('{value_column}')))\n"
  description: Matillion Pivot as Spark groupBy.pivot
  imports:
  - from pyspark.sql.functions import first
  confidence: 0.92
  role: transform
- type: Unpivot
  category: Unpivot
  template: "df_{name} = spark.sql(\"\"\"\n    SELECT {id_cols},\n           stack({n_cols}, {stack_expressions}) AS (attribute,\
    \ value)\n    FROM {{df_{input}.createOrReplaceTempView('_unpivot_in'); '_unpivot_in'}}\n\"\"\")\n"
  description: Matillion Unpivot as Spark SQL stack
  imports: []
  confidence: 0.88
  role: transform
- type: Distinct
  category: Deduplicate
  template: 'df_{name} = df_{input}.distinct()

    print(f''[DISTINCT] {{df_{name}.count()}} unique rows'')

    '
  description: Matillion Distinct as DataFrame distinct
  imports: []
  confidence: 0.95
  role: transform
- type: Sample
  category: Sample
  template: 'df_{name} = df_{input}.sample(fraction={fraction}, seed={seed})

    print(f''[SAMPLE] {{df_{name}.count()}} sampled rows'')

    '
  description: Matillion Sample as DataFrame sample
  imports: []
  confidence: 0.95
  role: transform
- type: Assert
  category: Data Quality
  template: '_check = spark.sql("""{assertion_sql}""").first()[0]

    assert {assertion_condition}, f''Assertion failed: {{_check}}''

    print(''[ASSERT] Passed'')

    '
  description: Matillion Assert as SQL-based assertion
  imports: []
  confidence: 0.9
  role: test
- type: Fixed Flow
  category: Static Data
  template: 'df_{name} = spark.createDataFrame({rows}, schema={schema})

    '
  description: Matillion Fixed Flow as createDataFrame from static data
  imports: []
  confidence: 0.92
  role: source
- type: Generate Sequence
  category: Sequence
  template: 'from pyspark.sql.functions import monotonically_increasing_id

    df_{name} = df_{input}.withColumn(''{seq_col}'', monotonically_increasing_id() + {start})

    '
  description: Matillion Generate Sequence as monotonically_increasing_id
  imports:
  - from pyspark.sql.functions import monotonically_increasing_id
  confidence: 0.9
  role: transform
- type: Convert Type
  category: Cast
  template: "from pyspark.sql.functions import col\ndf_{name} = df_{input}\nfor _col, _type in {type_mapping}.items():\n \
    \   df_{name} = df_{name}.withColumn(_col, col(_col).cast(_type))\n"
  description: Matillion Convert Type as DataFrame cast
  imports:
  - from pyspark.sql.functions import col
  confidence: 0.92
  role: transform
- type: Hash
  category: Hash Column
  template: "from pyspark.sql.functions import md5, concat_ws, col\ndf_{name} = df_{input}.withColumn(\n    '{hash_col}',\n\
    \    md5(concat_ws('||', *[col(c) for c in [{hash_columns}]])))\n"
  description: Matillion Hash as md5 of concatenated columns
  imports:
  - from pyspark.sql.functions import md5, concat_ws, col
  confidence: 0.92
  role: transform
- type: Multi Table Input
  category: Multi-Source Read
  template: "dfs = {}\nfor _table in [{tables}]:\n    dfs[_table] = spark.table(f'{catalog}.{schema}.{{_table}}')\nprint(f'[MULTI-IN]\
    \ Loaded {{len(dfs)}} tables')\n"
  description: Matillion Multi Table Input as multiple spark.table reads
  imports: []
  confidence: 0.9
  role: source
- type: Multi Table Output
  category: Multi-Target Write
  template: "for _name, _df in {dataframes}.items():\n    _df.write.format('delta').mode('{mode}').saveAsTable(f'{catalog}.{schema}.{{_name}}')\n\
    print(f'[MULTI-OUT] Written {{len({dataframes})}} tables')\n"
  description: Matillion Multi Table Output as multiple Delta writes
  imports: []
  confidence: 0.9
  role: sink
- type: Bash Script
  category: Shell Command
  template: "import subprocess\n_result = subprocess.run(\n    '{command}',\n    shell=True, capture_output=True, text=True,\
    \ timeout=3600)\nif _result.returncode != 0:\n    raise Exception(f'Bash failed: {_result.stderr[:500]}')\nprint(_result.stdout[:1000])\n"
  description: Matillion Bash Script as subprocess run
  imports:
  - import subprocess
  confidence: 0.9
  role: process
- type: SQS Message
  category: Message Queue
  template: 'import boto3

    _sqs = boto3.client(''sqs'', region_name=''{region}'')

    _sqs.send_message(QueueUrl=''{queue_url}'', MessageBody=''{message}'')

    print(''[SQS] Message sent'')

    '
  description: Matillion SQS Message as boto3 SQS send
  imports:
  - import boto3
  confidence: 0.85
  role: utility
- type: SNS Message
  category: Notification
  template: 'import boto3

    _sns = boto3.client(''sns'', region_name=''{region}'')

    _sns.publish(TopicArn=''{topic_arn}'', Subject=''{subject}'', Message=''{message}'')

    print(''[SNS] Notification sent'')

    '
  description: Matillion SNS Message as boto3 SNS publish
  imports:
  - import boto3
  confidence: 0.85
  role: utility
- type: API Extract
  category: HTTP Source
  template: 'import requests

    _response = requests.get(''{url}'', headers={headers}, timeout=60)

    _response.raise_for_status()

    df_{name} = spark.read.json(spark.sparkContext.parallelize([_response.text]))

    print(f''[API] Extracted {{df_{name}.count()}} rows'')

    '
  description: Matillion API Extract as HTTP GET with JSON parse
  imports:
  - import requests
  confidence: 0.85
  role: source
- type: Map Values
  category: Value Mapping
  template: "from pyspark.sql.functions import when, col, lit\n_mapping = {mapping}\n_expr = col('{source_col}')\nfor _old,\
    \ _new in _mapping.items():\n    _expr = when(col('{source_col}') == lit(_old), lit(_new)).otherwise(_expr)\ndf_{name}\
    \ = df_{input}.withColumn('{target_col}', _expr)\n"
  description: Matillion Map Values as when/otherwise chain
  imports:
  - from pyspark.sql.functions import when, col, lit
  confidence: 0.88
  role: transform
- type: Window Calculation
  category: Window Function
  template: 'from pyspark.sql.window import Window

    from pyspark.sql.functions import col, row_number, rank, dense_rank

    _window = Window.partitionBy({partition_cols}).orderBy({order_cols})

    df_{name} = df_{input}.withColumn(''{output_col}'', {window_fn}.over(_window))

    '
  description: Matillion Window Calculation as Spark window function
  imports:
  - from pyspark.sql.window import Window
  - from pyspark.sql.functions import col, row_number, rank, dense_rank
  confidence: 0.92
  role: transform
- type: If
  category: Conditional Branch
  template: "# Matillion If component -> conditional logic\nif {condition}:\n    print('[IF] Condition TRUE')\n    {true_branch}\n\
    else:\n    print('[IF] Condition FALSE')\n    {false_branch}\n"
  description: Matillion If component as Python conditional
  imports: []
  confidence: 0.92
  role: route
- type: End Success
  category: Exit
  template: 'print(''[SUCCESS] Pipeline completed successfully'')

    dbutils.notebook.exit(''SUCCESS'')

    '
  description: Matillion End Success as notebook exit
  imports: []
  confidence: 0.95
  role: utility
- type: End Failure
  category: Error
  template: 'print(''[FAILURE] Pipeline failed'')

    raise Exception(''{error_message}'')

    '
  description: Matillion End Failure as exception raise
  imports: []
  confidence: 0.95
  role: utility
- type: Grid Variable
  category: Variable
  template: '# Matillion Grid Variable -> Python list/dict

    _{var_name} = {grid_values}

    print(f''[VAR] {var_name} = {{_{var_name}}}'')

    '
  description: Matillion Grid Variable as Python list
  imports: []
  confidence: 0.88
  role: utility
- type: Iterator
  category: Loop
  template: "# Matillion Iterator -> for loop over values\nfor _item in {items}:\n    print(f'[ITER] Processing: {{_item}}')\n\
    \    {loop_body}\nprint('[ITER] Complete')\n"
  description: Matillion Iterator as Python for loop
  imports: []
  confidence: 0.9
  role: process
- type: CDC
  category: Change Data Capture
  template: "from delta.tables import DeltaTable\n_target = DeltaTable.forName(spark, '{catalog}.{schema}.{target_table}')\n\
    _target.alias('t').merge(\n    df_{input}.alias('s'),\n    't.{key} = s.{key}'\n).whenMatchedUpdateAll(\n).whenNotMatchedInsertAll(\n\
    ).execute()\nprint('[CDC] Merge complete')\n"
  description: Matillion CDC as Delta MERGE upsert
  imports:
  - from delta.tables import DeltaTable
  confidence: 0.9
  role: transform
