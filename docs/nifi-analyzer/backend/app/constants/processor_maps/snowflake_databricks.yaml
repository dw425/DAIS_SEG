mappings:
- type: CREATE TABLE
  category: Delta Lake Table
  template: "spark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table} (\n        {column_definitions}\n\
    \    )\n    USING delta\n    TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')\n\"\"\")\n"
  description: Snowflake CREATE TABLE as Delta Lake table with auto-optimize
  imports: []
  confidence: 0.92
  role: sink
- type: CREATE PIPE
  category: Auto Loader
  template: "df_{name} = (spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"{file_format}\"\
    )\n    .option(\"cloudFiles.schemaLocation\", \"/Volumes/{catalog}/{schema}/checkpoints/{name}\")\n    .load(\"{stage_path}\"\
    ))\n(df_{name}.writeStream\n    .format(\"delta\")\n    .option(\"checkpointLocation\", \"/Volumes/{catalog}/{schema}/checkpoints/{name}\"\
    )\n    .trigger(availableNow=True)\n    .toTable(\"{catalog}.{schema}.{target_table}\"))\n"
  description: Snowpipe replaced by Auto Loader streaming ingestion
  imports: []
  confidence: 0.92
  role: source
- type: CREATE TASK
  category: Databricks Workflow
  template: "# Snowflake TASK -> Databricks scheduled job\n# Configure via Databricks Jobs UI or SDK:\nfrom databricks.sdk\
    \ import WorkspaceClient\nw = WorkspaceClient()\n_job = w.jobs.create(\n    name=\"{task_name}\",\n    tasks=[{{\n   \
    \     \"task_key\": \"{task_name}\",\n        \"notebook_task\": {{\n            \"notebook_path\": \"{notebook_path}\"\
    \n        }},\n        \"existing_cluster_id\": \"{cluster_id}\"\n    }}],\n    schedule={{\n        \"quartz_cron_expression\"\
    : \"{cron_expression}\",\n        \"timezone_id\": \"UTC\"\n    }})\n"
  description: Snowflake TASK replaced by Databricks scheduled job
  imports:
  - from databricks.sdk import WorkspaceClient
  confidence: 0.88
  role: process
- type: CREATE STREAM
  category: Delta Change Data Feed
  template: "# Snowflake STREAM -> Delta Change Data Feed\nspark.sql(\"ALTER TABLE {catalog}.{schema}.{source_table} SET TBLPROPERTIES\
    \ (delta.enableChangeDataFeed = true)\")\ndf_{name} = (spark.readStream\n    .format(\"delta\")\n    .option(\"readChangeFeed\"\
    , \"true\")\n    .table(\"{catalog}.{schema}.{source_table}\"))\n"
  description: Snowflake STREAM replaced by Delta Change Data Feed
  imports: []
  confidence: 0.92
  role: source
- type: COPY INTO
  category: Auto Loader
  template: "# Snowflake COPY INTO -> Auto Loader or COPY INTO (Databricks)\nspark.sql(\"\"\"\n    COPY INTO {catalog}.{schema}.{table}\n\
    \    FROM '{stage_path}'\n    FILEFORMAT = {file_format}\n    FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n\
    \    COPY_OPTIONS ('mergeSchema' = 'true')\n\"\"\")\n"
  description: Snowflake COPY INTO as Databricks COPY INTO command
  imports: []
  confidence: 0.95
  role: source
- type: MERGE INTO
  category: Delta MERGE
  template: "spark.sql(\"\"\"\n    MERGE INTO {catalog}.{schema}.{target_table} AS t\n    USING {catalog}.{schema}.{source_table}\
    \ AS s\n    ON t.{merge_key} = s.{merge_key}\n    WHEN MATCHED THEN UPDATE SET *\n    WHEN NOT MATCHED THEN INSERT *\n\
    \"\"\")\n"
  description: Snowflake MERGE INTO directly supported in Delta Lake SQL
  imports: []
  confidence: 0.95
  role: process
- type: CREATE VIEW
  category: Spark SQL View
  template: "spark.sql(\"\"\"\n    CREATE OR REPLACE VIEW {catalog}.{schema}.{view_name} AS\n    {select_sql}\n\"\"\")\n"
  description: Snowflake VIEW as Databricks SQL view in Unity Catalog
  imports: []
  confidence: 0.95
  role: transform
- type: CREATE FUNCTION
  category: Spark UDF
  template: "spark.sql(\"\"\"\n    CREATE OR REPLACE FUNCTION {catalog}.{schema}.{function_name}({params})\n    RETURNS {return_type}\n\
    \    RETURN {function_body}\n\"\"\")\n"
  description: Snowflake UDF as Databricks SQL function
  imports: []
  confidence: 0.9
  role: transform
- type: CREATE STAGE
  category: External Location
  template: "# Snowflake STAGE -> Unity Catalog external location\n# Configure via Databricks UI or SQL:\nspark.sql(\"\"\"\
    \n    CREATE EXTERNAL LOCATION IF NOT EXISTS {stage_name}\n    URL '{cloud_path}'\n    WITH (STORAGE CREDENTIAL {credential_name})\n\
    \"\"\")\n"
  description: Snowflake STAGE replaced by UC external location
  imports: []
  confidence: 0.88
  role: utility
- type: CREATE MATERIALIZED VIEW
  category: DLT Materialized View
  template: "# Snowflake Materialized View -> DLT table or cached view\nspark.sql(\"\"\"\n    CREATE OR REPLACE TABLE {catalog}.{schema}.{mv_name}\
    \ AS\n    {select_sql}\n\"\"\")\n# For auto-refresh, use DLT:\n# @dlt.table\n# def {mv_name}():\n#     return spark.sql(\"\
    {select_sql}\")\n"
  description: Snowflake MV as Delta table or DLT materialized view
  imports: []
  confidence: 0.88
  role: transform
- type: FLATTEN
  category: DataFrame Explode
  template: "df_{name} = (df_{input}\n    .withColumn(\"_flat\", explode(col(\"{array_column}\")))\n    .select(\"*\", \"\
    _flat.*\")\n    .drop(\"_flat\"))\n"
  description: Snowflake FLATTEN replaced by PySpark explode
  imports:
  - from pyspark.sql.functions import explode, col
  confidence: 0.92
  role: transform
- type: GET_PATH / PARSE_JSON
  category: DataFrame API
  template: "df_{name} = df_{input}.withColumn(\"{output_col}\",\n    get_json_object(col(\"{json_col}\"), \"$.{json_path}\"\
    ))\n"
  description: Snowflake JSON functions via get_json_object
  imports:
  - from pyspark.sql.functions import get_json_object, col
  confidence: 0.92
  role: transform
- type: GET_DDL
  category: Schema Export
  template: '# Snowflake GET_DDL -> Spark catalog describe

    _ddl = spark.sql(''SHOW CREATE TABLE {catalog}.{schema}.{table}'').first()[0]

    print(_ddl)

    '
  description: Snowflake GET_DDL replaced by SHOW CREATE TABLE
  imports: []
  confidence: 0.88
  role: utility
- type: CREATE_EXTERNAL_TABLE
  category: External Table
  template: "spark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table}\n    USING {format}\n    LOCATION\
    \ '{location}'\n    OPTIONS ({options})\n\"\"\")\n"
  description: Snowflake external table as Spark external table with LOCATION
  imports: []
  confidence: 0.9
  role: sink
- type: CREATE_FILE_FORMAT
  category: Read Options
  template: "# Snowflake FILE FORMAT -> Spark read options dict\n_{name}_options = {{\n    'format': '{format}',\n    'header':\
    \ '{skip_header}',\n    'delimiter': '{field_delimiter}',\n    'quote': '{field_optionally_enclosed_by}',\n    'nullValue':\
    \ '{null_if}',\n    'compression': '{compression}'\n}}\n"
  description: Snowflake FILE FORMAT as Spark read options dictionary
  imports: []
  confidence: 0.88
  role: utility
- type: CLONE_TABLE
  category: Delta Clone
  template: "# Snowflake CLONE -> Delta DEEP CLONE\nspark.sql(\"\"\"\n    CREATE TABLE {catalog}.{schema}.{target_table}\n\
    \    DEEP CLONE {catalog}.{schema}.{source_table}\n\"\"\")\n"
  description: Snowflake CLONE as Delta DEEP CLONE
  imports: []
  confidence: 0.95
  role: process
- type: TIME_TRAVEL_QUERY
  category: Delta Time Travel
  template: "# Snowflake AT/BEFORE -> Delta time travel\ndf_{name} = spark.sql(\"\"\"\n    SELECT * FROM {catalog}.{schema}.{table}\n\
    \    VERSION AS OF {version}\n\"\"\")\n# Or by timestamp:\n# df_{name} = spark.sql(\"SELECT * FROM {table} TIMESTAMP AS\
    \ OF '{timestamp}'\")\n"
  description: Snowflake time travel as Delta VERSION/TIMESTAMP AS OF
  imports: []
  confidence: 0.95
  role: source
- type: LATERAL_FLATTEN
  category: Explode
  template: "from pyspark.sql.functions import explode, col\ndf_{name} = df_{input}.select(\n    '*',\n    explode(col('{array_column}')).alias('{value_alias}'))\n"
  description: Snowflake LATERAL FLATTEN as Spark explode
  imports:
  - from pyspark.sql.functions import explode, col
  confidence: 0.92
  role: transform
- type: PARSE_JSON
  category: JSON Parse
  template: "from pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import {schema_type}\ndf_{name} = df_{input}.withColumn(\n\
    \    '{output_col}',\n    from_json(col('{json_column}'), {json_schema}))\n"
  description: Snowflake PARSE_JSON as Spark from_json
  imports:
  - from pyspark.sql.functions import from_json, col
  confidence: 0.92
  role: transform
- type: OBJECT_CONSTRUCT
  category: JSON Build
  template: "from pyspark.sql.functions import struct, to_json, col\ndf_{name} = df_{input}.withColumn(\n    '{output_col}',\n\
    \    to_json(struct([col(c) for c in [{columns}]])))\n"
  description: Snowflake OBJECT_CONSTRUCT as Spark to_json(struct(...))
  imports:
  - from pyspark.sql.functions import struct, to_json, col
  confidence: 0.92
  role: transform
- type: ARRAY_AGG
  category: Aggregation
  template: "from pyspark.sql.functions import collect_list, col\ndf_{name} = df_{input}.groupBy({group_by}).agg(\n    collect_list(col('{agg_column}')).alias('{output_col}'))\n"
  description: Snowflake ARRAY_AGG as Spark collect_list
  imports:
  - from pyspark.sql.functions import collect_list, col
  confidence: 0.95
  role: transform
- type: CREATE_UDF
  category: Python UDF
  template: "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import {return_type}\n\n@udf(returnType={return_type}())\n\
    def {udf_name}({params}):\n    {body}\n    return result\n\nspark.udf.register('{udf_name}', {udf_name})\n"
  description: Snowflake UDF as PySpark UDF registered to Spark catalog
  imports:
  - from pyspark.sql.functions import udf
  confidence: 0.9
  role: transform
- type: CREATE_PROCEDURE
  category: Python Function
  template: "def {procedure_name}({params}):\n    \"\"\"{description}\"\"\"\n    {body}\n    return result\n\n# Execute\n\
    _{procedure_name}_result = {procedure_name}({args})\nprint(f'[PROC] Result: {_{procedure_name}_result}')\n"
  description: Snowflake stored procedure as Python function
  imports: []
  confidence: 0.88
  role: process
- type: SHARE_DATA
  category: Delta Sharing
  template: '# Snowflake data sharing -> Delta Sharing

    # Provider side:

    spark.sql("CREATE SHARE IF NOT EXISTS {share_name}")

    spark.sql("ALTER SHARE {share_name} ADD TABLE {catalog}.{schema}.{table}")

    spark.sql("GRANT SELECT ON SHARE {share_name} TO RECIPIENT {recipient}")

    '
  description: Snowflake data sharing as Delta Sharing
  imports: []
  confidence: 0.85
  role: utility
- type: READER_ACCOUNT
  category: Delta Sharing
  template: "# Snowflake reader account -> Delta Sharing recipient\n# Recipient reads shared data:\ndf_{name} = (spark.read\n\
    \    .format('deltaSharing')\n    .load('{profile_path}#{share_name}.{schema}.{table}'))\n"
  description: Snowflake reader account as Delta Sharing recipient read
  imports: []
  confidence: 0.82
  role: source
- type: SNOWPIPE_STREAMING
  category: Auto Loader Streaming
  template: "# Snowpipe Streaming -> Structured Streaming with Auto Loader\ndf_{name} = (spark.readStream\n    .format('cloudFiles')\n\
    \    .option('cloudFiles.format', '{file_format}')\n    .option('cloudFiles.schemaLocation', '/Volumes/{catalog}/{schema}/checkpoints/{name}')\n\
    \    .option('cloudFiles.maxFilesPerTrigger', '{max_files}')\n    .load('{landing_path}'))\n(df_{name}.writeStream\n \
    \   .format('delta')\n    .option('checkpointLocation', '/Volumes/{catalog}/{schema}/checkpoints/{name}')\n    .trigger(processingTime='{trigger_interval}')\n\
    \    .toTable('{catalog}.{schema}.{target_table}'))\n"
  description: Snowpipe Streaming as continuous Auto Loader ingestion
  imports: []
  confidence: 0.9
  role: source
- type: DYNAMIC_TABLE
  category: Materialized View / DLT
  template: "# Snowflake Dynamic Table -> Databricks Materialized View or DLT\nspark.sql(\"\"\"\n    CREATE OR REPLACE MATERIALIZED\
    \ VIEW {catalog}.{schema}.{table} AS\n    {select_sql}\n\"\"\")\n# Or use DLT:\n# @dlt.table\n# def {table}():\n#    \
    \ return spark.sql(\"\"\"{select_sql}\"\"\")\n"
  description: Snowflake Dynamic Table as Databricks materialized view or DLT
  imports: []
  confidence: 0.88
  role: transform
- type: ICEBERG_TABLE
  category: Delta Lake Table
  template: "# Snowflake Iceberg table -> Delta table (native format)\n# Delta Lake is Databricks' native format; Iceberg\
    \ tables convert directly\nspark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table} (\n        {column_definitions}\n\
    \    )\n    USING delta\n    TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')\n\"\"\")\n"
  description: Snowflake Iceberg table as Delta Lake table (native)
  imports: []
  confidence: 0.9
  role: sink
- type: CREATE_SEQUENCE
  category: Identity Column
  template: "# Snowflake SEQUENCE -> Delta GENERATED ALWAYS AS IDENTITY\nspark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS\
    \ {catalog}.{schema}.{table} (\n        {seq_column} BIGINT GENERATED ALWAYS AS IDENTITY,\n        {other_columns}\n \
    \   ) USING delta\n\"\"\")\n"
  description: Snowflake SEQUENCE as Delta GENERATED ALWAYS AS IDENTITY
  imports: []
  confidence: 0.9
  role: utility
- type: QUALIFY
  category: Window Filter
  template: "# Snowflake QUALIFY -> Spark subquery with window filter\ndf_{name} = spark.sql(\"\"\"\n    SELECT * FROM (\n\
    \        SELECT *, {window_function} AS _rn\n        FROM {catalog}.{schema}.{table}\n    ) WHERE _rn {condition}\n\"\"\
    \")\n"
  description: Snowflake QUALIFY as subquery with window function filter
  imports: []
  confidence: 0.92
  role: transform
- type: OBJECT_AGG
  category: Map Aggregation
  template: "from pyspark.sql.functions import map_from_entries, collect_list, struct, col\ndf_{name} = df_{input}.groupBy({group_by}).agg(\n\
    \    map_from_entries(collect_list(struct(col('{key_col}'), col('{val_col}')))).alias('{output_col}'))\n"
  description: Snowflake OBJECT_AGG as Spark map_from_entries + collect_list
  imports:
  - from pyspark.sql.functions import map_from_entries, collect_list, struct, col
  confidence: 0.88
  role: transform
- type: COPY_INTO_UNLOAD
  category: File Export
  template: '# Snowflake COPY INTO @stage (unload) -> Spark write to cloud storage

    df_{name} = spark.table(''{catalog}.{schema}.{table}'')

    df_{name}.write.format(''{format}'').mode(''overwrite'').save(''{output_path}'')

    print(''[UNLOAD] Export complete'')

    '
  description: Snowflake COPY INTO unload as Spark file write
  imports: []
  confidence: 0.92
  role: sink
- type: CREATE_ALERT
  category: Databricks SQL Alert
  template: '# Snowflake ALERT -> Databricks SQL Alert

    # Configure via Databricks SQL UI or SDK

    from databricks.sdk import WorkspaceClient

    w = WorkspaceClient()

    # Create query-based alert programmatically

    print(''[ALERT] Configure via Databricks SQL Alerts UI'')

    '
  description: Snowflake ALERT as Databricks SQL Alert
  imports:
  - from databricks.sdk import WorkspaceClient
  confidence: 0.8
  role: utility
- type: UNDROP_TABLE
  category: Delta Restore
  template: "# Snowflake UNDROP -> Delta RESTORE\nspark.sql(\"\"\"\n    RESTORE TABLE {catalog}.{schema}.{table}\n    TO VERSION\
    \ AS OF {version}\n\"\"\")\nprint('[RESTORE] Table restored')\n"
  description: Snowflake UNDROP as Delta RESTORE TABLE
  imports: []
  confidence: 0.92
  role: utility
- type: SHOW_TABLES
  category: Catalog List
  template: 'df_{name} = spark.sql("SHOW TABLES IN {catalog}.{schema}")

    display(df_{name})

    '
  description: Snowflake SHOW TABLES as Spark SQL SHOW TABLES
  imports: []
  confidence: 0.95
  role: utility
