# Generic SQL / Stored Procedures -> Databricks PySpark/SQL Mapping
# Maps common SQL patterns and SSIS/SSRS SQL constructs to Spark equivalents

mappings:
  - type: "SELECT"
    category: "Spark SQL"
    template: |
      df_{name} = spark.sql("""
          {select_statement}
      """)
    description: "SQL SELECT via spark.sql"
    imports: []
    confidence: 0.95
    role: "source"

  - type: "INSERT INTO"
    category: "Delta Write"
    template: |
      spark.sql("""
          INSERT INTO {catalog}.{schema}.{table}
          {select_or_values}
      """)
    description: "SQL INSERT INTO via Spark SQL"
    imports: []
    confidence: 0.95
    role: "sink"

  - type: "UPDATE"
    category: "Delta UPDATE"
    template: |
      spark.sql("""
          UPDATE {catalog}.{schema}.{table}
          SET {set_clause}
          WHERE {where_clause}
      """)
    description: "SQL UPDATE directly supported on Delta tables"
    imports: []
    confidence: 0.95
    role: "process"

  - type: "DELETE"
    category: "Delta DELETE"
    template: |
      spark.sql("""
          DELETE FROM {catalog}.{schema}.{table}
          WHERE {where_clause}
      """)
    description: "SQL DELETE directly supported on Delta tables"
    imports: []
    confidence: 0.95
    role: "process"

  - type: "MERGE"
    category: "Delta MERGE"
    template: |
      spark.sql("""
          MERGE INTO {catalog}.{schema}.{target_table} AS t
          USING {catalog}.{schema}.{source_table} AS s
          ON {merge_condition}
          WHEN MATCHED THEN UPDATE SET *
          WHEN NOT MATCHED THEN INSERT *
      """)
    description: "SQL MERGE directly supported on Delta tables"
    imports: []
    confidence: 0.95
    role: "process"

  - type: "CREATE TABLE AS SELECT"
    category: "Delta CTAS"
    template: |
      spark.sql("""
          CREATE OR REPLACE TABLE {catalog}.{schema}.{table} AS
          {select_statement}
      """)
    description: "CTAS with Delta Lake"
    imports: []
    confidence: 0.95
    role: "transform"

  - type: "Stored Procedure"
    category: "Python Function"
    template: |
      def {procedure_name}({params}):
          """Ported from SQL stored procedure"""
          # Port T-SQL/PL/SQL logic to Python + Spark SQL
          df = spark.sql("""
              {main_query}
          """)
          (df.write
              .format("delta")
              .mode("{write_mode}")
              .saveAsTable("{catalog}.{schema}.{output_table}"))
          return df.count()

      _rows = {procedure_name}({args})
      print(f"[SP] {procedure_name}: {_rows} rows affected")
    description: "Stored procedure ported to Python function"
    imports: []
    confidence: 0.82
    role: "process"

  - type: "Cursor Loop"
    category: "DataFrame Iteration"
    template: |
      # SQL cursor loop -> DataFrame collect + Python loop
      # WARNING: Avoid for large datasets; prefer set-based operations
      df_{name} = spark.sql("{select_statement}")
      for _row in df_{name}.collect():
          # Row-level processing
          spark.sql(f"UPDATE {catalog}.{schema}.{table} SET {set_clause} WHERE id = {{_row.id}}")
    description: "SQL cursor loop as DataFrame iteration (prefer set-based)"
    imports: []
    confidence: 0.75
    role: "process"

  - type: "Temp Table"
    category: "Temp View"
    template: |
      spark.sql("""
          CREATE OR REPLACE TEMP VIEW {temp_table} AS
          {select_statement}
      """)
    description: "SQL temp table as Spark temp view"
    imports: []
    confidence: 0.95
    role: "transform"

  - type: "Window Function"
    category: "Spark SQL"
    template: |
      df_{name} = spark.sql("""
          SELECT *,
              ROW_NUMBER() OVER (PARTITION BY {partition_by} ORDER BY {order_by}) AS rn,
              SUM({measure}) OVER (PARTITION BY {partition_by}) AS running_total
          FROM {catalog}.{schema}.{table}
      """)
    description: "SQL window functions natively supported in Spark SQL"
    imports: []
    confidence: 0.95
    role: "transform"
