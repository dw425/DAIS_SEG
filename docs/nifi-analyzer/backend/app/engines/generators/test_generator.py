"""Test generator — produces pytest test files for generated notebook code.

Generates a complete .py test file with SparkSession fixture, tests per
source/transform/sink mapping, and dbutils mocking.
"""

import logging
import re
from textwrap import dedent

from app.models.pipeline import AssessmentResult, MappingEntry, ParseResult

logger = logging.getLogger(__name__)


def generate_tests(
    parse_result: ParseResult,
    assessment: AssessmentResult,
) -> str:
    """Generate a pytest test file for the generated notebook code.

    Returns the complete .py file content as a string.
    """
    sections: list[str] = []

    # conftest / imports / fixtures
    sections.append(_build_header(parse_result))

    # dbutils mock
    sections.append(_build_dbutils_mock())

    # SparkSession fixture
    sections.append(_build_spark_fixture())

    # Tests for each mapping
    test_count = 0
    for mapping in assessment.mappings:
        if not mapping.code or mapping.code.startswith("# UNMAPPED"):
            continue

        safe_name = _safe_name(mapping.name)
        role = mapping.role

        if role == "source":
            sections.append(_build_source_test(mapping, safe_name))
        elif role == "transform":
            sections.append(_build_transform_test(mapping, safe_name))
        elif role == "sink":
            sections.append(_build_sink_test(mapping, safe_name))
        elif role in ("route", "process"):
            sections.append(_build_transform_test(mapping, safe_name))
        else:
            sections.append(_build_generic_test(mapping, safe_name))

        test_count += 1

    # If no tests were generated, add a placeholder
    if test_count == 0:
        sections.append(dedent('''\
            def test_placeholder():
                """Placeholder — no mapped processors found."""
                pytest.skip("No mapped processors found — no tests to generate")
        '''))

    logger.info("Test generation: %d test(s) generated for %d mappings", test_count, len(assessment.mappings))
    return "\n\n".join(sections) + "\n"


# ---------------------------------------------------------------------------
# Section builders
# ---------------------------------------------------------------------------

def _build_header(parse_result: ParseResult) -> str:
    source_file = parse_result.metadata.get("source_file", "unknown")
    return dedent(f'''\
        """Auto-generated tests for {parse_result.platform} migration: {source_file}.

        Generated by the ETL Migration Platform test generator.
        Run with: pytest {_safe_name(source_file)}_test.py -v
        """

        import tempfile

        import pytest
        from unittest.mock import MagicMock, patch
    ''')


def _build_dbutils_mock() -> str:
    return dedent('''\
        class MockDBUtils:
            """Mock dbutils for local testing."""

            class secrets:
                @staticmethod
                def get(scope: str, key: str) -> str:
                    return f"mock_secret_{key}"

                @staticmethod
                def list(scope: str) -> list:
                    return []

            class fs:
                @staticmethod
                def ls(path: str) -> list:
                    return []

                @staticmethod
                def mkdirs(path: str) -> bool:
                    return True

            class widgets:
                _values: dict = {}

                @classmethod
                def get(cls, name: str) -> str:
                    return cls._values.get(name, "")

                @classmethod
                def text(cls, name: str, default: str, label: str = "") -> None:
                    cls._values[name] = default

        dbutils = MockDBUtils()
    ''')


def _build_spark_fixture() -> str:
    return dedent('''\
        @pytest.fixture(scope="session")
        def spark():
            """Create a local SparkSession for testing."""
            try:
                from pyspark.sql import SparkSession

                session = (
                    SparkSession.builder
                    .master("local[2]")
                    .appName("migration_test")
                    .config("spark.sql.shuffle.partitions", "2")
                    .config("spark.default.parallelism", "2")
                    .config("spark.sql.warehouse.dir", tempfile.mkdtemp(prefix="spark-warehouse-test-"))
                    .config("spark.driver.extraJavaOptions", f"-Dderby.system.home={tempfile.mkdtemp(prefix='derby-test-')}")
                    .getOrCreate()
                )
                yield session
                session.stop()
            except ImportError:
                pytest.skip("PySpark not installed — skipping Spark tests")


        @pytest.fixture
        def sample_df(spark):
            """Create a sample DataFrame for testing."""
            data = [
                {"id": 1, "name": "Alice", "value": "100", "timestamp": "2024-01-01T00:00:00Z"},
                {"id": 2, "name": "Bob", "value": "200", "timestamp": "2024-01-02T00:00:00Z"},
                {"id": 3, "name": "Charlie", "value": "300", "timestamp": "2024-01-03T00:00:00Z"},
            ]
            return spark.createDataFrame(data)
    ''')


def _build_source_test(mapping: MappingEntry, safe_name: str) -> str:
    """Generate a test for a source processor mapping."""
    return dedent(f'''\
        def test_{safe_name}_source(spark, sample_df):
            """Test source processor: {mapping.name} ({mapping.type}).

            Verifies that the source read returns a non-empty DataFrame.
            """
            # Simulate source read with sample data
            df = sample_df
            assert df is not None, "Source DataFrame should not be None"
            assert df.count() > 0, "Source DataFrame should not be empty"
            assert len(df.columns) > 0, "Source DataFrame should have columns"

            # Verify schema is non-trivial
            schema_fields = df.schema.fieldNames()
            assert len(schema_fields) > 0, "Schema should have at least one field"
    ''')


def _build_transform_test(mapping: MappingEntry, safe_name: str) -> str:
    """Generate a test for a transform/route processor mapping."""
    return dedent(f'''\
        def test_{safe_name}_transform(spark, sample_df):
            """Test transform processor: {mapping.name} ({mapping.type}).

            Verifies transformation preserves schema structure and row count.
            """
            df_input = sample_df
            input_count = df_input.count()
            input_columns = set(df_input.columns)

            # Apply transformation (passthrough as baseline)
            df_output = df_input.select("*")

            assert df_output is not None, "Transform output should not be None"
            assert df_output.count() > 0, "Transform should produce rows"

            # Schema should be compatible (may add columns but should not lose all)
            output_columns = set(df_output.columns)
            assert len(output_columns) > 0, "Output should retain columns"
    ''')


def _build_sink_test(mapping: MappingEntry, safe_name: str) -> str:
    """Generate a test for a sink processor mapping."""
    return dedent(f'''\
        def test_{safe_name}_sink(spark, sample_df, tmp_path):
            """Test sink processor: {mapping.name} ({mapping.type}).

            Verifies that write operation does not raise an exception.
            """
            df = sample_df
            output_path = str(tmp_path / "{safe_name}_output")

            # Write should not raise
            try:
                df.write.format("parquet").mode("overwrite").save(output_path)
            except Exception as exc:
                pytest.fail(f"Sink write raised an exception: {{exc}}")

            # Verify data was written
            df_read = spark.read.parquet(output_path)
            assert df_read.count() == df.count(), "Written data should match input count"
    ''')


def _build_generic_test(mapping: MappingEntry, safe_name: str) -> str:
    """Generate a generic test for unmapped/utility processors."""
    return dedent(f'''\
        def test_{safe_name}_{mapping.role}(spark, sample_df):
            """Test processor: {mapping.name} ({mapping.type}, role={mapping.role}).

            Basic smoke test verifying processor logic does not raise.
            """
            df = sample_df
            assert df is not None
            assert df.count() > 0
    ''')


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _safe_name(name: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9_]", "_", name).strip("_")
    if not s or s[0].isdigit():
        s = "p_" + s
    return s.lower()
